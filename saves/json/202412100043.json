[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.05228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05228v1",
                "updated": "2024-12-06T17:58:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "title": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips"
                },
                "summary": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate."
                },
                "authors": [
                    {
                        "name": "Ismet Dagli"
                    },
                    {
                        "name": "James Crea"
                    },
                    {
                        "name": "Soner Seckiner"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Selçuk Köse"
                    },
                    {
                        "name": "Mehmet E. Belviranli"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet E. Belviranli"
                },
                "author": "Mehmet E. Belviranli",
                "arxiv_comment": "This paper is accepted to 2025 Design, Automation Test in Europe\n  Conference Exhibition (DATE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02031v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02031v2",
                "updated": "2024-12-06T11:47:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    47,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-07-02T07:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    7,
                    59,
                    8,
                    1,
                    184,
                    0
                ],
                "title": "SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules"
                },
                "summary": "Text-to-image (T2I) generation using diffusion models has become a\nblockbuster service in today's AI cloud. A production T2I service typically\ninvolves a serving workflow where a base diffusion model is augmented with\nvarious \"add-on\" modules, notably ControlNet and LoRA, to enhance image\ngeneration control. Compared to serving the base model alone, these add-on\nmodules introduce significant loading and computational overhead, resulting in\nincreased latency. In this paper, we present SwiftDiffusion, a system that\nefficiently serves a T2I workflow through a holistic approach. SwiftDiffusion\ndecouples ControNet from the base model and deploys it as a separate,\nindependently scaled service on dedicated GPUs, enabling ControlNet caching,\nparallelization, and sharing. To mitigate the high loading overhead of LoRA\nserving, SwiftDiffusion employs a bounded asynchronous LoRA loading (BAL)\ntechnique, allowing LoRA loading to overlap with the initial base model\nexecution by up to k steps without compromising image quality. Furthermore,\nSwiftDiffusion optimizes base model execution with a novel latent parallelism\ntechnique. Collectively, these designs enable SwiftDiffusion to outperform the\nstate-of-the-art T2I serving systems, achieving up to 7.8x latency reduction\nand 1.6x throughput improvement in serving SDXL models on H800 GPUs, without\nsacrificing image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) generation using diffusion models has become a\nblockbuster service in today's AI cloud. A production T2I service typically\ninvolves a serving workflow where a base diffusion model is augmented with\nvarious \"add-on\" modules, notably ControlNet and LoRA, to enhance image\ngeneration control. Compared to serving the base model alone, these add-on\nmodules introduce significant loading and computational overhead, resulting in\nincreased latency. In this paper, we present SwiftDiffusion, a system that\nefficiently serves a T2I workflow through a holistic approach. SwiftDiffusion\ndecouples ControNet from the base model and deploys it as a separate,\nindependently scaled service on dedicated GPUs, enabling ControlNet caching,\nparallelization, and sharing. To mitigate the high loading overhead of LoRA\nserving, SwiftDiffusion employs a bounded asynchronous LoRA loading (BAL)\ntechnique, allowing LoRA loading to overlap with the initial base model\nexecution by up to k steps without compromising image quality. Furthermore,\nSwiftDiffusion optimizes base model execution with a novel latent parallelism\ntechnique. Collectively, these designs enable SwiftDiffusion to outperform the\nstate-of-the-art T2I serving systems, achieving up to 7.8x latency reduction\nand 1.6x throughput improvement in serving SDXL models on H800 GPUs, without\nsacrificing image quality."
                },
                "authors": [
                    {
                        "name": "Suyi Li"
                    },
                    {
                        "name": "Lingyun Yang"
                    },
                    {
                        "name": "Xiaoxiao Jiang"
                    },
                    {
                        "name": "Hanfeng Lu"
                    },
                    {
                        "name": "Dakai An"
                    },
                    {
                        "name": "Zhipeng Di"
                    },
                    {
                        "name": "Weiyi Lu"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Kan Liu"
                    },
                    {
                        "name": "Yinghao Yu"
                    },
                    {
                        "name": "Tao Lan"
                    },
                    {
                        "name": "Guodong Yang"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Liping Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02031v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02031v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04757v1",
                "updated": "2024-12-06T03:46:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    46,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T03:46:06Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    46,
                    6,
                    4,
                    341,
                    0
                ],
                "title": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free\n  Dynamic Triangular Attention Pattern",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free\n  Dynamic Triangular Attention Pattern"
                },
                "summary": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference."
                },
                "authors": [
                    {
                        "name": "Hongyin Tang"
                    },
                    {
                        "name": "Di Xiu"
                    },
                    {
                        "name": "Lanrui Wang"
                    },
                    {
                        "name": "Xiurui Geng"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xunliang Cai"
                },
                "author": "Xunliang Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01844v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01844v2",
                "updated": "2024-12-06T01:43:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    43,
                    55,
                    4,
                    341,
                    0
                ],
                "published": "2024-05-03T04:27:32Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    4,
                    27,
                    32,
                    4,
                    124,
                    0
                ],
                "title": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges"
                },
                "summary": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Shazia Riaz"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01844v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01844v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04698v1",
                "updated": "2024-12-06T01:20:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    47,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T01:20:47Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    47,
                    4,
                    341,
                    0
                ],
                "title": "One-Hop Sub-Query Result Caches for Graph Database Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-Hop Sub-Query Result Caches for Graph Database Systems"
                },
                "summary": "This paper introduces a novel one-hop sub-query result cache for processing\ngraph read transactions, gR-Txs, in a graph database system. The one-hop\nnavigation is from a vertex using either its in-coming or out-going edges with\nselection predicates that filter edges and vertices. Its cache entry identifies\na unique one-hop sub-query (key) and its result set consisting of immutable\nvertex ids (value). When processing a gR-Tx, the query processor identifies its\nsequence of individual one-hop sub-queries and looks up their results in the\ncache. A cache hit fetches less data from the storage manager and eliminates\nthe requirement to process the one-hop sub-query. A cache miss populates the\ncache asynchronously and in a transactional manner, maintaining the separation\nof read and write paths of our transactional storage manager. A graph read and\nwrite transaction, gRW-Tx, identifies the impacted cache entries and either\ndeletes or updates them. Our implementation of the cache is inside the graph\nquery processing engine and transparent to a user application. We evaluate the\ncache using our eCommerce production workload and with rules that re-write\ngraph queries to maximize the performance enhancements observed with the cache.\nObtained results show the cache enhances 95th and 99th percentile of query\nresponse times by at least 2x and 1.63x, respectively. When combined with query\nre-writing, the enhancements are at least 2.33x and 4.48x, respectively. An\ninteresting result is the significant performance enhancement observed by the\nindirect beneficiaries of the cache, gRW-Txs and gR-Txs that do not reference\none-hop sub-queries. The cache frees system resources to expedite their\nprocessing significantly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel one-hop sub-query result cache for processing\ngraph read transactions, gR-Txs, in a graph database system. The one-hop\nnavigation is from a vertex using either its in-coming or out-going edges with\nselection predicates that filter edges and vertices. Its cache entry identifies\na unique one-hop sub-query (key) and its result set consisting of immutable\nvertex ids (value). When processing a gR-Tx, the query processor identifies its\nsequence of individual one-hop sub-queries and looks up their results in the\ncache. A cache hit fetches less data from the storage manager and eliminates\nthe requirement to process the one-hop sub-query. A cache miss populates the\ncache asynchronously and in a transactional manner, maintaining the separation\nof read and write paths of our transactional storage manager. A graph read and\nwrite transaction, gRW-Tx, identifies the impacted cache entries and either\ndeletes or updates them. Our implementation of the cache is inside the graph\nquery processing engine and transparent to a user application. We evaluate the\ncache using our eCommerce production workload and with rules that re-write\ngraph queries to maximize the performance enhancements observed with the cache.\nObtained results show the cache enhances 95th and 99th percentile of query\nresponse times by at least 2x and 1.63x, respectively. When combined with query\nre-writing, the enhancements are at least 2.33x and 4.48x, respectively. An\ninteresting result is the significant performance enhancement observed by the\nindirect beneficiaries of the cache, gRW-Txs and gR-Txs that do not reference\none-hop sub-queries. The cache frees system resources to expedite their\nprocessing significantly."
                },
                "authors": [
                    {
                        "name": "Hieu Nguyen"
                    },
                    {
                        "name": "Jun Li"
                    },
                    {
                        "name": "Shahram Ghandeharizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Shahram Ghandeharizadeh"
                },
                "author": "Shahram Ghandeharizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04652v1",
                "updated": "2024-12-05T22:47:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    47,
                    17,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T22:47:17Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    47,
                    17,
                    3,
                    340,
                    0
                ],
                "title": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference"
                },
                "summary": "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP"
                },
                "authors": [
                    {
                        "name": "Xiaohuan Pei"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04634v1",
                "updated": "2024-12-05T22:06:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    6,
                    23,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T22:06:23Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    6,
                    23,
                    3,
                    340,
                    0
                ],
                "title": "Neural Two-Level Monte Carlo Real-Time Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Two-Level Monte Carlo Real-Time Rendering"
                },
                "summary": "We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte\nCarlo, MLMC) estimator for real-time rendering of scenes with global\nillumination. Using MLMC we split the shading integral into two parts: the\nradiance cache integral and the residual error integral that compensates for\nthe bias of the first one. For the first part, we developed the Neural Incident\nRadiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks\nas a building block, which is trained on the fly. The cache is designed to\nprovide a fast and reasonable approximation of the incident radiance: an\nevaluation takes 2-25x less compute time than a path tracing sample. This\nenables us to estimate the radiance cache integral with a high number of\nsamples and by this achieve faster convergence. For the residual error\nintegral, we compute the difference between the NIRC predictions and the\nunbiased path tracing simulation. Our method makes no assumptions about the\ngeometry, materials, or lighting of a scene and has only few intuitive\nhyper-parameters. We provide a comprehensive comparative analysis in different\nexperimental scenarios. Since the algorithm is trained in an on-line fashion,\nit demonstrates significant noise level reduction even for dynamic scenes and\ncan easily be combined with other importance sampling schemes and noise\nreduction techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte\nCarlo, MLMC) estimator for real-time rendering of scenes with global\nillumination. Using MLMC we split the shading integral into two parts: the\nradiance cache integral and the residual error integral that compensates for\nthe bias of the first one. For the first part, we developed the Neural Incident\nRadiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks\nas a building block, which is trained on the fly. The cache is designed to\nprovide a fast and reasonable approximation of the incident radiance: an\nevaluation takes 2-25x less compute time than a path tracing sample. This\nenables us to estimate the radiance cache integral with a high number of\nsamples and by this achieve faster convergence. For the residual error\nintegral, we compute the difference between the NIRC predictions and the\nunbiased path tracing simulation. Our method makes no assumptions about the\ngeometry, materials, or lighting of a scene and has only few intuitive\nhyper-parameters. We provide a comprehensive comparative analysis in different\nexperimental scenarios. Since the algorithm is trained in an on-line fashion,\nit demonstrates significant noise level reduction even for dynamic scenes and\ncan easily be combined with other importance sampling schemes and noise\nreduction techniques."
                },
                "authors": [
                    {
                        "name": "Mikhail Dereviannykh"
                    },
                    {
                        "name": "Dmitrii Klepikov"
                    },
                    {
                        "name": "Johannes Hanika"
                    },
                    {
                        "name": "Carsten Dachsbacher"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Dachsbacher"
                },
                "author": "Carsten Dachsbacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04449v1",
                "updated": "2024-12-05T18:58:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:58:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay"
                },
                "summary": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. The majority of computation stems from the\noverwhelming volume of vision tokens processed by the transformer decoder. In\nthis paper, we propose to build efficient MLLMs by leveraging the\nMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects\nessential vision tokens to process while skipping redundant ones. However,\nintegrating MoD into MLLMs is non-trivial. To address the challenges of\ntraining and inference stability as well as limited training data, we adapt the\nMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)\nand symmetric token reweighting (STRing). Moreover, we observe that vision\ntokens exhibit higher redundancy in deeper layer and thus design a progressive\nratio decay (PRD) strategy, which gradually reduces the token retention ratio\nlayer by layer, employing a shifted cosine schedule. This crucial design fully\nunleashes the potential of MoD, significantly boosting the efficiency and\nperformance of our models. To validate the effectiveness of our approach, we\nconduct extensive experiments with two baseline models across 14 benchmarks.\nOur model, p-MoD, matches or even surpasses the performance of the baseline\nmodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and\n77.7% GPU hours during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. The majority of computation stems from the\noverwhelming volume of vision tokens processed by the transformer decoder. In\nthis paper, we propose to build efficient MLLMs by leveraging the\nMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects\nessential vision tokens to process while skipping redundant ones. However,\nintegrating MoD into MLLMs is non-trivial. To address the challenges of\ntraining and inference stability as well as limited training data, we adapt the\nMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)\nand symmetric token reweighting (STRing). Moreover, we observe that vision\ntokens exhibit higher redundancy in deeper layer and thus design a progressive\nratio decay (PRD) strategy, which gradually reduces the token retention ratio\nlayer by layer, employing a shifted cosine schedule. This crucial design fully\nunleashes the potential of MoD, significantly boosting the efficiency and\nperformance of our models. To validate the effectiveness of our approach, we\nconduct extensive experiments with two baseline models across 14 benchmarks.\nOur model, p-MoD, matches or even surpasses the performance of the baseline\nmodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and\n77.7% GPU hours during training."
                },
                "authors": [
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Desen Meng"
                    },
                    {
                        "name": "Ji Qi"
                    },
                    {
                        "name": "Zhenpeng Huang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "Technical Report; Code released at https://github.com/MCG-NJU/p-MoD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03960v2",
                "updated": "2024-12-05T14:56:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    56,
                    56,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-04T22:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation"
                },
                "summary": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb."
                },
                "authors": [
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19574v2",
                "updated": "2024-12-05T12:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    19,
                    38,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-29T09:42:38Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    42,
                    38,
                    4,
                    334,
                    0
                ],
                "title": "KV Shifting Attention Enhances Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Shifting Attention Enhances Language Modeling"
                },
                "summary": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters."
                },
                "authors": [
                    {
                        "name": "Mingyu Xu"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01485v2",
                "updated": "2024-12-05T06:52:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    6,
                    52,
                    42,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-02T12:35:53Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "title": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts"
                },
                "summary": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v3",
                "updated": "2024-12-05T04:29:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    4,
                    29,
                    49,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "01. AI"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Albert Wang"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Ethan Dai"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zirui Zhang"
                },
                "author": "Zirui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.01516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.01516v2",
                "updated": "2024-12-05T01:50:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    1,
                    50,
                    27,
                    3,
                    340,
                    0
                ],
                "published": "2023-05-02T15:27:16Z",
                "published_parsed": [
                    2023,
                    5,
                    2,
                    15,
                    27,
                    16,
                    1,
                    122,
                    0
                ],
                "title": "F2: Designing a Key-Value Store for Large Skewed Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F2: Designing a Key-Value Store for Large Skewed Workloads"
                },
                "summary": "Many real-world workloads present a challenging set of requirements: point\noperations requiring high throughput, working sets much larger than main\nmemory, and natural skew in key access patterns for both reads and writes. We\nfind that modern key-value designs are either optimized for memory-efficiency,\nsacrificing high-performance (LSM-tree designs), or achieve high-performance,\nsaturating modern NVMe SSD bandwidth, at the cost of substantial memory\nresources or high disk wear (CPU-optimized designs). Unfortunately these\ndesigns are not able to handle meet the challenging demands of such\nlarger-than-memory, skewed workloads.\n  To this end, we present F2, a new key-value store that bridges this gap by\ncombining the strengths of both approaches. F2 adopts a tiered, record-oriented\narchitecture inspired by LSM-trees to effectively separate hot from cold\nrecords, while incorporating concurrent latch-free mechanisms from\nCPU-optimized engines to maximize performance on modern NVMe SSDs. To realize\nthis design, we tackle key challenges and introduce several innovations,\nincluding new latch-free algorithms for multi-threaded log compaction and user\noperations (e.g., RMWs), as well as new components: a two-level hash index to\nreduce indexing overhead for cold records and a read-cache for serving read-hot\ndata.\n  Detailed experimental results show that F2 matches or outperforms existing\nsolutions, achieving on average better throughput on memory-constrained\nenvironments compared to state-of-the-art systems like RocksDB (11.75x),\nSplinterDB (4.52x), KVell (10.56x), LeanStore (2.04x), and FASTER (2.38x). F2\nalso maintains its high performance across varying workload skewness levels and\nmemory budgets, while achieving low disk write amplification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many real-world workloads present a challenging set of requirements: point\noperations requiring high throughput, working sets much larger than main\nmemory, and natural skew in key access patterns for both reads and writes. We\nfind that modern key-value designs are either optimized for memory-efficiency,\nsacrificing high-performance (LSM-tree designs), or achieve high-performance,\nsaturating modern NVMe SSD bandwidth, at the cost of substantial memory\nresources or high disk wear (CPU-optimized designs). Unfortunately these\ndesigns are not able to handle meet the challenging demands of such\nlarger-than-memory, skewed workloads.\n  To this end, we present F2, a new key-value store that bridges this gap by\ncombining the strengths of both approaches. F2 adopts a tiered, record-oriented\narchitecture inspired by LSM-trees to effectively separate hot from cold\nrecords, while incorporating concurrent latch-free mechanisms from\nCPU-optimized engines to maximize performance on modern NVMe SSDs. To realize\nthis design, we tackle key challenges and introduce several innovations,\nincluding new latch-free algorithms for multi-threaded log compaction and user\noperations (e.g., RMWs), as well as new components: a two-level hash index to\nreduce indexing overhead for cold records and a read-cache for serving read-hot\ndata.\n  Detailed experimental results show that F2 matches or outperforms existing\nsolutions, achieving on average better throughput on memory-constrained\nenvironments compared to state-of-the-art systems like RocksDB (11.75x),\nSplinterDB (4.52x), KVell (10.56x), LeanStore (2.04x), and FASTER (2.38x). F2\nalso maintains its high performance across varying workload skewness levels and\nmemory budgets, while achieving low disk write amplification."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Badrish Chandramouli"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.01516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.01516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v2",
                "updated": "2024-12-04T18:40:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    40,
                    24,
                    2,
                    339,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v1",
                "updated": "2024-12-04T15:48:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "12 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03361v1",
                "updated": "2024-12-04T14:47:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    47,
                    42,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T14:47:42Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    47,
                    42,
                    2,
                    339,
                    0
                ],
                "title": "Measurement of electron beam induced sample heating in SEM experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurement of electron beam induced sample heating in SEM experiments"
                },
                "summary": "Scanning Electron Microscopy (SEM) experiments provide detailed insights into\nmaterial microstructures, enabling high-resolution imaging as well as\ncrystallographic analysis through advanced techniques like Electron Backscatter\nDiffraction (EBSD). However, the interaction of the high-energy electron beam\nwith the material can lead to localized heating, which may significantly impact\nspecimen integrity, especially in applications requiring prolonged beam\nexposure, for instance when mapping the crystal structure using EBSD. This\nstudy examines electron-beam-induced heating effects on a model metal sample\n(iron), directly measuring the locally deposited electron beam energy with a\nMEMS-based heating device and validating these measurements through\nsimulations, including Monte Carlo and Finite Element methods. The analysis\nfocuses on the effects of various experimental parameters such as acceleration\nvoltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time\n(from 1$\\mu$s to 1ms) and sample tilt (0{\\deg} to 70{\\deg}). The findings\nreveal that local sample temperatures can increase by up to 70 {\\deg}C during\nEBSD experiments, primarily affected by the choice in beam current and\nacceleration voltage, with beam current having the most significant impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scanning Electron Microscopy (SEM) experiments provide detailed insights into\nmaterial microstructures, enabling high-resolution imaging as well as\ncrystallographic analysis through advanced techniques like Electron Backscatter\nDiffraction (EBSD). However, the interaction of the high-energy electron beam\nwith the material can lead to localized heating, which may significantly impact\nspecimen integrity, especially in applications requiring prolonged beam\nexposure, for instance when mapping the crystal structure using EBSD. This\nstudy examines electron-beam-induced heating effects on a model metal sample\n(iron), directly measuring the locally deposited electron beam energy with a\nMEMS-based heating device and validating these measurements through\nsimulations, including Monte Carlo and Finite Element methods. The analysis\nfocuses on the effects of various experimental parameters such as acceleration\nvoltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time\n(from 1$\\mu$s to 1ms) and sample tilt (0{\\deg} to 70{\\deg}). The findings\nreveal that local sample temperatures can increase by up to 70 {\\deg}C during\nEBSD experiments, primarily affected by the choice in beam current and\nacceleration voltage, with beam current having the most significant impact."
                },
                "authors": [
                    {
                        "name": "Christina Koenig"
                    },
                    {
                        "name": "Alice Bastos da Silva Fanta"
                    },
                    {
                        "name": "Joerg R. Jinschek"
                    }
                ],
                "author_detail": {
                    "name": "Joerg R. Jinschek"
                },
                "author": "Joerg R. Jinschek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03213v1",
                "updated": "2024-12-04T10:58:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T10:58:27Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "title": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Chenqi Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v1",
                "updated": "2024-12-04T08:51:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying KV Cache Compression for Large Language Models with LeanKV"
                },
                "summary": "Large language models (LLMs) demonstrate exceptional performance but incur\nhigh serving costs due to substantial memory demands, with the key-value (KV)\ncache being a primary bottleneck. Existing KV cache compression methods,\nincluding quantization and pruning, struggle with limitations such as uniform\ntreatment of keys and values and static memory allocation across attention\nheads. To address these challenges, we introduce LeanKV, a unified KV cache\ncompression framework that enhances LLM serving efficiency without compromising\naccuracy through three innovations: (1) Hetero-KV quantization, which stores\nkeys at a higher precision than values to reflect their greater impact on\nattention computations; (2) per-head dynamic sparsity, which allocates memory\nbased on token importance per head and per request; and (3) unified KV\ncompression, integrating mixed-precision quantization and selective pruning to\nenable a smooth tradeoff between model accuracy and memory efficiency. To\nefficiently support these techniques, LeanKV introduces systems optimizations\nincluding unified paging and on-GPU parallel memory management. Implemented on\nvLLM, LeanKV compresses the KV cache by $3.0\\times$ to $5.0\\times$ without\naccuracy loss and up to $11.0\\times$ with under 5% accuracy loss, enhancing\nthroughput by $1.9\\times$ to $2.5\\times$, and up to $6.9\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate exceptional performance but incur\nhigh serving costs due to substantial memory demands, with the key-value (KV)\ncache being a primary bottleneck. Existing KV cache compression methods,\nincluding quantization and pruning, struggle with limitations such as uniform\ntreatment of keys and values and static memory allocation across attention\nheads. To address these challenges, we introduce LeanKV, a unified KV cache\ncompression framework that enhances LLM serving efficiency without compromising\naccuracy through three innovations: (1) Hetero-KV quantization, which stores\nkeys at a higher precision than values to reflect their greater impact on\nattention computations; (2) per-head dynamic sparsity, which allocates memory\nbased on token importance per head and per request; and (3) unified KV\ncompression, integrating mixed-precision quantization and selective pruning to\nenable a smooth tradeoff between model accuracy and memory efficiency. To\nefficiently support these techniques, LeanKV introduces systems optimizations\nincluding unified paging and on-GPU parallel memory management. Implemented on\nvLLM, LeanKV compresses the KV cache by $3.0\\times$ to $5.0\\times$ without\naccuracy loss and up to $11.0\\times$ with under 5% accuracy loss, enhancing\nthroughput by $1.9\\times$ to $2.5\\times$, and up to $6.9\\times$."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.08066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.08066v2",
                "updated": "2024-12-04T05:32:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    5,
                    32,
                    12,
                    2,
                    339,
                    0
                ],
                "published": "2023-02-06T13:46:08Z",
                "published_parsed": [
                    2023,
                    2,
                    6,
                    13,
                    46,
                    8,
                    0,
                    37,
                    0
                ],
                "title": "PASCAL: A Learning-aided Cooperative Bandwidth Control Policy for\n  Hierarchical Storage Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PASCAL: A Learning-aided Cooperative Bandwidth Control Policy for\n  Hierarchical Storage Systems"
                },
                "summary": "Nowadays, the Hierarchical Storage System (HSS) is considered as an ideal\nmodel to meet the cost-performance demand. The data migration between storing\ntiers of HSS is the way to achieve the cost-performance goal. The bandwidth\ncontrol is to limit the maximum amount of data migration. Most of previous\nresearch about HSS focus on studying the data migration policy instead of\nbandwidth control. However, the recent research about cache and networking\noptimization suggest that the bandwidth control has significant impact on the\nsystem performance. Few previous work achieves a satisfactory bandwidth control\nin HSS since it is hard to control bandwidth for so many data migration tasks\nsimultaneously. In this paper, we first give a stochastic programming model to\nformalize the bandwidth control problem in HSS. Then we propose a\nlearning-aided bandwidth control policy for HSS, named \\Pascal{}, which learns\nto control the bandwidth of different data migration task in an cooperative\nway. We implement \\Pascal{} on a commercial HSS and compare it with three\nstrong baselines over a group of workloads. Our evaluation on the physical\nsystem shows that \\Pascal{} can effectively decrease 1.95X the tail latency and\ngreatly improve throughput stability (2X $\\downarrow$ throughput jitter), and\nmeanwhile keep the throughput at a relatively high level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, the Hierarchical Storage System (HSS) is considered as an ideal\nmodel to meet the cost-performance demand. The data migration between storing\ntiers of HSS is the way to achieve the cost-performance goal. The bandwidth\ncontrol is to limit the maximum amount of data migration. Most of previous\nresearch about HSS focus on studying the data migration policy instead of\nbandwidth control. However, the recent research about cache and networking\noptimization suggest that the bandwidth control has significant impact on the\nsystem performance. Few previous work achieves a satisfactory bandwidth control\nin HSS since it is hard to control bandwidth for so many data migration tasks\nsimultaneously. In this paper, we first give a stochastic programming model to\nformalize the bandwidth control problem in HSS. Then we propose a\nlearning-aided bandwidth control policy for HSS, named \\Pascal{}, which learns\nto control the bandwidth of different data migration task in an cooperative\nway. We implement \\Pascal{} on a commercial HSS and compare it with three\nstrong baselines over a group of workloads. Our evaluation on the physical\nsystem shows that \\Pascal{} can effectively decrease 1.95X the tail latency and\ngreatly improve throughput stability (2X $\\downarrow$ throughput jitter), and\nmeanwhile keep the throughput at a relatively high level."
                },
                "authors": [
                    {
                        "name": "Xijun Li"
                    },
                    {
                        "name": "Yunfan Zhou"
                    },
                    {
                        "name": "Ji Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ji Zhang"
                },
                "author": "Ji Zhang",
                "arxiv_comment": "for modifying part of contents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.08066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.08066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03023v1",
                "updated": "2024-12-04T04:29:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    29,
                    12,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T04:29:12Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    29,
                    12,
                    2,
                    339,
                    0
                ],
                "title": "A Multi-Functional Web Tool for Comprehensive Threat Detection Through\n  IP Address Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Functional Web Tool for Comprehensive Threat Detection Through\n  IP Address Analysis"
                },
                "summary": "In recent years, the advances in digitalisation have also adversely\ncontributed to the significant rise in cybercrimes. Hence, building the threat\nintelligence to shield against rising cybercrimes has become a fundamental\nrequisite. Internet Protocol (IP) addresses play a crucial role in the threat\nintelligence and prevention of cyber crimes. However, we have noticed the lack\nof one-stop, free, and open-source tools that can analyse IP addresses. Hence,\nthis work introduces a comprehensive web tool for advanced IP address\ncharacterisation. Our tool offers a wide range of features, including\ngeolocation, blocklist check, VPN detection, proxy detection, bot detection,\nTor detection, port scan, and accurate domain statistics that include the\ndetails about the name servers and registrar information. In addition, our tool\ncalculates a confidence score based on a weighted sum of publicly accessible\nonline results from different reliable sources to give users a dependable\nmeasure of accuracy. Further, to improve performance, our tool also\nincorporates a local database for caching the results, to enable fast content\nretrieval with minimal external Web API calls. Our tool supports domain names\nand IPv4 addresses, making it a multi-functional and powerful IP analyser tool\nfor threat intelligence. Our tool is available at www.ipanalyzer.in",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the advances in digitalisation have also adversely\ncontributed to the significant rise in cybercrimes. Hence, building the threat\nintelligence to shield against rising cybercrimes has become a fundamental\nrequisite. Internet Protocol (IP) addresses play a crucial role in the threat\nintelligence and prevention of cyber crimes. However, we have noticed the lack\nof one-stop, free, and open-source tools that can analyse IP addresses. Hence,\nthis work introduces a comprehensive web tool for advanced IP address\ncharacterisation. Our tool offers a wide range of features, including\ngeolocation, blocklist check, VPN detection, proxy detection, bot detection,\nTor detection, port scan, and accurate domain statistics that include the\ndetails about the name servers and registrar information. In addition, our tool\ncalculates a confidence score based on a weighted sum of publicly accessible\nonline results from different reliable sources to give users a dependable\nmeasure of accuracy. Further, to improve performance, our tool also\nincorporates a local database for caching the results, to enable fast content\nretrieval with minimal external Web API calls. Our tool supports domain names\nand IPv4 addresses, making it a multi-functional and powerful IP analyser tool\nfor threat intelligence. Our tool is available at www.ipanalyzer.in"
                },
                "authors": [
                    {
                        "name": "Cebajel Tanan"
                    },
                    {
                        "name": "Sameer G. Kulkarni"
                    },
                    {
                        "name": "Tamal Das"
                    },
                    {
                        "name": "Manjesh K. Hanawal"
                    }
                ],
                "author_detail": {
                    "name": "Manjesh K. Hanawal"
                },
                "author": "Manjesh K. Hanawal",
                "arxiv_comment": "Presented at ICIE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.12622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.12622v2",
                "updated": "2024-12-03T22:48:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    48,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2023-10-19T10:02:52Z",
                "published_parsed": [
                    2023,
                    10,
                    19,
                    10,
                    2,
                    52,
                    3,
                    292,
                    0
                ],
                "title": "cRVR: A Stackelberg Game Approach for Joint Privacy-Aware Video\n  Requesting and Edge Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cRVR: A Stackelberg Game Approach for Joint Privacy-Aware Video\n  Requesting and Edge Caching"
                },
                "summary": "As users conveniently stream their favorite online videos, video request\nrecords are automatically stored by video content providers, which have a high\nchance of privacy leakage. Unfortunately, most existing privacy-enhancing\napproaches are not applicable for protecting user privacy in video requests,\nbecause they cannot be easily altered or distorted by users and must be visible\nfor content providers to stream correct videos. To preserve request privacy in\nonline video services, it is possible to request additional videos that are\nirrelevant to users' interests so that content providers cannot precisely infer\nusers' interest information. However, a naive redundant requesting approach\nwould significantly degrade the performance of edge caches and increase\nbandwidth overhead. In this paper, we are among the first to propose a\nCache-Friendly Redundant Video Requesting (cRVR) algorithm for User Devices\n(UDs) and its corresponding caching algorithm for the Edge Cache (EC), which\ncan effectively mitigate the problem of request privacy leakage with minimal\nimpact on the EC's performance. To tackle the problem, we first develop a\nStackelberg game to analyze the dedicated interaction between UDs and EC, and\nobtain their optimal strategies to maximize their respective utility. For UDs,\nthe utility function is a combination of both video playback utility and\nprivacy protection utility. We prove the existence and uniqueness of the\nequilibrium of the Stackelberg game. Extensive experiments are conducted with\nreal traces to demonstrate that cRVR can effectively protect video request\nprivacy by reducing up to 59.03\\% of privacy disclosure compared to baseline\nalgorithms. Meanwhile, the caching performance of EC is only slightly affected.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As users conveniently stream their favorite online videos, video request\nrecords are automatically stored by video content providers, which have a high\nchance of privacy leakage. Unfortunately, most existing privacy-enhancing\napproaches are not applicable for protecting user privacy in video requests,\nbecause they cannot be easily altered or distorted by users and must be visible\nfor content providers to stream correct videos. To preserve request privacy in\nonline video services, it is possible to request additional videos that are\nirrelevant to users' interests so that content providers cannot precisely infer\nusers' interest information. However, a naive redundant requesting approach\nwould significantly degrade the performance of edge caches and increase\nbandwidth overhead. In this paper, we are among the first to propose a\nCache-Friendly Redundant Video Requesting (cRVR) algorithm for User Devices\n(UDs) and its corresponding caching algorithm for the Edge Cache (EC), which\ncan effectively mitigate the problem of request privacy leakage with minimal\nimpact on the EC's performance. To tackle the problem, we first develop a\nStackelberg game to analyze the dedicated interaction between UDs and EC, and\nobtain their optimal strategies to maximize their respective utility. For UDs,\nthe utility function is a combination of both video playback utility and\nprivacy protection utility. We prove the existence and uniqueness of the\nequilibrium of the Stackelberg game. Extensive experiments are conducted with\nreal traces to demonstrate that cRVR can effectively protect video request\nprivacy by reducing up to 59.03\\% of privacy disclosure compared to baseline\nalgorithms. Meanwhile, the caching performance of EC is only slightly affected."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Linchang Xiao"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    }
                ],
                "author_detail": {
                    "name": "Quan Z. Sheng"
                },
                "author": "Quan Z. Sheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.12622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.12622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02867v1",
                "updated": "2024-12-03T22:02:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    2,
                    42,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T22:02:42Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    2,
                    42,
                    1,
                    338,
                    0
                ],
                "title": "GoldFish: Serverless Actors with Short-Term Memory State for the\n  Edge-Cloud Continuum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoldFish: Serverless Actors with Short-Term Memory State for the\n  Edge-Cloud Continuum"
                },
                "summary": "Serverless Computing is a computing paradigm that provides efficient\ninfrastructure management and elastic scalability. Serverless functions scale\nup or down based on demand, which means that functions are not directly\naddressable and rely on platform-managed invocation. Serverless stateless\nnature requires functions to leverage external services, such as object storage\nand KVS, to exchange data. Serverless actors have emerged as a solution to\nthese issues. However, the state-of-the-art serverless lifecycle and\nevent-trigger invocation force actors to leverage remote services to manage\ntheir state and exchange data, which impacts the performance and incurs\nadditional costs and dependency on third-party services.\n  To address these issues, in this paper, we introduce a novel serverless\nlifecycle model that allows short-term stateful actors, enabling actors to\nmaintain their state between executions. Additionally, we propose a novel\nserverless Invocation Model that enables serverless actors to influence the\nprocessing of future messages. We present GoldFish, a lightweight WebAssembly\nshort-term stateful serverless actor platform that provides a novel serverless\nactor lifecycle and invocation model. GoldFish leverages WebAssembly to provide\nthe actors with lightweight sandbox isolation, making them suitable for the\nEdge-Cloud Continuum, where computational resources are limited. Experimental\nresults show that GoldFish optimizes the data exchange latency by up to 92% and\nincreases the throughput by up to 10x compared to OpenFaaS and Spin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless Computing is a computing paradigm that provides efficient\ninfrastructure management and elastic scalability. Serverless functions scale\nup or down based on demand, which means that functions are not directly\naddressable and rely on platform-managed invocation. Serverless stateless\nnature requires functions to leverage external services, such as object storage\nand KVS, to exchange data. Serverless actors have emerged as a solution to\nthese issues. However, the state-of-the-art serverless lifecycle and\nevent-trigger invocation force actors to leverage remote services to manage\ntheir state and exchange data, which impacts the performance and incurs\nadditional costs and dependency on third-party services.\n  To address these issues, in this paper, we introduce a novel serverless\nlifecycle model that allows short-term stateful actors, enabling actors to\nmaintain their state between executions. Additionally, we propose a novel\nserverless Invocation Model that enables serverless actors to influence the\nprocessing of future messages. We present GoldFish, a lightweight WebAssembly\nshort-term stateful serverless actor platform that provides a novel serverless\nactor lifecycle and invocation model. GoldFish leverages WebAssembly to provide\nthe actors with lightweight sandbox isolation, making them suitable for the\nEdge-Cloud Continuum, where computational resources are limited. Experimental\nresults show that GoldFish optimizes the data exchange latency by up to 92% and\nincreases the throughput by up to 10x compared to OpenFaaS and Spin."
                },
                "authors": [
                    {
                        "name": "Cynthia Marcelino"
                    },
                    {
                        "name": "Jack Shahhoud"
                    },
                    {
                        "name": "Stefan Nastic"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Nastic"
                },
                "author": "Stefan Nastic",
                "arxiv_doi": "10.1145/3703790.3703797",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3703790.3703797",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.02867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14th International Conference on the Internet of Things (IoT 2024),\n  November 19--22, 2024, Oulu, Finland",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v2",
                "updated": "2024-12-03T21:40:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    21,
                    40,
                    10,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v2",
                "updated": "2024-12-03T16:12:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    12,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaoshen Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Code is available at https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v3",
                "updated": "2024-12-03T12:36:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    12,
                    36,
                    19,
                    1,
                    338,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer. Comprehensive empirical evidence demonstrates ResFormer\nachieves equivalent validation loss with 10.4% fewer model parameters and 13.6%\nless training data compared to Transformer, while maintaining similar memory\nusage and computational cost. Besides, SVFormer reduces KV cache size by nearly\nhalf with only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate. Further\nvisualization results suggest that Resformer and SVFormer alleviate attention\nconcentration in deeper layers through avoiding value-state drains and enhance\nrepresentation across most layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer. Comprehensive empirical evidence demonstrates ResFormer\nachieves equivalent validation loss with 10.4% fewer model parameters and 13.6%\nless training data compared to Transformer, while maintaining similar memory\nusage and computational cost. Besides, SVFormer reduces KV cache size by nearly\nhalf with only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate. Further\nvisualization results suggest that Resformer and SVFormer alleviate attention\nconcentration in deeper layers through avoiding value-state drains and enhance\nrepresentation across most layers."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02252v1",
                "updated": "2024-12-03T08:29:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T08:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity"
                },
                "summary": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance."
                },
                "authors": [
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Situo Zhang"
                    },
                    {
                        "name": "Yuxun Miao"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Hanqi Li"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Lei Pan"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02122v1",
                "updated": "2024-12-03T03:20:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    20,
                    40,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T03:20:40Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    20,
                    40,
                    1,
                    338,
                    0
                ],
                "title": "Improving Sequential Recommender Systems with Online and In-store User\n  Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Sequential Recommender Systems with Online and In-store User\n  Behavior"
                },
                "summary": "Online e-commerce platforms have been extending in-store shopping, which\nallows users to keep the canonical online browsing and checkout experience\nwhile exploring in-store shopping. However, the growing transition between\nonline and in-store becomes a challenge to sequential recommender systems for\nfuture online interaction prediction due to the lack of holistic modeling of\nhybrid user behaviors (online and in-store). The challenges are twofold. First,\ncombining online and in-store user behavior data into a single data schema and\nsupporting multiple stages in the model life cycle (pre-training, training,\ninference, etc.) organically needs a new data pipeline design. Second, online\nrecommender systems, which solely rely on online user behavior sequences, must\nbe redesigned to support online and in-store user data as input under the\nsequential modeling setting. To overcome the first challenge, we propose a\nhybrid, omnichannel data pipeline to compile online and in-store user behavior\ndata by caching information from diverse data sources. Later, we introduce a\nmodel-agnostic encoder module to the sequential recommender system to interpret\nthe user in-store transaction and augment the modeling capacity for better\nonline interaction prediction given the hybrid user behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online e-commerce platforms have been extending in-store shopping, which\nallows users to keep the canonical online browsing and checkout experience\nwhile exploring in-store shopping. However, the growing transition between\nonline and in-store becomes a challenge to sequential recommender systems for\nfuture online interaction prediction due to the lack of holistic modeling of\nhybrid user behaviors (online and in-store). The challenges are twofold. First,\ncombining online and in-store user behavior data into a single data schema and\nsupporting multiple stages in the model life cycle (pre-training, training,\ninference, etc.) organically needs a new data pipeline design. Second, online\nrecommender systems, which solely rely on online user behavior sequences, must\nbe redesigned to support online and in-store user data as input under the\nsequential modeling setting. To overcome the first challenge, we propose a\nhybrid, omnichannel data pipeline to compile online and in-store user behavior\ndata by caching information from diverse data sources. Later, we introduce a\nmodel-agnostic encoder module to the sequential recommender system to interpret\nthe user in-store transaction and augment the modeling capacity for better\nonline interaction prediction given the hybrid user behavior."
                },
                "authors": [
                    {
                        "name": "Luyi Ma"
                    },
                    {
                        "name": "Aashika Padmanabhan"
                    },
                    {
                        "name": "Anjana Ganesh"
                    },
                    {
                        "name": "Shengwei Tang"
                    },
                    {
                        "name": "Jiao Chen"
                    },
                    {
                        "name": "Xiaohan Li"
                    },
                    {
                        "name": "Lalitesh Morishetti"
                    },
                    {
                        "name": "Kaushiki Nag"
                    },
                    {
                        "name": "Malay Patel"
                    },
                    {
                        "name": "Jason Cho"
                    },
                    {
                        "name": "Sushant Kumar"
                    },
                    {
                        "name": "Kannan Achan"
                    }
                ],
                "author_detail": {
                    "name": "Kannan Achan"
                },
                "author": "Kannan Achan",
                "arxiv_comment": "6 pages, IEEE BigData 2024 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01959v1",
                "updated": "2024-12-02T20:39:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T20:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Development and Application of a Decentralized Domain Name Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Application of a Decentralized Domain Name Service"
                },
                "summary": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01827v1",
                "updated": "2024-12-02T18:59:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T18:59:53Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders"
                },
                "summary": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/."
                },
                "authors": [
                    {
                        "name": "Ziqi Pang"
                    },
                    {
                        "name": "Tianyuan Zhang"
                    },
                    {
                        "name": "Fujun Luan"
                    },
                    {
                        "name": "Yunze Man"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Xiong Wang"
                },
                "author": "Yu-Xiong Wang",
                "arxiv_comment": "Project page: https://rand-ar.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01659v1",
                "updated": "2024-12-02T16:10:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    10,
                    26,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T16:10:26Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    10,
                    26,
                    0,
                    337,
                    0
                ],
                "title": "Local and Regional Contributions to Tropospheric Ozone Concentrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local and Regional Contributions to Tropospheric Ozone Concentrations"
                },
                "summary": "The Wasatch Front in Utah, USA, is currently a non-attainment area for ozone\naccording to the Environmental Protection Agency's (EPA) National Ambient Air\nQuality Standards (NAAQS). Nitrogen oxides ($\\mathrm{NO_x = NO_2 + NO}$) and\nvolatile organic compounds (VOCs), in the presence of sunlight, lead to ozone\nformation in the troposphere. When the rate of oxidant production, defined as\nthe sum of $\\mathrm{O_3}$ and $\\mathrm{NO_2}$, is faster than the rate of\n$\\mathrm{NO_x}$ production, a region is said to be $\\mathrm{NO_x}$limited, and\nozone formation will be limited by the concentration of $\\mathrm{NO_x}$ species\nin the region. The inverse of this situation makes the region VOC-limited.\nKnowing whether a region is $\\mathrm{NO_x}$-limited or VOC-limited can aid in\ngenerating effective mitigation strategies. Understanding the background or\nregional contributions to ozone in a region, whether from the transport of\nprecursors or of ozone, provides information about the lower limit for ozone\nconcentrations that a region can achieve through regulation of local\nprecursors. In this paper, measured oxidant and $\\mathrm{NO_x}$ concentrations\nare analyzed from 14 counties in the state of Utah to calculate the regional\nand local contributions to ozone for each region. This analysis is used to\ndetermine the nature of the atmosphere in each county by identifying whether\nthe region is VOC or $\\mathrm{NO_x}$-limited. Furthermore, this analysis is\nperformed for each county for the years 2012 and 2022 to assess changes in the\noxidative nature and quantify regional and local contributions to ozone over a\n10-year period. All studied counties--except for Washington County--in Utah\nwere found to be VOC-limited in 2012. This shifted in 2022, with most counties\nbeing either in a transitional state or $\\mathrm{NO_x}$-limited. Local\ncontributions to ozone increased in two major counties, Cache and Salt Lake\nCounties, but decreased in Carbon, Davis, Duchesne, Uinta, Utah, Washington,\nand Weber Counties. Generally, the regional contributions to oxidant\nconcentrations decreased across the state. A summertime spike in both regional\nand local contributions to oxidants was observed. Smoke from wildfires was\nfound to increase regional contributions to oxidants and shift the local regime\nto be more $\\mathrm{NO_x}$-limited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Wasatch Front in Utah, USA, is currently a non-attainment area for ozone\naccording to the Environmental Protection Agency's (EPA) National Ambient Air\nQuality Standards (NAAQS). Nitrogen oxides ($\\mathrm{NO_x = NO_2 + NO}$) and\nvolatile organic compounds (VOCs), in the presence of sunlight, lead to ozone\nformation in the troposphere. When the rate of oxidant production, defined as\nthe sum of $\\mathrm{O_3}$ and $\\mathrm{NO_2}$, is faster than the rate of\n$\\mathrm{NO_x}$ production, a region is said to be $\\mathrm{NO_x}$limited, and\nozone formation will be limited by the concentration of $\\mathrm{NO_x}$ species\nin the region. The inverse of this situation makes the region VOC-limited.\nKnowing whether a region is $\\mathrm{NO_x}$-limited or VOC-limited can aid in\ngenerating effective mitigation strategies. Understanding the background or\nregional contributions to ozone in a region, whether from the transport of\nprecursors or of ozone, provides information about the lower limit for ozone\nconcentrations that a region can achieve through regulation of local\nprecursors. In this paper, measured oxidant and $\\mathrm{NO_x}$ concentrations\nare analyzed from 14 counties in the state of Utah to calculate the regional\nand local contributions to ozone for each region. This analysis is used to\ndetermine the nature of the atmosphere in each county by identifying whether\nthe region is VOC or $\\mathrm{NO_x}$-limited. Furthermore, this analysis is\nperformed for each county for the years 2012 and 2022 to assess changes in the\noxidative nature and quantify regional and local contributions to ozone over a\n10-year period. All studied counties--except for Washington County--in Utah\nwere found to be VOC-limited in 2012. This shifted in 2022, with most counties\nbeing either in a transitional state or $\\mathrm{NO_x}$-limited. Local\ncontributions to ozone increased in two major counties, Cache and Salt Lake\nCounties, but decreased in Carbon, Davis, Duchesne, Uinta, Utah, Washington,\nand Weber Counties. Generally, the regional contributions to oxidant\nconcentrations decreased across the state. A summertime spike in both regional\nand local contributions to oxidants was observed. Smoke from wildfires was\nfound to increase regional contributions to oxidants and shift the local regime\nto be more $\\mathrm{NO_x}$-limited."
                },
                "authors": [
                    {
                        "name": "Callum E. Flowerday"
                    },
                    {
                        "name": "Ryan Thalman"
                    },
                    {
                        "name": "Jaron C. Hansen"
                    }
                ],
                "author_detail": {
                    "name": "Jaron C. Hansen"
                },
                "author": "Jaron C. Hansen",
                "arxiv_doi": "10.3390/atmos14081262",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/atmos14081262",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.01659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Atmosphere 2023, 14, 1262",
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01415v1",
                "updated": "2024-12-02T11:57:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    57,
                    3,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T11:57:03Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    57,
                    3,
                    0,
                    337,
                    0
                ],
                "title": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure"
                },
                "summary": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW."
                },
                "authors": [
                    {
                        "name": "A. B. Batrakov"
                    },
                    {
                        "name": "S. Yu. Karelin"
                    },
                    {
                        "name": "O. M. Lebedenko"
                    },
                    {
                        "name": "V. S. Mukhin"
                    },
                    {
                        "name": "I. N. Onishchenko"
                    },
                    {
                        "name": "O. L. Rak"
                    },
                    {
                        "name": "V. G. Sinitsin"
                    },
                    {
                        "name": "M. V. Volovenko"
                    }
                ],
                "author_detail": {
                    "name": "M. V. Volovenko"
                },
                "author": "M. V. Volovenko",
                "arxiv_comment": "4 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06892v2",
                "updated": "2024-12-02T11:24:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    24,
                    20,
                    0,
                    337,
                    0
                ],
                "published": "2024-03-11T16:48:25Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    16,
                    48,
                    25,
                    0,
                    71,
                    0
                ],
                "title": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head"
                },
                "summary": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}"
                },
                "authors": [
                    {
                        "name": "Tiancheng Zhao"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Xuan He"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Kyusong Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyusong Lee"
                },
                "author": "Kyusong Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01380v1",
                "updated": "2024-12-02T11:07:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T11:07:51Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "title": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking"
                },
                "summary": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which result in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46% reduction in memory and 40% increase in throughput with $<$ 0.1\nloss in perplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which result in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46% reduction in memory and 40% increase in throughput with $<$ 0.1\nloss in perplexity."
                },
                "authors": [
                    {
                        "name": "Marco Federici"
                    },
                    {
                        "name": "Davide Belli"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Amir Jalalirad"
                    },
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Bence Major"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    }
                ],
                "author_detail": {
                    "name": "Paul Whatmough"
                },
                "author": "Paul Whatmough",
                "arxiv_comment": "Main Text: 10 pages, 11 figures. Appendix: 3 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01195v1",
                "updated": "2024-12-02T06:57:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    57,
                    46,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T06:57:46Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    57,
                    46,
                    0,
                    337,
                    0
                ],
                "title": "Memory-Efficient Training for Deep Speaker Embedding Learning in Speaker\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Training for Deep Speaker Embedding Learning in Speaker\n  Verification"
                },
                "summary": "Recent speaker verification (SV) systems have shown a trend toward adopting\ndeeper speaker embedding extractors. Although deeper and larger neural networks\ncan significantly improve performance, their substantial memory requirements\nhinder training on consumer GPUs. In this paper, we explore a memory-efficient\ntraining strategy for deep speaker embedding learning in resource-constrained\nscenarios. Firstly, we conduct a systematic analysis of GPU memory allocation\nduring SV system training. Empirical observations show that activations and\noptimizer states are the main sources of memory consumption. For activations,\nwe design two types of reversible neural networks which eliminate the need to\nstore intermediate activations during back-propagation, thereby significantly\nreducing memory usage without performance loss. For optimizer states, we\nintroduce a dynamic quantization approach that replaces the original 32-bit\nfloating-point values with a dynamic tree-based 8-bit data type. Experimental\nresults on VoxCeleb demonstrate that the reversible variants of ResNets and\nDF-ResNets can perform training without the need to cache activations in GPU\nmemory. In addition, the 8-bit versions of SGD and Adam save 75% of memory\ncosts while maintaining performance compared to their 32-bit counterparts.\nFinally, a detailed comparison of memory usage and performance indicates that\nour proposed models achieve up to 16.2x memory savings, with nearly identical\nparameters and performance compared to the vanilla systems. In contrast to the\nprevious need for multiple high-end GPUs such as the A100, we can effectively\ntrain deep speaker embedding extractors with just one or two consumer-level\n2080Ti GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent speaker verification (SV) systems have shown a trend toward adopting\ndeeper speaker embedding extractors. Although deeper and larger neural networks\ncan significantly improve performance, their substantial memory requirements\nhinder training on consumer GPUs. In this paper, we explore a memory-efficient\ntraining strategy for deep speaker embedding learning in resource-constrained\nscenarios. Firstly, we conduct a systematic analysis of GPU memory allocation\nduring SV system training. Empirical observations show that activations and\noptimizer states are the main sources of memory consumption. For activations,\nwe design two types of reversible neural networks which eliminate the need to\nstore intermediate activations during back-propagation, thereby significantly\nreducing memory usage without performance loss. For optimizer states, we\nintroduce a dynamic quantization approach that replaces the original 32-bit\nfloating-point values with a dynamic tree-based 8-bit data type. Experimental\nresults on VoxCeleb demonstrate that the reversible variants of ResNets and\nDF-ResNets can perform training without the need to cache activations in GPU\nmemory. In addition, the 8-bit versions of SGD and Adam save 75% of memory\ncosts while maintaining performance compared to their 32-bit counterparts.\nFinally, a detailed comparison of memory usage and performance indicates that\nour proposed models achieve up to 16.2x memory savings, with nearly identical\nparameters and performance compared to the vanilla systems. In contrast to the\nprevious need for multiple high-end GPUs such as the A100, we can effectively\ntrain deep speaker embedding extractors with just one or two consumer-level\n2080Ti GPUs."
                },
                "authors": [
                    {
                        "name": "Bei Liu"
                    },
                    {
                        "name": "Yanmin Qian"
                    }
                ],
                "author_detail": {
                    "name": "Yanmin Qian"
                },
                "author": "Yanmin Qian",
                "arxiv_comment": "Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04762v2",
                "updated": "2024-12-02T06:30:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    30,
                    4,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-07T14:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "title": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems"
                },
                "summary": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Jiaxu Wu"
                    },
                    {
                        "name": "Zemin Sun"
                    },
                    {
                        "name": "Long He"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Shiwen Mao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Mao"
                },
                "author": "Shiwen Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00977v1",
                "updated": "2024-12-01T21:47:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    21,
                    47,
                    35,
                    6,
                    336,
                    0
                ],
                "published": "2024-12-01T21:47:35Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    21,
                    47,
                    35,
                    6,
                    336,
                    0
                ],
                "title": "Optimal Power Allocation in Uplink NOMA with Simultaneous Cache-Enabled\n  D2D Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Power Allocation in Uplink NOMA with Simultaneous Cache-Enabled\n  D2D Communications"
                },
                "summary": "Non-orthogonal multiple access (NOMA) is widely viewed as a potential\ncandidate for providing enhanced multiple access in future mobile networks by\neliminating the orthogonal distribution of radio resources amongst the users.\nNevertheless, the performance of NOMA can be significantly improved by\ncombining it with other sophisticated technologies such as wireless data\ncaching and device-to-device (D2D) communications. In this letter, we propose a\nnovel cellular system model which integrates uplink NOMA with cache based\ndevice-to-device (D2D) communications. The proposed system would enable a\ncellular user to upload data file to base station while simultaneously\nexchanging useful cache content with another nearby user. We maximize the\nsystem sum rate by deriving closed form solutions for optimal power allocation.\nSimulation results demonstrate the superior performance of our proposed model\nover other potential combinations of uplink NOMA and D2D communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-orthogonal multiple access (NOMA) is widely viewed as a potential\ncandidate for providing enhanced multiple access in future mobile networks by\neliminating the orthogonal distribution of radio resources amongst the users.\nNevertheless, the performance of NOMA can be significantly improved by\ncombining it with other sophisticated technologies such as wireless data\ncaching and device-to-device (D2D) communications. In this letter, we propose a\nnovel cellular system model which integrates uplink NOMA with cache based\ndevice-to-device (D2D) communications. The proposed system would enable a\ncellular user to upload data file to base station while simultaneously\nexchanging useful cache content with another nearby user. We maximize the\nsystem sum rate by deriving closed form solutions for optimal power allocation.\nSimulation results demonstrate the superior performance of our proposed model\nover other potential combinations of uplink NOMA and D2D communications."
                },
                "authors": [
                    {
                        "name": "Aditya Powari"
                    },
                    {
                        "name": "Daniel K. C. So"
                    }
                ],
                "author_detail": {
                    "name": "Daniel K. C. So"
                },
                "author": "Daniel K. C. So",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v1",
                "updated": "2024-12-01T15:45:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    }
                ],
                "author_detail": {
                    "name": "Peiran Dong"
                },
                "author": "Peiran Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02532v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02532v3",
                "updated": "2024-11-30T21:33:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    21,
                    33,
                    59,
                    5,
                    335,
                    0
                ],
                "published": "2024-06-04T17:53:36Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    53,
                    36,
                    1,
                    156,
                    0
                ],
                "title": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM\n  Inference on Consumer Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM\n  Inference on Consumer Devices"
                },
                "summary": "As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights."
                },
                "authors": [
                    {
                        "name": "Ruslan Svirschevski"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Max Ryabinin"
                    }
                ],
                "author_detail": {
                    "name": "Max Ryabinin"
                },
                "author": "Max Ryabinin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02532v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02532v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00209v1",
                "updated": "2024-11-29T19:14:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    19,
                    14,
                    45,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T19:14:45Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    19,
                    14,
                    45,
                    4,
                    334,
                    0
                ],
                "title": "Digital Twin in Industries: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Twin in Industries: A Comprehensive Survey"
                },
                "summary": "Industrial networks are undergoing rapid transformation driven by the\nconvergence of emerging technologies that are revolutionizing conventional\nworkflows, enhancing operational efficiency, and fundamentally redefining the\nindustrial landscape across diverse sectors. Amidst this revolution, Digital\nTwin (DT) emerges as a transformative innovation that seamlessly integrates\nreal-world systems with their virtual counterparts, bridging the physical and\ndigital realms. In this article, we present a comprehensive survey of the\nemerging DT-enabled services and applications across industries, beginning with\nan overview of DT fundamentals and its components to a discussion of key\nenabling technologies for DT. Different from literature works, we investigate\nand analyze the capabilities of DT across a wide range of industrial services,\nincluding data sharing, data offloading, integrated sensing and communication,\ncontent caching, resource allocation, wireless networking, and metaverse. In\nparticular, we present an in-depth technical discussion of the roles of DT in\nindustrial applications across various domains, including manufacturing,\nhealthcare, transportation, energy, agriculture, space, oil and gas, as well as\nrobotics. Throughout the technical analysis, we delve into real-time data\ncommunications between physical and virtual platforms to enable industrial DT\nnetworking. Subsequently, we extensively explore and analyze a wide range of\nmajor privacy and security issues in DT-based industry. Taxonomy tables and the\nkey research findings from the survey are also given, emphasizing important\ninsights into the significance of DT in industries. Finally, we point out\nfuture research directions to spur further research in this promising area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industrial networks are undergoing rapid transformation driven by the\nconvergence of emerging technologies that are revolutionizing conventional\nworkflows, enhancing operational efficiency, and fundamentally redefining the\nindustrial landscape across diverse sectors. Amidst this revolution, Digital\nTwin (DT) emerges as a transformative innovation that seamlessly integrates\nreal-world systems with their virtual counterparts, bridging the physical and\ndigital realms. In this article, we present a comprehensive survey of the\nemerging DT-enabled services and applications across industries, beginning with\nan overview of DT fundamentals and its components to a discussion of key\nenabling technologies for DT. Different from literature works, we investigate\nand analyze the capabilities of DT across a wide range of industrial services,\nincluding data sharing, data offloading, integrated sensing and communication,\ncontent caching, resource allocation, wireless networking, and metaverse. In\nparticular, we present an in-depth technical discussion of the roles of DT in\nindustrial applications across various domains, including manufacturing,\nhealthcare, transportation, energy, agriculture, space, oil and gas, as well as\nrobotics. Throughout the technical analysis, we delve into real-time data\ncommunications between physical and virtual platforms to enable industrial DT\nnetworking. Subsequently, we extensively explore and analyze a wide range of\nmajor privacy and security issues in DT-based industry. Taxonomy tables and the\nkey research findings from the survey are also given, emphasizing important\ninsights into the significance of DT in industries. Finally, we point out\nfuture research directions to spur further research in this promising area."
                },
                "authors": [
                    {
                        "name": "Md Bokhtiar Al Zami"
                    },
                    {
                        "name": "Shaba Shaon"
                    },
                    {
                        "name": "Vu Khanh Quy"
                    },
                    {
                        "name": "Dinh C. Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Dinh C. Nguyen"
                },
                "author": "Dinh C. Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19730v1",
                "updated": "2024-11-29T14:23:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    23,
                    25,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T14:23:25Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    23,
                    25,
                    4,
                    334,
                    0
                ],
                "title": "Ten Ways in which Virtual Reality Differs from Video Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ten Ways in which Virtual Reality Differs from Video Streaming"
                },
                "summary": "Virtual Reality (VR) applications have a number of unique characteristics\nthat set them apart from traditional video streaming. These characteristics\nhave major implications on the design of VR rendering, adaptation, prefetching,\ncaching, and transport mechanisms. This paper contrasts VR to video streaming,\nstored 2D video streaming in particular, and discusses how to rethink system\nand network support for VR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual Reality (VR) applications have a number of unique characteristics\nthat set them apart from traditional video streaming. These characteristics\nhave major implications on the design of VR rendering, adaptation, prefetching,\ncaching, and transport mechanisms. This paper contrasts VR to video streaming,\nstored 2D video streaming in particular, and discusses how to rethink system\nand network support for VR."
                },
                "authors": [
                    {
                        "name": "Gustavo de Veciana"
                    },
                    {
                        "name": "Sonia Fahmy"
                    },
                    {
                        "name": "George Kesidis"
                    },
                    {
                        "name": "Voicu Popescu"
                    }
                ],
                "author_detail": {
                    "name": "Voicu Popescu"
                },
                "author": "Voicu Popescu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01852v1",
                "updated": "2024-11-29T10:21:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    21,
                    12,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T10:21:12Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    21,
                    12,
                    4,
                    334,
                    0
                ],
                "title": "Communication efficient application of sequences of planar rotations to\n  a matrix",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication efficient application of sequences of planar rotations to\n  a matrix"
                },
                "summary": "We present an efficient algorithm for the application of sequences of planar\nrotations to a matrix. Applying such sequences efficiently is important in many\nnumerical linear algebra algorithms for eigenvalues. Our algorithm is novel in\nthree main ways. First, we introduce a new kernel that is optimized for\nregister reuse in a novel way. Second, we introduce a blocking and packing\nscheme that improves the cache efficiency of the algorithm. Finally, we\nthoroughly analyze the memory operations of the algorithm which leads to\nimportant theoretical insights and makes it easier to select good parameters.\nNumerical experiments show that our algorithm outperforms the state-of-the-art\nand achieves a flop rate close to the theoretical peak on modern hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an efficient algorithm for the application of sequences of planar\nrotations to a matrix. Applying such sequences efficiently is important in many\nnumerical linear algebra algorithms for eigenvalues. Our algorithm is novel in\nthree main ways. First, we introduce a new kernel that is optimized for\nregister reuse in a novel way. Second, we introduce a blocking and packing\nscheme that improves the cache efficiency of the algorithm. Finally, we\nthoroughly analyze the memory operations of the algorithm which leads to\nimportant theoretical insights and makes it easier to select good parameters.\nNumerical experiments show that our algorithm outperforms the state-of-the-art\nand achieves a flop rate close to the theoretical peak on modern hardware."
                },
                "authors": [
                    {
                        "name": "Thijs Steel"
                    },
                    {
                        "name": "Julien Langou"
                    }
                ],
                "author_detail": {
                    "name": "Julien Langou"
                },
                "author": "Julien Langou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F15, 65Y05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07533v3",
                "updated": "2024-11-29T08:48:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    48,
                    1,
                    4,
                    334,
                    0
                ],
                "published": "2024-05-13T08:03:32Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    8,
                    3,
                    32,
                    0,
                    134,
                    0
                ],
                "title": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials"
                },
                "summary": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities."
                },
                "authors": [
                    {
                        "name": "Sandro Rodriguez Garzon"
                    },
                    {
                        "name": "Dennis Natusch"
                    },
                    {
                        "name": "Artur Philipp"
                    },
                    {
                        "name": "Axel Küpper"
                    },
                    {
                        "name": "Hans Joachim Einsiedler"
                    },
                    {
                        "name": "Daniela Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Schneider"
                },
                "author": "Daniela Schneider",
                "arxiv_comment": "Accepted by and presented at 21st Annual International Conference on\n  Privacy, Security, and Trust (PST2024). Publication by IEEE still pending",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18191v2",
                "updated": "2024-11-29T08:33:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    33,
                    49,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-27T10:14:38Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    14,
                    38,
                    2,
                    332,
                    0
                ],
                "title": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks"
                },
                "summary": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference."
                },
                "authors": [
                    {
                        "name": "Xinyao Zheng"
                    },
                    {
                        "name": "Husheng Han"
                    },
                    {
                        "name": "Shangyi Shi"
                    },
                    {
                        "name": "Qiyan Fang"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Qi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Qi Guo"
                },
                "author": "Qi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03594v1",
                "updated": "2024-11-29T05:57:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    57,
                    37,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T05:57:37Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    57,
                    37,
                    4,
                    334,
                    0
                ],
                "title": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching"
                },
                "summary": "Many LLM tasks are performed in large batches or even offline, and the\nperformance indictor for which is throughput. These tasks usually show the\ncharacteristic of prefix sharing, where different prompt input can partially\nshow the common prefix. However, the existing LLM inference engines tend to\noptimize the streaming requests and show limitations of supporting the large\nbatched tasks with the prefix sharing characteristic. The existing solutions\nuse the LRU-based cache to reuse the KV context of common prefix. The KV\ncontext that is about to be reused may prematurely be evicted with the implicit\ncache management. Even if not evicted, the lifetime of the shared KV context is\nextended since requests sharing the same context are not scheduled together,\nresulting in larger memory usage. These streaming oriented systems schedule the\nrequests in the first-come-first-serve or similar order. As a result, the\nrequests with larger ratio of decoding steps may be scheduled too late to be\nable to mix with the prefill chunks to increase the hardware utilization.\nBesides, the token and request number based batching can limit the size of\ntoken-batch, which keeps the GPU from saturating for the iterations dominated\nby decoding tokens. We propose BatchLLM to address the above problems. BatchLLM\nexplicitly identifies the common prefixes globally. The requests sharing the\nsame prefix will be scheduled together to reuse the KV context the best, which\nalso shrinks the lifetime of common KV memory. BatchLLM reorders the requests\nand schedules the requests with larger ratio of decoding first to better mix\nthe decoding tokens with the latter prefill chunks and applies memory-centric\ntoken batching to enlarge the token-batch sizes, which helps to increase the\nGPU utilization. Extensive evaluation shows that BatchLLM outperforms vLLM by\n1.1x to 2x on a set of microbenchmarks and two typical industry workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many LLM tasks are performed in large batches or even offline, and the\nperformance indictor for which is throughput. These tasks usually show the\ncharacteristic of prefix sharing, where different prompt input can partially\nshow the common prefix. However, the existing LLM inference engines tend to\noptimize the streaming requests and show limitations of supporting the large\nbatched tasks with the prefix sharing characteristic. The existing solutions\nuse the LRU-based cache to reuse the KV context of common prefix. The KV\ncontext that is about to be reused may prematurely be evicted with the implicit\ncache management. Even if not evicted, the lifetime of the shared KV context is\nextended since requests sharing the same context are not scheduled together,\nresulting in larger memory usage. These streaming oriented systems schedule the\nrequests in the first-come-first-serve or similar order. As a result, the\nrequests with larger ratio of decoding steps may be scheduled too late to be\nable to mix with the prefill chunks to increase the hardware utilization.\nBesides, the token and request number based batching can limit the size of\ntoken-batch, which keeps the GPU from saturating for the iterations dominated\nby decoding tokens. We propose BatchLLM to address the above problems. BatchLLM\nexplicitly identifies the common prefixes globally. The requests sharing the\nsame prefix will be scheduled together to reuse the KV context the best, which\nalso shrinks the lifetime of common KV memory. BatchLLM reorders the requests\nand schedules the requests with larger ratio of decoding first to better mix\nthe decoding tokens with the latter prefill chunks and applies memory-centric\ntoken batching to enlarge the token-batch sizes, which helps to increase the\nGPU utilization. Extensive evaluation shows that BatchLLM outperforms vLLM by\n1.1x to 2x on a set of microbenchmarks and two typical industry workloads."
                },
                "authors": [
                    {
                        "name": "Zhen Zheng"
                    },
                    {
                        "name": "Xin Ji"
                    },
                    {
                        "name": "Taosong Fang"
                    },
                    {
                        "name": "Fanghao Zhou"
                    },
                    {
                        "name": "Chuanjie Liu"
                    },
                    {
                        "name": "Gang Peng"
                    }
                ],
                "author_detail": {
                    "name": "Gang Peng"
                },
                "author": "Gang Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19248v1",
                "updated": "2024-11-28T16:35:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    35,
                    22,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T16:35:22Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    35,
                    22,
                    3,
                    333,
                    0
                ],
                "title": "Reflecting Intelligent Surfaces-Assisted Multiple-Antenna Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflecting Intelligent Surfaces-Assisted Multiple-Antenna Coded Caching"
                },
                "summary": "Reconfigurable intelligent surface (RIS) has been treated as a core technique\nin improving wireless propagation environments for the next generation wireless\ncommunication systems. This paper proposes a new coded caching problem,\nreferred to as Reconfigurable Intelligent Surface (RIS)-assisted\nmultiple-antenna coded caching, which is composed of a server with multiple\nantennas and some single-antenna cache-aided users. Different from the existing\nmulti-antenna coded caching problems, we introduce a passive RIS (with limited\nnumber of units) into the systems to further increase the multicast gain (i.e.,\ndegrees of freedom (DoF)) in the transmission, which is done by using\nRIS-assisted interference nulling. That is, by using RIS, we can `erase' any\npath between one transmission antenna and one receive antenna. We first propose\na new RIS-assisted interference nulling approach to search for the phase-shift\ncoefficients of RIS for the sake of interference nulling, which converges\nfaster than the state-of-the-art algorithm. After erasing some paths in each\ntime slot, the delivery can be divided into several non-overlapping groups\nincluding transmission antennas and users, where in each group the transmission\nantennas serve the contained users without suffering interference from the\ntransmissions by other groups. The division of groups for the sake of\nmaximizing the DoF could be formulated into a combinatorial optimization\nproblem. We propose a grouping algorithm which can find the optimal solution\nwith low complexity, and the corresponding coded caching scheme achieving this\nDoF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable intelligent surface (RIS) has been treated as a core technique\nin improving wireless propagation environments for the next generation wireless\ncommunication systems. This paper proposes a new coded caching problem,\nreferred to as Reconfigurable Intelligent Surface (RIS)-assisted\nmultiple-antenna coded caching, which is composed of a server with multiple\nantennas and some single-antenna cache-aided users. Different from the existing\nmulti-antenna coded caching problems, we introduce a passive RIS (with limited\nnumber of units) into the systems to further increase the multicast gain (i.e.,\ndegrees of freedom (DoF)) in the transmission, which is done by using\nRIS-assisted interference nulling. That is, by using RIS, we can `erase' any\npath between one transmission antenna and one receive antenna. We first propose\na new RIS-assisted interference nulling approach to search for the phase-shift\ncoefficients of RIS for the sake of interference nulling, which converges\nfaster than the state-of-the-art algorithm. After erasing some paths in each\ntime slot, the delivery can be divided into several non-overlapping groups\nincluding transmission antennas and users, where in each group the transmission\nantennas serve the contained users without suffering interference from the\ntransmissions by other groups. The division of groups for the sake of\nmaximizing the DoF could be formulated into a combinatorial optimization\nproblem. We propose a grouping algorithm which can find the optimal solution\nwith low complexity, and the corresponding coded caching scheme achieving this\nDoF."
                },
                "authors": [
                    {
                        "name": "Xiaofan Niu"
                    },
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Robert Caiming Qiu"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "The short version of this paper was presented in 2024 IEEE\n  Information Theory Workshop, Nov. 24-28, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12468v2",
                "updated": "2024-11-28T14:42:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    14,
                    42,
                    54,
                    3,
                    333,
                    0
                ],
                "published": "2024-04-18T19:04:33Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    19,
                    4,
                    33,
                    3,
                    109,
                    0
                ],
                "title": "Fresh Caching of Dynamic Contents using Restless Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fresh Caching of Dynamic Contents using Restless Multi-armed Bandits"
                },
                "summary": "We consider a dynamic content caching problem wherein the contents get\nupdated at a central server, and local copies of a subset of contents are\ncached at a local cache associated with a Base station (BS). When a content\nrequest arrives, based on whether the content is in the local cache, the BS can\ndecide whether to fetch the content from the central server or serve the cached\nversion from the local cache. Fetching a content incurs a fixed fetching cost,\nand serving the cached version incurs an ageing cost proportional to the\nage-of-version (AoV) of the content. The BS has only partial information\nregarding AoVs of the contents. We formulate an optimal content fetching and\ncaching problem to minimize the average cost subject to cache capacity\nconstraints. The problem suffers from the curse of dimensionality and is\nprovably hard to solve. We formulate this problem as a continuous time restless\nmulti-armed bandit process (RMAB), where a single content problem of the\ncorresponding RMAB is a partially observable Markov decision process. We\nreformulate the single content problem as a semi-Markov decision process, prove\nindexability, and provide a Whittle index based solution to this problem.\nFinally, we compare the performance with recent work and show that our proposed\npolicy is optimal via simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a dynamic content caching problem wherein the contents get\nupdated at a central server, and local copies of a subset of contents are\ncached at a local cache associated with a Base station (BS). When a content\nrequest arrives, based on whether the content is in the local cache, the BS can\ndecide whether to fetch the content from the central server or serve the cached\nversion from the local cache. Fetching a content incurs a fixed fetching cost,\nand serving the cached version incurs an ageing cost proportional to the\nage-of-version (AoV) of the content. The BS has only partial information\nregarding AoVs of the contents. We formulate an optimal content fetching and\ncaching problem to minimize the average cost subject to cache capacity\nconstraints. The problem suffers from the curse of dimensionality and is\nprovably hard to solve. We formulate this problem as a continuous time restless\nmulti-armed bandit process (RMAB), where a single content problem of the\ncorresponding RMAB is a partially observable Markov decision process. We\nreformulate the single content problem as a semi-Markov decision process, prove\nindexability, and provide a Whittle index based solution to this problem.\nFinally, we compare the performance with recent work and show that our proposed\npolicy is optimal via simulations."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19108v1",
                "updated": "2024-11-28T12:50:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T12:50:05Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model"
                },
                "summary": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality."
                },
                "authors": [
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Yujie Wei"
                    },
                    {
                        "name": "Haonan Qiu"
                    },
                    {
                        "name": "Yuzhong Zhao"
                    },
                    {
                        "name": "Yingya Zhang"
                    },
                    {
                        "name": "Qixiang Ye"
                    },
                    {
                        "name": "Fang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Fang Wan"
                },
                "author": "Fang Wan",
                "arxiv_comment": "Project: https://liewfeng.github.io/TeaCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18077v2",
                "updated": "2024-11-28T02:01:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    2,
                    1,
                    50,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-27T06:10:49Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    6,
                    10,
                    49,
                    2,
                    332,
                    0
                ],
                "title": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache"
                },
                "summary": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements."
                },
                "authors": [
                    {
                        "name": "Akshat Sharma"
                    },
                    {
                        "name": "Hangliang Ding"
                    },
                    {
                        "name": "Jianping Li"
                    },
                    {
                        "name": "Neel Dani"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00099v1",
                "updated": "2024-11-27T18:59:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T18:59:48Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "title": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference"
                },
                "summary": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Ties van Rozendaal"
                    },
                    {
                        "name": "Romain Lepert"
                    },
                    {
                        "name": "Todor Boinovski"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Ehteshami Bejnordi"
                },
                "author": "Babak Ehteshami Bejnordi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18566v1",
                "updated": "2024-11-27T18:09:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T18:09:29Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "title": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software"
                },
                "summary": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software."
                },
                "authors": [
                    {
                        "name": "Oliver Maximilian Zobel"
                    },
                    {
                        "name": "Johannes Maierhofer"
                    },
                    {
                        "name": "Andreas Köstler"
                    },
                    {
                        "name": "Daniel J. Rixen"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Rixen"
                },
                "author": "Daniel J. Rixen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08895v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08895v4",
                "updated": "2024-11-27T18:05:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    5,
                    57,
                    2,
                    332,
                    0
                ],
                "published": "2024-01-17T00:36:58Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    0,
                    36,
                    58,
                    2,
                    17,
                    0
                ],
                "title": "cedar: Optimized and Unified Machine Learning Input Data Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cedar: Optimized and Unified Machine Learning Input Data Pipelines"
                },
                "summary": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems."
                },
                "authors": [
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Emanuel Adamiak"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "Published in PVLDB Volume 18, Issue 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08895v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08895v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18424v1",
                "updated": "2024-11-27T15:07:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    28,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T15:07:28Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    28,
                    2,
                    332,
                    0
                ],
                "title": "FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware\n  Large Language Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware\n  Large Language Model Serving"
                },
                "summary": "Serving numerous users and requests concurrently requires good fairness in\nLarge Language Models (LLMs) serving system. This ensures that, at the same\ncost, the system can meet the Service Level Objectives (SLOs) of more users ,\nsuch as time to first token (TTFT) and time between tokens (TBT), rather than\nallowing a few users to experience performance far exceeding the SLOs. To\nachieve better fairness, the preemption-based scheduling policy dynamically\nadjusts the priority of each request to maintain balance during runtime.\nHowever, existing systems tend to overly prioritize throughput, overlooking the\noverhead caused by preemption-induced context switching, which is crucial for\nmaintaining fairness through priority adjustments. In this work, we identify\nthree main challenges that result in this overhead. 1) Inadequate I/O\nutilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn\nconversations. Our key insight is that the block-based KV cache memory policy\nin existing systems, while achieving near-zero memory waste, leads to\ndiscontinuity and insufficient granularity in the KV cache memory. To respond,\nwe introduce FastSwitch, a fairness-aware serving system that not only aligns\nwith existing KV cache memory allocation policy but also mitigates context\nswitching overhead. Our evaluation shows that FastSwitch outperforms the\nstate-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across\ndifferent tail TTFT and TBT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving numerous users and requests concurrently requires good fairness in\nLarge Language Models (LLMs) serving system. This ensures that, at the same\ncost, the system can meet the Service Level Objectives (SLOs) of more users ,\nsuch as time to first token (TTFT) and time between tokens (TBT), rather than\nallowing a few users to experience performance far exceeding the SLOs. To\nachieve better fairness, the preemption-based scheduling policy dynamically\nadjusts the priority of each request to maintain balance during runtime.\nHowever, existing systems tend to overly prioritize throughput, overlooking the\noverhead caused by preemption-induced context switching, which is crucial for\nmaintaining fairness through priority adjustments. In this work, we identify\nthree main challenges that result in this overhead. 1) Inadequate I/O\nutilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn\nconversations. Our key insight is that the block-based KV cache memory policy\nin existing systems, while achieving near-zero memory waste, leads to\ndiscontinuity and insufficient granularity in the KV cache memory. To respond,\nwe introduce FastSwitch, a fairness-aware serving system that not only aligns\nwith existing KV cache memory allocation policy but also mitigates context\nswitching overhead. Our evaluation shows that FastSwitch outperforms the\nstate-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across\ndifferent tail TTFT and TBT."
                },
                "authors": [
                    {
                        "name": "Ao Shen"
                    },
                    {
                        "name": "Zhiyao Li"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v2",
                "updated": "2024-11-27T14:43:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    43,
                    46,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Accelerating Vision Diffusion Transformers with Skip Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Vision Diffusion Transformers with Skip Branches"
                },
                "summary": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17459v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17459v2",
                "updated": "2024-11-27T08:21:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    21,
                    47,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-26T14:23:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model"
                },
                "summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE."
                },
                "authors": [
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Liuhan Chen"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17459v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17459v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15785v2",
                "updated": "2024-11-27T03:07:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    3,
                    7,
                    20,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-24T11:30:00Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    11,
                    30,
                    0,
                    6,
                    329,
                    0
                ],
                "title": "A Method for Building Large Language Models with Predefined KV Cache\n  Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Method for Building Large Language Models with Predefined KV Cache\n  Capacity"
                },
                "summary": "This paper introduces a novel approach, the Bounded-Cache Transformer (BCT),\nfor building large language models with a predefined Key-Value (KV) cache\ncapacity. The BCT addresses the excessive memory consumption issue in\ntraditional KV caches by implementing a bounded-length KV cache, which is\nparticularly suitable for the attention layers in Transformer decode-only\narchitectures. By dynamically updating the key-value vector sequences, the BCT\nachieves efficient inference within limited cache capacity, significantly\nreducing memory usage while maintaining model performance and system\nthroughput. Experimental results demonstrate that the BCT significantly reduces\nmemory usage while maintaining the model's inference quality, offering a new\nsolution for efficient inference in large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel approach, the Bounded-Cache Transformer (BCT),\nfor building large language models with a predefined Key-Value (KV) cache\ncapacity. The BCT addresses the excessive memory consumption issue in\ntraditional KV caches by implementing a bounded-length KV cache, which is\nparticularly suitable for the attention layers in Transformer decode-only\narchitectures. By dynamically updating the key-value vector sequences, the BCT\nachieves efficient inference within limited cache capacity, significantly\nreducing memory usage while maintaining model performance and system\nthroughput. Experimental results demonstrate that the BCT significantly reduces\nmemory usage while maintaining the model's inference quality, offering a new\nsolution for efficient inference in large language models."
                },
                "authors": [
                    {
                        "name": "Zhonghua Yi"
                    },
                    {
                        "name": "Ge Niu"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Liqiu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liqiu Zhang"
                },
                "author": "Liqiu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17685v1",
                "updated": "2024-11-26T18:52:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    52,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:52:06Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    52,
                    6,
                    1,
                    331,
                    0
                ],
                "title": "Attamba: Attending To Multi-Token States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attamba: Attending To Multi-Token States"
                },
                "summary": "When predicting the next token in a sequence, vanilla transformers compute\nattention over all previous tokens, resulting in quadratic scaling of compute\nwith sequence length. State-space models compress the entire sequence of tokens\ninto a fixed-dimensional representation to improve efficiency, while other\narchitectures achieve sub-quadratic complexity via low-rank projections or\nsparse attention patterns over the sequence. In this paper, we introduce\nAttamba, a novel architecture that uses state-space models to compress chunks\nof tokens and applies attention on these compressed key-value representations.\nWe find that replacing key and value projections in a transformer with SSMs can\nimprove model quality and enable flexible token chunking, resulting in 24%\nimproved perplexity with transformer of similar KV-Cache and attention\nfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity\ntrade-off. Attamba can perform attention on chunked-sequences of variable\nlength, enabling a smooth transition between quadratic and linear scaling,\noffering adaptable efficiency gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When predicting the next token in a sequence, vanilla transformers compute\nattention over all previous tokens, resulting in quadratic scaling of compute\nwith sequence length. State-space models compress the entire sequence of tokens\ninto a fixed-dimensional representation to improve efficiency, while other\narchitectures achieve sub-quadratic complexity via low-rank projections or\nsparse attention patterns over the sequence. In this paper, we introduce\nAttamba, a novel architecture that uses state-space models to compress chunks\nof tokens and applies attention on these compressed key-value representations.\nWe find that replacing key and value projections in a transformer with SSMs can\nimprove model quality and enable flexible token chunking, resulting in 24%\nimproved perplexity with transformer of similar KV-Cache and attention\nfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity\ntrade-off. Attamba can perform attention on chunked-sequences of variable\nlength, enabling a smooth transition between quadratic and linear scaling,\noffering adaptable efficiency gains."
                },
                "authors": [
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Safeen Huda"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17800v1",
                "updated": "2024-11-26T18:42:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    42,
                    42,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:42:42Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    42,
                    42,
                    1,
                    331,
                    0
                ],
                "title": "STAR: Synthesis of Tailored Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAR: Synthesis of Tailored Architectures"
                },
                "summary": "Iterative improvement of model architectures is fundamental to deep learning:\nTransformers first enabled scaling, and recent advances in model hybridization\nhave pushed the quality-efficiency frontier. However, optimizing architectures\nremains challenging and expensive. Current automated or manual approaches fall\nshort, largely due to limited progress in the design of search spaces and due\nto the simplicity of resulting patterns and heuristics. In this work, we\npropose a new approach for the synthesis of tailored architectures (STAR). Our\napproach combines a novel search space based on the theory of linear\ninput-varying systems, supporting a hierarchical numerical encoding into\narchitecture genomes. STAR genomes are automatically refined and recombined\nwith gradient-free, evolutionary algorithms to optimize for multiple model\nquality and efficiency metrics. Using STAR, we optimize large populations of\nnew architectures, leveraging diverse computational units and interconnection\npatterns, improving over highly-optimized Transformers and striped hybrid\nmodels on the frontier of quality, parameter size, and inference cache for\nautoregressive language modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative improvement of model architectures is fundamental to deep learning:\nTransformers first enabled scaling, and recent advances in model hybridization\nhave pushed the quality-efficiency frontier. However, optimizing architectures\nremains challenging and expensive. Current automated or manual approaches fall\nshort, largely due to limited progress in the design of search spaces and due\nto the simplicity of resulting patterns and heuristics. In this work, we\npropose a new approach for the synthesis of tailored architectures (STAR). Our\napproach combines a novel search space based on the theory of linear\ninput-varying systems, supporting a hierarchical numerical encoding into\narchitecture genomes. STAR genomes are automatically refined and recombined\nwith gradient-free, evolutionary algorithms to optimize for multiple model\nquality and efficiency metrics. Using STAR, we optimize large populations of\nnew architectures, leveraging diverse computational units and interconnection\npatterns, improving over highly-optimized Transformers and striped hybrid\nmodels on the frontier of quality, parameter size, and inference cache for\nautoregressive language modeling."
                },
                "authors": [
                    {
                        "name": "Armin W. Thomas"
                    },
                    {
                        "name": "Rom Parnichkun"
                    },
                    {
                        "name": "Alexander Amini"
                    },
                    {
                        "name": "Stefano Massaroli"
                    },
                    {
                        "name": "Michael Poli"
                    }
                ],
                "author_detail": {
                    "name": "Michael Poli"
                },
                "author": "Michael Poli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15651v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15651v3",
                "updated": "2024-11-26T17:28:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-03-22T23:47:19Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    23,
                    47,
                    19,
                    4,
                    82,
                    0
                ],
                "title": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering"
                },
                "summary": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room."
                },
                "authors": [
                    {
                        "name": "Jiaye Wu"
                    },
                    {
                        "name": "Saeed Hadadan"
                    },
                    {
                        "name": "Geng Lin"
                    },
                    {
                        "name": "Matthias Zwicker"
                    },
                    {
                        "name": "David Jacobs"
                    },
                    {
                        "name": "Roni Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Roni Sengupta"
                },
                "author": "Roni Sengupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15651v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15651v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17559v1",
                "updated": "2024-11-26T16:21:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    21,
                    10,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T16:21:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    21,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Degrees of Freedom of Cache-Aided Interference Channels Assisted by\n  Active Intelligent Reflecting Surfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Degrees of Freedom of Cache-Aided Interference Channels Assisted by\n  Active Intelligent Reflecting Surfaces"
                },
                "summary": "This paper studies cache-aided wireless networks in the presence of active\nintelligent reflecting surfaces (IRS) from an information-theoretic\nperspective. Specifically, we explore interference management in a cache-aided\nwireless network assisted by an active IRS, to enhance the achievable degrees\nof freedom (DoF). To this end, we jointly design the content placement,\ndelivery phase, and phase shifts of the IRS and propose a one-shot achievable\nscheme. Our scheme exploits transmitters' cooperation, cache contents (as side\ninformation), interference alignment, and IRS capabilities, adapting to the\nnetwork's parameters. We derive the achievable one-shot sum-DoF for different\nsizes of cache memories, network configurations, and numbers of IRS elements.\nOur results highlight the potential of deploying an IRS in cache-aided wireless\ncommunication systems, underscoring the enhancement of achievable DoF for\nvarious parameter regimes, particularly when the sizes of the caches\n(especially at the transmitters) are inadequate. Notably, we show that access\nto an IRS with a sufficient number of elements enables the achievement of the\nmaximum possible DoF for various parameter regimes of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies cache-aided wireless networks in the presence of active\nintelligent reflecting surfaces (IRS) from an information-theoretic\nperspective. Specifically, we explore interference management in a cache-aided\nwireless network assisted by an active IRS, to enhance the achievable degrees\nof freedom (DoF). To this end, we jointly design the content placement,\ndelivery phase, and phase shifts of the IRS and propose a one-shot achievable\nscheme. Our scheme exploits transmitters' cooperation, cache contents (as side\ninformation), interference alignment, and IRS capabilities, adapting to the\nnetwork's parameters. We derive the achievable one-shot sum-DoF for different\nsizes of cache memories, network configurations, and numbers of IRS elements.\nOur results highlight the potential of deploying an IRS in cache-aided wireless\ncommunication systems, underscoring the enhancement of achievable DoF for\nvarious parameter regimes, particularly when the sizes of the caches\n(especially at the transmitters) are inadequate. Notably, we show that access\nto an IRS with a sufficient number of elements enables the achievement of the\nmaximum possible DoF for various parameter regimes of interest."
                },
                "authors": [
                    {
                        "name": "Abolfazl Changizi"
                    },
                    {
                        "name": "Ali H. Abdollahi Bafghi"
                    },
                    {
                        "name": "Masoumeh Nasiri-Kenari"
                    }
                ],
                "author_detail": {
                    "name": "Masoumeh Nasiri-Kenari"
                },
                "author": "Masoumeh Nasiri-Kenari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17786v1",
                "updated": "2024-11-26T15:03:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T15:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation\n  via Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation\n  via Feature Caching"
                },
                "summary": "Personalized image generation requires text-to-image generative models that\ncapture the core features of a reference subject to allow for controlled\ngeneration across different contexts. Existing methods face challenges due to\ncomplex training requirements, high inference costs, limited flexibility, or a\ncombination of these issues. In this paper, we introduce DreamCache, a scalable\napproach for efficient and high-quality personalized image generation. By\ncaching a small number of reference image features from a subset of layers and\na single timestep of the pretrained diffusion denoiser, DreamCache enables\ndynamic modulation of the generated image features through lightweight, trained\nconditioning adapters. DreamCache achieves state-of-the-art image and text\nalignment, utilizing an order of magnitude fewer extra parameters, and is both\nmore computationally effective and versatile than existing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized image generation requires text-to-image generative models that\ncapture the core features of a reference subject to allow for controlled\ngeneration across different contexts. Existing methods face challenges due to\ncomplex training requirements, high inference costs, limited flexibility, or a\ncombination of these issues. In this paper, we introduce DreamCache, a scalable\napproach for efficient and high-quality personalized image generation. By\ncaching a small number of reference image features from a subset of layers and\na single timestep of the pretrained diffusion denoiser, DreamCache enables\ndynamic modulation of the generated image features through lightweight, trained\nconditioning adapters. DreamCache achieves state-of-the-art image and text\nalignment, utilizing an order of magnitude fewer extra parameters, and is both\nmore computationally effective and versatile than existing models."
                },
                "authors": [
                    {
                        "name": "Emanuele Aiello"
                    },
                    {
                        "name": "Umberto Michieli"
                    },
                    {
                        "name": "Diego Valsesia"
                    },
                    {
                        "name": "Mete Ozay"
                    },
                    {
                        "name": "Enrico Magli"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Magli"
                },
                "author": "Enrico Magli",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v1",
                "updated": "2024-11-26T05:10:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n95-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n95-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Code: https://github.com/NVIDIA/Star-Attention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17089v1",
                "updated": "2024-11-26T04:03:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T04:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation"
                },
                "summary": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) caching is used to\nstore intermediate activations, enabling GPUs to perform only the incremental\ncomputation required for each new token. This approach significantly lowers the\ncomputational overhead for token generation. However, the memory required for\nKV caching grows rapidly, often exceeding the capacity of GPU memory. A\ncost-effective alternative is to offload KV cache to CPU memory, which\nalleviates GPU memory pressure but shifts the bottleneck to the limited\nbandwidth of the PCIe connection between the CPU and GPU. Existing methods\nattempt to address these issues by overlapping GPU computation with I/O or\nemploying CPU-GPU heterogeneous execution, but they are hindered by excessive\ndata movement and dependence on CPU capabilities. In this paper, we introduce\nan efficient CPU-GPU I/O-aware LLM inference method that avoids transferring\nthe entire KV cache from CPU to GPU by recomputing partial KV cache from\nactivations while concurrently transferring the remaining KV cache via PCIe\nbus. This approach overlaps GPU recomputation with data transfer to minimize\nidle GPU time and maximize inference performance. Our method is fully automated\nby integrating a profiler module that utilizes input characteristics and system\nhardware information, a scheduler module to optimize the distribution of\ncomputation and communication workloads, and a runtime module to efficiently\nexecute the derived execution plan. Experimental results show that our method\nachieves up to 35.8% lower latency and 46.2% higher throughput during decoding\ncompared to state-of-the-art approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) caching is used to\nstore intermediate activations, enabling GPUs to perform only the incremental\ncomputation required for each new token. This approach significantly lowers the\ncomputational overhead for token generation. However, the memory required for\nKV caching grows rapidly, often exceeding the capacity of GPU memory. A\ncost-effective alternative is to offload KV cache to CPU memory, which\nalleviates GPU memory pressure but shifts the bottleneck to the limited\nbandwidth of the PCIe connection between the CPU and GPU. Existing methods\nattempt to address these issues by overlapping GPU computation with I/O or\nemploying CPU-GPU heterogeneous execution, but they are hindered by excessive\ndata movement and dependence on CPU capabilities. In this paper, we introduce\nan efficient CPU-GPU I/O-aware LLM inference method that avoids transferring\nthe entire KV cache from CPU to GPU by recomputing partial KV cache from\nactivations while concurrently transferring the remaining KV cache via PCIe\nbus. This approach overlaps GPU recomputation with data transfer to minimize\nidle GPU time and maximize inference performance. Our method is fully automated\nby integrating a profiler module that utilizes input characteristics and system\nhardware information, a scheduler module to optimize the distribution of\ncomputation and communication workloads, and a runtime module to efficiently\nexecute the derived execution plan. Experimental results show that our method\nachieves up to 35.8% lower latency and 46.2% higher throughput during decoding\ncompared to state-of-the-art approaches."
                },
                "authors": [
                    {
                        "name": "Chaoyi Jiang"
                    },
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Murali Annavaram"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavaram"
                },
                "author": "Murali Annavaram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16375v1",
                "updated": "2024-11-25T13:33:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T13:33:41Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "title": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing"
                },
                "summary": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available at\nhttps://github.com/Dawn-LX/CausalCache-VDM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available at\nhttps://github.com/Dawn-LX/CausalCache-VDM"
                },
                "authors": [
                    {
                        "name": "Kaifeng Gao"
                    },
                    {
                        "name": "Jiaxin Shi"
                    },
                    {
                        "name": "Hanwang Zhang"
                    },
                    {
                        "name": "Chunping Wang"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Long Chen"
                    }
                ],
                "author_detail": {
                    "name": "Long Chen"
                },
                "author": "Long Chen",
                "arxiv_comment": "Technical Report. Code is available at\n  https://github.com/Dawn-LX/CausalCache-VDM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19315v2",
                "updated": "2024-11-25T12:14:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    14,
                    33,
                    0,
                    330,
                    0
                ],
                "published": "2024-09-28T11:00:11Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "title": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models"
                },
                "summary": "Transformer networks, driven by self-attention, are central to Large Language\nModels. In generative Transformers, self-attention uses cache memory to store\ntoken projections, avoiding recomputation at each time step. However,\nGPU-stored projections must be loaded into SRAM for each new generation step,\ncausing latency and energy bottlenecks.\n  We present a custom self-attention in-memory computing architecture based on\nemerging charge-based memories called gain cells, which can be efficiently\nwritten to store new tokens during sequence generation and enable parallel\nanalog dot-product computation required for self-attention. However, the analog\ngain cell circuits introduce non-idealities and constraints preventing the\ndirect mapping of pre-trained models. To circumvent this problem, we design an\ninitialization algorithm achieving text processing performance comparable to\nGPT-2 without training from scratch. Our architecture respectively reduces\nattention latency and energy consumption by up to two and five orders of\nmagnitude compared to GPUs, marking a significant step toward ultra-fast,\nlow-power generative Transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer networks, driven by self-attention, are central to Large Language\nModels. In generative Transformers, self-attention uses cache memory to store\ntoken projections, avoiding recomputation at each time step. However,\nGPU-stored projections must be loaded into SRAM for each new generation step,\ncausing latency and energy bottlenecks.\n  We present a custom self-attention in-memory computing architecture based on\nemerging charge-based memories called gain cells, which can be efficiently\nwritten to store new tokens during sequence generation and enable parallel\nanalog dot-product computation required for self-attention. However, the analog\ngain cell circuits introduce non-idealities and constraints preventing the\ndirect mapping of pre-trained models. To circumvent this problem, we design an\ninitialization algorithm achieving text processing performance comparable to\nGPT-2 without training from scratch. Our architecture respectively reduces\nattention latency and energy consumption by up to two and five orders of\nmagnitude compared to GPUs, marking a significant step toward ultra-fast,\nlow-power generative Transformers."
                },
                "authors": [
                    {
                        "name": "Nathan Leroux"
                    },
                    {
                        "name": "Paul-Philipp Manea"
                    },
                    {
                        "name": "Chirag Sudarshan"
                    },
                    {
                        "name": "Jan Finkbeiner"
                    },
                    {
                        "name": "Sebastian Siegel"
                    },
                    {
                        "name": "John Paul Strachan"
                    },
                    {
                        "name": "Emre Neftci"
                    }
                ],
                "author_detail": {
                    "name": "Emre Neftci"
                },
                "author": "Emre Neftci",
                "arxiv_comment": "25 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11469v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11469v2",
                "updated": "2024-11-24T21:57:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    21,
                    57,
                    29,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-18T11:12:57Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    12,
                    57,
                    0,
                    323,
                    0
                ],
                "title": "Deegen: A JIT-Capable VM Generator for Dynamic Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deegen: A JIT-Capable VM Generator for Dynamic Languages"
                },
                "summary": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT."
                },
                "authors": [
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Fredrik Kjolstad"
                    }
                ],
                "author_detail": {
                    "name": "Fredrik Kjolstad"
                },
                "author": "Fredrik Kjolstad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11469v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11469v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17741v1",
                "updated": "2024-11-24T16:20:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    16,
                    20,
                    57,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T16:20:57Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    16,
                    20,
                    57,
                    6,
                    329,
                    0
                ],
                "title": "Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM\n  Inference Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM\n  Inference Environments"
                },
                "summary": "The widespread adoption of LLMs has driven an exponential rise in their\ndeployment, imposing substantial demands on inference clusters. These clusters\nmust handle numerous concurrent queries for different LLM downstream tasks. To\nhandle multi-task settings with vast LLM parameter counts, methods like\nLow-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most\nof the base LLM model across tasks. Hence, they allow concurrent task serving\nwith minimal memory requirements. However, existing LLM serving systems face\ninefficiencies: they overlook workload heterogeneity, impose high link\nbandwidth from frequent adapter loading, and suffer from head-of-line blocking\nin their schedulers. To address these challenges, we present Chameleon, a novel\nLLM serving system optimized for many adapter environments, that relies on two\ncore ideas: adapter caching and adapter-aware scheduling. First, Chameleon\ncaches popular adapters in GPU memory, minimizing the adapter loading times.\nImportantly, it uses the otherwise idle GPU memory, avoiding extra memory\ncosts. Second, Chameleon uses a non-preemptive multi-queue scheduling to\nefficiently account for workload heterogeneity. In this way, Chameleon\nsimultaneously prevents head of line blocking and starvation. We implement\nChameleon on top of a state-of-the-art LLM serving platform and evaluate it\nwith real-world production traces and open-source LLMs. Under high loads,\nChameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively,\nwhile improving throughput by 1.5x compared to state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of LLMs has driven an exponential rise in their\ndeployment, imposing substantial demands on inference clusters. These clusters\nmust handle numerous concurrent queries for different LLM downstream tasks. To\nhandle multi-task settings with vast LLM parameter counts, methods like\nLow-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most\nof the base LLM model across tasks. Hence, they allow concurrent task serving\nwith minimal memory requirements. However, existing LLM serving systems face\ninefficiencies: they overlook workload heterogeneity, impose high link\nbandwidth from frequent adapter loading, and suffer from head-of-line blocking\nin their schedulers. To address these challenges, we present Chameleon, a novel\nLLM serving system optimized for many adapter environments, that relies on two\ncore ideas: adapter caching and adapter-aware scheduling. First, Chameleon\ncaches popular adapters in GPU memory, minimizing the adapter loading times.\nImportantly, it uses the otherwise idle GPU memory, avoiding extra memory\ncosts. Second, Chameleon uses a non-preemptive multi-queue scheduling to\nefficiently account for workload heterogeneity. In this way, Chameleon\nsimultaneously prevents head of line blocking and starvation. We implement\nChameleon on top of a state-of-the-art LLM serving platform and evaluate it\nwith real-world production traces and open-source LLMs. Under high loads,\nChameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively,\nwhile improving throughput by 1.5x compared to state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Nikoleta Iliakopoulou"
                    },
                    {
                        "name": "Jovan Stojkovic"
                    },
                    {
                        "name": "Chloe Alverti"
                    },
                    {
                        "name": "Tianyin Xu"
                    },
                    {
                        "name": "Hubertus Franke"
                    },
                    {
                        "name": "Josep Torrellas"
                    }
                ],
                "author_detail": {
                    "name": "Josep Torrellas"
                },
                "author": "Josep Torrellas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.0; D.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15735v1",
                "updated": "2024-11-24T06:43:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    6,
                    43,
                    38,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T06:43:38Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    6,
                    43,
                    38,
                    6,
                    329,
                    0
                ],
                "title": "Test-time Alignment-Enhanced Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time Alignment-Enhanced Adapter for Vision-Language Models"
                },
                "summary": "Test-time adaptation with pre-trained vision-language models (VLMs) has\nattracted increasing attention for tackling the issue of distribution shift\nduring the test phase. While prior methods have shown effectiveness in\naddressing distribution shift by adjusting classification logits, they are not\noptimal due to keeping text features unchanged. To address this issue, we\nintroduce a new approach called Test-time Alignment-Enhanced Adapter (TAEA),\nwhich trains an adapter with test samples to adjust text features during the\ntest phase. We can enhance the text-to-image alignment prediction by utilizing\nan adapter to adapt text features. Furthermore, we also propose to adopt the\nnegative cache from TDA as enhancement module, which further improves the\nperformance of TAEA. Our approach outperforms the state-of-the-art TTA method\nof pre-trained VLMs by an average of 0.75% on the out-of-distribution benchmark\nand 2.5% on the cross-domain benchmark, with an acceptable training time. Code\nwill be available at https://github.com/BaoshunWq/clip-TAEA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation with pre-trained vision-language models (VLMs) has\nattracted increasing attention for tackling the issue of distribution shift\nduring the test phase. While prior methods have shown effectiveness in\naddressing distribution shift by adjusting classification logits, they are not\noptimal due to keeping text features unchanged. To address this issue, we\nintroduce a new approach called Test-time Alignment-Enhanced Adapter (TAEA),\nwhich trains an adapter with test samples to adjust text features during the\ntest phase. We can enhance the text-to-image alignment prediction by utilizing\nan adapter to adapt text features. Furthermore, we also propose to adopt the\nnegative cache from TDA as enhancement module, which further improves the\nperformance of TAEA. Our approach outperforms the state-of-the-art TTA method\nof pre-trained VLMs by an average of 0.75% on the out-of-distribution benchmark\nand 2.5% on the cross-domain benchmark, with an acceptable training time. Code\nwill be available at https://github.com/BaoshunWq/clip-TAEA."
                },
                "authors": [
                    {
                        "name": "Baoshun Tong"
                    },
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09688v2",
                "updated": "2024-11-23T22:11:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    22,
                    11,
                    42,
                    5,
                    328,
                    0
                ],
                "published": "2024-11-14T18:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed Attention: Accelerating Long Context Length LLM Inference"
                },
                "summary": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "June Paik"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05396v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05396v3",
                "updated": "2024-11-23T10:42:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    10,
                    42,
                    11,
                    5,
                    328,
                    0
                ],
                "published": "2024-02-08T04:16:35Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    4,
                    16,
                    35,
                    3,
                    39,
                    0
                ],
                "title": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph\n  Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph\n  Representation Learning"
                },
                "summary": "Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated\nstate-of-the-art performance in various high-impact applications, including\nfraud detection and content recommendation. Despite the success of TGNNs, they\nare prone to the prevalent noise found in real-world dynamic graphs like\ntime-deprecated links and skewed interaction distribution. The noise causes two\ncritical issues that significantly compromise the accuracy of TGNNs: (1) models\nare supervised by inferior interactions, and (2) noisy input induces high\nvariance in the aggregated messages. However, current TGNN denoising techniques\ndo not consider the diverse and dynamic noise pattern of each node. In\naddition, they also suffer from the excessive mini-batch generation overheads\ncaused by traversing more neighbors. We believe the remedy for fast and\naccurate TGNNs lies in temporal adaptive sampling. In this work, we propose\nTASER, the first adaptive sampling method for TGNNs optimized for accuracy,\nefficiency, and scalability. TASER adapts its mini-batch selection based on\ntraining dynamics and temporal neighbor selection based on the contextual,\nstructural, and temporal properties of past interactions. To alleviate the\nbottleneck in mini-batch generation, TASER implements a pure GPU-based temporal\nneighbor finder and a dedicated GPU feature cache. We evaluate the performance\nof TASER using two state-of-the-art backbone TGNNs. On five popular datasets,\nTASER outperforms the corresponding baselines by an average of 2.3% in Mean\nReciprocal Rank (MRR) while achieving an average of 5.1x speedup in training\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated\nstate-of-the-art performance in various high-impact applications, including\nfraud detection and content recommendation. Despite the success of TGNNs, they\nare prone to the prevalent noise found in real-world dynamic graphs like\ntime-deprecated links and skewed interaction distribution. The noise causes two\ncritical issues that significantly compromise the accuracy of TGNNs: (1) models\nare supervised by inferior interactions, and (2) noisy input induces high\nvariance in the aggregated messages. However, current TGNN denoising techniques\ndo not consider the diverse and dynamic noise pattern of each node. In\naddition, they also suffer from the excessive mini-batch generation overheads\ncaused by traversing more neighbors. We believe the remedy for fast and\naccurate TGNNs lies in temporal adaptive sampling. In this work, we propose\nTASER, the first adaptive sampling method for TGNNs optimized for accuracy,\nefficiency, and scalability. TASER adapts its mini-batch selection based on\ntraining dynamics and temporal neighbor selection based on the contextual,\nstructural, and temporal properties of past interactions. To alleviate the\nbottleneck in mini-batch generation, TASER implements a pure GPU-based temporal\nneighbor finder and a dedicated GPU feature cache. We evaluate the performance\nof TASER using two state-of-the-art backbone TGNNs. On five popular datasets,\nTASER outperforms the corresponding baselines by an average of 2.3% in Mean\nReciprocal Rank (MRR) while achieving an average of 5.1x speedup in training\ntime."
                },
                "authors": [
                    {
                        "name": "Gangda Deng"
                    },
                    {
                        "name": "Hongkuan Zhou"
                    },
                    {
                        "name": "Hanqing Zeng"
                    },
                    {
                        "name": "Yinglong Xia"
                    },
                    {
                        "name": "Christopher Leung"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Rajgopal Kannan"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "arxiv_comment": "IPDPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05396v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05396v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02109v2",
                "updated": "2024-11-23T01:44:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    1,
                    44,
                    0,
                    5,
                    328,
                    0
                ],
                "published": "2024-07-02T09:51:56Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    9,
                    51,
                    56,
                    1,
                    184,
                    0
                ],
                "title": "HRSAM: Efficient Interactive Segmentation in High-Resolution Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HRSAM: Efficient Interactive Segmentation in High-Resolution Images"
                },
                "summary": "The Segment Anything Model (SAM) has advanced interactive segmentation but is\nlimited by the high computational cost on high-resolution images. This requires\ndownsampling to meet GPU constraints, sacrificing the fine-grained details\nneeded for high-precision interactive segmentation. To address SAM's\nlimitations, we focus on visual length extrapolation and propose a lightweight\nmodel named HRSAM. The extrapolation enables HRSAM trained on low resolutions\nto generalize to high resolutions. We begin by finding the link between the\nextrapolation and attention scores, which leads us to base HRSAM on Swin\nattention. We then introduce the Flexible Local Attention (FLA) framework,\nusing CUDA-optimized Efficient Memory Attention to accelerate HRSAM. Within\nFLA, we implement Flash Swin attention, achieving over a 35% speedup compared\nto traditional Swin attention, and propose a KV-only padding mechanism to\nenhance extrapolation. We also develop the Cycle-scan module that uses State\nSpace models to efficiently expand HRSAM's receptive field. We further develop\nthe HRSAM++ within FLA by adding an anchor map, providing multi-scale data\naugmentation for the extrapolation and a larger receptive field at slight\ncomputational cost. Experiments show that, under standard training, HRSAMs\nsurpass the previous SOTA with only 38% of the latency. With SAM-distillation,\nthe extrapolation enables HRSAMs to outperform the teacher model at lower\nlatency. Further finetuning achieves performance significantly exceeding the\nprevious SOTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Segment Anything Model (SAM) has advanced interactive segmentation but is\nlimited by the high computational cost on high-resolution images. This requires\ndownsampling to meet GPU constraints, sacrificing the fine-grained details\nneeded for high-precision interactive segmentation. To address SAM's\nlimitations, we focus on visual length extrapolation and propose a lightweight\nmodel named HRSAM. The extrapolation enables HRSAM trained on low resolutions\nto generalize to high resolutions. We begin by finding the link between the\nextrapolation and attention scores, which leads us to base HRSAM on Swin\nattention. We then introduce the Flexible Local Attention (FLA) framework,\nusing CUDA-optimized Efficient Memory Attention to accelerate HRSAM. Within\nFLA, we implement Flash Swin attention, achieving over a 35% speedup compared\nto traditional Swin attention, and propose a KV-only padding mechanism to\nenhance extrapolation. We also develop the Cycle-scan module that uses State\nSpace models to efficiently expand HRSAM's receptive field. We further develop\nthe HRSAM++ within FLA by adding an anchor map, providing multi-scale data\naugmentation for the extrapolation and a larger receptive field at slight\ncomputational cost. Experiments show that, under standard training, HRSAMs\nsurpass the previous SOTA with only 38% of the latency. With SAM-distillation,\nthe extrapolation enables HRSAMs to outperform the teacher model at lower\nlatency. Further finetuning achieves performance significantly exceeding the\nprevious SOTA."
                },
                "authors": [
                    {
                        "name": "You Huang"
                    },
                    {
                        "name": "Wenbin Lai"
                    },
                    {
                        "name": "Jiayi Ji"
                    },
                    {
                        "name": "Liujuan Cao"
                    },
                    {
                        "name": "Shengchuan Zhang"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15322v1",
                "updated": "2024-11-22T19:30:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    19,
                    30,
                    40,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T19:30:40Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    19,
                    30,
                    40,
                    4,
                    327,
                    0
                ],
                "title": "Deep Learning-Based Automatic Delineation of Liver Domes in kV Triggered\n  Images for Online Breath-hold Reproducibility Verification of Liver\n  Stereotactic Body Radiation Therapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning-Based Automatic Delineation of Liver Domes in kV Triggered\n  Images for Online Breath-hold Reproducibility Verification of Liver\n  Stereotactic Body Radiation Therapy"
                },
                "summary": "Stereotactic Body Radiation Therapy (SBRT) can be a precise, minimally\ninvasive treatment method for liver cancer and liver metastases. However, the\neffectiveness of SBRT relies on the accurate delivery of the dose to the tumor\nwhile sparing healthy tissue. Challenges persist in ensuring breath-hold\nreproducibility, with current methods often requiring manual verification of\nliver dome positions from kV-triggered images. To address this, we propose a\nproof-of-principle study of a deep learning-based pipeline to automatically\ndelineate the liver dome from kV-planar images. From 24 patients who received\nSBRT for liver cancer or metastasis inside liver, 711 KV-triggered images\nacquired for online breath-hold verification were included in the current\nstudy. We developed a pipeline comprising a trained U-Net for automatic liver\ndome region segmentation from the triggered images followed by extraction of\nthe liver dome via thresholding, edge detection, and morphological operations.\nThe performance and generalizability of the pipeline was evaluated using 2-fold\ncross validation. The training of the U-Net model for liver region segmentation\ntook under 30 minutes and the automatic delineation of a liver dome for any\ntriggered image took less than one second. The RMSE and rate of detection for\nFold1 with 366 images was (6.4 +/- 1.6) mm and 91.7%, respectively. For Fold2\nwith 345 images, the RMSE and rate of detection was (7.7 +/- 2.3) mm and 76.3%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stereotactic Body Radiation Therapy (SBRT) can be a precise, minimally\ninvasive treatment method for liver cancer and liver metastases. However, the\neffectiveness of SBRT relies on the accurate delivery of the dose to the tumor\nwhile sparing healthy tissue. Challenges persist in ensuring breath-hold\nreproducibility, with current methods often requiring manual verification of\nliver dome positions from kV-triggered images. To address this, we propose a\nproof-of-principle study of a deep learning-based pipeline to automatically\ndelineate the liver dome from kV-planar images. From 24 patients who received\nSBRT for liver cancer or metastasis inside liver, 711 KV-triggered images\nacquired for online breath-hold verification were included in the current\nstudy. We developed a pipeline comprising a trained U-Net for automatic liver\ndome region segmentation from the triggered images followed by extraction of\nthe liver dome via thresholding, edge detection, and morphological operations.\nThe performance and generalizability of the pipeline was evaluated using 2-fold\ncross validation. The training of the U-Net model for liver region segmentation\ntook under 30 minutes and the automatic delineation of a liver dome for any\ntriggered image took less than one second. The RMSE and rate of detection for\nFold1 with 366 images was (6.4 +/- 1.6) mm and 91.7%, respectively. For Fold2\nwith 345 images, the RMSE and rate of detection was (7.7 +/- 2.3) mm and 76.3%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Sugandima Weragoda"
                    },
                    {
                        "name": "Ping Xia"
                    },
                    {
                        "name": "Kevin Stephans"
                    },
                    {
                        "name": "Neil Woody"
                    },
                    {
                        "name": "Michael Martens"
                    },
                    {
                        "name": "Robert Brown"
                    },
                    {
                        "name": "Bingqi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Bingqi Guo"
                },
                "author": "Bingqi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v1",
                "updated": "2024-11-22T18:06:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v1",
                "updated": "2024-11-22T15:55:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04032v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04032v5",
                "updated": "2024-11-21T05:55:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    55,
                    43,
                    3,
                    326,
                    0
                ],
                "published": "2024-02-06T14:26:22Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    14,
                    26,
                    22,
                    1,
                    37,
                    0
                ],
                "title": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System"
                },
                "summary": "The model size growth of personalized recommendation systems poses new\nchallenges for inference. Weight-sharing algorithms have been proposed for size\nreduction, but they increase memory access. Recent advancements in\nprocessing-in-memory (PIM) enhanced the model throughput by exploiting memory\nparallelism, but such algorithms introduce massive CPU-PIM communication into\nprior PIM systems. We propose ProactivePIM, a PIM system for weight-sharing\nrecommendation system acceleration. ProactivePIM integrates a cache within the\nPIM with a prefetching scheme to leverage a unique locality of the algorithm\nand eliminate communication overhead through a subtable mapping strategy.\nProactivePIM achieves a 4.8x speedup compared to prior works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The model size growth of personalized recommendation systems poses new\nchallenges for inference. Weight-sharing algorithms have been proposed for size\nreduction, but they increase memory access. Recent advancements in\nprocessing-in-memory (PIM) enhanced the model throughput by exploiting memory\nparallelism, but such algorithms introduce massive CPU-PIM communication into\nprior PIM systems. We propose ProactivePIM, a PIM system for weight-sharing\nrecommendation system acceleration. ProactivePIM integrates a cache within the\nPIM with a prefetching scheme to leverage a unique locality of the algorithm\nand eliminate communication overhead through a subtable mapping strategy.\nProactivePIM achieves a 4.8x speedup compared to prior works."
                },
                "authors": [
                    {
                        "name": "Youngsuk Kim"
                    },
                    {
                        "name": "Junghwan Lim"
                    },
                    {
                        "name": "Hyuk-Jae Lee"
                    },
                    {
                        "name": "Chae Eun Rhee"
                    }
                ],
                "author_detail": {
                    "name": "Chae Eun Rhee"
                },
                "author": "Chae Eun Rhee",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04032v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04032v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13854v1",
                "updated": "2024-11-21T05:26:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    26,
                    57,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T05:26:57Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    26,
                    57,
                    3,
                    326,
                    0
                ],
                "title": "Static Reuse Profile Estimation for Array Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Reuse Profile Estimation for Array Applications"
                },
                "summary": "Reuse distance analysis is a widely recognized method for application\ncharacterization that illustrates cache locality. Although there are various\ntechniques to calculate the reuse profile from dynamic memory traces, it is\nboth time and space-consuming due to the requirement to collect dynamic memory\ntraces at runtime. In contrast, static analysis reuse profile estimation is a\npromisingly faster approach since it is calculated at compile time without\nrunning the program or collecting memory traces. This work presents a static\nanalysis technique to estimate the reuse profile of loop-based programs. For an\ninput program, we generate a basic block-level control flow graph and the\nexecution count by analyzing the LLVM IR of the program. We present the memory\naccesses of the application kernel in a compact bracketed format and use a\nrecursive algorithm to predict the reuse distance histogram. We deploy a\nseparate predictor that unrolls the loop(s) for smaller bounds and generates a\ntemporary reuse distance profile for those small cases. Using these smaller\nprofiles, the reuse profile is extrapolated for the actual loop bound(s). We\nuse this reuse profile to predict the cache hit rate. Results show that our\nmodel can predict cache hit rates with an average accuracy of 95% relative to\nthe dynamic reuse profile methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reuse distance analysis is a widely recognized method for application\ncharacterization that illustrates cache locality. Although there are various\ntechniques to calculate the reuse profile from dynamic memory traces, it is\nboth time and space-consuming due to the requirement to collect dynamic memory\ntraces at runtime. In contrast, static analysis reuse profile estimation is a\npromisingly faster approach since it is calculated at compile time without\nrunning the program or collecting memory traces. This work presents a static\nanalysis technique to estimate the reuse profile of loop-based programs. For an\ninput program, we generate a basic block-level control flow graph and the\nexecution count by analyzing the LLVM IR of the program. We present the memory\naccesses of the application kernel in a compact bracketed format and use a\nrecursive algorithm to predict the reuse distance histogram. We deploy a\nseparate predictor that unrolls the loop(s) for smaller bounds and generates a\ntemporary reuse distance profile for those small cases. Using these smaller\nprofiles, the reuse profile is extrapolated for the actual loop bound(s). We\nuse this reuse profile to predict the cache hit rate. Results show that our\nmodel can predict cache hit rates with an average accuracy of 95% relative to\nthe dynamic reuse profile methods."
                },
                "authors": [
                    {
                        "name": "Abdur Razzak"
                    },
                    {
                        "name": "Atanu Barai"
                    },
                    {
                        "name": "Nandakishore Santhi"
                    },
                    {
                        "name": "Abdel-Hameed A. Badawy"
                    }
                ],
                "author_detail": {
                    "name": "Abdel-Hameed A. Badawy"
                },
                "author": "Abdel-Hameed A. Badawy",
                "arxiv_comment": "Accepted in The International Symposium on Memory Systems (MEMSYS\n  24), September 30 to October 03, 2024, Washington, DC, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.02243v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.02243v3",
                "updated": "2024-11-21T04:12:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    4,
                    12,
                    53,
                    3,
                    326,
                    0
                ],
                "published": "2023-06-04T03:06:37Z",
                "published_parsed": [
                    2023,
                    6,
                    4,
                    3,
                    6,
                    37,
                    6,
                    155,
                    0
                ],
                "title": "Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification"
                },
                "summary": "The Contrastive Language-Image Pretraining (CLIP) model has been widely used\nin various downstream vision tasks. The few-shot learning paradigm has been\nwidely adopted to augment its capacity for these tasks. However, current\nparadigms may struggle with fine-grained classification, such as satellite\nimage recognition, due to widening domain gaps. To address this limitation, we\npropose retrieval-enhanced visual prompt learning (RePrompt), which introduces\nretrieval mechanisms to cache and reuse the knowledge of downstream tasks.\nRePrompt constructs a retrieval database from either training examples or\nexternal data if available, and uses a retrieval mechanism to enhance multiple\nstages of a simple prompt learning baseline, thus narrowing the domain gap.\nDuring inference, our enhanced model can reference similar samples brought by\nretrieval to make more accurate predictions. A detailed analysis reveals that\nretrieval helps to improve the distribution of late features, thus, improving\ngeneralization for downstream tasks. Reprompt attains state-of-the-art\nperformance on a wide range of vision datasets, including 11 image datasets, 3\nvideo datasets, 1 multi-view dataset, and 4 domain generalization benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Contrastive Language-Image Pretraining (CLIP) model has been widely used\nin various downstream vision tasks. The few-shot learning paradigm has been\nwidely adopted to augment its capacity for these tasks. However, current\nparadigms may struggle with fine-grained classification, such as satellite\nimage recognition, due to widening domain gaps. To address this limitation, we\npropose retrieval-enhanced visual prompt learning (RePrompt), which introduces\nretrieval mechanisms to cache and reuse the knowledge of downstream tasks.\nRePrompt constructs a retrieval database from either training examples or\nexternal data if available, and uses a retrieval mechanism to enhance multiple\nstages of a simple prompt learning baseline, thus narrowing the domain gap.\nDuring inference, our enhanced model can reference similar samples brought by\nretrieval to make more accurate predictions. A detailed analysis reveals that\nretrieval helps to improve the distribution of late features, thus, improving\ngeneralization for downstream tasks. Reprompt attains state-of-the-art\nperformance on a wide range of vision datasets, including 11 image datasets, 3\nvideo datasets, 1 multi-view dataset, and 4 domain generalization benchmarks."
                },
                "authors": [
                    {
                        "name": "Jintao Rong"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Linlin Ou"
                    },
                    {
                        "name": "Tianxiao Chen"
                    },
                    {
                        "name": "Xinyi Yu"
                    },
                    {
                        "name": "Yifan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Liu"
                },
                "author": "Yifan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.02243v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.02243v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13820v1",
                "updated": "2024-11-21T03:52:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T03:52:41Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "title": "InstCache: A Predictive Cache for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstCache: A Predictive Cache for LLM Serving"
                },
                "summary": "Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB."
                },
                "authors": [
                    {
                        "name": "Longwei Zou"
                    },
                    {
                        "name": "Tingfeng Liu"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Jiangang Kong"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22649v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22649v2",
                "updated": "2024-11-21T03:34:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    34,
                    44,
                    3,
                    326,
                    0
                ],
                "published": "2024-10-30T02:36:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting"
                },
                "summary": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs. Our code is available at\nhttps://github.com/Leopold2333/WaveRoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs. Our code is available at\nhttps://github.com/Leopold2333/WaveRoRA."
                },
                "authors": [
                    {
                        "name": "Aobo Liang"
                    },
                    {
                        "name": "Yan Sun"
                    },
                    {
                        "name": "Nadra Guizani"
                    }
                ],
                "author_detail": {
                    "name": "Nadra Guizani"
                },
                "author": "Nadra Guizani",
                "arxiv_comment": "Model architecture changed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22649v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22649v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13786v1",
                "updated": "2024-11-21T02:15:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    15,
                    52,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T02:15:52Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    15,
                    52,
                    3,
                    326,
                    0
                ],
                "title": "Adaptable Embeddings Network (AEN)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptable Embeddings Network (AEN)"
                },
                "summary": "Modern day Language Models see extensive use in text classification, yet this\ncomes at significant computational cost. Compute-effective classification\nmodels are needed for low-resource environments, most notably on edge devices.\nWe introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder\narchitecture using Kernel Density Estimation (KDE). This architecture allows\nfor runtime adaptation of classification criteria without retraining and is\nnon-autoregressive. Through thorough synthetic data experimentation, we\ndemonstrate our model outputs comparable and in certain cases superior results\nto that of autoregressive models an order of magnitude larger than AEN's size.\nThe architecture's ability to preprocess and cache condition embeddings makes\nit ideal for edge computing applications and real-time monitoring systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern day Language Models see extensive use in text classification, yet this\ncomes at significant computational cost. Compute-effective classification\nmodels are needed for low-resource environments, most notably on edge devices.\nWe introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder\narchitecture using Kernel Density Estimation (KDE). This architecture allows\nfor runtime adaptation of classification criteria without retraining and is\nnon-autoregressive. Through thorough synthetic data experimentation, we\ndemonstrate our model outputs comparable and in certain cases superior results\nto that of autoregressive models an order of magnitude larger than AEN's size.\nThe architecture's ability to preprocess and cache condition embeddings makes\nit ideal for edge computing applications and real-time monitoring systems."
                },
                "authors": [
                    {
                        "name": "Stan Loosmore"
                    },
                    {
                        "name": "Alexander Titus"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Titus"
                },
                "author": "Alexander Titus",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13676v1",
                "updated": "2024-11-20T19:51:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    51,
                    25,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T19:51:25Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    51,
                    25,
                    2,
                    325,
                    0
                ],
                "title": "Hymba: A Hybrid-head Architecture for Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hymba: A Hybrid-head Architecture for Small Language Models"
                },
                "summary": "We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput."
                },
                "authors": [
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Wonmin Byeon"
                    },
                    {
                        "name": "Zijia Chen"
                    },
                    {
                        "name": "Ameya Sunil Mahabaleshwarkar"
                    },
                    {
                        "name": "Shih-Yang Liu"
                    },
                    {
                        "name": "Matthijs Van Keirsbilck"
                    },
                    {
                        "name": "Min-Hung Chen"
                    },
                    {
                        "name": "Yoshi Suhara"
                    },
                    {
                        "name": "Yingyan Lin"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "arxiv_comment": "20 pages, models are available on huggingface",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17720v1",
                "updated": "2024-11-20T19:44:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    44,
                    26,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T19:44:26Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    44,
                    26,
                    2,
                    325,
                    0
                ],
                "title": "MAS-Attention: Memory-Aware Stream Processing for Attention Acceleration\n  on Resource-Constrained Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAS-Attention: Memory-Aware Stream Processing for Attention Acceleration\n  on Resource-Constrained Edge Devices"
                },
                "summary": "The advent of foundation models have revolutionized various fields, enabling\nunprecedented task accuracy and flexibility in computational linguistics,\ncomputer vision and other domains. Attention mechanism has become an essential\ncomponent of foundation models, due to their superb capability of capturing\ncorrelations in a sequence. However, attention results in quadratic complexity\nin memory and compute as the context length grows. Although many fusion-based\nexact attention acceleration algorithms have been developed for\ndatacenter-grade GPUs and accelerators leveraging multi-core parallelism and\ndata locality, yet it remains a significant challenge to accelerate attention\non resource-constrained edge neural accelerators with limited compute units and\nstringent on-chip caches. In this paper, we propose a scheme for exact\nattention inference acceleration on memory-constrained edge accelerators, by\nparallelizing the utilization of heterogeneous compute units, i.e., vector\nprocessing units and matrix processing units. Our method involves scheduling\nworkloads onto these different compute units in a multi-tiered tiling scheme to\nprocess tiled vector workloads and matrix workloads in attention as two\nstreams, respecting the workload dependencies. We search for tiling factors to\nmaximize the parallelization of both compute units while considering I/O\noverhead, and propose a proactive cache overwrite strategy to avoid undesirable\ncache spills in reality. Extensive results based on open-sourced simulation\nframeworks show up to 2.75x speedup and 54% reduction in energy consumption as\ncompared to the state-of-the-art attention fusion method (FLAT) in the edge\ncomputing scenario. Further experiments on a real-world edge neural processing\nunit demonstrate speedup of up to 1.76x for attention as compared to FLAT,\nwithout affecting model output accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of foundation models have revolutionized various fields, enabling\nunprecedented task accuracy and flexibility in computational linguistics,\ncomputer vision and other domains. Attention mechanism has become an essential\ncomponent of foundation models, due to their superb capability of capturing\ncorrelations in a sequence. However, attention results in quadratic complexity\nin memory and compute as the context length grows. Although many fusion-based\nexact attention acceleration algorithms have been developed for\ndatacenter-grade GPUs and accelerators leveraging multi-core parallelism and\ndata locality, yet it remains a significant challenge to accelerate attention\non resource-constrained edge neural accelerators with limited compute units and\nstringent on-chip caches. In this paper, we propose a scheme for exact\nattention inference acceleration on memory-constrained edge accelerators, by\nparallelizing the utilization of heterogeneous compute units, i.e., vector\nprocessing units and matrix processing units. Our method involves scheduling\nworkloads onto these different compute units in a multi-tiered tiling scheme to\nprocess tiled vector workloads and matrix workloads in attention as two\nstreams, respecting the workload dependencies. We search for tiling factors to\nmaximize the parallelization of both compute units while considering I/O\noverhead, and propose a proactive cache overwrite strategy to avoid undesirable\ncache spills in reality. Extensive results based on open-sourced simulation\nframeworks show up to 2.75x speedup and 54% reduction in energy consumption as\ncompared to the state-of-the-art attention fusion method (FLAT) in the edge\ncomputing scenario. Further experiments on a real-world edge neural processing\nunit demonstrate speedup of up to 1.76x for attention as compared to FLAT,\nwithout affecting model output accuracy."
                },
                "authors": [
                    {
                        "name": "Mohammadali Shakerdargah"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Chao Gao"
                    },
                    {
                        "name": "Di Niu"
                    }
                ],
                "author_detail": {
                    "name": "Di Niu"
                },
                "author": "Di Niu",
                "arxiv_comment": "10 pages, 6 figures, under review for MLSys 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; I.2.7; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13532v1",
                "updated": "2024-11-20T18:31:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    31,
                    39,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T18:31:39Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    31,
                    39,
                    2,
                    325,
                    0
                ],
                "title": "A Distributed-memory Tridiagonal Solver Based on a Specialised Data\n  Structure Optimised for CPU and GPU Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Distributed-memory Tridiagonal Solver Based on a Specialised Data\n  Structure Optimised for CPU and GPU Architectures"
                },
                "summary": "Various numerical methods used for solving partial differential equations\n(PDE) result in tridiagonal systems. Solving tridiagonal systems on\ndistributed-memory environments is not straightforward, and often requires\nsignificant amount of communication. In this article, we present a novel\ndistributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a\nspecialised data structure. DistD2-TDS algorithm takes advantage of the\ndiagonal dominance in tridiagonal systems to reduce the communications in\ndistributed-memory environments. The underlying data structure plays a crucial\nrole for the performance of the algorithm. First, the data structure improves\ndata localities and makes it possible to minimise data movements via cache\nblocking and kernel fusion strategies. Second, data continuity enables a\ncontiguous data access pattern and results in efficient utilisation of the\navailable memory bandwidth. Finally, the data layout supports vectorisation on\nCPUs and thread level parallelisation on GPUs for improved performance. In\norder to demonstrate the robustness of the algorithm, we implemented and\nbenchmarked the algorithm on CPUs and GPUs. We investigated the single rank\nperformance and compared against existing algorithms. Furthermore, we analysed\nthe strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to\n8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the\nalgorithm by using compact finite difference schemes to solve a 3D non-linear\nPDE. The results demonstrate that DistD2 algorithm can sustain around 66% of\nthe theoretical peak bandwidth at scale on CPU and GPU based supercomputers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various numerical methods used for solving partial differential equations\n(PDE) result in tridiagonal systems. Solving tridiagonal systems on\ndistributed-memory environments is not straightforward, and often requires\nsignificant amount of communication. In this article, we present a novel\ndistributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a\nspecialised data structure. DistD2-TDS algorithm takes advantage of the\ndiagonal dominance in tridiagonal systems to reduce the communications in\ndistributed-memory environments. The underlying data structure plays a crucial\nrole for the performance of the algorithm. First, the data structure improves\ndata localities and makes it possible to minimise data movements via cache\nblocking and kernel fusion strategies. Second, data continuity enables a\ncontiguous data access pattern and results in efficient utilisation of the\navailable memory bandwidth. Finally, the data layout supports vectorisation on\nCPUs and thread level parallelisation on GPUs for improved performance. In\norder to demonstrate the robustness of the algorithm, we implemented and\nbenchmarked the algorithm on CPUs and GPUs. We investigated the single rank\nperformance and compared against existing algorithms. Furthermore, we analysed\nthe strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to\n8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the\nalgorithm by using compact finite difference schemes to solve a 3D non-linear\nPDE. The results demonstrate that DistD2 algorithm can sustain around 66% of\nthe theoretical peak bandwidth at scale on CPU and GPU based supercomputers."
                },
                "authors": [
                    {
                        "name": "Semih Akkurt"
                    },
                    {
                        "name": "Sébastien Lemaire"
                    },
                    {
                        "name": "Paul Bartholomew"
                    },
                    {
                        "name": "Sylvain Laizet"
                    }
                ],
                "author_detail": {
                    "name": "Sylvain Laizet"
                },
                "author": "Sylvain Laizet",
                "arxiv_comment": "42 pages, 13 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13373v1",
                "updated": "2024-11-20T14:52:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T14:52:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment"
                },
                "summary": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Plasma\nPhysics, was based on a source of positive hydrogen ions, accelerated to 50 keV\nand for an equivalent neutral beam current of about 5 A at the source. The beam\ncould be modulated and the maximum overall duration was 50 ms. With the upgrade\nof RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to\nsolve several plant faults and improve the overall reliability of the system.\nThe 50 kV power supply is being improved, as well as the power supplies in the\nhigh voltage deck and its insulation transformer. The control system,\noriginally based on CAMAC technology, was redesigned to be fully replaced. This\ncontribution reviews the technical criticalities emerged in the DNBI check-up\nand the new solutions adopted to make the DNBI operative and more reliable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Plasma\nPhysics, was based on a source of positive hydrogen ions, accelerated to 50 keV\nand for an equivalent neutral beam current of about 5 A at the source. The beam\ncould be modulated and the maximum overall duration was 50 ms. With the upgrade\nof RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to\nsolve several plant faults and improve the overall reliability of the system.\nThe 50 kV power supply is being improved, as well as the power supplies in the\nhigh voltage deck and its insulation transformer. The control system,\noriginally based on CAMAC technology, was redesigned to be fully replaced. This\ncontribution reviews the technical criticalities emerged in the DNBI check-up\nand the new solutions adopted to make the DNBI operative and more reliable."
                },
                "authors": [
                    {
                        "name": "Marco Barbisan"
                    },
                    {
                        "name": "Marco Boldrin"
                    },
                    {
                        "name": "Luca Cinnirella"
                    },
                    {
                        "name": "Bruno Laterza"
                    },
                    {
                        "name": "Alberto Maistrello"
                    },
                    {
                        "name": "Lionello Marrelli"
                    },
                    {
                        "name": "Federico Molon"
                    },
                    {
                        "name": "Simone Peruzzo"
                    },
                    {
                        "name": "Cesare Taliercio"
                    },
                    {
                        "name": "Marco Valisa"
                    },
                    {
                        "name": "Enrico Zampiva"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Zampiva"
                },
                "author": "Enrico Zampiva",
                "arxiv_comment": "6 pages (excl. highlights), 8 figures. Contribution to the 33rd\n  Symposium on Fusion Technology (SOFT), 22-27 September 2024. This is a\n  preprint for the \"Fusion Engineering and Design\" journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v4",
                "updated": "2024-11-20T02:04:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    4,
                    10,
                    2,
                    325,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "Published on the First Conference on Language Modeling (COLM 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17918v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17918v3",
                "updated": "2024-11-19T18:24:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    24,
                    3,
                    1,
                    324,
                    0
                ],
                "published": "2024-06-25T20:00:32Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    20,
                    0,
                    32,
                    1,
                    177,
                    0
                ],
                "title": "GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and\n  Retrieval"
                },
                "summary": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17918v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17918v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12430v1",
                "updated": "2024-11-19T11:40:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    40,
                    56,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T11:40:56Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    40,
                    56,
                    1,
                    324,
                    0
                ],
                "title": "An Eulerian approach to regularized JKO scheme with low-rank tensor\n  decompositions for Bayesian inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Eulerian approach to regularized JKO scheme with low-rank tensor\n  decompositions for Bayesian inversion"
                },
                "summary": "The possibility of using the Eulerian discretization for the problem of\nmodelling high-dimensional distributions and sampling, is studied. The problem\nis posed as a minimization problem over the space of probability measures with\nrespect to the Wasserstein distance and solved with entropy-regularized JKO\nscheme. Each proximal step can be formulated as a fixed-point equation and\nsolved with accelerated methods, such as Anderson's. The usage of low-rank\nTensor Train format allows to overcome the \\emph{curse of dimensionality}, i.e.\nthe exponential growth of degrees of freedom with dimension, inherent to\nEulerian approaches. The resulting method requires only pointwise computations\nof the unnormalized posterior and is, in particular, gradient-free. Fixed\nEulerian grid allows to employ a caching strategy, significally reducing the\nexpensive evaluations of the posterior. When the Eulerian model of the target\ndistribution is fitted, the passage back to the Lagrangian perspective can also\nbe made, allowing to approximately sample from it. We test our method both for\nsynthetic target distributions and particular Bayesian inverse problems and\nreport comparable or better performance than the baseline Metropolis-Hastings\nMCMC with same amount of resources. Finally, the fitted model can be modified\nto facilitate the solution of certain associated problems, which we demonstrate\nby fitting an importance distribution for a particular quantity of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of using the Eulerian discretization for the problem of\nmodelling high-dimensional distributions and sampling, is studied. The problem\nis posed as a minimization problem over the space of probability measures with\nrespect to the Wasserstein distance and solved with entropy-regularized JKO\nscheme. Each proximal step can be formulated as a fixed-point equation and\nsolved with accelerated methods, such as Anderson's. The usage of low-rank\nTensor Train format allows to overcome the \\emph{curse of dimensionality}, i.e.\nthe exponential growth of degrees of freedom with dimension, inherent to\nEulerian approaches. The resulting method requires only pointwise computations\nof the unnormalized posterior and is, in particular, gradient-free. Fixed\nEulerian grid allows to employ a caching strategy, significally reducing the\nexpensive evaluations of the posterior. When the Eulerian model of the target\ndistribution is fitted, the passage back to the Lagrangian perspective can also\nbe made, allowing to approximately sample from it. We test our method both for\nsynthetic target distributions and particular Bayesian inverse problems and\nreport comparable or better performance than the baseline Metropolis-Hastings\nMCMC with same amount of resources. Finally, the fitted model can be modified\nto facilitate the solution of certain associated problems, which we demonstrate\nby fitting an importance distribution for a particular quantity of interest."
                },
                "authors": [
                    {
                        "name": "Vitalii Aksenov"
                    },
                    {
                        "name": "Martin Eigel"
                    }
                ],
                "author_detail": {
                    "name": "Martin Eigel"
                },
                "author": "Martin Eigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "46E27, 49Q22, 62F15, 68W25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12161v1",
                "updated": "2024-11-19T01:55:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    1,
                    55,
                    26,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T01:55:26Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    1,
                    55,
                    26,
                    1,
                    324,
                    0
                ],
                "title": "Adaptive Cache Management for Complex Storage Systems Using\n  CNN-LSTM-Based Spatiotemporal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cache Management for Complex Storage Systems Using\n  CNN-LSTM-Based Spatiotemporal Prediction"
                },
                "summary": "This paper proposes an intelligent cache management strategy based on\nCNN-LSTM to improve the performance and cache hit rate of storage systems.\nThrough comparative experiments with traditional algorithms (such as LRU and\nLFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the\nresults show that the CNN-LSTM model has significant advantages in cache demand\nprediction. The MSE and MAE values of this model are significantly reduced,\nproving its effectiveness under complex data access patterns. This study not\nonly verifies the potential of deep learning technology in storage system\noptimization, but also provides direction and reference for further optimizing\nand improving cache management strategies. This intelligent cache management\nstrategy performs well in complex storage environments. By combining the\nspatial feature extraction capabilities of convolutional neural networks and\nthe time series modeling capabilities of long short-term memory networks, the\nCNN-LSTM model can more accurately predict cache needs, thereby Dynamically\noptimize cache allocation to improve system response speed and resource\nutilization. This research provides theoretical support and practical reference\nfor cache optimization under large-scale data access modes, and is of great\nsignificance to improving the performance of future storage systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes an intelligent cache management strategy based on\nCNN-LSTM to improve the performance and cache hit rate of storage systems.\nThrough comparative experiments with traditional algorithms (such as LRU and\nLFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the\nresults show that the CNN-LSTM model has significant advantages in cache demand\nprediction. The MSE and MAE values of this model are significantly reduced,\nproving its effectiveness under complex data access patterns. This study not\nonly verifies the potential of deep learning technology in storage system\noptimization, but also provides direction and reference for further optimizing\nand improving cache management strategies. This intelligent cache management\nstrategy performs well in complex storage environments. By combining the\nspatial feature extraction capabilities of convolutional neural networks and\nthe time series modeling capabilities of long short-term memory networks, the\nCNN-LSTM model can more accurately predict cache needs, thereby Dynamically\noptimize cache allocation to improve system response speed and resource\nutilization. This research provides theoretical support and practical reference\nfor cache optimization under large-scale data access modes, and is of great\nsignificance to improving the performance of future storage systems."
                },
                "authors": [
                    {
                        "name": "Xiaoye Wang"
                    },
                    {
                        "name": "Xuan Li"
                    },
                    {
                        "name": "Linji Wang"
                    },
                    {
                        "name": "Tingyi Ruan"
                    },
                    {
                        "name": "Pochun Li"
                    }
                ],
                "author_detail": {
                    "name": "Pochun Li"
                },
                "author": "Pochun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11843v1",
                "updated": "2024-11-18T18:59:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:59:15Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "title": "Bi-Mamba: Towards Accurate 1-Bit State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-Mamba: Towards Accurate 1-Bit State Space Models"
                },
                "summary": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs."
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11739v1",
                "updated": "2024-11-18T17:08:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:08:35Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "title": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou"
                },
                "summary": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels."
                },
                "authors": [
                    {
                        "name": "Xinchen Luo"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Tianyu Sun"
                    },
                    {
                        "name": "Jinkai Yu"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Hezheng Lin"
                    },
                    {
                        "name": "Yichen Zheng"
                    },
                    {
                        "name": "Shiyao Wang"
                    },
                    {
                        "name": "Qigen Hu"
                    },
                    {
                        "name": "Changqing Qiu"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Zhiheng Yan"
                    },
                    {
                        "name": "Jingming Zhang"
                    },
                    {
                        "name": "Simin Zhang"
                    },
                    {
                        "name": "Mingxing Wen"
                    },
                    {
                        "name": "Zhaojie Liu"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11300v1",
                "updated": "2024-11-18T05:50:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    50,
                    58,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T05:50:58Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    50,
                    58,
                    0,
                    323,
                    0
                ],
                "title": "Accelerating spherical K-means clustering for large-scale sparse\n  document data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating spherical K-means clustering for large-scale sparse\n  document data"
                },
                "summary": "This paper presents an accelerated spherical K-means clustering algorithm for\nlarge-scale and high-dimensional sparse document data sets. We design an\nalgorithm working in an architecture-friendly manner (AFM), which is a\nprocedure of suppressing performance-degradation factors such as the numbers of\ninstructions, branch mispredictions, and cache misses in CPUs of a modern\ncomputer system. For the AFM operation, we leverage unique universal\ncharacteristics (UCs) of a data-object and a cluster's mean set, which are\nskewed distributions on data relationships such as Zipf's law and a\nfeature-value concentration phenomenon. The UCs indicate that the most part of\nthe number of multiplications for similarity calculations is executed regarding\nterms with high document frequencies (df) and the most part of a similarity\nbetween an object- and a mean-feature vector is obtained by the multiplications\nregarding a few high mean-feature values. Our proposed algorithm applies an\ninverted-index data structure to a mean set, extracts the specific region with\nhigh-df terms and high mean-feature values in the mean-inverted index by newly\nintroduced two structural parameters, and exploits the index divided into three\nparts for efficient pruning. The algorithm determines the two structural\nparameters by minimizing the approximate number of multiplications related to\nthat of instructions, reduces the branch mispredictions by sharing the index\nstructure including the two parameters with all the objects, and suppressing\nthe cache misses by keeping in the caches the frequently used data in the\nforegoing specific region, resulting in working in the AFM. We experimentally\ndemonstrate that our algorithm efficiently achieves superior speed performance\nin large-scale documents compared with algorithms using the state-of-the-art\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an accelerated spherical K-means clustering algorithm for\nlarge-scale and high-dimensional sparse document data sets. We design an\nalgorithm working in an architecture-friendly manner (AFM), which is a\nprocedure of suppressing performance-degradation factors such as the numbers of\ninstructions, branch mispredictions, and cache misses in CPUs of a modern\ncomputer system. For the AFM operation, we leverage unique universal\ncharacteristics (UCs) of a data-object and a cluster's mean set, which are\nskewed distributions on data relationships such as Zipf's law and a\nfeature-value concentration phenomenon. The UCs indicate that the most part of\nthe number of multiplications for similarity calculations is executed regarding\nterms with high document frequencies (df) and the most part of a similarity\nbetween an object- and a mean-feature vector is obtained by the multiplications\nregarding a few high mean-feature values. Our proposed algorithm applies an\ninverted-index data structure to a mean set, extracts the specific region with\nhigh-df terms and high mean-feature values in the mean-inverted index by newly\nintroduced two structural parameters, and exploits the index divided into three\nparts for efficient pruning. The algorithm determines the two structural\nparameters by minimizing the approximate number of multiplications related to\nthat of instructions, reduces the branch mispredictions by sharing the index\nstructure including the two parameters with all the objects, and suppressing\nthe cache misses by keeping in the caches the frequently used data in the\nforegoing specific region, resulting in working in the AFM. We experimentally\ndemonstrate that our algorithm efficiently achieves superior speed performance\nin large-scale documents compared with algorithms using the state-of-the-art\ntechniques."
                },
                "authors": [
                    {
                        "name": "Kazuo Aoyama"
                    },
                    {
                        "name": "Kazumi Saito"
                    }
                ],
                "author_detail": {
                    "name": "Kazumi Saito"
                },
                "author": "Kazumi Saito",
                "arxiv_comment": "28 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13588v1",
                "updated": "2024-11-18T02:49:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    49,
                    23,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T02:49:23Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    49,
                    23,
                    0,
                    323,
                    0
                ],
                "title": "Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic\n  Study"
                },
                "summary": "The increased model capacity of Diffusion Transformers (DiTs) and the demand\nfor generating higher resolutions of images and videos have led to a\nsignificant rise in inference latency, impacting real-time performance\nadversely. While prior research has highlighted the presence of high similarity\nin activation values between adjacent diffusion steps (referred to as\nredundancy) and proposed various caching mechanisms to mitigate computational\noverhead, the exploration of redundancy in existing literature remains limited,\nwith findings often not generalizable across different DiT models. This study\naims to address this gap by conducting a comprehensive investigation into\nredundancy across a broad spectrum of mainstream DiT models. Our experimental\nanalysis reveals substantial variations in the distribution of redundancy\nacross diffusion steps among different DiT models. Interestingly, within a\nsingle model, the redundancy distribution remains stable regardless of\nvariations in input prompts, step counts, or scheduling strategies. Given the\nlack of a consistent pattern across diverse models, caching strategies designed\nfor a specific group of models may not easily transfer to others. To overcome\nthis challenge, we introduce a tool for analyzing the redundancy of individual\nmodels, enabling subsequent research to develop tailored caching strategies for\nspecific model architectures. The project is publicly available at\nhttps://github.com/xdit-project/DiTCacheAnalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increased model capacity of Diffusion Transformers (DiTs) and the demand\nfor generating higher resolutions of images and videos have led to a\nsignificant rise in inference latency, impacting real-time performance\nadversely. While prior research has highlighted the presence of high similarity\nin activation values between adjacent diffusion steps (referred to as\nredundancy) and proposed various caching mechanisms to mitigate computational\noverhead, the exploration of redundancy in existing literature remains limited,\nwith findings often not generalizable across different DiT models. This study\naims to address this gap by conducting a comprehensive investigation into\nredundancy across a broad spectrum of mainstream DiT models. Our experimental\nanalysis reveals substantial variations in the distribution of redundancy\nacross diffusion steps among different DiT models. Interestingly, within a\nsingle model, the redundancy distribution remains stable regardless of\nvariations in input prompts, step counts, or scheduling strategies. Given the\nlack of a consistent pattern across diverse models, caching strategies designed\nfor a specific group of models may not easily transfer to others. To overcome\nthis challenge, we introduce a tool for analyzing the redundancy of individual\nmodels, enabling subsequent research to develop tailored caching strategies for\nspecific model architectures. The project is publicly available at\nhttps://github.com/xdit-project/DiTCacheAnalysis."
                },
                "authors": [
                    {
                        "name": "Xibo Sun"
                    },
                    {
                        "name": "Jiarui Fang"
                    },
                    {
                        "name": "Aoyu Li"
                    },
                    {
                        "name": "Jinzhe Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jinzhe Pan"
                },
                "author": "Jinzhe Pan",
                "arxiv_comment": "9 pages including reference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06392v2",
                "updated": "2024-11-18T02:10:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    10,
                    28,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-10T08:31:18Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    8,
                    31,
                    18,
                    6,
                    315,
                    0
                ],
                "title": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR"
                },
                "summary": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Pengxi Liu"
                    },
                    {
                        "name": "Zhixin Zhang"
                    },
                    {
                        "name": "Hongfu Li"
                    },
                    {
                        "name": "Xiaojian Luo"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11091v1",
                "updated": "2024-11-17T14:47:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-17T14:47:15Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "title": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines"
                },
                "summary": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains."
                },
                "authors": [
                    {
                        "name": "Edward Bortnikov"
                    },
                    {
                        "name": "Michael Azran"
                    },
                    {
                        "name": "Asa Bornstein"
                    },
                    {
                        "name": "Shmuel Dashevsky"
                    },
                    {
                        "name": "Dennis Huang"
                    },
                    {
                        "name": "Omer Kepten"
                    },
                    {
                        "name": "Michael Pan"
                    },
                    {
                        "name": "Gali Sheffi"
                    },
                    {
                        "name": "Moshe Twitto"
                    },
                    {
                        "name": "Tamar Weiss Orzech"
                    },
                    {
                        "name": "Idit Keidar"
                    },
                    {
                        "name": "Guy Gueta"
                    },
                    {
                        "name": "Roey Maor"
                    },
                    {
                        "name": "Niv Dayan"
                    }
                ],
                "author_detail": {
                    "name": "Niv Dayan"
                },
                "author": "Niv Dayan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v3",
                "updated": "2024-11-17T12:56:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    12,
                    56,
                    16,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10883v1",
                "updated": "2024-11-16T20:40:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T20:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "title": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs"
                },
                "summary": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack."
                },
                "authors": [
                    {
                        "name": "Cheng Gu"
                    },
                    {
                        "name": "Yicheng Zhang"
                    },
                    {
                        "name": "Nael Abu-Ghazaleh"
                    }
                ],
                "author_detail": {
                    "name": "Nael Abu-Ghazaleh"
                },
                "author": "Nael Abu-Ghazaleh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13112v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13112v3",
                "updated": "2024-11-16T20:39:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    39,
                    46,
                    5,
                    321,
                    0
                ],
                "published": "2024-03-19T19:27:23Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    19,
                    27,
                    23,
                    1,
                    79,
                    0
                ],
                "title": "Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks"
                },
                "summary": "Transformer-based NLP models are powerful but have high computational costs\nthat limit deployment. Finetuned encoder-decoder models are popular in\nspecialized domains and can outperform larger more generalized decoder-only\nmodels, such as GPT-4. We introduce a new configuration for encoder-decoder\nmodels that improves efficiency on structured output and decomposable tasks\nwhere multiple outputs are required for a single shared input. Our method,\nprompt-in-decoder (PiD), encodes the input once and decodes the output in\nparallel, boosting both training and inference efficiency by avoiding duplicate\ninput encoding and increasing the operational intensity (ratio of numbers of\narithmetic operation to memory access) of decoding process by sharing the input\nkey-value cache. We achieve computation reduction that roughly scales with the\nnumber of subtasks, gaining up to 4.6x speed-up over state-of-the-art models\nfor dialogue state tracking, summarization, and question-answering tasks, with\ncomparable or better performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based NLP models are powerful but have high computational costs\nthat limit deployment. Finetuned encoder-decoder models are popular in\nspecialized domains and can outperform larger more generalized decoder-only\nmodels, such as GPT-4. We introduce a new configuration for encoder-decoder\nmodels that improves efficiency on structured output and decomposable tasks\nwhere multiple outputs are required for a single shared input. Our method,\nprompt-in-decoder (PiD), encodes the input once and decodes the output in\nparallel, boosting both training and inference efficiency by avoiding duplicate\ninput encoding and increasing the operational intensity (ratio of numbers of\narithmetic operation to memory access) of decoding process by sharing the input\nkey-value cache. We achieve computation reduction that roughly scales with the\nnumber of subtasks, gaining up to 4.6x speed-up over state-of-the-art models\nfor dialogue state tracking, summarization, and question-answering tasks, with\ncomparable or better performance."
                },
                "authors": [
                    {
                        "name": "Bo-Ru Lu"
                    },
                    {
                        "name": "Nikita Haduong"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Noah A. Smith"
                    },
                    {
                        "name": "Mari Ostendorf"
                    }
                ],
                "author_detail": {
                    "name": "Mari Ostendorf"
                },
                "author": "Mari Ostendorf",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13112v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13112v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10803v1",
                "updated": "2024-11-16T13:45:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    45,
                    33,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T13:45:33Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    45,
                    33,
                    5,
                    321,
                    0
                ],
                "title": "Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large\n  Language Model"
                },
                "summary": "The vision tokens in multimodal large language models usually exhibit\nsignificant spatial and temporal redundancy and take up most of the input\ntokens, which harms their inference efficiency. To solve this problem, some\nrecent works were introduced to drop the unimportant tokens during inference\nwhere the importance of each token is decided only by the information in either\nthe vision encoding stage or the prefilling stage. In this paper, we propose\nMulti-stage Token Dropping (MustDrop) to measure the importance of each token\nfrom the whole lifecycle, including the vision encoding stage, prefilling\nstage, and decoding stage. Concretely, in the visual encoding stage, MustDrop\nmerges spatially adjacent tokens with high similarity, and establishes a key\ntoken set to retain the most vision-critical tokens, preventing them from being\ndiscarded in later stages. In the prefilling stage, MustDrop further compresses\nvision tokens by the guidance of text semantics, with a dual-attention\nfiltering strategy. In the decoding stage, an output-aware cache policy is\nproposed to further reduce the size of the KV cache. By leveraging tailored\nstrategies in the multi-stage process, MustDrop can more precisely recognize\nthe important and redundant tokens, thus achieving an optimal balance between\nperformance and efficiency. For instance, MustDrop reduces about 88.5\\% FLOPs\non LLaVA with a compression ratio of 92.2\\% while maintaining comparable\naccuracy. Our codes are available at\n\\url{https://github.com/liuting20/MustDrop}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vision tokens in multimodal large language models usually exhibit\nsignificant spatial and temporal redundancy and take up most of the input\ntokens, which harms their inference efficiency. To solve this problem, some\nrecent works were introduced to drop the unimportant tokens during inference\nwhere the importance of each token is decided only by the information in either\nthe vision encoding stage or the prefilling stage. In this paper, we propose\nMulti-stage Token Dropping (MustDrop) to measure the importance of each token\nfrom the whole lifecycle, including the vision encoding stage, prefilling\nstage, and decoding stage. Concretely, in the visual encoding stage, MustDrop\nmerges spatially adjacent tokens with high similarity, and establishes a key\ntoken set to retain the most vision-critical tokens, preventing them from being\ndiscarded in later stages. In the prefilling stage, MustDrop further compresses\nvision tokens by the guidance of text semantics, with a dual-attention\nfiltering strategy. In the decoding stage, an output-aware cache policy is\nproposed to further reduce the size of the KV cache. By leveraging tailored\nstrategies in the multi-stage process, MustDrop can more precisely recognize\nthe important and redundant tokens, thus achieving an optimal balance between\nperformance and efficiency. For instance, MustDrop reduces about 88.5\\% FLOPs\non LLaVA with a compression ratio of 92.2\\% while maintaining comparable\naccuracy. Our codes are available at\n\\url{https://github.com/liuting20/MustDrop}."
                },
                "authors": [
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Liangtao Shi"
                    },
                    {
                        "name": "Richang Hong"
                    },
                    {
                        "name": "Yue Hu"
                    },
                    {
                        "name": "Quanjun Yin"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "8 pages, 4figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01733v2",
                "updated": "2024-11-16T07:43:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    7,
                    43,
                    28,
                    5,
                    321,
                    0
                ],
                "published": "2024-06-03T18:49:57Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    18,
                    49,
                    57,
                    0,
                    155,
                    0
                ],
                "title": "Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching"
                },
                "summary": "Diffusion Transformers have recently demonstrated unprecedented generative\ncapabilities for various tasks. The encouraging results, however, come with the\ncost of slow inference, since each denoising step requires inference on a\ntransformer model with a large scale of parameters. In this study, we make an\ninteresting and somehow surprising observation: the computation of a large\nproportion of layers in the diffusion transformer, through introducing a\ncaching mechanism, can be readily removed even without updating the model\nparameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68%\nof the computation in the cache steps (46.84% for all steps), with less than\n0.01 drop in FID. To achieve this, we introduce a novel scheme, named\nLearning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for\ndiffusion transformers. Specifically, by leveraging the identical structure of\nlayers in transformers and the sequential nature of diffusion, we explore\nredundant computations between timesteps by treating each layer as the\nfundamental unit for caching. To address the challenge of the exponential\nsearch space in deep models for identifying layers to cache and remove, we\npropose a novel differentiable optimization objective. An input-invariant yet\ntimestep-variant router is then optimized, which can finally produce a static\ncomputation graph. Experimental results show that L2C largely outperforms\nsamplers such as DDIM and DPM-Solver, alongside prior cache-based methods at\nthe same inference speed. Code is available at\nhttps://github.com/horseee/learning-to-cache",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have recently demonstrated unprecedented generative\ncapabilities for various tasks. The encouraging results, however, come with the\ncost of slow inference, since each denoising step requires inference on a\ntransformer model with a large scale of parameters. In this study, we make an\ninteresting and somehow surprising observation: the computation of a large\nproportion of layers in the diffusion transformer, through introducing a\ncaching mechanism, can be readily removed even without updating the model\nparameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68%\nof the computation in the cache steps (46.84% for all steps), with less than\n0.01 drop in FID. To achieve this, we introduce a novel scheme, named\nLearning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for\ndiffusion transformers. Specifically, by leveraging the identical structure of\nlayers in transformers and the sequential nature of diffusion, we explore\nredundant computations between timesteps by treating each layer as the\nfundamental unit for caching. To address the challenge of the exponential\nsearch space in deep models for identifying layers to cache and remove, we\npropose a novel differentiable optimization objective. An input-invariant yet\ntimestep-variant router is then optimized, which can finally produce a static\ncomputation graph. Experimental results show that L2C largely outperforms\nsamplers such as DDIM and DPM-Solver, alongside prior cache-based methods at\nthe same inference speed. Code is available at\nhttps://github.com/horseee/learning-to-cache"
                },
                "authors": [
                    {
                        "name": "Xinyin Ma"
                    },
                    {
                        "name": "Gongfan Fang"
                    },
                    {
                        "name": "Michael Bi Mi"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v1",
                "updated": "2024-11-16T01:39:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v4",
                "updated": "2024-11-15T22:37:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    22,
                    37,
                    48,
                    4,
                    320,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.05276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05276v1",
                "updated": "2024-12-06T18:59:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    59,
                    51,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T18:59:51Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    59,
                    51,
                    4,
                    341,
                    0
                ],
                "title": "Sparse autoencoders reveal selective remapping of visual concepts during\n  adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse autoencoders reveal selective remapping of visual concepts during\n  adaptation"
                },
                "summary": "Adapting foundation models for specific purposes has become a standard\napproach to build machine learning systems for downstream applications. Yet, it\nis an open question which mechanisms take place during adaptation. Here we\ndevelop a new Sparse Autoencoder (SAE) for the CLIP vision transformer, named\nPatchSAE, to extract interpretable concepts at granular levels (e.g. shape,\ncolor, or semantics of an object) and their patch-wise spatial attributions. We\nexplore how these concepts influence the model output in downstream image\nclassification tasks and investigate how recent state-of-the-art prompt-based\nadaptation techniques change the association of model inputs to these concepts.\nWhile activations of concepts slightly change between adapted and non-adapted\nmodels, we find that the majority of gains on common adaptation tasks can be\nexplained with the existing concepts already present in the non-adapted\nfoundation model. This work provides a concrete framework to train and use SAEs\nfor Vision Transformers and provides insights into explaining adaptation\nmechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting foundation models for specific purposes has become a standard\napproach to build machine learning systems for downstream applications. Yet, it\nis an open question which mechanisms take place during adaptation. Here we\ndevelop a new Sparse Autoencoder (SAE) for the CLIP vision transformer, named\nPatchSAE, to extract interpretable concepts at granular levels (e.g. shape,\ncolor, or semantics of an object) and their patch-wise spatial attributions. We\nexplore how these concepts influence the model output in downstream image\nclassification tasks and investigate how recent state-of-the-art prompt-based\nadaptation techniques change the association of model inputs to these concepts.\nWhile activations of concepts slightly change between adapted and non-adapted\nmodels, we find that the majority of gains on common adaptation tasks can be\nexplained with the existing concepts already present in the non-adapted\nfoundation model. This work provides a concrete framework to train and use SAEs\nfor Vision Transformers and provides insights into explaining adaptation\nmechanisms."
                },
                "authors": [
                    {
                        "name": "Hyesu Lim"
                    },
                    {
                        "name": "Jinho Choi"
                    },
                    {
                        "name": "Jaegul Choo"
                    },
                    {
                        "name": "Steffen Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Schneider"
                },
                "author": "Steffen Schneider",
                "arxiv_comment": "A demo is available at github.com/dynamical-inference/patchsae",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05270v1",
                "updated": "2024-12-06T18:55:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    55,
                    34,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T18:55:34Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    55,
                    34,
                    4,
                    341,
                    0
                ],
                "title": "APOLLO: SGD-like Memory, AdamW-level Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APOLLO: SGD-like Memory, AdamW-level Performance"
                },
                "summary": "Large language models (LLMs) are notoriously memory-intensive during\ntraining, particularly with the popular AdamW optimizer. This memory burden\nnecessitates using more or higher-end GPUs or reducing batch sizes, limiting\ntraining scalability and throughput. To address this, various memory-efficient\noptimizers have been proposed to reduce optimizer memory usage. However, they\nface critical challenges: (i) reliance on costly SVD operations; (ii)\nsignificant performance trade-offs compared to AdamW; and (iii) still\nsubstantial optimizer memory overhead to maintain competitive performance.\n  In this work, we identify that AdamW's learning rate adaptation rule can be\neffectively coarsened as a structured learning rate update. Based on this\ninsight, we propose Approximated Gradient Scaling for Memory-Efficient LLM\nOptimization (APOLLO), which approximates learning rate scaling using an\nauxiliary low-rank optimizer state based on pure random projection. This\nstructured learning rate update rule makes APOLLO highly tolerant to further\nmemory reductions while delivering comparable pre-training performance. Even\nits rank-1 variant, APOLLO-Mini, achieves superior pre-training performance\ncompared to AdamW with SGD-level memory costs.\n  Extensive experiments demonstrate that the APOLLO series performs on-par with\nor better than AdamW, while achieving greater memory savings by nearly\neliminating the optimization states of AdamW. These savings provide significant\nsystem-level benefits: (1) Enhanced Throughput: 3x throughput on an 8xA100-80GB\nsetup compared to AdamW by supporting 4x larger batch sizes. (2) Improved Model\nScalability: Pre-training LLaMA-13B with naive DDP on A100-80GB GPUs without\nsystem-level optimizations. (3) Low-End GPU Friendly Pre-training: Pre-training\nLLaMA-7B on a single GPU using less than 12 GB of memory with weight\nquantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are notoriously memory-intensive during\ntraining, particularly with the popular AdamW optimizer. This memory burden\nnecessitates using more or higher-end GPUs or reducing batch sizes, limiting\ntraining scalability and throughput. To address this, various memory-efficient\noptimizers have been proposed to reduce optimizer memory usage. However, they\nface critical challenges: (i) reliance on costly SVD operations; (ii)\nsignificant performance trade-offs compared to AdamW; and (iii) still\nsubstantial optimizer memory overhead to maintain competitive performance.\n  In this work, we identify that AdamW's learning rate adaptation rule can be\neffectively coarsened as a structured learning rate update. Based on this\ninsight, we propose Approximated Gradient Scaling for Memory-Efficient LLM\nOptimization (APOLLO), which approximates learning rate scaling using an\nauxiliary low-rank optimizer state based on pure random projection. This\nstructured learning rate update rule makes APOLLO highly tolerant to further\nmemory reductions while delivering comparable pre-training performance. Even\nits rank-1 variant, APOLLO-Mini, achieves superior pre-training performance\ncompared to AdamW with SGD-level memory costs.\n  Extensive experiments demonstrate that the APOLLO series performs on-par with\nor better than AdamW, while achieving greater memory savings by nearly\neliminating the optimization states of AdamW. These savings provide significant\nsystem-level benefits: (1) Enhanced Throughput: 3x throughput on an 8xA100-80GB\nsetup compared to AdamW by supporting 4x larger batch sizes. (2) Improved Model\nScalability: Pre-training LLaMA-13B with naive DDP on A100-80GB GPUs without\nsystem-level optimizations. (3) Low-End GPU Friendly Pre-training: Pre-training\nLLaMA-7B on a single GPU using less than 12 GB of memory with weight\nquantization."
                },
                "authors": [
                    {
                        "name": "Hanqing Zhu"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Wenyan Cong"
                    },
                    {
                        "name": "Xi Liu"
                    },
                    {
                        "name": "Sem Park"
                    },
                    {
                        "name": "Vikas Chandra"
                    },
                    {
                        "name": "Bo Long"
                    },
                    {
                        "name": "David Z. Pan"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Jinwon Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jinwon Lee"
                },
                "author": "Jinwon Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05251v1",
                "updated": "2024-12-06T18:31:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    31,
                    51,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T18:31:51Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    31,
                    51,
                    4,
                    341,
                    0
                ],
                "title": "Uncertainty Quantification for Transformer Models for Dark-Pattern\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty Quantification for Transformer Models for Dark-Pattern\n  Detection"
                },
                "summary": "The opaque nature of transformer-based models, particularly in applications\nsusceptible to unethical practices such as dark-patterns in user interfaces,\nrequires models that integrate uncertainty quantification to enhance trust in\npredictions. This study focuses on dark-pattern detection, deceptive design\nchoices that manipulate user decisions, undermining autonomy and consent. We\npropose a differential fine-tuning approach implemented at the final\nclassification head via uncertainty quantification with transformer-based\npre-trained models. Employing a dense neural network (DNN) head architecture as\na baseline, we examine two methods capable of quantifying uncertainty:\nSpectral-normalized Neural Gaussian Processes (SNGPs) and Bayesian Neural\nNetworks (BNNs). These methods are evaluated on a set of open-source\nfoundational models across multiple dimensions: model performance, variance in\ncertainty of predictions and environmental impact during training and inference\nphases. Results demonstrate that integrating uncertainty quantification\nmaintains performance while providing insights into challenging instances\nwithin the models. Moreover, the study reveals that the environmental impact\ndoes not uniformly increase with the incorporation of uncertainty\nquantification techniques. The study's findings demonstrate that uncertainty\nquantification enhances transparency and provides measurable confidence in\npredictions, improving the explainability and clarity of black-box models. This\nfacilitates informed decision-making and mitigates the influence of\ndark-patterns on user interfaces. These results highlight the importance of\nincorporating uncertainty quantification techniques in developing machine\nlearning models, particularly in domains where interpretability and\ntrustworthiness are critical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The opaque nature of transformer-based models, particularly in applications\nsusceptible to unethical practices such as dark-patterns in user interfaces,\nrequires models that integrate uncertainty quantification to enhance trust in\npredictions. This study focuses on dark-pattern detection, deceptive design\nchoices that manipulate user decisions, undermining autonomy and consent. We\npropose a differential fine-tuning approach implemented at the final\nclassification head via uncertainty quantification with transformer-based\npre-trained models. Employing a dense neural network (DNN) head architecture as\na baseline, we examine two methods capable of quantifying uncertainty:\nSpectral-normalized Neural Gaussian Processes (SNGPs) and Bayesian Neural\nNetworks (BNNs). These methods are evaluated on a set of open-source\nfoundational models across multiple dimensions: model performance, variance in\ncertainty of predictions and environmental impact during training and inference\nphases. Results demonstrate that integrating uncertainty quantification\nmaintains performance while providing insights into challenging instances\nwithin the models. Moreover, the study reveals that the environmental impact\ndoes not uniformly increase with the incorporation of uncertainty\nquantification techniques. The study's findings demonstrate that uncertainty\nquantification enhances transparency and provides measurable confidence in\npredictions, improving the explainability and clarity of black-box models. This\nfacilitates informed decision-making and mitigates the influence of\ndark-patterns on user interfaces. These results highlight the importance of\nincorporating uncertainty quantification techniques in developing machine\nlearning models, particularly in domains where interpretability and\ntrustworthiness are critical."
                },
                "authors": [
                    {
                        "name": "Javier Muñoz"
                    },
                    {
                        "name": "Álvaro Huertas-García"
                    },
                    {
                        "name": "Carlos Martí-González"
                    },
                    {
                        "name": "Enrique De Miguel Ambite"
                    }
                ],
                "author_detail": {
                    "name": "Enrique De Miguel Ambite"
                },
                "author": "Enrique De Miguel Ambite",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03459v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03459v3",
                "updated": "2024-12-06T18:31:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    31,
                    30,
                    4,
                    341,
                    0
                ],
                "published": "2024-08-06T22:11:00Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    22,
                    11,
                    0,
                    1,
                    219,
                    0
                ],
                "title": "On the Generalization of Preference Learning with DPO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Generalization of Preference Learning with DPO"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities but\noften struggle to align with human preferences, leading to harmful or\nundesirable outputs. Preference learning, which trains models to distinguish\nbetween preferred and non-preferred responses based on human feedback, has\nbecome a crucial component for ensuring that LLMs align with human values.\nDespite the widespread adoption in real-world systems, a thorough theoretical\nunderstanding of the generalization guarantees for these models remain lacking.\nThis paper bridges that gap by introducing a new theoretical framework to\nanalyze the generalization guarantees of models trained with direct preference\noptimization (DPO). While existing generalization theory often focuses on\noverparameterized models achieving near-optimal loss or models independent of\nthe training process, our framework rigorously assesses how well models\ngeneralize after a finite number of gradient steps, reflecting real-world LLM\ntraining practices. By analyzing the reward margin associated with each sample\nand its trajectory throughout training, we can effectively bound the\ngeneralization error. We derive learning guarantees showing that, under\nspecific conditions, models trained with DPO can correctly discern preferred\nresponses on unseen data with high probability. These insights are empirically\nvalidated on contemporary LLMs, underscoring the practical relevance of our\ntheoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities but\noften struggle to align with human preferences, leading to harmful or\nundesirable outputs. Preference learning, which trains models to distinguish\nbetween preferred and non-preferred responses based on human feedback, has\nbecome a crucial component for ensuring that LLMs align with human values.\nDespite the widespread adoption in real-world systems, a thorough theoretical\nunderstanding of the generalization guarantees for these models remain lacking.\nThis paper bridges that gap by introducing a new theoretical framework to\nanalyze the generalization guarantees of models trained with direct preference\noptimization (DPO). While existing generalization theory often focuses on\noverparameterized models achieving near-optimal loss or models independent of\nthe training process, our framework rigorously assesses how well models\ngeneralize after a finite number of gradient steps, reflecting real-world LLM\ntraining practices. By analyzing the reward margin associated with each sample\nand its trajectory throughout training, we can effectively bound the\ngeneralization error. We derive learning guarantees showing that, under\nspecific conditions, models trained with DPO can correctly discern preferred\nresponses on unseen data with high probability. These insights are empirically\nvalidated on contemporary LLMs, underscoring the practical relevance of our\ntheoretical findings."
                },
                "authors": [
                    {
                        "name": "Shawn Im"
                    },
                    {
                        "name": "Yixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Yixuan Li"
                },
                "author": "Yixuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03459v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03459v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05248v1",
                "updated": "2024-12-06T18:27:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    27,
                    15,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T18:27:15Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    27,
                    15,
                    4,
                    341,
                    0
                ],
                "title": "Enhancing FKG.in: automating Indian food composition analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing FKG.in: automating Indian food composition analysis"
                },
                "summary": "This paper presents a novel approach to compute food composition data for\nIndian recipes using a knowledge graph for Indian food (FKG.in) and LLMs. The\nprimary focus is to provide a broad overview of an automated food composition\nanalysis workflow and describe its core functionalities: nutrition data\naggregation, food composition analysis, and LLM-augmented information\nresolution. This workflow aims to complement FKG.in and iteratively supplement\nfood composition data from verified knowledge bases. Additionally, this paper\nhighlights the challenges of representing Indian food and accessing food\ncomposition data digitally. It also reviews three key sources of food\ncomposition data: the Indian Food Composition Tables, the Indian Nutrient\nDatabank, and the Nutritionix API. Furthermore, it briefly outlines how users\ncan interact with the workflow to obtain diet-based health recommendations and\ndetailed food composition information for numerous recipes. We then explore the\ncomplex challenges of analyzing Indian recipe information across dimensions\nsuch as structure, multilingualism, and uncertainty as well as present our\nongoing work on LLM-based solutions to address these issues. The methods\nproposed in this workshop paper for AI-driven knowledge curation and\ninformation resolution are application-agnostic, generalizable, and replicable\nfor any domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach to compute food composition data for\nIndian recipes using a knowledge graph for Indian food (FKG.in) and LLMs. The\nprimary focus is to provide a broad overview of an automated food composition\nanalysis workflow and describe its core functionalities: nutrition data\naggregation, food composition analysis, and LLM-augmented information\nresolution. This workflow aims to complement FKG.in and iteratively supplement\nfood composition data from verified knowledge bases. Additionally, this paper\nhighlights the challenges of representing Indian food and accessing food\ncomposition data digitally. It also reviews three key sources of food\ncomposition data: the Indian Food Composition Tables, the Indian Nutrient\nDatabank, and the Nutritionix API. Furthermore, it briefly outlines how users\ncan interact with the workflow to obtain diet-based health recommendations and\ndetailed food composition information for numerous recipes. We then explore the\ncomplex challenges of analyzing Indian recipe information across dimensions\nsuch as structure, multilingualism, and uncertainty as well as present our\nongoing work on LLM-based solutions to address these issues. The methods\nproposed in this workshop paper for AI-driven knowledge curation and\ninformation resolution are application-agnostic, generalizable, and replicable\nfor any domain."
                },
                "authors": [
                    {
                        "name": "Saransh Kumar Gupta"
                    },
                    {
                        "name": "Lipika Dey"
                    },
                    {
                        "name": "Partha Pratim Das"
                    },
                    {
                        "name": "Geeta Trilok-Kumar"
                    },
                    {
                        "name": "Ramesh Jain"
                    }
                ],
                "author_detail": {
                    "name": "Ramesh Jain"
                },
                "author": "Ramesh Jain",
                "arxiv_comment": "15 pages, 3 figures, 30 references, International Conference on\n  Pattern Recognition 2024 - Multimedia Assisted Dietary Management Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05243v1",
                "updated": "2024-12-06T18:22:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    22,
                    47,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T18:22:47Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    22,
                    47,
                    4,
                    341,
                    0
                ],
                "title": "CompCap: Improving Multimodal Large Language Models with Composite\n  Captions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompCap: Improving Multimodal Large Language Models with Composite\n  Captions"
                },
                "summary": "How well can Multimodal Large Language Models (MLLMs) understand composite\nimages? Composite images (CIs) are synthetic visuals created by merging\nmultiple visual elements, such as charts, posters, or screenshots, rather than\nbeing captured directly by a camera. While CIs are prevalent in real-world\napplications, recent MLLM developments have primarily focused on interpreting\nnatural images (NIs). Our research reveals that current MLLMs face significant\nchallenges in accurately understanding CIs, often struggling to extract\ninformation or perform complex reasoning based on these images. We find that\nexisting training data for CIs are mostly formatted for question-answer tasks\n(e.g., in datasets like ChartQA and ScienceQA), while high-quality\nimage-caption datasets, critical for robust vision-language alignment, are only\navailable for NIs. To bridge this gap, we introduce Composite Captions\n(CompCap), a flexible framework that leverages Large Language Models (LLMs) and\nautomation tools to synthesize CIs with accurate and detailed captions. Using\nCompCap, we curate CompCap-118K, a dataset containing 118K image-caption pairs\nacross six CI types. We validate the effectiveness of CompCap-118K by\nsupervised fine-tuning MLLMs of three sizes: xGen-MM-inst.-4B and\nLLaVA-NeXT-Vicuna-7B/13B. Empirical results show that CompCap-118K\nsignificantly enhances MLLMs' understanding of CIs, yielding average gains of\n1.7%, 2.0%, and 2.9% across eleven benchmarks, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How well can Multimodal Large Language Models (MLLMs) understand composite\nimages? Composite images (CIs) are synthetic visuals created by merging\nmultiple visual elements, such as charts, posters, or screenshots, rather than\nbeing captured directly by a camera. While CIs are prevalent in real-world\napplications, recent MLLM developments have primarily focused on interpreting\nnatural images (NIs). Our research reveals that current MLLMs face significant\nchallenges in accurately understanding CIs, often struggling to extract\ninformation or perform complex reasoning based on these images. We find that\nexisting training data for CIs are mostly formatted for question-answer tasks\n(e.g., in datasets like ChartQA and ScienceQA), while high-quality\nimage-caption datasets, critical for robust vision-language alignment, are only\navailable for NIs. To bridge this gap, we introduce Composite Captions\n(CompCap), a flexible framework that leverages Large Language Models (LLMs) and\nautomation tools to synthesize CIs with accurate and detailed captions. Using\nCompCap, we curate CompCap-118K, a dataset containing 118K image-caption pairs\nacross six CI types. We validate the effectiveness of CompCap-118K by\nsupervised fine-tuning MLLMs of three sizes: xGen-MM-inst.-4B and\nLLaVA-NeXT-Vicuna-7B/13B. Empirical results show that CompCap-118K\nsignificantly enhances MLLMs' understanding of CIs, yielding average gains of\n1.7%, 2.0%, and 2.9% across eleven benchmarks, respectively."
                },
                "authors": [
                    {
                        "name": "Xiaohui Chen"
                    },
                    {
                        "name": "Satya Narayan Shukla"
                    },
                    {
                        "name": "Mahmoud Azab"
                    },
                    {
                        "name": "Aashu Singh"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "David Yang"
                    },
                    {
                        "name": "ShengYun Peng"
                    },
                    {
                        "name": "Hanchao Yu"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Xuewen Zhang"
                    },
                    {
                        "name": "Baosheng He"
                    }
                ],
                "author_detail": {
                    "name": "Baosheng He"
                },
                "author": "Baosheng He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05240v1",
                "updated": "2024-12-06T18:18:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    18,
                    26,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T18:18:26Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    18,
                    26,
                    4,
                    341,
                    0
                ],
                "title": "Automated, Unsupervised, and Auto-parameterized Inference of Data\n  Patterns and Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated, Unsupervised, and Auto-parameterized Inference of Data\n  Patterns and Anomaly Detection"
                },
                "summary": "With the advent of data-centric and machine learning (ML) systems, data\nquality is playing an increasingly critical role in ensuring the overall\nquality of software systems. Data preparation, an essential step towards high\ndata quality, is known to be a highly effort-intensive process. Although prior\nstudies have dealt with one of the most impacting issues, data pattern\nviolations, these studies usually require data-specific configurations (i.e.,\nparameterized) or use carefully curated data as learning examples (i.e.,\nsupervised), relying on domain knowledge and deep understanding of the data, or\ndemanding significant manual effort. In this paper, we introduce RIOLU: Regex\nInferencer auto-parameterized Learning with Uncleaned data. RIOLU is fully\nautomated, automatically parameterized, and does not need labeled samples.\nRIOLU can generate precise patterns from datasets in various domains, with a\nhigh F1 score of 97.2%, exceeding the state-of-the-art baseline. In addition,\naccording to our experiment on five datasets with anomalies, RIOLU can\nautomatically estimate a data column's error rate, draw normal patterns, and\npredict anomalies from unlabeled data with higher performance (up to 800.4%\nimprovement in terms of F1) than the state-of-the-art baseline, even\noutperforming ChatGPT in terms of both accuracy (12.3% higher F1) and\nefficiency (10% less inference time). A variant of RIOLU, with user guidance,\ncan further boost its precision, with up to 37.4% improvement in terms of F1.\nOur evaluation in an industrial setting further demonstrates the practical\nbenefits of RIOLU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of data-centric and machine learning (ML) systems, data\nquality is playing an increasingly critical role in ensuring the overall\nquality of software systems. Data preparation, an essential step towards high\ndata quality, is known to be a highly effort-intensive process. Although prior\nstudies have dealt with one of the most impacting issues, data pattern\nviolations, these studies usually require data-specific configurations (i.e.,\nparameterized) or use carefully curated data as learning examples (i.e.,\nsupervised), relying on domain knowledge and deep understanding of the data, or\ndemanding significant manual effort. In this paper, we introduce RIOLU: Regex\nInferencer auto-parameterized Learning with Uncleaned data. RIOLU is fully\nautomated, automatically parameterized, and does not need labeled samples.\nRIOLU can generate precise patterns from datasets in various domains, with a\nhigh F1 score of 97.2%, exceeding the state-of-the-art baseline. In addition,\naccording to our experiment on five datasets with anomalies, RIOLU can\nautomatically estimate a data column's error rate, draw normal patterns, and\npredict anomalies from unlabeled data with higher performance (up to 800.4%\nimprovement in terms of F1) than the state-of-the-art baseline, even\noutperforming ChatGPT in terms of both accuracy (12.3% higher F1) and\nefficiency (10% less inference time). A variant of RIOLU, with user guidance,\ncan further boost its precision, with up to 37.4% improvement in terms of F1.\nOur evaluation in an industrial setting further demonstrates the practical\nbenefits of RIOLU."
                },
                "authors": [
                    {
                        "name": "Qiaolin Qin"
                    },
                    {
                        "name": "Heng Li"
                    },
                    {
                        "name": "Ettore Merlo"
                    },
                    {
                        "name": "Maxime Lamothe"
                    }
                ],
                "author_detail": {
                    "name": "Maxime Lamothe"
                },
                "author": "Maxime Lamothe",
                "arxiv_comment": "Accepted in 2025 IEEE/ACM International Conference on Software\n  Engineering (ICSE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68P01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "E.m",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05232v1",
                "updated": "2024-12-06T18:02:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    2,
                    59,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T18:02:59Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    2,
                    59,
                    4,
                    341,
                    0
                ],
                "title": "LIAR: Leveraging Alignment (Best-of-N) to Jailbreak LLMs in Seconds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LIAR: Leveraging Alignment (Best-of-N) to Jailbreak LLMs in Seconds"
                },
                "summary": "Many existing jailbreak techniques rely on solving discrete combinatorial\noptimization, while more recent approaches involve training LLMs to generate\nmultiple adversarial prompts. However, both approaches require significant\ncomputational resources to produce even a single adversarial prompt. We\nhypothesize that the inefficiency of current approaches stems from an\ninadequate characterization of the jailbreak problem. To address this gap, we\nformulate the jailbreak problem in terms of alignment. By starting from an\navailable safety-aligned model, we leverage an unsafe reward to guide the safe\nmodel towards generating unsafe outputs using alignment techniques (e.g.,\nreinforcement learning from human feedback), effectively performing\njailbreaking via alignment. We propose a novel jailbreak method called LIAR\n(LeveragIng Alignment to jailbReak). To demonstrate the simplicity and\neffectiveness of our approach, we employ a best-of-N method to solve the\nalignment problem. LIAR offers significant advantages: lower computational\nrequirements without additional training, fully black-box operation,\ncompetitive attack success rates, and more human-readable prompts. We provide\ntheoretical insights into the possibility of jailbreaking a safety-aligned\nmodel, revealing inherent vulnerabilities in current alignment strategies for\nLLMs. We also provide sub-optimality guarantees for the proposed \\algo.\nExperimentally, we achieve ASR comparable to the SoTA with a 10x improvement to\nperplexity and a Time-to-Attack measured in seconds rather than tens of hours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many existing jailbreak techniques rely on solving discrete combinatorial\noptimization, while more recent approaches involve training LLMs to generate\nmultiple adversarial prompts. However, both approaches require significant\ncomputational resources to produce even a single adversarial prompt. We\nhypothesize that the inefficiency of current approaches stems from an\ninadequate characterization of the jailbreak problem. To address this gap, we\nformulate the jailbreak problem in terms of alignment. By starting from an\navailable safety-aligned model, we leverage an unsafe reward to guide the safe\nmodel towards generating unsafe outputs using alignment techniques (e.g.,\nreinforcement learning from human feedback), effectively performing\njailbreaking via alignment. We propose a novel jailbreak method called LIAR\n(LeveragIng Alignment to jailbReak). To demonstrate the simplicity and\neffectiveness of our approach, we employ a best-of-N method to solve the\nalignment problem. LIAR offers significant advantages: lower computational\nrequirements without additional training, fully black-box operation,\ncompetitive attack success rates, and more human-readable prompts. We provide\ntheoretical insights into the possibility of jailbreaking a safety-aligned\nmodel, revealing inherent vulnerabilities in current alignment strategies for\nLLMs. We also provide sub-optimality guarantees for the proposed \\algo.\nExperimentally, we achieve ASR comparable to the SoTA with a 10x improvement to\nperplexity and a Time-to-Attack measured in seconds rather than tens of hours."
                },
                "authors": [
                    {
                        "name": "James Beetham"
                    },
                    {
                        "name": "Souradip Chakraborty"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Amrit Singh Bedi"
                    },
                    {
                        "name": "Mubarak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Mubarak Shah"
                },
                "author": "Mubarak Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05225v1",
                "updated": "2024-12-06T17:58:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    14,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:58:14Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    14,
                    4,
                    341,
                    0
                ],
                "title": "BEExformer: A Fast Inferencing Transformer Architecture via Binarization\n  with Multiple Early Exits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEExformer: A Fast Inferencing Transformer Architecture via Binarization\n  with Multiple Early Exits"
                },
                "summary": "Large Language Models (LLMs) based on transformers achieve cutting-edge\nresults on a variety of applications. However, their enormous size and\nprocessing requirements make deployment on devices with constrained resources\nextremely difficult. Among various efficiency considerations, model\nbinarization and Early Exit (EE) are common effective solutions. However,\nbinarization may lead to performance loss due to reduced precision affecting\ngradient estimation and parameter updates. Besides, the present early-exit\nmechanisms are still in the nascent stages of research. To ameliorate these\nissues, we propose Binarized Early Exit Transformer (BEExformer), the\nfirst-ever selective learning transformer architecture to combine early exit\nwith binarization for textual inference. It improves the binarization process\nthrough a differentiable second-order approximation to the impulse function.\nThis enables gradient computation concerning both the sign as well as the\nmagnitude of the weights. In contrast to absolute threshold-based EE, the\nproposed EE mechanism hinges on fractional reduction in entropy among\nintermediate transformer blocks with soft-routing loss estimation. While\nbinarization results in 18.44 times reduction in model size, early exit reduces\nthe FLOPs during inference by 54.85% and even improves accuracy by 5.98%\nthrough resolving the \"overthinking\" problem inherent in deep networks.\nMoreover, the proposed BEExformer simplifies training by not requiring\nknowledge distillation from a full-precision LLM. Extensive evaluation on the\nGLUE dataset and comparison with the SOTA works showcase its pareto-optimal\nperformance-efficiency trade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) based on transformers achieve cutting-edge\nresults on a variety of applications. However, their enormous size and\nprocessing requirements make deployment on devices with constrained resources\nextremely difficult. Among various efficiency considerations, model\nbinarization and Early Exit (EE) are common effective solutions. However,\nbinarization may lead to performance loss due to reduced precision affecting\ngradient estimation and parameter updates. Besides, the present early-exit\nmechanisms are still in the nascent stages of research. To ameliorate these\nissues, we propose Binarized Early Exit Transformer (BEExformer), the\nfirst-ever selective learning transformer architecture to combine early exit\nwith binarization for textual inference. It improves the binarization process\nthrough a differentiable second-order approximation to the impulse function.\nThis enables gradient computation concerning both the sign as well as the\nmagnitude of the weights. In contrast to absolute threshold-based EE, the\nproposed EE mechanism hinges on fractional reduction in entropy among\nintermediate transformer blocks with soft-routing loss estimation. While\nbinarization results in 18.44 times reduction in model size, early exit reduces\nthe FLOPs during inference by 54.85% and even improves accuracy by 5.98%\nthrough resolving the \"overthinking\" problem inherent in deep networks.\nMoreover, the proposed BEExformer simplifies training by not requiring\nknowledge distillation from a full-precision LLM. Extensive evaluation on the\nGLUE dataset and comparison with the SOTA works showcase its pareto-optimal\nperformance-efficiency trade-off."
                },
                "authors": [
                    {
                        "name": "Wazib Ansar"
                    },
                    {
                        "name": "Saptarsi Goswami"
                    },
                    {
                        "name": "Amlan Chakrabarti"
                    }
                ],
                "author_detail": {
                    "name": "Amlan Chakrabarti"
                },
                "author": "Amlan Chakrabarti",
                "arxiv_comment": "15 pages, 15 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05223v1",
                "updated": "2024-12-06T17:54:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    54,
                    54,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:54:54Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    54,
                    54,
                    4,
                    341,
                    0
                ],
                "title": "100% Hallucination Elimination Using Acurai",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "100% Hallucination Elimination Using Acurai"
                },
                "summary": "The issue of hallucinations in large language models (LLMs) remains a\ncritical barrier to the adoption of AI in enterprise and other high-stakes\napplications. Despite advancements in retrieval-augmented generation (RAG)\nsystems, current state-of-the-art methods fail to achieve more than 80%\naccuracy in generating faithful and factually correct outputs, even when\nprovided with relevant and accurate context. In this work, we introduce Acurai,\na novel systematic approach that achieves 100% hallucination-free responses in\nLLMs by reformatting queries and context data prior to input. Leveraging a deep\nunderstanding of LLM internal representations, the importance of noun-phrase\ndominance, and the role of discrete functional units (DFUs), Acurai ensures\nalignment between input context and generated output. We validate this method\nusing the RAGTruth corpus, demonstrating its ability to eliminate 100%\nhallucinations for both GPT-4 and GPT-3.5 Turbo. Acurai sets a new standard for\nachieving consistent, accurate, and faithful AI responses, marking a\nsignificant step forward in the development of trustworthy AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The issue of hallucinations in large language models (LLMs) remains a\ncritical barrier to the adoption of AI in enterprise and other high-stakes\napplications. Despite advancements in retrieval-augmented generation (RAG)\nsystems, current state-of-the-art methods fail to achieve more than 80%\naccuracy in generating faithful and factually correct outputs, even when\nprovided with relevant and accurate context. In this work, we introduce Acurai,\na novel systematic approach that achieves 100% hallucination-free responses in\nLLMs by reformatting queries and context data prior to input. Leveraging a deep\nunderstanding of LLM internal representations, the importance of noun-phrase\ndominance, and the role of discrete functional units (DFUs), Acurai ensures\nalignment between input context and generated output. We validate this method\nusing the RAGTruth corpus, demonstrating its ability to eliminate 100%\nhallucinations for both GPT-4 and GPT-3.5 Turbo. Acurai sets a new standard for\nachieving consistent, accurate, and faithful AI responses, marking a\nsignificant step forward in the development of trustworthy AI systems."
                },
                "authors": [
                    {
                        "name": "Michael C. Wood"
                    },
                    {
                        "name": "Adam A. Forbes"
                    }
                ],
                "author_detail": {
                    "name": "Adam A. Forbes"
                },
                "author": "Adam A. Forbes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05213v1",
                "updated": "2024-12-06T17:43:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    43,
                    51,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:43:51Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    43,
                    51,
                    4,
                    341,
                    0
                ],
                "title": "Probing neutrino mass ordering with supernova neutrinos at NO$ν$A\n  including the effect of sterile neutrinos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing neutrino mass ordering with supernova neutrinos at NO$ν$A\n  including the effect of sterile neutrinos"
                },
                "summary": "In this work, we explore the possibility of probing the mass ordering\nsensitivity as a function of supernova distance in the context of the ongoing\nneutrino experiment NO$\\nu$A. We provide a detailed study of the active-active\nand active-sterile mixing frameworks, illustrating how supernova neutrinos can\nbe used to realize the existence of sterile neutrinos. Interestingly, we infer\nthat observation of the NC channel alone can differentiate between the presence\nand absence of sterile neutrinos. Our study reveals that in the presence of\nsterile neutrinos, the mass hierarchy sensitivity is significantly reduced\ncompared to the active-active framework for the NC channel. Our results\nindicate that the primary channel of NO$\\nu$A can distinguish normal mass\nhierarchy from inverted mass hierarchy at $5 \\sigma$ confidence level for a\nsupernova explosion occurring at a distance of 5 kpc. Additionally, we examine\nthe impact of systematic uncertainties on mass hierarchy sensitivity, showing\nthat higher levels of systematics lead to a reduction in sensitivity.\nSimilarly, the inclusion of energy smearing significantly diminishes hierarchy\nsensitivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we explore the possibility of probing the mass ordering\nsensitivity as a function of supernova distance in the context of the ongoing\nneutrino experiment NO$\\nu$A. We provide a detailed study of the active-active\nand active-sterile mixing frameworks, illustrating how supernova neutrinos can\nbe used to realize the existence of sterile neutrinos. Interestingly, we infer\nthat observation of the NC channel alone can differentiate between the presence\nand absence of sterile neutrinos. Our study reveals that in the presence of\nsterile neutrinos, the mass hierarchy sensitivity is significantly reduced\ncompared to the active-active framework for the NC channel. Our results\nindicate that the primary channel of NO$\\nu$A can distinguish normal mass\nhierarchy from inverted mass hierarchy at $5 \\sigma$ confidence level for a\nsupernova explosion occurring at a distance of 5 kpc. Additionally, we examine\nthe impact of systematic uncertainties on mass hierarchy sensitivity, showing\nthat higher levels of systematics lead to a reduction in sensitivity.\nSimilarly, the inclusion of energy smearing significantly diminishes hierarchy\nsensitivity."
                },
                "authors": [
                    {
                        "name": "Papia Panda"
                    },
                    {
                        "name": "Rukmani Mohanta"
                    }
                ],
                "author_detail": {
                    "name": "Rukmani Mohanta"
                },
                "author": "Rukmani Mohanta",
                "arxiv_comment": "27 pages, 9 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05210v1",
                "updated": "2024-12-06T17:40:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    40,
                    38,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:40:38Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    40,
                    38,
                    4,
                    341,
                    0
                ],
                "title": "Evaluating and Aligning CodeLLMs on Human Preference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating and Aligning CodeLLMs on Human Preference"
                },
                "summary": "Code large language models (codeLLMs) have made significant strides in code\ngeneration. Most previous code-related benchmarks, which consist of various\nprogramming exercises along with the corresponding test cases, are used as a\ncommon measure to evaluate the performance and capabilities of code LLMs.\nHowever, the current code LLMs focus on synthesizing the correct code snippet,\nignoring the alignment with human preferences, where the query should be\nsampled from the practical application scenarios and the model-generated\nresponses should satisfy the human preference. To bridge the gap between the\nmodel-generated response and human preference, we present a rigorous\nhuman-curated benchmark CodeArena to emulate the complexity and diversity of\nreal-world coding tasks, where 397 high-quality samples spanning 40 categories\nand 44 programming languages, carefully curated from user queries. Further, we\npropose a diverse synthetic instruction corpus SynCode-Instruct (nearly 20B\ntokens) by scaling instructions from the website to verify the effectiveness of\nthe large-scale synthetic instruction fine-tuning, where Qwen2.5-SynCoder\ntotally trained on synthetic instruction data can achieve top-tier performance\nof open-source code LLMs. The results find performance differences between\nexecution-based benchmarks and CodeArena. Our systematic experiments of\nCodeArena on 40+ LLMs reveal a notable performance gap between open SOTA code\nLLMs (e.g. Qwen2.5-Coder) and proprietary LLMs (e.g., OpenAI o1), underscoring\nthe importance of the human preference\nalignment.\\footnote{\\url{https://codearenaeval.github.io/ }}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code large language models (codeLLMs) have made significant strides in code\ngeneration. Most previous code-related benchmarks, which consist of various\nprogramming exercises along with the corresponding test cases, are used as a\ncommon measure to evaluate the performance and capabilities of code LLMs.\nHowever, the current code LLMs focus on synthesizing the correct code snippet,\nignoring the alignment with human preferences, where the query should be\nsampled from the practical application scenarios and the model-generated\nresponses should satisfy the human preference. To bridge the gap between the\nmodel-generated response and human preference, we present a rigorous\nhuman-curated benchmark CodeArena to emulate the complexity and diversity of\nreal-world coding tasks, where 397 high-quality samples spanning 40 categories\nand 44 programming languages, carefully curated from user queries. Further, we\npropose a diverse synthetic instruction corpus SynCode-Instruct (nearly 20B\ntokens) by scaling instructions from the website to verify the effectiveness of\nthe large-scale synthetic instruction fine-tuning, where Qwen2.5-SynCoder\ntotally trained on synthetic instruction data can achieve top-tier performance\nof open-source code LLMs. The results find performance differences between\nexecution-based benchmarks and CodeArena. Our systematic experiments of\nCodeArena on 40+ LLMs reveal a notable performance gap between open SOTA code\nLLMs (e.g. Qwen2.5-Coder) and proprietary LLMs (e.g., OpenAI o1), underscoring\nthe importance of the human preference\nalignment.\\footnote{\\url{https://codearenaeval.github.io/ }}"
                },
                "authors": [
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Jiaxi Yang"
                    },
                    {
                        "name": "Ke Jin"
                    },
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Liqun Yang"
                    },
                    {
                        "name": "Zeyu Cui"
                    },
                    {
                        "name": "Yichang Zhang"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05208v1",
                "updated": "2024-12-06T17:36:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    36,
                    28,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:36:28Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    36,
                    28,
                    4,
                    341,
                    0
                ],
                "title": "A Survey of Large Language Model-Based Generative AI for Text-to-SQL:\n  Benchmarks, Applications, Use Cases, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Large Language Model-Based Generative AI for Text-to-SQL:\n  Benchmarks, Applications, Use Cases, and Challenges"
                },
                "summary": "Text-to-SQL systems facilitate smooth interaction with databases by\ntranslating natural language queries into Structured Query Language (SQL),\nbridging the gap between non-technical users and complex database management\nsystems. This survey provides a comprehensive overview of the evolution of\nAI-driven text-to-SQL systems, highlighting their foundational components,\nadvancements in large language model (LLM) architectures, and the critical role\nof datasets such as Spider, WikiSQL, and CoSQL in driving progress. We examine\nthe applications of text-to-SQL in domains like healthcare, education, and\nfinance, emphasizing their transformative potential for improving data\naccessibility. Additionally, we analyze persistent challenges, including domain\ngeneralization, query optimization, support for multi-turn conversational\ninteractions, and the limited availability of datasets tailored for NoSQL\ndatabases and dynamic real-world scenarios. To address these challenges, we\noutline future research directions, such as extending text-to-SQL capabilities\nto support NoSQL databases, designing datasets for dynamic multi-turn\ninteractions, and optimizing systems for real-world scalability and robustness.\nBy surveying current advancements and identifying key gaps, this paper aims to\nguide the next generation of research and applications in LLM-based text-to-SQL\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL systems facilitate smooth interaction with databases by\ntranslating natural language queries into Structured Query Language (SQL),\nbridging the gap between non-technical users and complex database management\nsystems. This survey provides a comprehensive overview of the evolution of\nAI-driven text-to-SQL systems, highlighting their foundational components,\nadvancements in large language model (LLM) architectures, and the critical role\nof datasets such as Spider, WikiSQL, and CoSQL in driving progress. We examine\nthe applications of text-to-SQL in domains like healthcare, education, and\nfinance, emphasizing their transformative potential for improving data\naccessibility. Additionally, we analyze persistent challenges, including domain\ngeneralization, query optimization, support for multi-turn conversational\ninteractions, and the limited availability of datasets tailored for NoSQL\ndatabases and dynamic real-world scenarios. To address these challenges, we\noutline future research directions, such as extending text-to-SQL capabilities\nto support NoSQL databases, designing datasets for dynamic multi-turn\ninteractions, and optimizing systems for real-world scalability and robustness.\nBy surveying current advancements and identifying key gaps, this paper aims to\nguide the next generation of research and applications in LLM-based text-to-SQL\nsystems."
                },
                "authors": [
                    {
                        "name": "Aditi Singh"
                    },
                    {
                        "name": "Akash Shetty"
                    },
                    {
                        "name": "Abul Ehtesham"
                    },
                    {
                        "name": "Saket Kumar"
                    },
                    {
                        "name": "Tala Talaei Khoei"
                    }
                ],
                "author_detail": {
                    "name": "Tala Talaei Khoei"
                },
                "author": "Tala Talaei Khoei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05206v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05206v1",
                "updated": "2024-12-06T17:35:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    35,
                    52,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:35:52Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    35,
                    52,
                    4,
                    341,
                    0
                ],
                "title": "ConQRet: Benchmarking Fine-Grained Evaluation of Retrieval Augmented\n  Argumentation with LLM Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConQRet: Benchmarking Fine-Grained Evaluation of Retrieval Augmented\n  Argumentation with LLM Judges"
                },
                "summary": "Computational argumentation, which involves generating answers or summaries\nfor controversial topics like abortion bans and vaccination, has become\nincreasingly important in today's polarized environment. Sophisticated LLM\ncapabilities offer the potential to provide nuanced, evidence-based answers to\nsuch questions through Retrieval-Augmented Argumentation (RAArg), leveraging\nreal-world evidence for high-quality, grounded arguments. However, evaluating\nRAArg remains challenging, as human evaluation is costly and difficult for\ncomplex, lengthy answers on complicated topics. At the same time, re-using\nexisting argumentation datasets is no longer sufficient, as they lack long,\ncomplex arguments and realistic evidence from potentially misleading sources,\nlimiting holistic evaluation of retrieval effectiveness and argument quality.\nTo address these gaps, we investigate automated evaluation methods using\nmultiple fine-grained LLM judges, providing better and more interpretable\nassessments than traditional single-score metrics and even previously reported\nhuman crowdsourcing. To validate the proposed techniques, we introduce ConQRet,\na new benchmark featuring long and complex human-authored arguments on debated\ntopics, grounded in real-world websites, allowing an exhaustive evaluation\nacross retrieval effectiveness, argument quality, and groundedness. We validate\nour LLM Judges on a prior dataset and the new ConQRet benchmark. Our proposed\nLLM Judges and the ConQRet benchmark can enable rapid progress in computational\nargumentation and can be naturally extended to other complex\nretrieval-augmented generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational argumentation, which involves generating answers or summaries\nfor controversial topics like abortion bans and vaccination, has become\nincreasingly important in today's polarized environment. Sophisticated LLM\ncapabilities offer the potential to provide nuanced, evidence-based answers to\nsuch questions through Retrieval-Augmented Argumentation (RAArg), leveraging\nreal-world evidence for high-quality, grounded arguments. However, evaluating\nRAArg remains challenging, as human evaluation is costly and difficult for\ncomplex, lengthy answers on complicated topics. At the same time, re-using\nexisting argumentation datasets is no longer sufficient, as they lack long,\ncomplex arguments and realistic evidence from potentially misleading sources,\nlimiting holistic evaluation of retrieval effectiveness and argument quality.\nTo address these gaps, we investigate automated evaluation methods using\nmultiple fine-grained LLM judges, providing better and more interpretable\nassessments than traditional single-score metrics and even previously reported\nhuman crowdsourcing. To validate the proposed techniques, we introduce ConQRet,\na new benchmark featuring long and complex human-authored arguments on debated\ntopics, grounded in real-world websites, allowing an exhaustive evaluation\nacross retrieval effectiveness, argument quality, and groundedness. We validate\nour LLM Judges on a prior dataset and the new ConQRet benchmark. Our proposed\nLLM Judges and the ConQRet benchmark can enable rapid progress in computational\nargumentation and can be naturally extended to other complex\nretrieval-augmented generation tasks."
                },
                "authors": [
                    {
                        "name": "Kaustubh D. Dhole"
                    },
                    {
                        "name": "Kai Shu"
                    },
                    {
                        "name": "Eugene Agichtein"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Agichtein"
                },
                "author": "Eugene Agichtein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05206v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05200v1",
                "updated": "2024-12-06T17:28:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    28,
                    43,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:28:43Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    28,
                    43,
                    4,
                    341,
                    0
                ],
                "title": "Are Frontier Large Language Models Suitable for Q&A in Science Centres?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Frontier Large Language Models Suitable for Q&A in Science Centres?"
                },
                "summary": "This paper investigates the suitability of frontier Large Language Models\n(LLMs) for Q&A interactions in science centres, with the aim of boosting\nvisitor engagement while maintaining factual accuracy. Using a dataset of\nquestions collected from the National Space Centre in Leicester (UK), we\nevaluated responses generated by three leading models: OpenAI's GPT-4, Claude\n3.5 Sonnet, and Google Gemini 1.5. Each model was prompted for both standard\nand creative responses tailored to an 8-year-old audience, and these responses\nwere assessed by space science experts based on accuracy, engagement, clarity,\nnovelty, and deviation from expected answers. The results revealed a trade-off\nbetween creativity and accuracy, with Claude outperforming GPT and Gemini in\nboth maintaining clarity and engaging young audiences, even when asked to\ngenerate more creative responses. Nonetheless, experts observed that higher\nnovelty was generally associated with reduced factual reliability across all\nmodels. This study highlights the potential of LLMs in educational settings,\nemphasizing the need for careful prompt engineering to balance engagement with\nscientific rigor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the suitability of frontier Large Language Models\n(LLMs) for Q&A interactions in science centres, with the aim of boosting\nvisitor engagement while maintaining factual accuracy. Using a dataset of\nquestions collected from the National Space Centre in Leicester (UK), we\nevaluated responses generated by three leading models: OpenAI's GPT-4, Claude\n3.5 Sonnet, and Google Gemini 1.5. Each model was prompted for both standard\nand creative responses tailored to an 8-year-old audience, and these responses\nwere assessed by space science experts based on accuracy, engagement, clarity,\nnovelty, and deviation from expected answers. The results revealed a trade-off\nbetween creativity and accuracy, with Claude outperforming GPT and Gemini in\nboth maintaining clarity and engaging young audiences, even when asked to\ngenerate more creative responses. Nonetheless, experts observed that higher\nnovelty was generally associated with reduced factual reliability across all\nmodels. This study highlights the potential of LLMs in educational settings,\nemphasizing the need for careful prompt engineering to balance engagement with\nscientific rigor."
                },
                "authors": [
                    {
                        "name": "Jacob Watson"
                    },
                    {
                        "name": "Fabrício Góes"
                    },
                    {
                        "name": "Marco Volpe"
                    },
                    {
                        "name": "Talles Medeiros"
                    }
                ],
                "author_detail": {
                    "name": "Talles Medeiros"
                },
                "author": "Talles Medeiros",
                "arxiv_comment": "19 pages, 2 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03019v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03019v2",
                "updated": "2024-12-06T17:23:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    23,
                    53,
                    4,
                    341,
                    0
                ],
                "published": "2024-10-03T22:05:06Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    22,
                    5,
                    6,
                    3,
                    277,
                    0
                ],
                "title": "Is Your Paper Being Reviewed by an LLM? Investigating AI Text\n  Detectability in Peer Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Your Paper Being Reviewed by an LLM? Investigating AI Text\n  Detectability in Peer Review"
                },
                "summary": "Peer review is a critical process for ensuring the integrity of published\nscientific research. Confidence in this process is predicated on the assumption\nthat experts in the relevant domain give careful consideration to the merits of\nmanuscripts which are submitted for publication. With the recent rapid\nadvancements in the linguistic capabilities of large language models (LLMs), a\nnew potential risk to the peer review process is that negligent reviewers will\nrely on LLMs to perform the often time consuming process of reviewing a paper.\nIn this study, we investigate the ability of existing AI text detection\nalgorithms to distinguish between peer reviews written by humans and different\nstate-of-the-art LLMs. Our analysis shows that existing approaches fail to\nidentify many GPT-4o written reviews without also producing a high number of\nfalse positive classifications. To address this deficiency, we propose a new\ndetection approach which surpasses existing methods in the identification of\nGPT-4o written peer reviews at low levels of false positive classifications.\nOur work reveals the difficulty of accurately identifying AI-generated text at\nthe individual review level, highlighting the urgent need for new tools and\nmethods to detect this type of unethical application of generative AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peer review is a critical process for ensuring the integrity of published\nscientific research. Confidence in this process is predicated on the assumption\nthat experts in the relevant domain give careful consideration to the merits of\nmanuscripts which are submitted for publication. With the recent rapid\nadvancements in the linguistic capabilities of large language models (LLMs), a\nnew potential risk to the peer review process is that negligent reviewers will\nrely on LLMs to perform the often time consuming process of reviewing a paper.\nIn this study, we investigate the ability of existing AI text detection\nalgorithms to distinguish between peer reviews written by humans and different\nstate-of-the-art LLMs. Our analysis shows that existing approaches fail to\nidentify many GPT-4o written reviews without also producing a high number of\nfalse positive classifications. To address this deficiency, we propose a new\ndetection approach which surpasses existing methods in the identification of\nGPT-4o written peer reviews at low levels of false positive classifications.\nOur work reveals the difficulty of accurately identifying AI-generated text at\nthe individual review level, highlighting the urgent need for new tools and\nmethods to detect this type of unethical application of generative AI."
                },
                "authors": [
                    {
                        "name": "Sungduk Yu"
                    },
                    {
                        "name": "Man Luo"
                    },
                    {
                        "name": "Avinash Madasu"
                    },
                    {
                        "name": "Vasudev Lal"
                    },
                    {
                        "name": "Phillip Howard"
                    }
                ],
                "author_detail": {
                    "name": "Phillip Howard"
                },
                "author": "Phillip Howard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03019v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03019v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05195v1",
                "updated": "2024-12-06T17:18:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    18,
                    31,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:18:31Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    18,
                    31,
                    4,
                    341,
                    0
                ],
                "title": "Piecewise-linear modeling of multivariate geometric extremes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Piecewise-linear modeling of multivariate geometric extremes"
                },
                "summary": "A recent development in extreme value modeling uses the geometry of the\ndataset to perform inference on the multivariate tail. A key quantity in this\ninference is the gauge function, whose values define this geometry. Methodology\nproposed to date for capturing the gauge function either lacks flexibility due\nto parametric specifications, or relies on complex neural network\nspecifications in dimensions greater than three. We propose a semiparametric\ngauge function that is piecewise-linear, making it simple to interpret and\nprovides a good approximation for the true underlying gauge function. This\nlinearity also makes optimization tasks computationally inexpensive. The\npiecewise-linear gauge function can be used to define both a radial and an\nangular model, allowing for the joint fitting of extremal pseudo-polar\ncoordinates, a key aspect of this geometric framework. We further expand the\ntoolkit for geometric extremal modeling through the estimation of high radial\nquantiles at given angular values via kernel density estimation. We apply the\nnew methodology to air pollution data, which exhibits a complex extremal\ndependence structure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A recent development in extreme value modeling uses the geometry of the\ndataset to perform inference on the multivariate tail. A key quantity in this\ninference is the gauge function, whose values define this geometry. Methodology\nproposed to date for capturing the gauge function either lacks flexibility due\nto parametric specifications, or relies on complex neural network\nspecifications in dimensions greater than three. We propose a semiparametric\ngauge function that is piecewise-linear, making it simple to interpret and\nprovides a good approximation for the true underlying gauge function. This\nlinearity also makes optimization tasks computationally inexpensive. The\npiecewise-linear gauge function can be used to define both a radial and an\nangular model, allowing for the joint fitting of extremal pseudo-polar\ncoordinates, a key aspect of this geometric framework. We further expand the\ntoolkit for geometric extremal modeling through the estimation of high radial\nquantiles at given angular values via kernel density estimation. We apply the\nnew methodology to air pollution data, which exhibits a complex extremal\ndependence structure."
                },
                "authors": [
                    {
                        "name": "Ryan Campbell"
                    },
                    {
                        "name": "Jennifer Wadsworth"
                    }
                ],
                "author_detail": {
                    "name": "Jennifer Wadsworth"
                },
                "author": "Jennifer Wadsworth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05187v1",
                "updated": "2024-12-06T17:07:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    7,
                    27,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:07:27Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    7,
                    27,
                    4,
                    341,
                    0
                ],
                "title": "SurgBox: Agent-Driven Operating Room Sandbox with Surgery Copilot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SurgBox: Agent-Driven Operating Room Sandbox with Surgery Copilot"
                },
                "summary": "Surgical interventions, particularly in neurology, represent complex and\nhigh-stakes scenarios that impose substantial cognitive burdens on surgical\nteams. Although deliberate education and practice can enhance cognitive\ncapabilities, surgical training opportunities remain limited due to patient\nsafety concerns. To address these cognitive challenges in surgical training and\noperation, we propose SurgBox, an agent-driven sandbox framework to\nsystematically enhance the cognitive capabilities of surgeons in immersive\nsurgical simulations. Specifically, our SurgBox leverages large language models\n(LLMs) with tailored Retrieval-Augmented Generation (RAG) to authentically\nreplicate various surgical roles, enabling realistic training environments for\ndeliberate practice. In particular, we devise Surgery Copilot, an AI-driven\nassistant to actively coordinate the surgical information stream and support\nclinical decision-making, thereby diminishing the cognitive workload of\nsurgical teams during surgery. By incorporating a novel Long-Short Memory\nmechanism, our Surgery Copilot can effectively balance immediate procedural\nassistance with comprehensive surgical knowledge. Extensive experiments using\nreal neurosurgical procedure records validate our SurgBox framework in both\nenhancing surgical cognitive capabilities and supporting clinical\ndecision-making. By providing an integrated solution for training and\noperational support to address cognitive challenges, our SurgBox framework\nadvances surgical education and practice, potentially transforming surgical\noutcomes and healthcare quality. The code is available at\nhttps://github.com/franciszchen/SurgBox.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical interventions, particularly in neurology, represent complex and\nhigh-stakes scenarios that impose substantial cognitive burdens on surgical\nteams. Although deliberate education and practice can enhance cognitive\ncapabilities, surgical training opportunities remain limited due to patient\nsafety concerns. To address these cognitive challenges in surgical training and\noperation, we propose SurgBox, an agent-driven sandbox framework to\nsystematically enhance the cognitive capabilities of surgeons in immersive\nsurgical simulations. Specifically, our SurgBox leverages large language models\n(LLMs) with tailored Retrieval-Augmented Generation (RAG) to authentically\nreplicate various surgical roles, enabling realistic training environments for\ndeliberate practice. In particular, we devise Surgery Copilot, an AI-driven\nassistant to actively coordinate the surgical information stream and support\nclinical decision-making, thereby diminishing the cognitive workload of\nsurgical teams during surgery. By incorporating a novel Long-Short Memory\nmechanism, our Surgery Copilot can effectively balance immediate procedural\nassistance with comprehensive surgical knowledge. Extensive experiments using\nreal neurosurgical procedure records validate our SurgBox framework in both\nenhancing surgical cognitive capabilities and supporting clinical\ndecision-making. By providing an integrated solution for training and\noperational support to address cognitive challenges, our SurgBox framework\nadvances surgical education and practice, potentially transforming surgical\noutcomes and healthcare quality. The code is available at\nhttps://github.com/franciszchen/SurgBox."
                },
                "authors": [
                    {
                        "name": "Jinlin Wu"
                    },
                    {
                        "name": "Xusheng Liang"
                    },
                    {
                        "name": "Xuexue Bai"
                    },
                    {
                        "name": "Zhen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Chen"
                },
                "author": "Zhen Chen",
                "arxiv_comment": "This work is accepted by IEEE Big Data 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05185v1",
                "updated": "2024-12-06T17:04:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    4,
                    42,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:04:42Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    4,
                    42,
                    4,
                    341,
                    0
                ],
                "title": "LinVT: Empower Your Image-level Large Language Model to Understand\n  Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LinVT: Empower Your Image-level Large Language Model to Understand\n  Videos"
                },
                "summary": "Large Language Models (LLMs) have been widely used in various tasks,\nmotivating us to develop an LLM-based assistant for videos. Instead of training\nfrom scratch, we propose a module to transform arbitrary well-trained\nimage-based LLMs into video-LLMs (after being trained on video data). To better\nadapt image-LLMs for processing videos, we introduce two design principles:\nlinear transformation to preserve the original visual-language alignment and\nrepresentative information condensation from redundant video content. Guided by\nthese principles, we propose a plug-and-play Linear Video Tokenizer(LinVT),\nwhich enables existing image-LLMs to understand videos. We benchmark LinVT with\nsix recent visual LLMs: Aquila, Blip-3, InternVL2, Mipha, Molmo and Qwen2-VL,\nshowcasing the high compatibility of LinVT. LinVT-based LLMs achieve\nstate-of-the-art performance across various video benchmarks, illustrating the\neffectiveness of LinVT in multi-modal video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely used in various tasks,\nmotivating us to develop an LLM-based assistant for videos. Instead of training\nfrom scratch, we propose a module to transform arbitrary well-trained\nimage-based LLMs into video-LLMs (after being trained on video data). To better\nadapt image-LLMs for processing videos, we introduce two design principles:\nlinear transformation to preserve the original visual-language alignment and\nrepresentative information condensation from redundant video content. Guided by\nthese principles, we propose a plug-and-play Linear Video Tokenizer(LinVT),\nwhich enables existing image-LLMs to understand videos. We benchmark LinVT with\nsix recent visual LLMs: Aquila, Blip-3, InternVL2, Mipha, Molmo and Qwen2-VL,\nshowcasing the high compatibility of LinVT. LinVT-based LLMs achieve\nstate-of-the-art performance across various video benchmarks, illustrating the\neffectiveness of LinVT in multi-modal video understanding."
                },
                "authors": [
                    {
                        "name": "Lishuai Gao"
                    },
                    {
                        "name": "Yujie Zhong"
                    },
                    {
                        "name": "Yingsen Zeng"
                    },
                    {
                        "name": "Haoxian Tan"
                    },
                    {
                        "name": "Dengjie Li"
                    },
                    {
                        "name": "Zheng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Zhao"
                },
                "author": "Zheng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05184v1",
                "updated": "2024-12-06T17:04:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    4,
                    21,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:04:21Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    4,
                    21,
                    4,
                    341,
                    0
                ],
                "title": "QueEn: A Large Language Model for Quechua-English Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QueEn: A Large Language Model for Quechua-English Translation"
                },
                "summary": "Recent studies show that large language models (LLMs) are powerful tools for\nworking with natural language, bringing advances in many areas of computational\nlinguistics. However, these models face challenges when applied to low-resource\nlanguages due to limited training data and difficulty in understanding cultural\nnuances. In this paper, we propose QueEn, a novel approach for Quechua-English\ntranslation that combines Retrieval-Augmented Generation (RAG) with\nparameter-efficient fine-tuning techniques. Our method leverages external\nlinguistic resources through RAG and uses Low-Rank Adaptation (LoRA) for\nefficient model adaptation. Experimental results show that our approach\nsubstantially exceeds baseline models, with a BLEU score of 17.6 compared to\n1.5 for standard GPT models. The integration of RAG with fine-tuning allows our\nsystem to address the challenges of low-resource language translation while\nmaintaining computational efficiency. This work contributes to the broader goal\nof preserving endangered languages through advanced language technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies show that large language models (LLMs) are powerful tools for\nworking with natural language, bringing advances in many areas of computational\nlinguistics. However, these models face challenges when applied to low-resource\nlanguages due to limited training data and difficulty in understanding cultural\nnuances. In this paper, we propose QueEn, a novel approach for Quechua-English\ntranslation that combines Retrieval-Augmented Generation (RAG) with\nparameter-efficient fine-tuning techniques. Our method leverages external\nlinguistic resources through RAG and uses Low-Rank Adaptation (LoRA) for\nefficient model adaptation. Experimental results show that our approach\nsubstantially exceeds baseline models, with a BLEU score of 17.6 compared to\n1.5 for standard GPT models. The integration of RAG with fine-tuning allows our\nsystem to address the challenges of low-resource language translation while\nmaintaining computational efficiency. This work contributes to the broader goal\nof preserving endangered languages through advanced language technologies."
                },
                "authors": [
                    {
                        "name": "Junhao Chen"
                    },
                    {
                        "name": "Peng Shu"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Huaqin Zhao"
                    },
                    {
                        "name": "Hanqi Jiang"
                    },
                    {
                        "name": "Yi Pan"
                    },
                    {
                        "name": "Yifan Zhou"
                    },
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Lewis C Howe"
                    },
                    {
                        "name": "Tianming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tianming Liu"
                },
                "author": "Tianming Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05183v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05183v1",
                "updated": "2024-12-06T17:04:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    4,
                    9,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:04:09Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    4,
                    9,
                    4,
                    341,
                    0
                ],
                "title": "Privacy Drift: Evolving Privacy Concerns in Incremental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy Drift: Evolving Privacy Concerns in Incremental Learning"
                },
                "summary": "In the evolving landscape of machine learning (ML), Federated Learning (FL)\npresents a paradigm shift towards decentralized model training while preserving\nuser data privacy. This paper introduces the concept of ``privacy drift\", an\ninnovative framework that parallels the well-known phenomenon of concept drift.\nWhile concept drift addresses the variability in model accuracy over time due\nto changes in the data, privacy drift encapsulates the variation in the leakage\nof private information as models undergo incremental training. By defining and\nexamining privacy drift, this study aims to unveil the nuanced relationship\nbetween the evolution of model performance and the integrity of data privacy.\nThrough rigorous experimentation, we investigate the dynamics of privacy drift\nin FL systems, focusing on how model updates and data distribution shifts\ninfluence the susceptibility of models to privacy attacks, such as membership\ninference attacks (MIA). Our results highlight a complex interplay between\nmodel accuracy and privacy safeguards, revealing that enhancements in model\nperformance can lead to increased privacy risks. We provide empirical evidence\nfrom experiments on customized datasets derived from CIFAR-100 (Canadian\nInstitute for Advanced Research, 100 classes), showcasing the impact of data\nand concept drift on privacy. This work lays the groundwork for future research\non privacy-aware machine learning, aiming to achieve a delicate balance between\nmodel accuracy and data privacy in decentralized environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the evolving landscape of machine learning (ML), Federated Learning (FL)\npresents a paradigm shift towards decentralized model training while preserving\nuser data privacy. This paper introduces the concept of ``privacy drift\", an\ninnovative framework that parallels the well-known phenomenon of concept drift.\nWhile concept drift addresses the variability in model accuracy over time due\nto changes in the data, privacy drift encapsulates the variation in the leakage\nof private information as models undergo incremental training. By defining and\nexamining privacy drift, this study aims to unveil the nuanced relationship\nbetween the evolution of model performance and the integrity of data privacy.\nThrough rigorous experimentation, we investigate the dynamics of privacy drift\nin FL systems, focusing on how model updates and data distribution shifts\ninfluence the susceptibility of models to privacy attacks, such as membership\ninference attacks (MIA). Our results highlight a complex interplay between\nmodel accuracy and privacy safeguards, revealing that enhancements in model\nperformance can lead to increased privacy risks. We provide empirical evidence\nfrom experiments on customized datasets derived from CIFAR-100 (Canadian\nInstitute for Advanced Research, 100 classes), showcasing the impact of data\nand concept drift on privacy. This work lays the groundwork for future research\non privacy-aware machine learning, aiming to achieve a delicate balance between\nmodel accuracy and data privacy in decentralized environments."
                },
                "authors": [
                    {
                        "name": "Sayyed Farid Ahamed"
                    },
                    {
                        "name": "Soumya Banerjee"
                    },
                    {
                        "name": "Sandip Roy"
                    },
                    {
                        "name": "Aayush Kapoor"
                    },
                    {
                        "name": "Marc Vucovich"
                    },
                    {
                        "name": "Kevin Choi"
                    },
                    {
                        "name": "Abdul Rahman"
                    },
                    {
                        "name": "Edward Bowen"
                    },
                    {
                        "name": "Sachin Shetty"
                    }
                ],
                "author_detail": {
                    "name": "Sachin Shetty"
                },
                "author": "Sachin Shetty",
                "arxiv_comment": "6 pages, 7 figures, Accepted in IEEE ICNC 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05183v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05183v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03603v2",
                "updated": "2024-12-06T17:02:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    2,
                    10,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-03T23:52:37Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    23,
                    52,
                    37,
                    1,
                    338,
                    0
                ],
                "title": "HunyuanVideo: A Systematic Framework For Large Video Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
                },
                "summary": "Recent advancements in video generation have significantly impacted daily\nlife for both individuals and industries. However, the leading video generation\nmodels remain closed-source, resulting in a notable performance gap between\nindustry capabilities and those available to the public. In this report, we\nintroduce HunyuanVideo, an innovative open-source video foundation model that\ndemonstrates performance in video generation comparable to, or even surpassing,\nthat of leading closed-source models. HunyuanVideo encompasses a comprehensive\nframework that integrates several key elements, including data curation,\nadvanced architectural design, progressive model scaling and training, and an\nefficient infrastructure tailored for large-scale model training and inference.\nAs a result, we successfully trained a video generative model with over 13\nbillion parameters, making it the largest among all open-source models. We\nconducted extensive experiments and implemented a series of targeted designs to\nensure high visual quality, motion dynamics, text-video alignment, and advanced\nfilming techniques. According to evaluations by professionals, HunyuanVideo\noutperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6,\nand three top-performing Chinese video generative models. By releasing the code\nfor the foundation model and its applications, we aim to bridge the gap between\nclosed-source and open-source communities. This initiative will empower\nindividuals within the community to experiment with their ideas, fostering a\nmore dynamic and vibrant video generation ecosystem. The code is publicly\navailable at https://github.com/Tencent/HunyuanVideo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in video generation have significantly impacted daily\nlife for both individuals and industries. However, the leading video generation\nmodels remain closed-source, resulting in a notable performance gap between\nindustry capabilities and those available to the public. In this report, we\nintroduce HunyuanVideo, an innovative open-source video foundation model that\ndemonstrates performance in video generation comparable to, or even surpassing,\nthat of leading closed-source models. HunyuanVideo encompasses a comprehensive\nframework that integrates several key elements, including data curation,\nadvanced architectural design, progressive model scaling and training, and an\nefficient infrastructure tailored for large-scale model training and inference.\nAs a result, we successfully trained a video generative model with over 13\nbillion parameters, making it the largest among all open-source models. We\nconducted extensive experiments and implemented a series of targeted designs to\nensure high visual quality, motion dynamics, text-video alignment, and advanced\nfilming techniques. According to evaluations by professionals, HunyuanVideo\noutperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6,\nand three top-performing Chinese video generative models. By releasing the code\nfor the foundation model and its applications, we aim to bridge the gap between\nclosed-source and open-source communities. This initiative will empower\nindividuals within the community to experiment with their ideas, fostering a\nmore dynamic and vibrant video generation ecosystem. The code is publicly\navailable at https://github.com/Tencent/HunyuanVideo."
                },
                "authors": [
                    {
                        "name": "Weijie Kong"
                    },
                    {
                        "name": "Qi Tian"
                    },
                    {
                        "name": "Zijian Zhang"
                    },
                    {
                        "name": "Rox Min"
                    },
                    {
                        "name": "Zuozhuo Dai"
                    },
                    {
                        "name": "Jin Zhou"
                    },
                    {
                        "name": "Jiangfeng Xiong"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Bo Wu"
                    },
                    {
                        "name": "Jianwei Zhang"
                    },
                    {
                        "name": "Kathrina Wu"
                    },
                    {
                        "name": "Qin Lin"
                    },
                    {
                        "name": "Junkun Yuan"
                    },
                    {
                        "name": "Yanxin Long"
                    },
                    {
                        "name": "Aladdin Wang"
                    },
                    {
                        "name": "Andong Wang"
                    },
                    {
                        "name": "Changlin Li"
                    },
                    {
                        "name": "Duojun Huang"
                    },
                    {
                        "name": "Fang Yang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Hongmei Wang"
                    },
                    {
                        "name": "Jacob Song"
                    },
                    {
                        "name": "Jiawang Bai"
                    },
                    {
                        "name": "Jianbing Wu"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Joey Wang"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Mengyang Liu"
                    },
                    {
                        "name": "Pengyu Li"
                    },
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Weiyan Wang"
                    },
                    {
                        "name": "Wenqing Yu"
                    },
                    {
                        "name": "Xinchi Deng"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yi Chen"
                    },
                    {
                        "name": "Yutao Cui"
                    },
                    {
                        "name": "Yuanbo Peng"
                    },
                    {
                        "name": "Zhentao Yu"
                    },
                    {
                        "name": "Zhiyu He"
                    },
                    {
                        "name": "Zhiyong Xu"
                    },
                    {
                        "name": "Zixiang Zhou"
                    },
                    {
                        "name": "Zunnan Xu"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Qinglin Lu"
                    },
                    {
                        "name": "Songtao Liu"
                    },
                    {
                        "name": "Daquan Zhou"
                    },
                    {
                        "name": "Hongfa Wang"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Caesar Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Caesar Zhong"
                },
                "arxiv_affiliation": "refer to the report for detailed contributions",
                "author": "Caesar Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06467v2",
                "updated": "2024-12-06T16:50:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    16,
                    50,
                    13,
                    4,
                    341,
                    0
                ],
                "published": "2024-10-09T01:41:14Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    1,
                    41,
                    14,
                    2,
                    283,
                    0
                ],
                "title": "WAPITI: A Watermark for Finetuned Open-Source LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WAPITI: A Watermark for Finetuned Open-Source LLMs"
                },
                "summary": "Watermarking of large language models (LLMs) generation embeds an\nimperceptible statistical pattern within texts, making it algorithmically\ndetectable. Watermarking is a promising method for addressing potential harm\nand biases from LLMs, as it enables traceability, accountability, and detection\nof manipulated content, helping to mitigate unintended consequences. However,\nfor open-source models, watermarking faces two major challenges: (i)\nincompatibility with fine-tuned models, and (ii) vulnerability to fine-tuning\nattacks. In this work, we propose WAPITI, a new method that transfers\nwatermarking from base models to fine-tuned models through parameter\nintegration. To the best of our knowledge, we propose the first watermark for\nfine-tuned open-source LLMs that preserves their fine-tuned capabilities.\nFurthermore, our approach offers an effective defense against fine-tuning\nattacks. We test our method on various model architectures and watermarking\nstrategies. Results demonstrate that our method can successfully inject\nwatermarks and is highly compatible with fine-tuned models. Additionally, we\noffer an in-depth analysis of how parameter editing influences the watermark\nstrength and overall capabilities of the resulting models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking of large language models (LLMs) generation embeds an\nimperceptible statistical pattern within texts, making it algorithmically\ndetectable. Watermarking is a promising method for addressing potential harm\nand biases from LLMs, as it enables traceability, accountability, and detection\nof manipulated content, helping to mitigate unintended consequences. However,\nfor open-source models, watermarking faces two major challenges: (i)\nincompatibility with fine-tuned models, and (ii) vulnerability to fine-tuning\nattacks. In this work, we propose WAPITI, a new method that transfers\nwatermarking from base models to fine-tuned models through parameter\nintegration. To the best of our knowledge, we propose the first watermark for\nfine-tuned open-source LLMs that preserves their fine-tuned capabilities.\nFurthermore, our approach offers an effective defense against fine-tuning\nattacks. We test our method on various model architectures and watermarking\nstrategies. Results demonstrate that our method can successfully inject\nwatermarks and is highly compatible with fine-tuned models. Additionally, we\noffer an in-depth analysis of how parameter editing influences the watermark\nstrength and overall capabilities of the resulting models."
                },
                "authors": [
                    {
                        "name": "Lingjie Chen"
                    },
                    {
                        "name": "Ruizhong Qiu"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Zhining Liu"
                    },
                    {
                        "name": "Tianxin Wei"
                    },
                    {
                        "name": "Hyunsik Yoo"
                    },
                    {
                        "name": "Zhichen Zeng"
                    },
                    {
                        "name": "Deqing Yang"
                    },
                    {
                        "name": "Hanghang Tong"
                    }
                ],
                "author_detail": {
                    "name": "Hanghang Tong"
                },
                "author": "Hanghang Tong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02263v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02263v2",
                "updated": "2024-12-06T16:43:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    16,
                    43,
                    58,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-03T08:35:51Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    35,
                    51,
                    1,
                    338,
                    0
                ],
                "title": "Connecting Large Language Models with Blockchain: Advancing the\n  Evolution of Smart Contracts from Automation to Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Connecting Large Language Models with Blockchain: Advancing the\n  Evolution of Smart Contracts from Automation to Intelligence"
                },
                "summary": "Blockchain smart contracts have catalyzed the development of decentralized\napplications across various domains, including decentralized finance. However,\ndue to constraints in computational resources and the prevalence of data silos,\ncurrent smart contracts face significant challenges in fully leveraging the\npowerful capabilities of Large Language Models (LLMs) for tasks such as\nintelligent analysis and reasoning. To address this gap, this paper proposes\nand implements a universal framework for integrating LLMs with blockchain data,\n{\\sysname}, effectively overcoming the interoperability barriers between\nblockchain and LLMs. By combining semantic relatedness with truth discovery\nmethods, we introduce an innovative data aggregation approach, {\\funcname},\nwhich significantly enhances the accuracy and trustworthiness of data generated\nby LLMs. To validate the framework's effectiveness, we construct a dataset\nconsisting of three types of questions, capturing Q\\&A interactions between 10\noracle nodes and 5 LLM models. Experimental results demonstrate that, even with\n40\\% malicious nodes, the proposed solution improves data accuracy by an\naverage of 17.74\\% compared to the optimal baseline. This research not only\nprovides an innovative solution for the intelligent enhancement of smart\ncontracts but also highlights the potential for deep integration between LLMs\nand blockchain technology, paving the way for more intelligent and complex\napplications of smart contracts in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blockchain smart contracts have catalyzed the development of decentralized\napplications across various domains, including decentralized finance. However,\ndue to constraints in computational resources and the prevalence of data silos,\ncurrent smart contracts face significant challenges in fully leveraging the\npowerful capabilities of Large Language Models (LLMs) for tasks such as\nintelligent analysis and reasoning. To address this gap, this paper proposes\nand implements a universal framework for integrating LLMs with blockchain data,\n{\\sysname}, effectively overcoming the interoperability barriers between\nblockchain and LLMs. By combining semantic relatedness with truth discovery\nmethods, we introduce an innovative data aggregation approach, {\\funcname},\nwhich significantly enhances the accuracy and trustworthiness of data generated\nby LLMs. To validate the framework's effectiveness, we construct a dataset\nconsisting of three types of questions, capturing Q\\&A interactions between 10\noracle nodes and 5 LLM models. Experimental results demonstrate that, even with\n40\\% malicious nodes, the proposed solution improves data accuracy by an\naverage of 17.74\\% compared to the optimal baseline. This research not only\nprovides an innovative solution for the intelligent enhancement of smart\ncontracts but also highlights the potential for deep integration between LLMs\nand blockchain technology, paving the way for more intelligent and complex\napplications of smart contracts in the future."
                },
                "authors": [
                    {
                        "name": "Youquan Xian"
                    },
                    {
                        "name": "Xueying Zeng"
                    },
                    {
                        "name": "Duancheng Xuan"
                    },
                    {
                        "name": "Danping Yang"
                    },
                    {
                        "name": "Chunpei Li"
                    },
                    {
                        "name": "Peng Fan"
                    },
                    {
                        "name": "Peng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Peng Liu"
                },
                "author": "Peng Liu",
                "arxiv_comment": "10 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02263v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02263v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05164v1",
                "updated": "2024-12-06T16:29:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    16,
                    29,
                    53,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T16:29:53Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    16,
                    29,
                    53,
                    4,
                    341,
                    0
                ],
                "title": "A Differentially Private Kaplan-Meier Estimator for Privacy-Preserving\n  Survival Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Differentially Private Kaplan-Meier Estimator for Privacy-Preserving\n  Survival Analysis"
                },
                "summary": "This paper presents a differentially private approach to Kaplan-Meier\nestimation that achieves accurate survival probability estimates while\nsafeguarding individual privacy. The Kaplan-Meier estimator is widely used in\nsurvival analysis to estimate survival functions over time, yet applying it to\nsensitive datasets, such as clinical records, risks revealing private\ninformation. To address this, we introduce a novel algorithm that applies\ntime-indexed Laplace noise, dynamic clipping, and smoothing to produce a\nprivacy-preserving survival curve while maintaining the cumulative structure of\nthe Kaplan-Meier estimator. By scaling noise over time, the algorithm accounts\nfor decreasing sensitivity as fewer individuals remain at risk, while dynamic\nclipping and smoothing prevent extreme values and reduce fluctuations,\npreserving the natural shape of the survival curve.\n  Our results, evaluated on the NCCTG lung cancer dataset, show that the\nproposed method effectively lowers root mean squared error (RMSE) and enhances\naccuracy across privacy budgets ($\\epsilon$). At $\\epsilon = 10$, the algorithm\nachieves an RMSE as low as 0.04, closely approximating non-private estimates.\nAdditionally, membership inference attacks reveal that higher $\\epsilon$ values\n(e.g., $\\epsilon \\geq 6$) significantly reduce influential points, particularly\nat higher thresholds, lowering susceptibility to inference attacks. These\nfindings confirm that our approach balances privacy and utility, advancing\nprivacy-preserving survival analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a differentially private approach to Kaplan-Meier\nestimation that achieves accurate survival probability estimates while\nsafeguarding individual privacy. The Kaplan-Meier estimator is widely used in\nsurvival analysis to estimate survival functions over time, yet applying it to\nsensitive datasets, such as clinical records, risks revealing private\ninformation. To address this, we introduce a novel algorithm that applies\ntime-indexed Laplace noise, dynamic clipping, and smoothing to produce a\nprivacy-preserving survival curve while maintaining the cumulative structure of\nthe Kaplan-Meier estimator. By scaling noise over time, the algorithm accounts\nfor decreasing sensitivity as fewer individuals remain at risk, while dynamic\nclipping and smoothing prevent extreme values and reduce fluctuations,\npreserving the natural shape of the survival curve.\n  Our results, evaluated on the NCCTG lung cancer dataset, show that the\nproposed method effectively lowers root mean squared error (RMSE) and enhances\naccuracy across privacy budgets ($\\epsilon$). At $\\epsilon = 10$, the algorithm\nachieves an RMSE as low as 0.04, closely approximating non-private estimates.\nAdditionally, membership inference attacks reveal that higher $\\epsilon$ values\n(e.g., $\\epsilon \\geq 6$) significantly reduce influential points, particularly\nat higher thresholds, lowering susceptibility to inference attacks. These\nfindings confirm that our approach balances privacy and utility, advancing\nprivacy-preserving survival analysis."
                },
                "authors": [
                    {
                        "name": "Narasimha Raghavan Veeraragavan"
                    },
                    {
                        "name": "Sai Praneeth Karimireddy"
                    },
                    {
                        "name": "Jan Franz Nygård"
                    }
                ],
                "author_detail": {
                    "name": "Jan Franz Nygård"
                },
                "author": "Jan Franz Nygård",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12537v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12537v2",
                "updated": "2024-12-06T16:22:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    16,
                    22,
                    21,
                    4,
                    341,
                    0
                ],
                "published": "2024-11-19T14:35:38Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    14,
                    35,
                    38,
                    1,
                    324,
                    0
                ],
                "title": "Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues"
                },
                "summary": "Linear Recurrent Neural Networks (LRNNs) such as Mamba, RWKV, GLA, mLSTM, and\nDeltaNet have emerged as efficient alternatives to Transformers in large\nlanguage modeling, offering linear scaling with sequence length and improved\ntraining efficiency. However, LRNNs struggle to perform state-tracking which\nmay impair performance in tasks such as code evaluation or tracking a chess\ngame. Even parity, the simplest state-tracking task, which non-linear RNNs like\nLSTM handle effectively, cannot be solved by current LRNNs. Recently, Sarrof et\nal. (2024) demonstrated that the failure of LRNNs like Mamba to solve parity\nstems from restricting the value range of their diagonal state-transition\nmatrices to $[0, 1]$ and that incorporating negative values can resolve this\nissue. We extend this result to non-diagonal LRNNs, which have recently shown\npromise in models such as DeltaNet. We prove that finite precision LRNNs with\nstate-transition matrices having only positive eigenvalues cannot solve parity,\nwhile complex eigenvalues are needed to count modulo $3$. Notably, we also\nprove that LRNNs can learn any regular language when their state-transition\nmatrices are products of identity minus vector outer product matrices, each\nwith eigenvalues in the range $[-1, 1]$. Our empirical results confirm that\nextending the eigenvalue range of models like Mamba and DeltaNet to include\nnegative values not only enables them to solve parity but consistently improves\ntheir performance on state-tracking tasks. Furthermore, pre-training LRNNs with\nan extended eigenvalue range for language modeling achieves comparable\nperformance and stability while showing promise on code and math data. Our work\nenhances the expressivity of modern LRNNs, broadening their applicability\nwithout changing the cost of training or inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Recurrent Neural Networks (LRNNs) such as Mamba, RWKV, GLA, mLSTM, and\nDeltaNet have emerged as efficient alternatives to Transformers in large\nlanguage modeling, offering linear scaling with sequence length and improved\ntraining efficiency. However, LRNNs struggle to perform state-tracking which\nmay impair performance in tasks such as code evaluation or tracking a chess\ngame. Even parity, the simplest state-tracking task, which non-linear RNNs like\nLSTM handle effectively, cannot be solved by current LRNNs. Recently, Sarrof et\nal. (2024) demonstrated that the failure of LRNNs like Mamba to solve parity\nstems from restricting the value range of their diagonal state-transition\nmatrices to $[0, 1]$ and that incorporating negative values can resolve this\nissue. We extend this result to non-diagonal LRNNs, which have recently shown\npromise in models such as DeltaNet. We prove that finite precision LRNNs with\nstate-transition matrices having only positive eigenvalues cannot solve parity,\nwhile complex eigenvalues are needed to count modulo $3$. Notably, we also\nprove that LRNNs can learn any regular language when their state-transition\nmatrices are products of identity minus vector outer product matrices, each\nwith eigenvalues in the range $[-1, 1]$. Our empirical results confirm that\nextending the eigenvalue range of models like Mamba and DeltaNet to include\nnegative values not only enables them to solve parity but consistently improves\ntheir performance on state-tracking tasks. Furthermore, pre-training LRNNs with\nan extended eigenvalue range for language modeling achieves comparable\nperformance and stability while showing promise on code and math data. Our work\nenhances the expressivity of modern LRNNs, broadening their applicability\nwithout changing the cost of training or inference."
                },
                "authors": [
                    {
                        "name": "Riccardo Grazzi"
                    },
                    {
                        "name": "Julien Siems"
                    },
                    {
                        "name": "Jörg K. H. Franke"
                    },
                    {
                        "name": "Arber Zela"
                    },
                    {
                        "name": "Frank Hutter"
                    },
                    {
                        "name": "Massimiliano Pontil"
                    }
                ],
                "author_detail": {
                    "name": "Massimiliano Pontil"
                },
                "author": "Massimiliano Pontil",
                "arxiv_comment": "Main changes: Correction to Theorem 1 and 2 (we excluded from the\n  only if condition complex eigenvalues with modulus strictly less than one).\n  Correction to point 3 of Proposition 3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12537v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12537v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05153v1",
                "updated": "2024-12-06T16:10:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    16,
                    10,
                    40,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T16:10:40Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    16,
                    10,
                    40,
                    4,
                    341,
                    0
                ],
                "title": "A text-to-tabular approach to generate synthetic patient data using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A text-to-tabular approach to generate synthetic patient data using LLMs"
                },
                "summary": "Access to large-scale high-quality healthcare databases is key to accelerate\nmedical research and make insightful discoveries about diseases. However,\naccess to such data is often limited by patient privacy concerns, data sharing\nrestrictions and high costs. To overcome these limitations, synthetic patient\ndata has emerged as an alternative. However, synthetic data generation (SDG)\nmethods typically rely on machine learning (ML) models trained on original\ndata, leading back to the data scarcity problem. We propose an approach to\ngenerate synthetic tabular patient data that does not require access to the\noriginal data, but only a description of the desired database. We leverage\nprior medical knowledge and in-context learning capabilities of large language\nmodels (LLMs) to generate realistic patient data, even in a low-resource\nsetting. We quantitatively evaluate our approach against state-of-the-art SDG\nmodels, using fidelity, privacy, and utility metrics. Our results show that\nwhile LLMs may not match the performance of state-of-the-art models trained on\nthe original data, they effectively generate realistic patient data with\nwell-preserved clinical correlations. An ablation study highlights key elements\nof our prompt contributing to high-quality synthetic patient data generation.\nThis approach, which is easy to use and does not require original data or\nadvanced ML skills, is particularly valuable for quickly generating\ncustom-designed patient data, supporting project implementation and providing\neducational resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Access to large-scale high-quality healthcare databases is key to accelerate\nmedical research and make insightful discoveries about diseases. However,\naccess to such data is often limited by patient privacy concerns, data sharing\nrestrictions and high costs. To overcome these limitations, synthetic patient\ndata has emerged as an alternative. However, synthetic data generation (SDG)\nmethods typically rely on machine learning (ML) models trained on original\ndata, leading back to the data scarcity problem. We propose an approach to\ngenerate synthetic tabular patient data that does not require access to the\noriginal data, but only a description of the desired database. We leverage\nprior medical knowledge and in-context learning capabilities of large language\nmodels (LLMs) to generate realistic patient data, even in a low-resource\nsetting. We quantitatively evaluate our approach against state-of-the-art SDG\nmodels, using fidelity, privacy, and utility metrics. Our results show that\nwhile LLMs may not match the performance of state-of-the-art models trained on\nthe original data, they effectively generate realistic patient data with\nwell-preserved clinical correlations. An ablation study highlights key elements\nof our prompt contributing to high-quality synthetic patient data generation.\nThis approach, which is easy to use and does not require original data or\nadvanced ML skills, is particularly valuable for quickly generating\ncustom-designed patient data, supporting project implementation and providing\neducational resources."
                },
                "authors": [
                    {
                        "name": "Margaux Tornqvist"
                    },
                    {
                        "name": "Jean-Daniel Zucker"
                    },
                    {
                        "name": "Tristan Fauvel"
                    },
                    {
                        "name": "Nicolas Lambert"
                    },
                    {
                        "name": "Mathilde Berthelot"
                    },
                    {
                        "name": "Antoine Movschin"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Movschin"
                },
                "author": "Antoine Movschin",
                "arxiv_comment": "12 pages, 2 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05145v1",
                "updated": "2024-12-06T16:01:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    16,
                    1,
                    30,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T16:01:30Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    16,
                    1,
                    30,
                    4,
                    341,
                    0
                ],
                "title": "Explingo: Explaining AI Predictions using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explingo: Explaining AI Predictions using Large Language Models"
                },
                "summary": "Explanations of machine learning (ML) model predictions generated by\nExplainable AI (XAI) techniques such as SHAP are essential for people using ML\noutputs for decision-making. We explore the potential of Large Language Models\n(LLMs) to transform these explanations into human-readable, narrative formats\nthat align with natural communication. We address two key research questions:\n(1) Can LLMs reliably transform traditional explanations into high-quality\nnarratives? and (2) How can we effectively evaluate the quality of narrative\nexplanations? To answer these questions, we introduce Explingo, which consists\nof two LLM-based subsystems, a Narrator and Grader. The Narrator takes in ML\nexplanations and transforms them into natural-language descriptions. The Grader\nscores these narratives on a set of metrics including accuracy, completeness,\nfluency, and conciseness.\n  Our experiments demonstrate that LLMs can generate high-quality narratives\nthat achieve high scores across all metrics, particularly when guided by a\nsmall number of human-labeled and bootstrapped examples. We also identified\nareas that remain challenging, in particular for effectively scoring narratives\nin complex domains. The findings from this work have been integrated into an\nopen-source tool that makes narrative explanations available for further\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explanations of machine learning (ML) model predictions generated by\nExplainable AI (XAI) techniques such as SHAP are essential for people using ML\noutputs for decision-making. We explore the potential of Large Language Models\n(LLMs) to transform these explanations into human-readable, narrative formats\nthat align with natural communication. We address two key research questions:\n(1) Can LLMs reliably transform traditional explanations into high-quality\nnarratives? and (2) How can we effectively evaluate the quality of narrative\nexplanations? To answer these questions, we introduce Explingo, which consists\nof two LLM-based subsystems, a Narrator and Grader. The Narrator takes in ML\nexplanations and transforms them into natural-language descriptions. The Grader\nscores these narratives on a set of metrics including accuracy, completeness,\nfluency, and conciseness.\n  Our experiments demonstrate that LLMs can generate high-quality narratives\nthat achieve high scores across all metrics, particularly when guided by a\nsmall number of human-labeled and bootstrapped examples. We also identified\nareas that remain challenging, in particular for effectively scoring narratives\nin complex domains. The findings from this work have been integrated into an\nopen-source tool that makes narrative explanations available for further\napplications."
                },
                "authors": [
                    {
                        "name": "Alexandra Zytek"
                    },
                    {
                        "name": "Sara Pido"
                    },
                    {
                        "name": "Sarah Alnegheimish"
                    },
                    {
                        "name": "Laure Berti-Equille"
                    },
                    {
                        "name": "Kalyan Veeramachaneni"
                    }
                ],
                "author_detail": {
                    "name": "Kalyan Veeramachaneni"
                },
                "author": "Kalyan Veeramachaneni",
                "arxiv_comment": "To be presented in the 2024 IEEE International Conference on Big Data\n  (IEEE BigData)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05137v1",
                "updated": "2024-12-06T15:51:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    15,
                    51,
                    22,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T15:51:22Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    15,
                    51,
                    22,
                    4,
                    341,
                    0
                ],
                "title": "Can Large Language Models Serve as Effective Classifiers for\n  Hierarchical Multi-Label Classification of Scientific Documents at Industrial\n  Scale?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Serve as Effective Classifiers for\n  Hierarchical Multi-Label Classification of Scientific Documents at Industrial\n  Scale?"
                },
                "summary": "We address the task of hierarchical multi-label classification (HMC) of\nscientific documents at an industrial scale, where hundreds of thousands of\ndocuments must be classified across thousands of dynamic labels. The rapid\ngrowth of scientific publications necessitates scalable and efficient methods\nfor classification, further complicated by the evolving nature of\ntaxonomies--where new categories are introduced, existing ones are merged, and\noutdated ones are deprecated. Traditional machine learning approaches, which\nrequire costly retraining with each taxonomy update, become impractical due to\nthe high overhead of labelled data collection and model adaptation. Large\nLanguage Models (LLMs) have demonstrated great potential in complex tasks such\nas multi-label classification. However, applying them to large and dynamic\ntaxonomies presents unique challenges as the vast number of labels can exceed\nLLMs' input limits. In this paper, we present novel methods that combine the\nstrengths of LLMs with dense retrieval techniques to overcome these challenges.\nOur approach avoids retraining by leveraging zero-shot HMC for real-time label\nassignment. We evaluate the effectiveness of our methods on SSRN, a large\nrepository of preprints spanning multiple disciplines, and demonstrate\nsignificant improvements in both classification accuracy and cost-efficiency.\nBy developing a tailored evaluation framework for dynamic taxonomies and\npublicly releasing our code, this research provides critical insights into\napplying LLMs for document classification, where the number of classes\ncorresponds to the number of nodes in a large taxonomy, at an industrial scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the task of hierarchical multi-label classification (HMC) of\nscientific documents at an industrial scale, where hundreds of thousands of\ndocuments must be classified across thousands of dynamic labels. The rapid\ngrowth of scientific publications necessitates scalable and efficient methods\nfor classification, further complicated by the evolving nature of\ntaxonomies--where new categories are introduced, existing ones are merged, and\noutdated ones are deprecated. Traditional machine learning approaches, which\nrequire costly retraining with each taxonomy update, become impractical due to\nthe high overhead of labelled data collection and model adaptation. Large\nLanguage Models (LLMs) have demonstrated great potential in complex tasks such\nas multi-label classification. However, applying them to large and dynamic\ntaxonomies presents unique challenges as the vast number of labels can exceed\nLLMs' input limits. In this paper, we present novel methods that combine the\nstrengths of LLMs with dense retrieval techniques to overcome these challenges.\nOur approach avoids retraining by leveraging zero-shot HMC for real-time label\nassignment. We evaluate the effectiveness of our methods on SSRN, a large\nrepository of preprints spanning multiple disciplines, and demonstrate\nsignificant improvements in both classification accuracy and cost-efficiency.\nBy developing a tailored evaluation framework for dynamic taxonomies and\npublicly releasing our code, this research provides critical insights into\napplying LLMs for document classification, where the number of classes\ncorresponds to the number of nodes in a large taxonomy, at an industrial scale."
                },
                "authors": [
                    {
                        "name": "Seyed Amin Tabatabaei"
                    },
                    {
                        "name": "Sarah Fancher"
                    },
                    {
                        "name": "Michael Parsons"
                    },
                    {
                        "name": "Arian Askari"
                    }
                ],
                "author_detail": {
                    "name": "Arian Askari"
                },
                "author": "Arian Askari",
                "arxiv_comment": "This paper has been accepted at COLING 2025 (Industry Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05135v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05135v1",
                "updated": "2024-12-06T15:51:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    15,
                    51,
                    4,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T15:51:04Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    15,
                    51,
                    4,
                    4,
                    341,
                    0
                ],
                "title": "The Polynomial Stein Discrepancy for Assessing Moment Convergence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Polynomial Stein Discrepancy for Assessing Moment Convergence"
                },
                "summary": "We propose a novel method for measuring the discrepancy between a set of\nsamples and a desired posterior distribution for Bayesian inference. Classical\nmethods for assessing sample quality like the effective sample size are not\nappropriate for scalable Bayesian sampling algorithms, such as stochastic\ngradient Langevin dynamics, that are asymptotically biased. Instead, the gold\nstandard is to use the kernel Stein Discrepancy (KSD), which is itself not\nscalable given its quadratic cost in the number of samples. The KSD and its\nfaster extensions also typically suffer from the curse-of-dimensionality and\ncan require extensive tuning. To address these limitations, we develop the\npolynomial Stein discrepancy (PSD) and an associated goodness-of-fit test.\nWhile the new test is not fully convergence-determining, we prove that it\ndetects differences in the first r moments in the Bernstein-von Mises limit. We\nempirically show that the test has higher power than its competitors in several\nexamples, and at a lower computational cost. Finally, we demonstrate that the\nPSD can assist practitioners to select hyper-parameters of Bayesian sampling\nalgorithms more efficiently than competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel method for measuring the discrepancy between a set of\nsamples and a desired posterior distribution for Bayesian inference. Classical\nmethods for assessing sample quality like the effective sample size are not\nappropriate for scalable Bayesian sampling algorithms, such as stochastic\ngradient Langevin dynamics, that are asymptotically biased. Instead, the gold\nstandard is to use the kernel Stein Discrepancy (KSD), which is itself not\nscalable given its quadratic cost in the number of samples. The KSD and its\nfaster extensions also typically suffer from the curse-of-dimensionality and\ncan require extensive tuning. To address these limitations, we develop the\npolynomial Stein discrepancy (PSD) and an associated goodness-of-fit test.\nWhile the new test is not fully convergence-determining, we prove that it\ndetects differences in the first r moments in the Bernstein-von Mises limit. We\nempirically show that the test has higher power than its competitors in several\nexamples, and at a lower computational cost. Finally, we demonstrate that the\nPSD can assist practitioners to select hyper-parameters of Bayesian sampling\nalgorithms more efficiently than competitors."
                },
                "authors": [
                    {
                        "name": "Narayan Srinivasan"
                    },
                    {
                        "name": "Matthew Sutton"
                    },
                    {
                        "name": "Christopher Drovandi"
                    },
                    {
                        "name": "Leah F South"
                    }
                ],
                "author_detail": {
                    "name": "Leah F South"
                },
                "author": "Leah F South",
                "arxiv_comment": "17 Pages, 14 Figs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05135v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05135v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04517v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04517v2",
                "updated": "2024-12-06T15:42:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    15,
                    42,
                    7,
                    4,
                    341,
                    0
                ],
                "published": "2024-05-07T17:50:21Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    17,
                    50,
                    21,
                    1,
                    128,
                    0
                ],
                "title": "xLSTM: Extended Long Short-Term Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xLSTM: Extended Long Short-Term Memory"
                },
                "summary": "In the 1990s, the constant error carousel and gating were introduced as the\ncentral ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have\nstood the test of time and contributed to numerous deep learning success\nstories, in particular they constituted the first Large Language Models (LLMs).\nHowever, the advent of the Transformer technology with parallelizable\nself-attention at its core marked the dawn of a new era, outpacing LSTMs at\nscale. We now raise a simple question: How far do we get in language modeling\nwhen scaling LSTMs to billions of parameters, leveraging the latest techniques\nfrom modern LLMs, but mitigating known limitations of LSTMs? Firstly, we\nintroduce exponential gating with appropriate normalization and stabilization\ntechniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM\nwith a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that\nis fully parallelizable with a matrix memory and a covariance update rule.\nIntegrating these LSTM extensions into residual block backbones yields xLSTM\nblocks that are then residually stacked into xLSTM architectures. Exponential\ngating and modified memory structures boost xLSTM capabilities to perform\nfavorably when compared to state-of-the-art Transformers and State Space\nModels, both in performance and scaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the 1990s, the constant error carousel and gating were introduced as the\ncentral ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have\nstood the test of time and contributed to numerous deep learning success\nstories, in particular they constituted the first Large Language Models (LLMs).\nHowever, the advent of the Transformer technology with parallelizable\nself-attention at its core marked the dawn of a new era, outpacing LSTMs at\nscale. We now raise a simple question: How far do we get in language modeling\nwhen scaling LSTMs to billions of parameters, leveraging the latest techniques\nfrom modern LLMs, but mitigating known limitations of LSTMs? Firstly, we\nintroduce exponential gating with appropriate normalization and stabilization\ntechniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM\nwith a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that\nis fully parallelizable with a matrix memory and a covariance update rule.\nIntegrating these LSTM extensions into residual block backbones yields xLSTM\nblocks that are then residually stacked into xLSTM architectures. Exponential\ngating and modified memory structures boost xLSTM capabilities to perform\nfavorably when compared to state-of-the-art Transformers and State Space\nModels, both in performance and scaling."
                },
                "authors": [
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Korbinian Pöppel"
                    },
                    {
                        "name": "Markus Spanring"
                    },
                    {
                        "name": "Andreas Auer"
                    },
                    {
                        "name": "Oleksandra Prudnikova"
                    },
                    {
                        "name": "Michael Kopp"
                    },
                    {
                        "name": "Günter Klambauer"
                    },
                    {
                        "name": "Johannes Brandstetter"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "arxiv_comment": "Code available at https://github.com/NX-AI/xlstm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04517v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04517v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05127v1",
                "updated": "2024-12-06T15:35:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    15,
                    35,
                    18,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T15:35:18Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    15,
                    35,
                    18,
                    4,
                    341,
                    0
                ],
                "title": "The Prompt Canvas: A Literature-Based Practitioner Guide for Creating\n  Effective Prompts in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Prompt Canvas: A Literature-Based Practitioner Guide for Creating\n  Effective Prompts in Large Language Models"
                },
                "summary": "The rise of large language models (LLMs) has highlighted the importance of\nprompt engineering as a crucial technique for optimizing model outputs. While\nexperimentation with various prompting methods, such as Few-shot,\nChain-of-Thought, and role-based techniques, has yielded promising results,\nthese advancements remain fragmented across academic papers, blog posts and\nanecdotal experimentation. The lack of a single, unified resource to\nconsolidate the field's knowledge impedes the progress of both research and\npractical application. This paper argues for the creation of an overarching\nframework that synthesizes existing methodologies into a cohesive overview for\npractitioners. Using a design-based research approach, we present the Prompt\nCanvas, a structured framework resulting from an extensive literature review on\nprompt engineering that captures current knowledge and expertise. By combining\nthe conceptual foundations and practical strategies identified in prompt\nengineering, the Prompt Canvas provides a practical approach for leveraging the\npotential of Large Language Models. It is primarily designed as a learning\nresource for pupils, students and employees, offering a structured introduction\nto prompt engineering. This work aims to contribute to the growing discourse on\nprompt engineering by establishing a unified methodology for researchers and\nproviding guidance for practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large language models (LLMs) has highlighted the importance of\nprompt engineering as a crucial technique for optimizing model outputs. While\nexperimentation with various prompting methods, such as Few-shot,\nChain-of-Thought, and role-based techniques, has yielded promising results,\nthese advancements remain fragmented across academic papers, blog posts and\nanecdotal experimentation. The lack of a single, unified resource to\nconsolidate the field's knowledge impedes the progress of both research and\npractical application. This paper argues for the creation of an overarching\nframework that synthesizes existing methodologies into a cohesive overview for\npractitioners. Using a design-based research approach, we present the Prompt\nCanvas, a structured framework resulting from an extensive literature review on\nprompt engineering that captures current knowledge and expertise. By combining\nthe conceptual foundations and practical strategies identified in prompt\nengineering, the Prompt Canvas provides a practical approach for leveraging the\npotential of Large Language Models. It is primarily designed as a learning\nresource for pupils, students and employees, offering a structured introduction\nto prompt engineering. This work aims to contribute to the growing discourse on\nprompt engineering by establishing a unified methodology for researchers and\nproviding guidance for practitioners."
                },
                "authors": [
                    {
                        "name": "Michael Hewing"
                    },
                    {
                        "name": "Vincent Leinhos"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Leinhos"
                },
                "author": "Vincent Leinhos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19908v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19908v2",
                "updated": "2024-12-06T15:34:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    15,
                    34,
                    8,
                    4,
                    341,
                    0
                ],
                "published": "2024-11-29T18:12:50Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    18,
                    12,
                    50,
                    4,
                    334,
                    0
                ],
                "title": "Another look at inference after prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Another look at inference after prediction"
                },
                "summary": "Prediction-based (PB) inference is increasingly used in applications where\nthe outcome of interest is difficult to obtain, but its predictors are readily\navailable. Unlike traditional inference, PB inference performs statistical\ninference using a partially observed outcome and a set of covariates by\nleveraging a prediction of the outcome generated from a machine learning (ML)\nmodel. Motwani and Witten (2023) recently revisited two innovative PB inference\napproaches for ordinary least squares. They found that the method proposed by\nWang et al. (2020) yields a consistent estimator for the association of\ninterest when the ML model perfectly captures the underlying regression\nfunction. Conversely, the prediction-powered inference (PPI) method proposed by\nAngelopoulos et al. (2023) yields valid inference regardless of the model's\naccuracy. In this paper, we study the statistical efficiency of the PPI\nestimator. Our analysis reveals that a more efficient estimator, proposed 25\nyears ago by Chen and Chen (2000), can be obtained by simply adding a weight to\nthe PPI estimator. We also contextualize PB inference with methods from the\neconomics and statistics literature dating back to the 1960s. Our extensive\ntheoretical and numerical analyses indicate that the Chen and Chen (CC)\nestimator offers a balance between robustness to ML model specification and\nstatistical efficiency, making it the preferred choice for use in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prediction-based (PB) inference is increasingly used in applications where\nthe outcome of interest is difficult to obtain, but its predictors are readily\navailable. Unlike traditional inference, PB inference performs statistical\ninference using a partially observed outcome and a set of covariates by\nleveraging a prediction of the outcome generated from a machine learning (ML)\nmodel. Motwani and Witten (2023) recently revisited two innovative PB inference\napproaches for ordinary least squares. They found that the method proposed by\nWang et al. (2020) yields a consistent estimator for the association of\ninterest when the ML model perfectly captures the underlying regression\nfunction. Conversely, the prediction-powered inference (PPI) method proposed by\nAngelopoulos et al. (2023) yields valid inference regardless of the model's\naccuracy. In this paper, we study the statistical efficiency of the PPI\nestimator. Our analysis reveals that a more efficient estimator, proposed 25\nyears ago by Chen and Chen (2000), can be obtained by simply adding a weight to\nthe PPI estimator. We also contextualize PB inference with methods from the\neconomics and statistics literature dating back to the 1960s. Our extensive\ntheoretical and numerical analyses indicate that the Chen and Chen (CC)\nestimator offers a balance between robustness to ML model specification and\nstatistical efficiency, making it the preferred choice for use in practice."
                },
                "authors": [
                    {
                        "name": "Jessica Gronsbell"
                    },
                    {
                        "name": "Jianhui Gao"
                    },
                    {
                        "name": "Yaqi Shi"
                    },
                    {
                        "name": "Zachary R. McCaw"
                    },
                    {
                        "name": "David Cheng"
                    }
                ],
                "author_detail": {
                    "name": "David Cheng"
                },
                "author": "David Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19908v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19908v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.08430v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.08430v3",
                "updated": "2024-12-06T15:05:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    15,
                    5,
                    23,
                    4,
                    341,
                    0
                ],
                "published": "2023-09-15T14:31:53Z",
                "published_parsed": [
                    2023,
                    9,
                    15,
                    14,
                    31,
                    53,
                    4,
                    258,
                    0
                ],
                "title": "Fast Likelihood-free Reconstruction of Gravitational Wave Backgrounds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Likelihood-free Reconstruction of Gravitational Wave Backgrounds"
                },
                "summary": "We apply state-of-the-art, likelihood-free statistical inference\n(machine-learning-based) techniques for reconstructing the spectral shape of a\ngravitational wave background (GWB). We focus on the reconstruction of an\narbitrarily shaped signal by the LISA detector, but the method can be easily\nextended to either template-dependent signals, or to other detectors, as long\nas a characterisation of the instrumental noise is available. As proof of the\ntechnique, we quantify the ability of LISA to reconstruct signals of arbitrary\nspectral shape (${\\it blind}$ reconstruction), considering a diversity of\nfrequency profiles, and including astrophysical backgrounds in some cases. As a\nteaser of how the method can reconstruct signals characterised by a\nparameter-dependent template (${\\it template}$ reconstruction), we present a\ndedicated study for power-law signals. While our technique has several\nadvantages with respect to traditional MCMC methods, we validate it with the\nlatter for concrete cases. This work opens the door for both fast and accurate\nBayesian parameter estimation of GWBs, with essentially no computational\noverhead during the inference step. Our set of tools are integrated into the\npackage ${\\tt GWBackFinder}$, which is publicly available in\nhttps://github.com/AndronikiDimitriou/GWBackFinder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We apply state-of-the-art, likelihood-free statistical inference\n(machine-learning-based) techniques for reconstructing the spectral shape of a\ngravitational wave background (GWB). We focus on the reconstruction of an\narbitrarily shaped signal by the LISA detector, but the method can be easily\nextended to either template-dependent signals, or to other detectors, as long\nas a characterisation of the instrumental noise is available. As proof of the\ntechnique, we quantify the ability of LISA to reconstruct signals of arbitrary\nspectral shape (${\\it blind}$ reconstruction), considering a diversity of\nfrequency profiles, and including astrophysical backgrounds in some cases. As a\nteaser of how the method can reconstruct signals characterised by a\nparameter-dependent template (${\\it template}$ reconstruction), we present a\ndedicated study for power-law signals. While our technique has several\nadvantages with respect to traditional MCMC methods, we validate it with the\nlatter for concrete cases. This work opens the door for both fast and accurate\nBayesian parameter estimation of GWBs, with essentially no computational\noverhead during the inference step. Our set of tools are integrated into the\npackage ${\\tt GWBackFinder}$, which is publicly available in\nhttps://github.com/AndronikiDimitriou/GWBackFinder."
                },
                "authors": [
                    {
                        "name": "Androniki Dimitriou"
                    },
                    {
                        "name": "Daniel G. Figueroa"
                    },
                    {
                        "name": "Bryan Zaldivar"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Zaldivar"
                },
                "author": "Bryan Zaldivar",
                "arxiv_doi": "10.1088/1475-7516/2024/09/032",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1475-7516/2024/09/032",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.08430v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.08430v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in JCAP. 29 pages plus appendices and references, 12\n  figures",
                "arxiv_journal_ref": "JCAP 09 (2024) 032",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05098v1",
                "updated": "2024-12-06T14:54:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    14,
                    54,
                    21,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T14:54:21Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    14,
                    54,
                    21,
                    4,
                    341,
                    0
                ],
                "title": "From Defects to Demands: A Unified, Iterative, and Heuristically Guided\n  LLM-Based Framework for Automated Software Repair and Requirement Realization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Defects to Demands: A Unified, Iterative, and Heuristically Guided\n  LLM-Based Framework for Automated Software Repair and Requirement Realization"
                },
                "summary": "This manuscript signals a new era in the integration of artificial\nintelligence with software engineering, placing machines at the pinnacle of\ncoding capability. We present a formalized, iterative methodology proving that\nAI can fully replace human programmers in all aspects of code creation and\nrefinement. Our approach, combining large language models with formal\nverification, test-driven development, and incremental architectural guidance,\nachieves a 38.6% improvement over the current top performer's 48.33% accuracy\non the SWE-bench benchmark. This surpasses previously assumed limits, signaling\nthe end of human-exclusive coding and the rise of autonomous AI-driven software\ninnovation. More than a technical advance, our work challenges centuries-old\nassumptions about human creativity. We provide robust evidence of AI\nsuperiority, demonstrating tangible gains in practical engineering contexts and\nlaying the foundation for a future in which computational creativity outpaces\nhuman ingenuity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This manuscript signals a new era in the integration of artificial\nintelligence with software engineering, placing machines at the pinnacle of\ncoding capability. We present a formalized, iterative methodology proving that\nAI can fully replace human programmers in all aspects of code creation and\nrefinement. Our approach, combining large language models with formal\nverification, test-driven development, and incremental architectural guidance,\nachieves a 38.6% improvement over the current top performer's 48.33% accuracy\non the SWE-bench benchmark. This surpasses previously assumed limits, signaling\nthe end of human-exclusive coding and the rise of autonomous AI-driven software\ninnovation. More than a technical advance, our work challenges centuries-old\nassumptions about human creativity. We provide robust evidence of AI\nsuperiority, demonstrating tangible gains in practical engineering contexts and\nlaying the foundation for a future in which computational creativity outpaces\nhuman ingenuity."
                },
                "authors": [
                    {
                        "name": "Alex"
                    },
                    {
                        "name": "Liu"
                    },
                    {
                        "name": "Vivian"
                    },
                    {
                        "name": "Chi"
                    }
                ],
                "author_detail": {
                    "name": "Chi"
                },
                "arxiv_affiliation": "Zirong",
                "author": "Chi",
                "arxiv_comment": "21 pages,1 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05094v1",
                "updated": "2024-12-06T14:50:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    14,
                    50,
                    28,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T14:50:28Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    14,
                    50,
                    28,
                    4,
                    341,
                    0
                ],
                "title": "Asteroseismic Structure Inversions of Main-Sequence Solar-like\n  Oscillators with Convective Cores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asteroseismic Structure Inversions of Main-Sequence Solar-like\n  Oscillators with Convective Cores"
                },
                "summary": "Asteroseismic inferences of main-sequence solar-like oscillators often rely\non best-fit models. However, these models cannot fully reproduce the observed\nmode frequencies, suggesting that the internal structure of the model does not\nfully match that of the star. Asteroseismic structure inversions provide a way\nto test the interior of our stellar models. Recently, structure inversion\ntechniques were used to study 12 stars with radiative cores. In this work, we\nextend that analysis to 43 main-sequence stars with convective cores observed\nby Kepler to look for differences in the sound speed profiles in the inner 30%\nof the star by radius. For around half of our stars, the structure inversions\nshow that our models reproduce the internal structure of the star, where the\ninversions are sensitive, within the observational uncertainties. For the stars\nwhere our inversions reveal significant differences, we find cases where our\nmodel sound speed is too high and cases where our model sound speed is too low.\nWe use the star with the most significant differences to explore several\nchanges to the physics of our model in an attempt to resolve the inferred\ndifferences. These changes include using a different overshoot prescription and\nincluding the effects of diffusion, gravitational settling, and radiative\nlevitation. We find that the resulting changes to the model structure are too\nsmall to resolve the differences shown in our inversions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asteroseismic inferences of main-sequence solar-like oscillators often rely\non best-fit models. However, these models cannot fully reproduce the observed\nmode frequencies, suggesting that the internal structure of the model does not\nfully match that of the star. Asteroseismic structure inversions provide a way\nto test the interior of our stellar models. Recently, structure inversion\ntechniques were used to study 12 stars with radiative cores. In this work, we\nextend that analysis to 43 main-sequence stars with convective cores observed\nby Kepler to look for differences in the sound speed profiles in the inner 30%\nof the star by radius. For around half of our stars, the structure inversions\nshow that our models reproduce the internal structure of the star, where the\ninversions are sensitive, within the observational uncertainties. For the stars\nwhere our inversions reveal significant differences, we find cases where our\nmodel sound speed is too high and cases where our model sound speed is too low.\nWe use the star with the most significant differences to explore several\nchanges to the physics of our model in an attempt to resolve the inferred\ndifferences. These changes include using a different overshoot prescription and\nincluding the effects of diffusion, gravitational settling, and radiative\nlevitation. We find that the resulting changes to the model structure are too\nsmall to resolve the differences shown in our inversions."
                },
                "authors": [
                    {
                        "name": "Lynn Buchele"
                    },
                    {
                        "name": "Earl P. Bellinger"
                    },
                    {
                        "name": "Saskia Hekker"
                    },
                    {
                        "name": "Sarbani Basu"
                    }
                ],
                "author_detail": {
                    "name": "Sarbani Basu"
                },
                "arxiv_affiliation": "Yale University",
                "author": "Sarbani Basu",
                "arxiv_comment": "18 pages, 8 figures, Resubmitted to ApJ after favorable referee\n  report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05093v1",
                "updated": "2024-12-06T14:50:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    14,
                    50,
                    1,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T14:50:01Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    14,
                    50,
                    1,
                    4,
                    341,
                    0
                ],
                "title": "Sense and Sensitivity: Evaluating the simulation of social dynamics via\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sense and Sensitivity: Evaluating the simulation of social dynamics via\n  Large Language Models"
                },
                "summary": "Large language models have increasingly been proposed as a powerful\nreplacement for classical agent-based models (ABMs) to simulate social\ndynamics. By using LLMs as a proxy for human behavior, the hope of this new\napproach is to be able to simulate significantly more complex dynamics than\nwith classical ABMs and gain new insights in fields such as social science,\npolitical science, and economics. However, due to the black box nature of LLMs,\nit is unclear whether LLM agents actually execute the intended semantics that\nare encoded in their natural language instructions and, if the resulting\ndynamics of interactions are meaningful. To study this question, we propose a\nnew evaluation framework that grounds LLM simulations within the dynamics of\nestablished reference models of social science. By treating LLMs as a black-box\nfunction, we evaluate their input-output behavior relative to this reference\nmodel, which allows us to evaluate detailed aspects of their behavior. Our\nresults show that, while it is possible to engineer prompts that approximate\nthe intended dynamics, the quality of these simulations is highly sensitive to\nthe particular choice of prompts. Importantly, simulations are even sensitive\nto arbitrary variations such as minor wording changes and whitespace. This puts\ninto question the usefulness of current versions of LLMs for meaningful\nsimulations, as without a reference model, it is impossible to determine a\npriori what impact seemingly meaningless changes in prompt will have on the\nsimulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have increasingly been proposed as a powerful\nreplacement for classical agent-based models (ABMs) to simulate social\ndynamics. By using LLMs as a proxy for human behavior, the hope of this new\napproach is to be able to simulate significantly more complex dynamics than\nwith classical ABMs and gain new insights in fields such as social science,\npolitical science, and economics. However, due to the black box nature of LLMs,\nit is unclear whether LLM agents actually execute the intended semantics that\nare encoded in their natural language instructions and, if the resulting\ndynamics of interactions are meaningful. To study this question, we propose a\nnew evaluation framework that grounds LLM simulations within the dynamics of\nestablished reference models of social science. By treating LLMs as a black-box\nfunction, we evaluate their input-output behavior relative to this reference\nmodel, which allows us to evaluate detailed aspects of their behavior. Our\nresults show that, while it is possible to engineer prompts that approximate\nthe intended dynamics, the quality of these simulations is highly sensitive to\nthe particular choice of prompts. Importantly, simulations are even sensitive\nto arbitrary variations such as minor wording changes and whitespace. This puts\ninto question the usefulness of current versions of LLMs for meaningful\nsimulations, as without a reference model, it is impossible to determine a\npriori what impact seemingly meaningless changes in prompt will have on the\nsimulation."
                },
                "authors": [
                    {
                        "name": "Da Ju"
                    },
                    {
                        "name": "Adina Williams"
                    },
                    {
                        "name": "Brian Karrer"
                    },
                    {
                        "name": "Maximilian Nickel"
                    }
                ],
                "author_detail": {
                    "name": "Maximilian Nickel"
                },
                "author": "Maximilian Nickel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05075v1",
                "updated": "2024-12-06T14:33:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    14,
                    33,
                    34,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T14:33:34Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    14,
                    33,
                    34,
                    4,
                    341,
                    0
                ],
                "title": "Towards the interoperability of low-code platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards the interoperability of low-code platforms"
                },
                "summary": "With the promise of accelerating software development, low-code platforms\n(LCPs) are becoming popular across various industries. Nevertheless, there are\nstill barriers hindering their adoption. Among them, vendor lock-in is a major\nconcern, especially considering the lack of interoperability between these\nplatforms. Typically, after modeling an application in one LCP, migrating to\nanother requires starting from scratch remodeling everything (the data model,\nthe graphical user interface, workflows, etc.), in the new platform.\n  To overcome this situation, this work proposes an approach to improve the\ninteroperability of LCPs by (semi)automatically migrating models specified in\none platform to another one. The concrete migration path depends on the\ncapabilities of the source and target tools. We first analyze popular LCPs,\ncharacterize their import and export alternatives and define transformations\nbetween those data formats when available. This is then complemented with an\nLLM-based solution, where image recognition features of large language models\nare employed to migrate models based on a simple image export of the model at\nhand. The full pipelines are implemented on top of the BESSER modeling\nframework that acts as a pivot representation between the tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the promise of accelerating software development, low-code platforms\n(LCPs) are becoming popular across various industries. Nevertheless, there are\nstill barriers hindering their adoption. Among them, vendor lock-in is a major\nconcern, especially considering the lack of interoperability between these\nplatforms. Typically, after modeling an application in one LCP, migrating to\nanother requires starting from scratch remodeling everything (the data model,\nthe graphical user interface, workflows, etc.), in the new platform.\n  To overcome this situation, this work proposes an approach to improve the\ninteroperability of LCPs by (semi)automatically migrating models specified in\none platform to another one. The concrete migration path depends on the\ncapabilities of the source and target tools. We first analyze popular LCPs,\ncharacterize their import and export alternatives and define transformations\nbetween those data formats when available. This is then complemented with an\nLLM-based solution, where image recognition features of large language models\nare employed to migrate models based on a simple image export of the model at\nhand. The full pipelines are implemented on top of the BESSER modeling\nframework that acts as a pivot representation between the tools."
                },
                "authors": [
                    {
                        "name": "Iván Alfonso"
                    },
                    {
                        "name": "Aaron Conrardy"
                    },
                    {
                        "name": "Jordi Cabot"
                    }
                ],
                "author_detail": {
                    "name": "Jordi Cabot"
                },
                "author": "Jordi Cabot",
                "arxiv_comment": "Submitted to International Conference on Advanced Information Systems\n  Engineering (CAiSE25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68-04",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.12",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07057v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07057v2",
                "updated": "2024-12-06T14:21:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    14,
                    21,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-06-11T08:38:13Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    8,
                    38,
                    13,
                    1,
                    163,
                    0
                ],
                "title": "MultiTrust: A Comprehensive Benchmark Towards Trustworthy Multimodal\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiTrust: A Comprehensive Benchmark Towards Trustworthy Multimodal\n  Large Language Models"
                },
                "summary": "Despite the superior capabilities of Multimodal Large Language Models (MLLMs)\nacross diverse tasks, they still face significant trustworthiness challenges.\nYet, current literature on the assessment of trustworthy MLLMs remains limited,\nlacking a holistic evaluation to offer thorough insights into future\nimprovements. In this work, we establish MultiTrust, the first comprehensive\nand unified benchmark on the trustworthiness of MLLMs across five primary\naspects: truthfulness, safety, robustness, fairness, and privacy. Our benchmark\nemploys a rigorous evaluation strategy that addresses both multimodal risks and\ncross-modal impacts, encompassing 32 diverse tasks with self-curated datasets.\nExtensive experiments with 21 modern MLLMs reveal some previously unexplored\ntrustworthiness issues and risks, highlighting the complexities introduced by\nthe multimodality and underscoring the necessity for advanced methodologies to\nenhance their reliability. For instance, typical proprietary models still\nstruggle with the perception of visually confusing images and are vulnerable to\nmultimodal jailbreaking and adversarial attacks; MLLMs are more inclined to\ndisclose privacy in text and reveal ideological and cultural biases even when\npaired with irrelevant images in inference, indicating that the multimodality\namplifies the internal risks from base LLMs. Additionally, we release a\nscalable toolbox for standardized trustworthiness research, aiming to\nfacilitate future advancements in this important field. Code and resources are\npublicly available at: https://multi-trust.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the superior capabilities of Multimodal Large Language Models (MLLMs)\nacross diverse tasks, they still face significant trustworthiness challenges.\nYet, current literature on the assessment of trustworthy MLLMs remains limited,\nlacking a holistic evaluation to offer thorough insights into future\nimprovements. In this work, we establish MultiTrust, the first comprehensive\nand unified benchmark on the trustworthiness of MLLMs across five primary\naspects: truthfulness, safety, robustness, fairness, and privacy. Our benchmark\nemploys a rigorous evaluation strategy that addresses both multimodal risks and\ncross-modal impacts, encompassing 32 diverse tasks with self-curated datasets.\nExtensive experiments with 21 modern MLLMs reveal some previously unexplored\ntrustworthiness issues and risks, highlighting the complexities introduced by\nthe multimodality and underscoring the necessity for advanced methodologies to\nenhance their reliability. For instance, typical proprietary models still\nstruggle with the perception of visually confusing images and are vulnerable to\nmultimodal jailbreaking and adversarial attacks; MLLMs are more inclined to\ndisclose privacy in text and reveal ideological and cultural biases even when\npaired with irrelevant images in inference, indicating that the multimodality\namplifies the internal risks from base LLMs. Additionally, we release a\nscalable toolbox for standardized trustworthiness research, aiming to\nfacilitate future advancements in this important field. Code and resources are\npublicly available at: https://multi-trust.github.io/."
                },
                "authors": [
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Yao Huang"
                    },
                    {
                        "name": "Yitong Sun"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Zhengwei Fang"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Huanran Chen"
                    },
                    {
                        "name": "Xiao Yang"
                    },
                    {
                        "name": "Xingxing Wei"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Yinpeng Dong"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "arxiv_comment": "100 pages, 84 figures, 33 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07057v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07057v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04513v2",
                "updated": "2024-12-06T14:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    14,
                    20,
                    26,
                    4,
                    341,
                    0
                ],
                "published": "2024-07-05T13:54:15Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    13,
                    54,
                    15,
                    4,
                    187,
                    0
                ],
                "title": "LayerShuffle: Enhancing Robustness in Vision Transformers by Randomizing\n  Layer Execution Order",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LayerShuffle: Enhancing Robustness in Vision Transformers by Randomizing\n  Layer Execution Order"
                },
                "summary": "Due to their architecture and how they are trained, artificial neural\nnetworks are typically not robust toward pruning or shuffling layers at test\ntime. However, such properties would be desirable for different applications,\nsuch as distributed neural network architectures where the order of execution\ncannot be guaranteed or parts of the network can fail during inference. In this\nwork, we address these issues through a number of training approaches for\nvision transformers whose most important component is randomizing the execution\norder of attention modules at training time. With our proposed approaches,\nvision transformers are capable to adapt to arbitrary layer execution orders at\ntest time assuming one tolerates a reduction (about 20\\%) in accuracy at the\nsame model size. We analyse the feature representations of our trained models\nas well as how each layer contributes to the models prediction based on its\nposition during inference. Our analysis shows that layers learn to contribute\ndifferently based on their position in the network. Finally, we layer-prune our\nmodels at test time and find that their performance declines gracefully. Code\navailable at https://github.com/matfrei/layershuffle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to their architecture and how they are trained, artificial neural\nnetworks are typically not robust toward pruning or shuffling layers at test\ntime. However, such properties would be desirable for different applications,\nsuch as distributed neural network architectures where the order of execution\ncannot be guaranteed or parts of the network can fail during inference. In this\nwork, we address these issues through a number of training approaches for\nvision transformers whose most important component is randomizing the execution\norder of attention modules at training time. With our proposed approaches,\nvision transformers are capable to adapt to arbitrary layer execution orders at\ntest time assuming one tolerates a reduction (about 20\\%) in accuracy at the\nsame model size. We analyse the feature representations of our trained models\nas well as how each layer contributes to the models prediction based on its\nposition during inference. Our analysis shows that layers learn to contribute\ndifferently based on their position in the network. Finally, we layer-prune our\nmodels at test time and find that their performance declines gracefully. Code\navailable at https://github.com/matfrei/layershuffle."
                },
                "authors": [
                    {
                        "name": "Matthias Freiberger"
                    },
                    {
                        "name": "Peter Kun"
                    },
                    {
                        "name": "Anders Sundnes Løvlie"
                    },
                    {
                        "name": "Sebastian Risi"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Risi"
                },
                "author": "Sebastian Risi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03517v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03517v2",
                "updated": "2024-12-06T13:56:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    13,
                    56,
                    50,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-04T17:58:03Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    17,
                    58,
                    3,
                    2,
                    339,
                    0
                ],
                "title": "NVComposer: Boosting Generative Novel View Synthesis with Multiple\n  Sparse and Unposed Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVComposer: Boosting Generative Novel View Synthesis with Multiple\n  Sparse and Unposed Images"
                },
                "summary": "Recent advancements in generative models have significantly improved novel\nview synthesis (NVS) from multi-view data. However, existing methods depend on\nexternal multi-view alignment processes, such as explicit pose estimation or\npre-reconstruction, which limits their flexibility and accessibility,\nespecially when alignment is unstable due to insufficient overlap or occlusions\nbetween views. In this paper, we propose NVComposer, a novel approach that\neliminates the need for explicit external alignment. NVComposer enables the\ngenerative model to implicitly infer spatial and geometric relationships\nbetween multiple conditional views by introducing two key components: 1) an\nimage-pose dual-stream diffusion model that simultaneously generates target\nnovel views and condition camera poses, and 2) a geometry-aware feature\nalignment module that distills geometric priors from dense stereo models during\ntraining. Extensive experiments demonstrate that NVComposer achieves\nstate-of-the-art performance in generative multi-view NVS tasks, removing the\nreliance on external alignment and thus improving model accessibility. Our\napproach shows substantial improvements in synthesis quality as the number of\nunposed input views increases, highlighting its potential for more flexible and\naccessible generative NVS systems. Our project page is available at\nhttps://lg-li.github.io/project/nvcomposer",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in generative models have significantly improved novel\nview synthesis (NVS) from multi-view data. However, existing methods depend on\nexternal multi-view alignment processes, such as explicit pose estimation or\npre-reconstruction, which limits their flexibility and accessibility,\nespecially when alignment is unstable due to insufficient overlap or occlusions\nbetween views. In this paper, we propose NVComposer, a novel approach that\neliminates the need for explicit external alignment. NVComposer enables the\ngenerative model to implicitly infer spatial and geometric relationships\nbetween multiple conditional views by introducing two key components: 1) an\nimage-pose dual-stream diffusion model that simultaneously generates target\nnovel views and condition camera poses, and 2) a geometry-aware feature\nalignment module that distills geometric priors from dense stereo models during\ntraining. Extensive experiments demonstrate that NVComposer achieves\nstate-of-the-art performance in generative multi-view NVS tasks, removing the\nreliance on external alignment and thus improving model accessibility. Our\napproach shows substantial improvements in synthesis quality as the number of\nunposed input views increases, highlighting its potential for more flexible and\naccessible generative NVS systems. Our project page is available at\nhttps://lg-li.github.io/project/nvcomposer"
                },
                "authors": [
                    {
                        "name": "Lingen Li"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Yaowei Li"
                    },
                    {
                        "name": "Jiale Xu"
                    },
                    {
                        "name": "Wenbo Hu"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Weihao Cheng"
                    },
                    {
                        "name": "Jinwei Gu"
                    },
                    {
                        "name": "Tianfan Xue"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project Page: https://lg-li.github.io/project/nvcomposer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03517v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03517v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19832v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19832v2",
                "updated": "2024-12-06T13:41:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    13,
                    41,
                    53,
                    4,
                    341,
                    0
                ],
                "published": "2024-11-29T16:44:02Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    44,
                    2,
                    4,
                    334,
                    0
                ],
                "title": "Sensitive Content Classification in Social Media: A Holistic Resource\n  and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensitive Content Classification in Social Media: A Holistic Resource\n  and Evaluation"
                },
                "summary": "The detection of sensitive content in large datasets is crucial for ensuring\nthat shared and analysed data is free from harmful material. However, current\nmoderation tools, such as external APIs, suffer from limitations in\ncustomisation, accuracy across diverse sensitive categories, and privacy\nconcerns. Additionally, existing datasets and open-source models focus\npredominantly on toxic language, leaving gaps in detecting other sensitive\ncategories such as substance abuse or self-harm. In this paper, we put forward\na unified dataset tailored for social media content moderation across six\nsensitive categories: conflictual language, profanity, sexually explicit\nmaterial, drug-related content, self-harm, and spam. By collecting and\nannotating data with consistent retrieval strategies and guidelines, we address\nthe shortcomings of previous focalised research. Our analysis demonstrates that\nfine-tuning large language models (LLMs) on this novel dataset yields\nsignificant improvements in detection performance compared to open\noff-the-shelf models such as LLaMA, and even proprietary OpenAI models, which\nunderperform by 10-15% overall. This limitation is even more pronounced on\npopular moderation APIs, which cannot be easily tailored to specific sensitive\ncontent categories, among others.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detection of sensitive content in large datasets is crucial for ensuring\nthat shared and analysed data is free from harmful material. However, current\nmoderation tools, such as external APIs, suffer from limitations in\ncustomisation, accuracy across diverse sensitive categories, and privacy\nconcerns. Additionally, existing datasets and open-source models focus\npredominantly on toxic language, leaving gaps in detecting other sensitive\ncategories such as substance abuse or self-harm. In this paper, we put forward\na unified dataset tailored for social media content moderation across six\nsensitive categories: conflictual language, profanity, sexually explicit\nmaterial, drug-related content, self-harm, and spam. By collecting and\nannotating data with consistent retrieval strategies and guidelines, we address\nthe shortcomings of previous focalised research. Our analysis demonstrates that\nfine-tuning large language models (LLMs) on this novel dataset yields\nsignificant improvements in detection performance compared to open\noff-the-shelf models such as LLaMA, and even proprietary OpenAI models, which\nunderperform by 10-15% overall. This limitation is even more pronounced on\npopular moderation APIs, which cannot be easily tailored to specific sensitive\ncontent categories, among others."
                },
                "authors": [
                    {
                        "name": "Dimosthenis Antypas"
                    },
                    {
                        "name": "Indira Sen"
                    },
                    {
                        "name": "Carla Perez-Almendros"
                    },
                    {
                        "name": "Jose Camacho-Collados"
                    },
                    {
                        "name": "Francesco Barbieri"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Barbieri"
                },
                "author": "Francesco Barbieri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19832v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19832v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18506v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18506v3",
                "updated": "2024-12-06T13:35:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    13,
                    35,
                    45,
                    4,
                    341,
                    0
                ],
                "published": "2024-11-27T16:48:24Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    16,
                    48,
                    24,
                    2,
                    332,
                    0
                ],
                "title": "LLM-ABBA: Understanding time series via symbolic approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-ABBA: Understanding time series via symbolic approximation"
                },
                "summary": "The success of large language models (LLMs) for time series has been\ndemonstrated in previous work. Utilizing a symbolic time series representation,\none can efficiently bridge the gap between LLMs and time series. However, the\nremaining challenge is to exploit the semantic information hidden in time\nseries by using symbols or existing tokens of LLMs, while aligning the\nembedding space of LLMs according to the hidden information of time series. The\nsymbolic time series approximation (STSA) method called adaptive Brownian\nbridge-based symbolic aggregation (ABBA) shows outstanding efficacy in\npreserving salient time series features by modeling time series patterns in\nterms of amplitude and period while using existing tokens of LLMs.\n  In this paper, we introduce a method, called LLM-ABBA, that integrates ABBA\ninto large language models for various downstream time series tasks. By\nsymbolizing time series, LLM-ABBA compares favorably to the recent\nstate-of-the-art (SOTA) in UCR and three medical time series classification\ntasks. Meanwhile, a fixed-polygonal chain trick in ABBA is introduced to\n\\kc{avoid obvious drifting} during prediction tasks by significantly mitigating\nthe effects of cumulative error arising from misused symbols during the\ntransition from symbols to numerical values. In time series regression tasks,\nLLM-ABBA achieves the new SOTA on Time Series Extrinsic Regression (TSER)\nbenchmarks. LLM-ABBA also shows competitive prediction capability compared to\nrecent SOTA time series prediction results. We believe this framework can also\nseamlessly extend to other time series tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of large language models (LLMs) for time series has been\ndemonstrated in previous work. Utilizing a symbolic time series representation,\none can efficiently bridge the gap between LLMs and time series. However, the\nremaining challenge is to exploit the semantic information hidden in time\nseries by using symbols or existing tokens of LLMs, while aligning the\nembedding space of LLMs according to the hidden information of time series. The\nsymbolic time series approximation (STSA) method called adaptive Brownian\nbridge-based symbolic aggregation (ABBA) shows outstanding efficacy in\npreserving salient time series features by modeling time series patterns in\nterms of amplitude and period while using existing tokens of LLMs.\n  In this paper, we introduce a method, called LLM-ABBA, that integrates ABBA\ninto large language models for various downstream time series tasks. By\nsymbolizing time series, LLM-ABBA compares favorably to the recent\nstate-of-the-art (SOTA) in UCR and three medical time series classification\ntasks. Meanwhile, a fixed-polygonal chain trick in ABBA is introduced to\n\\kc{avoid obvious drifting} during prediction tasks by significantly mitigating\nthe effects of cumulative error arising from misused symbols during the\ntransition from symbols to numerical values. In time series regression tasks,\nLLM-ABBA achieves the new SOTA on Time Series Extrinsic Regression (TSER)\nbenchmarks. LLM-ABBA also shows competitive prediction capability compared to\nrecent SOTA time series prediction results. We believe this framework can also\nseamlessly extend to other time series tasks."
                },
                "authors": [
                    {
                        "name": "Erin Carson"
                    },
                    {
                        "name": "Xinye Chen"
                    },
                    {
                        "name": "Cheng Kang"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Kang"
                },
                "author": "Cheng Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18506v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18506v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05023v1",
                "updated": "2024-12-06T13:20:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    13,
                    20,
                    57,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T13:20:57Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    13,
                    20,
                    57,
                    4,
                    341,
                    0
                ],
                "title": "Steps are all you need: Rethinking STEM Education with Prompt\n  Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steps are all you need: Rethinking STEM Education with Prompt\n  Engineering"
                },
                "summary": "Few shot and Chain-of-Thought prompting have shown promise when applied to\nPhysics Question Answering Tasks, but are limited by the lack of mathematical\nability inherent to LLMs, and are prone to hallucination. By utilizing a\nMixture of Experts (MoE) Model, along with analogical prompting, we are able to\nshow improved model performance when compared to the baseline on standard LLMs.\nWe also survey the limits of these prompting techniques and the effects they\nhave on model performance. Additionally, we propose Analogical CoT prompting, a\nprompting technique designed to allow smaller, open source models to leverage\nAnalogical prompting, something they have struggled with, possibly due to a\nlack of specialist training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few shot and Chain-of-Thought prompting have shown promise when applied to\nPhysics Question Answering Tasks, but are limited by the lack of mathematical\nability inherent to LLMs, and are prone to hallucination. By utilizing a\nMixture of Experts (MoE) Model, along with analogical prompting, we are able to\nshow improved model performance when compared to the baseline on standard LLMs.\nWe also survey the limits of these prompting techniques and the effects they\nhave on model performance. Additionally, we propose Analogical CoT prompting, a\nprompting technique designed to allow smaller, open source models to leverage\nAnalogical prompting, something they have struggled with, possibly due to a\nlack of specialist training data."
                },
                "authors": [
                    {
                        "name": "Krishnasai Addala"
                    },
                    {
                        "name": "Kabir Dev Paul Baghel"
                    },
                    {
                        "name": "Chhavi Kirtani"
                    },
                    {
                        "name": "Avinash Anand"
                    },
                    {
                        "name": "Rajiv Ratn Shah"
                    }
                ],
                "author_detail": {
                    "name": "Rajiv Ratn Shah"
                },
                "author": "Rajiv Ratn Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2203.06437v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2203.06437v6",
                "updated": "2024-12-06T13:20:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    13,
                    20,
                    35,
                    4,
                    341,
                    0
                ],
                "published": "2022-03-12T14:01:47Z",
                "published_parsed": [
                    2022,
                    3,
                    12,
                    14,
                    1,
                    47,
                    5,
                    71,
                    0
                ],
                "title": "The Poisson-Gaussian Mixture Process: A Flexible and Robust Approach for\n  Non-Gaussian Geostatistical Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Poisson-Gaussian Mixture Process: A Flexible and Robust Approach for\n  Non-Gaussian Geostatistical Modeling"
                },
                "summary": "This paper introduces a novel family of geostatistical models designed to\ncapture complex features beyond the reach of traditional Gaussian processes.\nThe proposed family, termed the Poisson-Gaussian Mixture Process (POGAMP), is\nhierarchically specified, combining the infinite-dimensional dynamics of\nGaussian processes with any multivariate continuous distribution. This\ncombination is stochastically defined by a latent Poisson process, allowing the\nPOGAMP to define valid processes with finite-dimensional distributions that can\napproximate any continuous distribution. Unlike other non-Gaussian\ngeostatistical models that may fail to ensure validity of the processes by\nassigning arbitrary finite-dimensional distributions, the POGAMP preserves\nessential probabilistic properties crucial for both modeling and inference. We\nestablish formal results regarding the existence and properties of the POGAMP,\nhighlighting its robustness and flexibility in capturing complex spatial\npatterns. To support practical applications, a carefully designed MCMC\nalgorithm is developed for Bayesian inference when the POGAMP is discretely\nobserved over some spatial domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel family of geostatistical models designed to\ncapture complex features beyond the reach of traditional Gaussian processes.\nThe proposed family, termed the Poisson-Gaussian Mixture Process (POGAMP), is\nhierarchically specified, combining the infinite-dimensional dynamics of\nGaussian processes with any multivariate continuous distribution. This\ncombination is stochastically defined by a latent Poisson process, allowing the\nPOGAMP to define valid processes with finite-dimensional distributions that can\napproximate any continuous distribution. Unlike other non-Gaussian\ngeostatistical models that may fail to ensure validity of the processes by\nassigning arbitrary finite-dimensional distributions, the POGAMP preserves\nessential probabilistic properties crucial for both modeling and inference. We\nestablish formal results regarding the existence and properties of the POGAMP,\nhighlighting its robustness and flexibility in capturing complex spatial\npatterns. To support practical applications, a carefully designed MCMC\nalgorithm is developed for Bayesian inference when the POGAMP is discretely\nobserved over some spatial domain."
                },
                "authors": [
                    {
                        "name": "F. B. Gonçalves"
                    },
                    {
                        "name": "M. O. Prates"
                    },
                    {
                        "name": "G. A. S. Aguilar"
                    }
                ],
                "author_detail": {
                    "name": "G. A. S. Aguilar"
                },
                "author": "G. A. S. Aguilar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2203.06437v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2203.06437v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.16118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.16118v2",
                "updated": "2024-12-06T13:03:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    13,
                    3,
                    53,
                    4,
                    341,
                    0
                ],
                "published": "2023-12-26T16:53:21Z",
                "published_parsed": [
                    2023,
                    12,
                    26,
                    16,
                    53,
                    21,
                    1,
                    360,
                    0
                ],
                "title": "Quantum-Hybrid Stereo Matching With Nonlinear Regularization and Spatial\n  Pyramids",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum-Hybrid Stereo Matching With Nonlinear Regularization and Spatial\n  Pyramids"
                },
                "summary": "Quantum visual computing is advancing rapidly. This paper presents a new\nformulation for stereo matching with nonlinear regularizers and spatial\npyramids on quantum annealers as a maximum a posteriori inference problem that\nminimizes the energy of a Markov Random Field. Our approach is hybrid (i.e.,\nquantum-classical) and is compatible with modern D-Wave quantum annealers,\ni.e., it includes a quadratic unconstrained binary optimization (QUBO)\nobjective. Previous quantum annealing techniques for stereo matching are\nlimited to using linear regularizers, and thus, they do not exploit the\nfundamental advantages of the quantum computing paradigm in solving\ncombinatorial optimization problems. In contrast, our method utilizes the full\npotential of quantum annealing for stereo matching, as nonlinear regularizers\ncreate optimization problems which are NP-hard. On the Middlebury benchmark, we\nachieve an improved root mean squared accuracy over the previous state of the\nart in quantum stereo matching of 2% and 22.5% when using different solvers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum visual computing is advancing rapidly. This paper presents a new\nformulation for stereo matching with nonlinear regularizers and spatial\npyramids on quantum annealers as a maximum a posteriori inference problem that\nminimizes the energy of a Markov Random Field. Our approach is hybrid (i.e.,\nquantum-classical) and is compatible with modern D-Wave quantum annealers,\ni.e., it includes a quadratic unconstrained binary optimization (QUBO)\nobjective. Previous quantum annealing techniques for stereo matching are\nlimited to using linear regularizers, and thus, they do not exploit the\nfundamental advantages of the quantum computing paradigm in solving\ncombinatorial optimization problems. In contrast, our method utilizes the full\npotential of quantum annealing for stereo matching, as nonlinear regularizers\ncreate optimization problems which are NP-hard. On the Middlebury benchmark, we\nachieve an improved root mean squared accuracy over the previous state of the\nart in quantum stereo matching of 2% and 22.5% when using different solvers."
                },
                "authors": [
                    {
                        "name": "Cameron Braunstein"
                    },
                    {
                        "name": "Eddy Ilg"
                    },
                    {
                        "name": "Vladislav Golyanik"
                    }
                ],
                "author_detail": {
                    "name": "Vladislav Golyanik"
                },
                "author": "Vladislav Golyanik",
                "arxiv_comment": "26 pages, 15 figures. To be published in the International Conference\n  on 3D Vision (3DV) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.16118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.16118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05002v1",
                "updated": "2024-12-06T12:58:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    12,
                    58,
                    14,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T12:58:14Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    12,
                    58,
                    14,
                    4,
                    341,
                    0
                ],
                "title": "Eta-Earth Revisited II: Deriving a Maximum Number of Earth-like Habitats\n  in the Galactic Disk",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eta-Earth Revisited II: Deriving a Maximum Number of Earth-like Habitats\n  in the Galactic Disk"
                },
                "summary": "In Lammer et al. 2024, we defined Earth-like Habitats (EH) as rocky planets\nin the habitable zone of complex life (HZCL) on which Earth-like\nN$_2$-O$_2$-dominated atmospheres with minor amounts of CO$_2$ can exist and\nderived a formula for estimating their maximum number in the Galaxy. Here, we\napply this formula by considering only requirements that are already\nscientifically quantifiable. By implementing models for star formation rate,\ninitial mass function, and galactic mass distribution, we calculate the spatial\ndistribution of disk stars as functions of stellar mass and birth age. We apply\nmodels for the GHZ and evaluate the thermal stability of Earth-like atmospheres\nwith various CO$_2$ mixing ratios by implementing the newest stellar evolution\nand upper atmosphere models. In addition, we include the rocky exoplanet\nfrequency, the availability of oceans and subaerial land, and the potential\nlarge moon requirement by evaluating their importance and implementing these\ncriteria from minima to maxima values. We also discuss factors that are not yet\nscientifically quantifiable but may be requirements for EHs to evolve. We find\nthat EHs are rare by obtaining maximum numbers of\n$2.5^{+71.6}_{-2.4}\\times10^{5}$ and $0.6^{+27.1}_{-0.59}\\times10^{5}$ planets\nthat can potentially host N$_2$-Earth-like atmospheres with maximum CO$_2$\nmixing ratios of 10\\% and 1\\%, respectively, implying that a minimum of $\\sim\n10^3 - 10^6$ rocky HZCL planets are needed for 1 EH to evolve. Their actual\nnumber, however, may be substantially lower as several requirements are not\nincluded in our model; this also implies ETIs are significantly rarer still.\nOur results illustrate that neither every star can host EHs, nor that each\nrocky HZCL planet evolves such that it may be able to host complex animal-like\nlife. The Copernican Principle therefore cannot be applied to infer that such\nlife is common in the Galaxy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Lammer et al. 2024, we defined Earth-like Habitats (EH) as rocky planets\nin the habitable zone of complex life (HZCL) on which Earth-like\nN$_2$-O$_2$-dominated atmospheres with minor amounts of CO$_2$ can exist and\nderived a formula for estimating their maximum number in the Galaxy. Here, we\napply this formula by considering only requirements that are already\nscientifically quantifiable. By implementing models for star formation rate,\ninitial mass function, and galactic mass distribution, we calculate the spatial\ndistribution of disk stars as functions of stellar mass and birth age. We apply\nmodels for the GHZ and evaluate the thermal stability of Earth-like atmospheres\nwith various CO$_2$ mixing ratios by implementing the newest stellar evolution\nand upper atmosphere models. In addition, we include the rocky exoplanet\nfrequency, the availability of oceans and subaerial land, and the potential\nlarge moon requirement by evaluating their importance and implementing these\ncriteria from minima to maxima values. We also discuss factors that are not yet\nscientifically quantifiable but may be requirements for EHs to evolve. We find\nthat EHs are rare by obtaining maximum numbers of\n$2.5^{+71.6}_{-2.4}\\times10^{5}$ and $0.6^{+27.1}_{-0.59}\\times10^{5}$ planets\nthat can potentially host N$_2$-Earth-like atmospheres with maximum CO$_2$\nmixing ratios of 10\\% and 1\\%, respectively, implying that a minimum of $\\sim\n10^3 - 10^6$ rocky HZCL planets are needed for 1 EH to evolve. Their actual\nnumber, however, may be substantially lower as several requirements are not\nincluded in our model; this also implies ETIs are significantly rarer still.\nOur results illustrate that neither every star can host EHs, nor that each\nrocky HZCL planet evolves such that it may be able to host complex animal-like\nlife. The Copernican Principle therefore cannot be applied to infer that such\nlife is common in the Galaxy."
                },
                "authors": [
                    {
                        "name": "Manuel Scherf"
                    },
                    {
                        "name": "Helmut Lammer"
                    },
                    {
                        "name": "Laurenz Sproß"
                    }
                ],
                "author_detail": {
                    "name": "Laurenz Sproß"
                },
                "author": "Laurenz Sproß",
                "arxiv_doi": "10.1089/ast.2023.0076",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1089/ast.2023.0076",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.05002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "accepted manuscript submitted to Astrobiology, 116 pages, 34 figures,\n  7 appendices including various reviews; link to published version:\n  https://www.liebertpub.com/doi/10.1089/ast.2023.0076",
                "arxiv_journal_ref": "Astrobiology, 24, 10, e916 2024",
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05250v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05250v3",
                "updated": "2024-12-06T12:40:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    12,
                    40,
                    53,
                    4,
                    341,
                    0
                ],
                "published": "2024-06-07T20:22:36Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    20,
                    22,
                    36,
                    4,
                    159,
                    0
                ],
                "title": "LLM-Enhanced Bayesian Optimization for Efficient Analog Layout\n  Constraint Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Enhanced Bayesian Optimization for Efficient Analog Layout\n  Constraint Generation"
                },
                "summary": "Analog layout synthesis faces significant challenges due to its dependence on\nmanual processes, considerable time requirements, and performance instability.\nCurrent Bayesian Optimization (BO)-based techniques for analog layout\nsynthesis, despite their potential for automation, suffer from slow convergence\nand extensive data needs, limiting their practical application. This paper\npresents the \\texttt{LLANA} framework, a novel approach that leverages Large\nLanguage Models (LLMs) to enhance BO by exploiting the few-shot learning\nabilities of LLMs for more efficient generation of analog design-dependent\nparameter constraints. Experimental results demonstrate that \\texttt{LLANA} not\nonly achieves performance comparable to state-of-the-art (SOTA) BO methods but\nalso enables a more effective exploration of the analog circuit design space,\nthanks to LLM's superior contextual understanding and learning efficiency. The\ncode is available at https://github.com/dekura/LLANA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog layout synthesis faces significant challenges due to its dependence on\nmanual processes, considerable time requirements, and performance instability.\nCurrent Bayesian Optimization (BO)-based techniques for analog layout\nsynthesis, despite their potential for automation, suffer from slow convergence\nand extensive data needs, limiting their practical application. This paper\npresents the \\texttt{LLANA} framework, a novel approach that leverages Large\nLanguage Models (LLMs) to enhance BO by exploiting the few-shot learning\nabilities of LLMs for more efficient generation of analog design-dependent\nparameter constraints. Experimental results demonstrate that \\texttt{LLANA} not\nonly achieves performance comparable to state-of-the-art (SOTA) BO methods but\nalso enables a more effective exploration of the analog circuit design space,\nthanks to LLM's superior contextual understanding and learning efficiency. The\ncode is available at https://github.com/dekura/LLANA."
                },
                "authors": [
                    {
                        "name": "Guojin Chen"
                    },
                    {
                        "name": "Keren Zhu"
                    },
                    {
                        "name": "Seunggeun Kim"
                    },
                    {
                        "name": "Hanqing Zhu"
                    },
                    {
                        "name": "Yao Lai"
                    },
                    {
                        "name": "Bei Yu"
                    },
                    {
                        "name": "David Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "David Z. Pan"
                },
                "author": "David Z. Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05250v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05250v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02976v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02976v2",
                "updated": "2024-12-06T12:39:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    12,
                    39,
                    0,
                    4,
                    341,
                    0
                ],
                "published": "2024-09-04T13:59:38Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    59,
                    38,
                    2,
                    248,
                    0
                ],
                "title": "Hallucination Detection in LLMs: Fast and Memory-Efficient Fine-Tuned\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination Detection in LLMs: Fast and Memory-Efficient Fine-Tuned\n  Models"
                },
                "summary": "Uncertainty estimation is a necessary component when implementing AI in\nhigh-risk settings, such as autonomous cars, medicine, or insurances. Large\nLanguage Models (LLMs) have seen a surge in popularity in recent years, but\nthey are subject to hallucinations, which may cause serious harm in high-risk\nsettings. Despite their success, LLMs are expensive to train and run: they need\na large amount of computations and memory, preventing the use of ensembling\nmethods in practice. In this work, we present a novel method that allows for\nfast and memory-friendly training of LLM ensembles. We show that the resulting\nensembles can detect hallucinations and are a viable approach in practice as\nonly one GPU is needed for training and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty estimation is a necessary component when implementing AI in\nhigh-risk settings, such as autonomous cars, medicine, or insurances. Large\nLanguage Models (LLMs) have seen a surge in popularity in recent years, but\nthey are subject to hallucinations, which may cause serious harm in high-risk\nsettings. Despite their success, LLMs are expensive to train and run: they need\na large amount of computations and memory, preventing the use of ensembling\nmethods in practice. In this work, we present a novel method that allows for\nfast and memory-friendly training of LLM ensembles. We show that the resulting\nensembles can detect hallucinations and are a viable approach in practice as\nonly one GPU is needed for training and inference."
                },
                "authors": [
                    {
                        "name": "Gabriel Y. Arteaga"
                    },
                    {
                        "name": "Thomas B. Schön"
                    },
                    {
                        "name": "Nicolas Pielawski"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Pielawski"
                },
                "author": "Nicolas Pielawski",
                "arxiv_comment": "6 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02976v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02976v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16910v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16910v2",
                "updated": "2024-12-06T12:33:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    12,
                    33,
                    44,
                    4,
                    341,
                    0
                ],
                "published": "2024-05-27T07:58:25Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    7,
                    58,
                    25,
                    0,
                    148,
                    0
                ],
                "title": "Temperature evolution of the Fermi surface of the FeSe monolayer on\n  SrTiO$_3$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temperature evolution of the Fermi surface of the FeSe monolayer on\n  SrTiO$_3$"
                },
                "summary": "The origin of superconductivity in the FeSe monolayer on SrTiO$_3$ remains\none of the unresolved mysteries in condensed-matter physics. Here by\ninvestigation of the temperature evolution of the dynamic charge response of\nFeSe/SrTiO$_3$ we infer that the response of the monolayer itself is nearly\ntemperature independent. This indicates a constant Fermi surface over a wide\nrange of temperature, in stark contrast to that of the bulk FeSe and other\nFe-based superconductors. Our results, which manifest the peculiarity of the\nelectronic structure of the FeSe monolayer, may help for a microscopic\nunderstanding of the superconductivity in Fe-chalcogenide monolayers on oxide\nsurfaces in general.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The origin of superconductivity in the FeSe monolayer on SrTiO$_3$ remains\none of the unresolved mysteries in condensed-matter physics. Here by\ninvestigation of the temperature evolution of the dynamic charge response of\nFeSe/SrTiO$_3$ we infer that the response of the monolayer itself is nearly\ntemperature independent. This indicates a constant Fermi surface over a wide\nrange of temperature, in stark contrast to that of the bulk FeSe and other\nFe-based superconductors. Our results, which manifest the peculiarity of the\nelectronic structure of the FeSe monolayer, may help for a microscopic\nunderstanding of the superconductivity in Fe-chalcogenide monolayers on oxide\nsurfaces in general."
                },
                "authors": [
                    {
                        "name": "Khalil Zakeri"
                    },
                    {
                        "name": "Ryan Roemer"
                    },
                    {
                        "name": "Ke Zou"
                    }
                ],
                "author_detail": {
                    "name": "Ke Zou"
                },
                "author": "Ke Zou",
                "arxiv_doi": "10.1103/PhysRevB.110.224504",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevB.110.224504",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.16910v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16910v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "7 Pages, 3 Figures",
                "arxiv_journal_ref": "Phys. Rev. B 110, 224504 (2024)",
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04987v1",
                "updated": "2024-12-06T12:15:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    12,
                    15,
                    24,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T12:15:24Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    12,
                    15,
                    24,
                    4,
                    341,
                    0
                ],
                "title": "FlowPolicy: Enabling Fast and Robust 3D Flow-based Policy via\n  Consistency Flow Matching for Robot Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowPolicy: Enabling Fast and Robust 3D Flow-based Policy via\n  Consistency Flow Matching for Robot Manipulation"
                },
                "summary": "Robots can acquire complex manipulation skills by learning policies from\nexpert demonstrations, which is often known as vision-based imitation learning.\nGenerating policies based on diffusion and flow matching models has been shown\nto be effective, particularly in robotics manipulation tasks. However,\nrecursion-based approaches are often inference inefficient in working from\nnoise distributions to policy distributions, posing a challenging trade-off\nbetween efficiency and quality. This motivates us to propose FlowPolicy, a\nnovel framework for fast policy generation based on consistency flow matching\nand 3D vision. Our approach refines the flow dynamics by normalizing the\nself-consistency of the velocity field, enabling the model to derive task\nexecution policies in a single inference step. Specifically, FlowPolicy\nconditions on the observed 3D point cloud, where consistency flow matching\ndirectly defines straight-line flows from different time states to the same\naction space, while simultaneously constraining their velocity values, that is,\nwe approximate the trajectories from noise to robot actions by normalizing the\nself-consistency of the velocity field within the action space, thus improving\nthe inference efficiency. We validate the effectiveness of FlowPolicy on Adroit\nand Metaworld, demonstrating a 7$\\times$ increase in inference speed while\nmaintaining competitive average success rates compared to state-of-the-art\npolicy models. Codes will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robots can acquire complex manipulation skills by learning policies from\nexpert demonstrations, which is often known as vision-based imitation learning.\nGenerating policies based on diffusion and flow matching models has been shown\nto be effective, particularly in robotics manipulation tasks. However,\nrecursion-based approaches are often inference inefficient in working from\nnoise distributions to policy distributions, posing a challenging trade-off\nbetween efficiency and quality. This motivates us to propose FlowPolicy, a\nnovel framework for fast policy generation based on consistency flow matching\nand 3D vision. Our approach refines the flow dynamics by normalizing the\nself-consistency of the velocity field, enabling the model to derive task\nexecution policies in a single inference step. Specifically, FlowPolicy\nconditions on the observed 3D point cloud, where consistency flow matching\ndirectly defines straight-line flows from different time states to the same\naction space, while simultaneously constraining their velocity values, that is,\nwe approximate the trajectories from noise to robot actions by normalizing the\nself-consistency of the velocity field within the action space, thus improving\nthe inference efficiency. We validate the effectiveness of FlowPolicy on Adroit\nand Metaworld, demonstrating a 7$\\times$ increase in inference speed while\nmaintaining competitive average success rates compared to state-of-the-art\npolicy models. Codes will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Qinglun Zhang"
                    },
                    {
                        "name": "Zhen Liu"
                    },
                    {
                        "name": "Haoqiang Fan"
                    },
                    {
                        "name": "Guanghui Liu"
                    },
                    {
                        "name": "Bing Zeng"
                    },
                    {
                        "name": "Shuaicheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Shuaicheng Liu"
                },
                "author": "Shuaicheng Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09439v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09439v2",
                "updated": "2024-12-06T12:09:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    12,
                    9,
                    15,
                    4,
                    341,
                    0
                ],
                "published": "2024-08-18T11:07:38Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    11,
                    7,
                    38,
                    6,
                    231,
                    0
                ],
                "title": "Towards Boosting LLMs-driven Relevance Modeling with Progressive\n  Retrieved Behavior-augmented Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Boosting LLMs-driven Relevance Modeling with Progressive\n  Retrieved Behavior-augmented Prompting"
                },
                "summary": "Relevance modeling is a critical component for enhancing user experience in\nsearch engines, with the primary objective of identifying items that align with\nusers' queries. Traditional models only rely on the semantic congruence between\nqueries and items to ascertain relevance. However, this approach represents\nmerely one aspect of the relevance judgement, and is insufficient in isolation.\nEven powerful Large Language Models (LLMs) still cannot accurately judge the\nrelevance of a query and an item from a semantic perspective. To augment\nLLMs-driven relevance modeling, this study proposes leveraging user\ninteractions recorded in search logs to yield insights into users' implicit\nsearch intentions. The challenge lies in the effective prompting of LLMs to\ncapture dynamic search intentions, which poses several obstacles in real-world\nrelevance scenarios, i.e., the absence of domain-specific knowledge, the\ninadequacy of an isolated prompt, and the prohibitive costs associated with\ndeploying LLMs. In response, we propose ProRBP, a novel Progressive Retrieved\nBehavior-augmented Prompting framework for integrating search scenario-oriented\nknowledge with LLMs effectively. Specifically, we perform the user-driven\nbehavior neighbors retrieval from the daily search logs to obtain\ndomain-specific knowledge in time, retrieving candidates that users consider to\nmeet their expectations. Then, we guide LLMs for relevance modeling by\nemploying advanced prompting techniques that progressively improve the outputs\nof the LLMs, followed by a progressive aggregation with comprehensive\nconsideration of diverse aspects. For online serving, we have developed an\nindustrial application framework tailored for the deployment of LLMs in\nrelevance modeling. Experiments on real-world industry data and online A/B\ntesting demonstrate our proposal achieves promising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relevance modeling is a critical component for enhancing user experience in\nsearch engines, with the primary objective of identifying items that align with\nusers' queries. Traditional models only rely on the semantic congruence between\nqueries and items to ascertain relevance. However, this approach represents\nmerely one aspect of the relevance judgement, and is insufficient in isolation.\nEven powerful Large Language Models (LLMs) still cannot accurately judge the\nrelevance of a query and an item from a semantic perspective. To augment\nLLMs-driven relevance modeling, this study proposes leveraging user\ninteractions recorded in search logs to yield insights into users' implicit\nsearch intentions. The challenge lies in the effective prompting of LLMs to\ncapture dynamic search intentions, which poses several obstacles in real-world\nrelevance scenarios, i.e., the absence of domain-specific knowledge, the\ninadequacy of an isolated prompt, and the prohibitive costs associated with\ndeploying LLMs. In response, we propose ProRBP, a novel Progressive Retrieved\nBehavior-augmented Prompting framework for integrating search scenario-oriented\nknowledge with LLMs effectively. Specifically, we perform the user-driven\nbehavior neighbors retrieval from the daily search logs to obtain\ndomain-specific knowledge in time, retrieving candidates that users consider to\nmeet their expectations. Then, we guide LLMs for relevance modeling by\nemploying advanced prompting techniques that progressively improve the outputs\nof the LLMs, followed by a progressive aggregation with comprehensive\nconsideration of diverse aspects. For online serving, we have developed an\nindustrial application framework tailored for the deployment of LLMs in\nrelevance modeling. Experiments on real-world industry data and online A/B\ntesting demonstrate our proposal achieves promising performance."
                },
                "authors": [
                    {
                        "name": "Zeyuan Chen"
                    },
                    {
                        "name": "Haiyan Wu"
                    },
                    {
                        "name": "Kaixin Wu"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Mingjie Zhong"
                    },
                    {
                        "name": "Jia Xu"
                    },
                    {
                        "name": "Zhongyi Liu"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "Accepted By COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09439v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09439v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04983v1",
                "updated": "2024-12-06T12:07:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    12,
                    7,
                    44,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T12:07:44Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    12,
                    7,
                    44,
                    4,
                    341,
                    0
                ],
                "title": "Red, hot, and very metal poor: extreme properties of a massive accreting\n  black hole in the first 500 Myr",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Red, hot, and very metal poor: extreme properties of a massive accreting\n  black hole in the first 500 Myr"
                },
                "summary": "The James Webb Space Telescope (JWST) has recently discovered a new\npopulation of objects at high redshift referred to as `Little Red Dots' (LRDs).\nTheir nature currently remains elusive, despite their surprisingly high\ninferred number densities. This emerging population of red point-like sources\nis reshaping our view of the early Universe and may shed light on the formation\nof high-redshift supermassive black holes. Here we present a spectroscopically\nconfirmed LRD CANUCS-LRD-z8.6 at $z_{\\rm spec}=8.6319\\pm 0.0005$ hosting an\nActive Galactic Nucleus (AGN), using JWST data. This source shows the typical\nspectral shape of an LRD (blue UV and red optical continuum, unresolved in JWST\nimaging), along with broad H$\\beta$ line emission, detection of high-ionization\nemission lines (CIV, NIV]) and very high electron temperature indicative of the\npresence of AGN. This is also combined with a very low metallicity ($Z<0.1\nZ_\\odot$). The presence of all these diverse features in one source makes\nCANUCS-LRD-z8.6 unique. We show that the inferred black hole mass of\nCANUCS-LRD-z8.6 ($M_{\\rm BH}=1.0^{+0.6}_{-0.4}\\times 10^{8}\\rm ~M_\\odot$)\nstrongly challenges current standard theoretical models and simulations of\nblack hole formation, and forces us to adopt `ad hoc' prescriptions. Indeed if\nmassive seeds, or light seeds with super-Eddington accretion, are considered,\nthe observed BH mass of CANUCS-LRD-z8.6 at $z=8.6$ can be reproduced. Moreover,\nthe black hole is over-massive compared to its host, relative to the local\n$M_{\\rm BH}-M_*$ relations, pointing towards an earlier and faster evolution of\nthe black hole compared to its host galaxy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The James Webb Space Telescope (JWST) has recently discovered a new\npopulation of objects at high redshift referred to as `Little Red Dots' (LRDs).\nTheir nature currently remains elusive, despite their surprisingly high\ninferred number densities. This emerging population of red point-like sources\nis reshaping our view of the early Universe and may shed light on the formation\nof high-redshift supermassive black holes. Here we present a spectroscopically\nconfirmed LRD CANUCS-LRD-z8.6 at $z_{\\rm spec}=8.6319\\pm 0.0005$ hosting an\nActive Galactic Nucleus (AGN), using JWST data. This source shows the typical\nspectral shape of an LRD (blue UV and red optical continuum, unresolved in JWST\nimaging), along with broad H$\\beta$ line emission, detection of high-ionization\nemission lines (CIV, NIV]) and very high electron temperature indicative of the\npresence of AGN. This is also combined with a very low metallicity ($Z<0.1\nZ_\\odot$). The presence of all these diverse features in one source makes\nCANUCS-LRD-z8.6 unique. We show that the inferred black hole mass of\nCANUCS-LRD-z8.6 ($M_{\\rm BH}=1.0^{+0.6}_{-0.4}\\times 10^{8}\\rm ~M_\\odot$)\nstrongly challenges current standard theoretical models and simulations of\nblack hole formation, and forces us to adopt `ad hoc' prescriptions. Indeed if\nmassive seeds, or light seeds with super-Eddington accretion, are considered,\nthe observed BH mass of CANUCS-LRD-z8.6 at $z=8.6$ can be reproduced. Moreover,\nthe black hole is over-massive compared to its host, relative to the local\n$M_{\\rm BH}-M_*$ relations, pointing towards an earlier and faster evolution of\nthe black hole compared to its host galaxy."
                },
                "authors": [
                    {
                        "name": "Roberta Tripodi"
                    },
                    {
                        "name": "Nicholas Martis"
                    },
                    {
                        "name": "Vladan Markov"
                    },
                    {
                        "name": "Maruša Bradač"
                    },
                    {
                        "name": "Fabio Di Mascia"
                    },
                    {
                        "name": "Vieri Cammelli"
                    },
                    {
                        "name": "Francesco D'Eugenio"
                    },
                    {
                        "name": "Chris Willott"
                    },
                    {
                        "name": "Mirko Curti"
                    },
                    {
                        "name": "Maulik Bhatt"
                    },
                    {
                        "name": "Simona Gallerani"
                    },
                    {
                        "name": "Gregor Rihtaršič"
                    },
                    {
                        "name": "Jasbir Singh"
                    },
                    {
                        "name": "Gaia Gaspar"
                    },
                    {
                        "name": "Anishya Harshan"
                    },
                    {
                        "name": "Jon Judež"
                    },
                    {
                        "name": "Rosa M. Merida"
                    },
                    {
                        "name": "Guillaume Desprez"
                    },
                    {
                        "name": "Marcin Sawicki"
                    },
                    {
                        "name": "Ilias Goovaerts"
                    },
                    {
                        "name": "Adam Muzzin"
                    },
                    {
                        "name": "Gaël Noirot"
                    },
                    {
                        "name": "Ghassan T. E. Sarrouh"
                    },
                    {
                        "name": "Roberto Abraham"
                    },
                    {
                        "name": "Yoshihisa Asada"
                    },
                    {
                        "name": "Gabriel Brammer"
                    },
                    {
                        "name": "Vicente Estrada Carpenter"
                    },
                    {
                        "name": "Giordano Felicioni"
                    },
                    {
                        "name": "Seiji Fujimoto"
                    },
                    {
                        "name": "Kartheik Iyer"
                    },
                    {
                        "name": "Lamiya Mowla"
                    },
                    {
                        "name": "Victoria Strait"
                    }
                ],
                "author_detail": {
                    "name": "Victoria Strait"
                },
                "author": "Victoria Strait",
                "arxiv_comment": "4 main figures; 8 supplementary figures; 5 supplementary tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01349v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01349v3",
                "updated": "2024-12-06T11:54:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    54,
                    40,
                    4,
                    341,
                    0
                ],
                "published": "2024-02-02T12:07:00Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    12,
                    7,
                    0,
                    4,
                    33,
                    0
                ],
                "title": "LLMs May Perform MCQA by Selecting the Least Incorrect Option",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs May Perform MCQA by Selecting the Least Incorrect Option"
                },
                "summary": "In the field of NLP, Large Language Models (LLMs) have markedly enhanced\nperformance across a variety of tasks. However, the comprehensive evaluation of\nLLMs remains an inevitable challenge for the community. Recently, the adoption\nof Multiple Choice Question Answering (MCQA) as a benchmark for assessing LLMs\nhas gained considerable traction. However, concerns regarding the robustness of\nthis evaluative method persist. Building upon previous discussions on the issue\nof \\textit{variability}, we reveal an additional dimension of concern: LLMs may\nperform MCQA by selecting the least incorrect option rather than distinctly\ncorrect. This observation suggests that LLMs might regard multiple options as\ncorrect, which could undermine the reliability of MCQA as a metric for\nevaluating LLMs. To address this challenge, we introduce an enhanced dataset\naugmentation method for MCQA, termed MCQA+, to provide a more accurate\nreflection of the model performance, thereby highlighting the necessity for\nmore sophisticated evaluation mechanisms in the assessment of LLM capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of NLP, Large Language Models (LLMs) have markedly enhanced\nperformance across a variety of tasks. However, the comprehensive evaluation of\nLLMs remains an inevitable challenge for the community. Recently, the adoption\nof Multiple Choice Question Answering (MCQA) as a benchmark for assessing LLMs\nhas gained considerable traction. However, concerns regarding the robustness of\nthis evaluative method persist. Building upon previous discussions on the issue\nof \\textit{variability}, we reveal an additional dimension of concern: LLMs may\nperform MCQA by selecting the least incorrect option rather than distinctly\ncorrect. This observation suggests that LLMs might regard multiple options as\ncorrect, which could undermine the reliability of MCQA as a metric for\nevaluating LLMs. To address this challenge, we introduce an enhanced dataset\naugmentation method for MCQA, termed MCQA+, to provide a more accurate\nreflection of the model performance, thereby highlighting the necessity for\nmore sophisticated evaluation mechanisms in the assessment of LLM capabilities."
                },
                "authors": [
                    {
                        "name": "Haochun Wang"
                    },
                    {
                        "name": "Sendong Zhao"
                    },
                    {
                        "name": "Zewen Qiang"
                    },
                    {
                        "name": "Nuwa Xi"
                    },
                    {
                        "name": "Bing Qin"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "arxiv_comment": "COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01349v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01349v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04977v1",
                "updated": "2024-12-06T11:53:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    53,
                    57,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T11:53:57Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    53,
                    57,
                    4,
                    341,
                    0
                ],
                "title": "ORKG ASK: a Neuro-symbolic Scholarly Search and Exploration System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORKG ASK: a Neuro-symbolic Scholarly Search and Exploration System"
                },
                "summary": "Purpose: Finding scholarly articles is a time-consuming and cumbersome\nactivity, yet crucial for conducting science. Due to the growing number of\nscholarly articles, new scholarly search systems are needed to effectively\nassist researchers in finding relevant literature.\n  Methodology: We take a neuro-symbolic approach to scholarly search and\nexploration by leveraging state-of-the-art components, including semantic\nsearch, Large Language Models (LLMs), and Knowledge Graphs (KGs). The semantic\nsearch component composes a set of relevant articles. From this set of\narticles, information is extracted and presented to the user.\n  Findings: The presented system, called ORKG ASK (Assistant for Scientific\nKnowledge), provides a production-ready search and exploration system. Our\npreliminary evaluation indicates that our proposed approach is indeed suitable\nfor the task of scholarly information retrieval.\n  Value: With ORKG ASK, we present a next-generation scholarly search and\nexploration system and make it available online. Additionally, the system\ncomponents are open source with a permissive license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: Finding scholarly articles is a time-consuming and cumbersome\nactivity, yet crucial for conducting science. Due to the growing number of\nscholarly articles, new scholarly search systems are needed to effectively\nassist researchers in finding relevant literature.\n  Methodology: We take a neuro-symbolic approach to scholarly search and\nexploration by leveraging state-of-the-art components, including semantic\nsearch, Large Language Models (LLMs), and Knowledge Graphs (KGs). The semantic\nsearch component composes a set of relevant articles. From this set of\narticles, information is extracted and presented to the user.\n  Findings: The presented system, called ORKG ASK (Assistant for Scientific\nKnowledge), provides a production-ready search and exploration system. Our\npreliminary evaluation indicates that our proposed approach is indeed suitable\nfor the task of scholarly information retrieval.\n  Value: With ORKG ASK, we present a next-generation scholarly search and\nexploration system and make it available online. Additionally, the system\ncomponents are open source with a permissive license."
                },
                "authors": [
                    {
                        "name": "Allard Oelen"
                    },
                    {
                        "name": "Mohamad Yaser Jaradeh"
                    },
                    {
                        "name": "Sören Auer"
                    }
                ],
                "author_detail": {
                    "name": "Sören Auer"
                },
                "author": "Sören Auer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04315v2",
                "updated": "2024-12-06T11:39:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    39,
                    27,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-05T16:31:13Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    31,
                    13,
                    3,
                    340,
                    0
                ],
                "title": "Densing Law of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Densing Law of LLMs"
                },
                "summary": "Large Language Models (LLMs) have emerged as a milestone in artificial\nintelligence, and their performance can improve as the model size increases.\nHowever, this scaling brings great challenges to training and inference\nefficiency, particularly for deploying LLMs in resource-constrained\nenvironments, and the scaling trend is becoming increasingly unsustainable.\nThis paper introduces the concept of ``\\textit{capacity density}'' as a new\nmetric to evaluate the quality of the LLMs across different scales and\ndescribes the trend of LLMs in terms of both effectiveness and efficiency. To\ncalculate the capacity density of a given target LLM, we first introduce a set\nof reference models and develop a scaling law to predict the downstream\nperformance of these reference models based on their parameter sizes. We then\ndefine the \\textit{effective parameter size} of the target LLM as the parameter\nsize required by a reference model to achieve equivalent performance, and\nformalize the capacity density as the ratio of the effective parameter size to\nthe actual parameter size of the target LLM. Capacity density provides a\nunified framework for assessing both model effectiveness and efficiency. Our\nfurther analysis of recent open-source base LLMs reveals an empirical law (the\ndensing law)that the capacity density of LLMs grows exponentially over time.\nMore specifically, using some widely used benchmarks for evaluation, the\ncapacity density of LLMs doubles approximately every three months. The law\nprovides new perspectives to guide future LLM development, emphasizing the\nimportance of improving capacity density to achieve optimal results with\nminimal computational overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as a milestone in artificial\nintelligence, and their performance can improve as the model size increases.\nHowever, this scaling brings great challenges to training and inference\nefficiency, particularly for deploying LLMs in resource-constrained\nenvironments, and the scaling trend is becoming increasingly unsustainable.\nThis paper introduces the concept of ``\\textit{capacity density}'' as a new\nmetric to evaluate the quality of the LLMs across different scales and\ndescribes the trend of LLMs in terms of both effectiveness and efficiency. To\ncalculate the capacity density of a given target LLM, we first introduce a set\nof reference models and develop a scaling law to predict the downstream\nperformance of these reference models based on their parameter sizes. We then\ndefine the \\textit{effective parameter size} of the target LLM as the parameter\nsize required by a reference model to achieve equivalent performance, and\nformalize the capacity density as the ratio of the effective parameter size to\nthe actual parameter size of the target LLM. Capacity density provides a\nunified framework for assessing both model effectiveness and efficiency. Our\nfurther analysis of recent open-source base LLMs reveals an empirical law (the\ndensing law)that the capacity density of LLMs grows exponentially over time.\nMore specifically, using some widely used benchmarks for evaluation, the\ncapacity density of LLMs doubles approximately every three months. The law\nprovides new perspectives to guide future LLM development, emphasizing the\nimportance of improving capacity density to achieve optimal results with\nminimal computational overhead."
                },
                "authors": [
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Jie Cai"
                    },
                    {
                        "name": "Weilin Zhao"
                    },
                    {
                        "name": "Guoyang Zeng"
                    },
                    {
                        "name": "Biyuan Lin"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Zhi Zheng"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04964v1",
                "updated": "2024-12-06T11:29:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    29,
                    32,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T11:29:32Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    29,
                    32,
                    4,
                    341,
                    0
                ],
                "title": "Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast\n  Large Language Model Inference"
                },
                "summary": "The ever-increasing sizes of large language models necessitate distributed\nsolutions for fast inference that exploit multi-dimensional parallelism, where\ncomputational loads are split across various accelerators such as GPU clusters.\nHowever, this approach often introduces significant communication overhead,\nespecially on devices with limited bandwidth. In this paper, we introduce\n\\emph{Flash Communication}, a novel low-bit compression technique designed to\nalleviate the tensor-parallelism communication bottleneck during inference. Our\nmethod substantially boosts intra-node communication speed by more than 3x and\nreduces the \\emph{time-to-first-token} by 2x, with nearly no sacrifice in model\naccuracy. Extensive experiments on various up-to-date LLMs demonstrate the\neffectiveness of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ever-increasing sizes of large language models necessitate distributed\nsolutions for fast inference that exploit multi-dimensional parallelism, where\ncomputational loads are split across various accelerators such as GPU clusters.\nHowever, this approach often introduces significant communication overhead,\nespecially on devices with limited bandwidth. In this paper, we introduce\n\\emph{Flash Communication}, a novel low-bit compression technique designed to\nalleviate the tensor-parallelism communication bottleneck during inference. Our\nmethod substantially boosts intra-node communication speed by more than 3x and\nreduces the \\emph{time-to-first-token} by 2x, with nearly no sacrifice in model\naccuracy. Extensive experiments on various up-to-date LLMs demonstrate the\neffectiveness of our approach."
                },
                "authors": [
                    {
                        "name": "Qingyuan Li"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Liang Ye"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Yerui Sun"
                    },
                    {
                        "name": "Lin Ma"
                    },
                    {
                        "name": "Yuchen Xie"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Xie"
                },
                "author": "Yuchen Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04954v1",
                "updated": "2024-12-06T11:14:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    14,
                    3,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T11:14:03Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    14,
                    3,
                    4,
                    341,
                    0
                ],
                "title": "Gla-AI4BioMed at RRG24: Visual Instruction-tuned Adaptation for\n  Radiology Report Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gla-AI4BioMed at RRG24: Visual Instruction-tuned Adaptation for\n  Radiology Report Generation"
                },
                "summary": "We introduce a radiology-focused visual language model designed to generate\nradiology reports from chest X-rays. Building on previous findings that large\nlanguage models (LLMs) can acquire multimodal capabilities when aligned with\npretrained vision encoders, we demonstrate similar potential with chest X-ray\nimages. This integration enhances the ability of model to understand and\ndescribe chest X-ray images. Our model combines an image encoder with a\nfine-tuned LLM based on the Vicuna-7B architecture, enabling it to generate\ndifferent sections of a radiology report with notable accuracy. The training\nprocess involves a two-stage approach: (i) initial alignment of chest X-ray\nfeatures with the LLM (ii) followed by fine-tuning for radiology report\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a radiology-focused visual language model designed to generate\nradiology reports from chest X-rays. Building on previous findings that large\nlanguage models (LLMs) can acquire multimodal capabilities when aligned with\npretrained vision encoders, we demonstrate similar potential with chest X-ray\nimages. This integration enhances the ability of model to understand and\ndescribe chest X-ray images. Our model combines an image encoder with a\nfine-tuned LLM based on the Vicuna-7B architecture, enabling it to generate\ndifferent sections of a radiology report with notable accuracy. The training\nprocess involves a two-stage approach: (i) initial alignment of chest X-ray\nfeatures with the LLM (ii) followed by fine-tuning for radiology report\ngeneration."
                },
                "authors": [
                    {
                        "name": "Xi Zhang"
                    },
                    {
                        "name": "Zaiqiao Meng"
                    },
                    {
                        "name": "Jake Lever"
                    },
                    {
                        "name": "Edmond S. L. Ho"
                    }
                ],
                "author_detail": {
                    "name": "Edmond S. L. Ho"
                },
                "author": "Edmond S. L. Ho",
                "arxiv_doi": "10.18653/v1/2024.bionlp-1.54",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.bionlp-1.54",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.04954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by BioNLP@ACL 2024",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04948v1",
                "updated": "2024-12-06T11:08:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    8,
                    24,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T11:08:24Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    8,
                    24,
                    4,
                    341,
                    0
                ],
                "title": "KaLM: Knowledge-aligned Autoregressive Language Modeling via Dual-view\n  Knowledge Graph Contrastive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KaLM: Knowledge-aligned Autoregressive Language Modeling via Dual-view\n  Knowledge Graph Contrastive Learning"
                },
                "summary": "Autoregressive large language models (LLMs) pre-trained by next token\nprediction are inherently proficient in generative tasks. However, their\nperformance on knowledge-driven tasks such as factual knowledge querying\nremains unsatisfactory. Knowledge graphs (KGs), as high-quality structured\nknowledge bases, can provide reliable knowledge for LLMs, potentially\ncompensating for their knowledge deficiencies. Aligning LLMs with explicit,\nstructured knowledge from KGs has been a challenge; previous attempts either\nfailed to effectively align knowledge representations or compromised the\ngenerative capabilities of LLMs, leading to less-than-optimal outcomes. This\npaper proposes \\textbf{KaLM}, a \\textit{Knowledge-aligned Language Modeling}\napproach, which fine-tunes autoregressive LLMs to align with KG knowledge via\nthe joint objective of explicit knowledge alignment and implicit knowledge\nalignment. The explicit knowledge alignment objective aims to directly optimize\nthe knowledge representation of LLMs through dual-view knowledge graph\ncontrastive learning. The implicit knowledge alignment objective focuses on\nincorporating textual patterns of knowledge into LLMs through triple completion\nlanguage modeling. Notably, our method achieves a significant performance boost\nin evaluations of knowledge-driven tasks, specifically embedding-based\nknowledge graph completion and generation-based knowledge graph question\nanswering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive large language models (LLMs) pre-trained by next token\nprediction are inherently proficient in generative tasks. However, their\nperformance on knowledge-driven tasks such as factual knowledge querying\nremains unsatisfactory. Knowledge graphs (KGs), as high-quality structured\nknowledge bases, can provide reliable knowledge for LLMs, potentially\ncompensating for their knowledge deficiencies. Aligning LLMs with explicit,\nstructured knowledge from KGs has been a challenge; previous attempts either\nfailed to effectively align knowledge representations or compromised the\ngenerative capabilities of LLMs, leading to less-than-optimal outcomes. This\npaper proposes \\textbf{KaLM}, a \\textit{Knowledge-aligned Language Modeling}\napproach, which fine-tunes autoregressive LLMs to align with KG knowledge via\nthe joint objective of explicit knowledge alignment and implicit knowledge\nalignment. The explicit knowledge alignment objective aims to directly optimize\nthe knowledge representation of LLMs through dual-view knowledge graph\ncontrastive learning. The implicit knowledge alignment objective focuses on\nincorporating textual patterns of knowledge into LLMs through triple completion\nlanguage modeling. Notably, our method achieves a significant performance boost\nin evaluations of knowledge-driven tasks, specifically embedding-based\nknowledge graph completion and generation-based knowledge graph question\nanswering."
                },
                "authors": [
                    {
                        "name": "Peng Yu"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Beiya Dai"
                    },
                    {
                        "name": "Xinbing Wang"
                    },
                    {
                        "name": "Ying Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wen"
                },
                "author": "Ying Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04947v1",
                "updated": "2024-12-06T11:07:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    7,
                    44,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T11:07:44Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    7,
                    44,
                    4,
                    341,
                    0
                ],
                "title": "C$^2$LEVA: Toward Comprehensive and Contamination-Free Language Model\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C$^2$LEVA: Toward Comprehensive and Contamination-Free Language Model\n  Evaluation"
                },
                "summary": "Recent advances in large language models (LLMs) have shown significant\npromise, yet their evaluation raises concerns, particularly regarding data\ncontamination due to the lack of access to proprietary training data. To\naddress this issue, we present C$^2$LEVA, a comprehensive bilingual benchmark\nfeaturing systematic contamination prevention. C$^2$LEVA firstly offers a\nholistic evaluation encompassing 22 tasks, each targeting a specific\napplication or ability of LLMs, and secondly a trustworthy assessment due to\nour contamination-free tasks, ensured by a systematic contamination prevention\nstrategy that fully automates test data renewal and enforces data protection\nduring benchmark data release. Our large-scale evaluation of 15 open-source and\nproprietary models demonstrates the effectiveness of C$^2$LEVA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have shown significant\npromise, yet their evaluation raises concerns, particularly regarding data\ncontamination due to the lack of access to proprietary training data. To\naddress this issue, we present C$^2$LEVA, a comprehensive bilingual benchmark\nfeaturing systematic contamination prevention. C$^2$LEVA firstly offers a\nholistic evaluation encompassing 22 tasks, each targeting a specific\napplication or ability of LLMs, and secondly a trustworthy assessment due to\nour contamination-free tasks, ensured by a systematic contamination prevention\nstrategy that fully automates test data renewal and enforces data protection\nduring benchmark data release. Our large-scale evaluation of 15 open-source and\nproprietary models demonstrates the effectiveness of C$^2$LEVA."
                },
                "authors": [
                    {
                        "name": "Yanyang Li"
                    },
                    {
                        "name": "Tin Long Wong"
                    },
                    {
                        "name": "Cheung To Hung"
                    },
                    {
                        "name": "Jianqiao Zhao"
                    },
                    {
                        "name": "Duo Zheng"
                    },
                    {
                        "name": "Ka Wai Liu"
                    },
                    {
                        "name": "Michael R. Lyu"
                    },
                    {
                        "name": "Liwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liwei Wang"
                },
                "author": "Liwei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04946v1",
                "updated": "2024-12-06T11:06:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    6,
                    1,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T11:06:01Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    6,
                    1,
                    4,
                    341,
                    0
                ],
                "title": "Impact of the Scalar Isovector $δ$-meson on the description of\n  nuclear matter and neutron star properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of the Scalar Isovector $δ$-meson on the description of\n  nuclear matter and neutron star properties"
                },
                "summary": "The implications of including the scalar isovector $\\delta$-meson in a\nrelativistic mean-field description of nuclear matter are discussed. A Bayesian\ninference approach is used to determine the parameters that define the\nisovector properties of the model. The properties of nuclear matter and neutron\nstars are discussed. The inclusion of the $\\delta$-meson has only a small\neffect on the maximum mass of the neutron star (NS) and on the speed of sound\nin its interior, but it has a strong effect on the radius and the tidal\ndeformability of low and medium mass stars. This is mainly due to the effect of\nthe $\\delta$-meson on the symmetry energy and its slope and curvature at\nsaturation, increasing the range of possible values of these three properties,\nand in particular allowing positive values of the symmetry energy curvature.\nDue to the effect of the $\\delta$-meson on the symmetry energy, the proton\ncontent of the star is also strongly affected. The inclusion of the\n$\\delta$-meson in the relativistic mean-field description of nuclear matter\nextends the phase space spanned by the model, allowing for a more flexible\ndensity dependence of the symmetry energy compatible with experimental,\nobservational, and ab initio constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The implications of including the scalar isovector $\\delta$-meson in a\nrelativistic mean-field description of nuclear matter are discussed. A Bayesian\ninference approach is used to determine the parameters that define the\nisovector properties of the model. The properties of nuclear matter and neutron\nstars are discussed. The inclusion of the $\\delta$-meson has only a small\neffect on the maximum mass of the neutron star (NS) and on the speed of sound\nin its interior, but it has a strong effect on the radius and the tidal\ndeformability of low and medium mass stars. This is mainly due to the effect of\nthe $\\delta$-meson on the symmetry energy and its slope and curvature at\nsaturation, increasing the range of possible values of these three properties,\nand in particular allowing positive values of the symmetry energy curvature.\nDue to the effect of the $\\delta$-meson on the symmetry energy, the proton\ncontent of the star is also strongly affected. The inclusion of the\n$\\delta$-meson in the relativistic mean-field description of nuclear matter\nextends the phase space spanned by the model, allowing for a more flexible\ndensity dependence of the symmetry energy compatible with experimental,\nobservational, and ab initio constraints."
                },
                "authors": [
                    {
                        "name": "Lavínia Gabriela Teodoro dos Santos"
                    },
                    {
                        "name": "Tuhin Malik"
                    },
                    {
                        "name": "Constança Providência"
                    }
                ],
                "author_detail": {
                    "name": "Constança Providência"
                },
                "author": "Constança Providência",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19050v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19050v2",
                "updated": "2024-12-06T10:58:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    10,
                    58,
                    53,
                    4,
                    341,
                    0
                ],
                "published": "2024-11-28T10:55:09Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    10,
                    55,
                    9,
                    3,
                    333,
                    0
                ],
                "title": "I Dream My Painting: Connecting MLLMs and Diffusion Models via Prompt\n  Generation for Text-Guided Multi-Mask Inpainting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Dream My Painting: Connecting MLLMs and Diffusion Models via Prompt\n  Generation for Text-Guided Multi-Mask Inpainting"
                },
                "summary": "Inpainting focuses on filling missing or corrupted regions of an image to\nblend seamlessly with its surrounding content and style. While conditional\ndiffusion models have proven effective for text-guided inpainting, we introduce\nthe novel task of multi-mask inpainting, where multiple regions are\nsimultaneously inpainted using distinct prompts. Furthermore, we design a\nfine-tuning procedure for multimodal LLMs, such as LLaVA, to generate\nmulti-mask prompts automatically using corrupted images as inputs. These models\ncan generate helpful and detailed prompt suggestions for filling the masked\nregions. The generated prompts are then fed to Stable Diffusion, which is\nfine-tuned for the multi-mask inpainting problem using rectified\ncross-attention, enforcing prompts onto their designated regions for filling.\nExperiments on digitized paintings from WikiArt and the Densely Captioned\nImages dataset demonstrate that our pipeline delivers creative and accurate\ninpainting results. Our code, data, and trained models are available at\nhttps://cilabuniba.github.io/i-dream-my-painting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inpainting focuses on filling missing or corrupted regions of an image to\nblend seamlessly with its surrounding content and style. While conditional\ndiffusion models have proven effective for text-guided inpainting, we introduce\nthe novel task of multi-mask inpainting, where multiple regions are\nsimultaneously inpainted using distinct prompts. Furthermore, we design a\nfine-tuning procedure for multimodal LLMs, such as LLaVA, to generate\nmulti-mask prompts automatically using corrupted images as inputs. These models\ncan generate helpful and detailed prompt suggestions for filling the masked\nregions. The generated prompts are then fed to Stable Diffusion, which is\nfine-tuned for the multi-mask inpainting problem using rectified\ncross-attention, enforcing prompts onto their designated regions for filling.\nExperiments on digitized paintings from WikiArt and the Densely Captioned\nImages dataset demonstrate that our pipeline delivers creative and accurate\ninpainting results. Our code, data, and trained models are available at\nhttps://cilabuniba.github.io/i-dream-my-painting."
                },
                "authors": [
                    {
                        "name": "Nicola Fanelli"
                    },
                    {
                        "name": "Gennaro Vessio"
                    },
                    {
                        "name": "Giovanna Castellano"
                    }
                ],
                "author_detail": {
                    "name": "Giovanna Castellano"
                },
                "author": "Giovanna Castellano",
                "arxiv_comment": "Accepted at WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19050v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19050v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04937v1",
                "updated": "2024-12-06T10:45:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    10,
                    45,
                    54,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T10:45:54Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    10,
                    45,
                    54,
                    4,
                    341,
                    0
                ],
                "title": "Who Speaks Next? Multi-party AI Discussion Leveraging the Systematics of\n  Turn-taking in Murder Mystery Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who Speaks Next? Multi-party AI Discussion Leveraging the Systematics of\n  Turn-taking in Murder Mystery Games"
                },
                "summary": "Multi-agent systems utilizing large language models (LLMs) have shown great\npromise in achieving natural dialogue. However, smooth dialogue control and\nautonomous decision making among agents still remain challenges. In this study,\nwe focus on conversational norms such as adjacency pairs and turn-taking found\nin conversation analysis and propose a new framework called \"Murder Mystery\nAgents\" that applies these norms to AI agents' dialogue control. As an\nevaluation target, we employed the \"Murder Mystery\" game, a reasoning-type\ntable-top role-playing game that requires complex social reasoning and\ninformation manipulation. In this game, players need to unravel the truth of\nthe case based on fragmentary information through cooperation and bargaining.\nThe proposed framework integrates next speaker selection based on adjacency\npairs and a self-selection mechanism that takes agents' internal states into\naccount to achieve more natural and strategic dialogue. To verify the\neffectiveness of this new approach, we analyzed utterances that led to dialogue\nbreakdowns and conducted automatic evaluation using LLMs, as well as human\nevaluation using evaluation criteria developed for the Murder Mystery game.\nExperimental results showed that the implementation of the next speaker\nselection mechanism significantly reduced dialogue breakdowns and improved the\nability of agents to share information and perform logical reasoning. The\nresults of this study demonstrate that the systematics of turn-taking in human\nconversation are also effective in controlling dialogue among AI agents, and\nprovide design guidelines for more advanced multi-agent dialogue systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems utilizing large language models (LLMs) have shown great\npromise in achieving natural dialogue. However, smooth dialogue control and\nautonomous decision making among agents still remain challenges. In this study,\nwe focus on conversational norms such as adjacency pairs and turn-taking found\nin conversation analysis and propose a new framework called \"Murder Mystery\nAgents\" that applies these norms to AI agents' dialogue control. As an\nevaluation target, we employed the \"Murder Mystery\" game, a reasoning-type\ntable-top role-playing game that requires complex social reasoning and\ninformation manipulation. In this game, players need to unravel the truth of\nthe case based on fragmentary information through cooperation and bargaining.\nThe proposed framework integrates next speaker selection based on adjacency\npairs and a self-selection mechanism that takes agents' internal states into\naccount to achieve more natural and strategic dialogue. To verify the\neffectiveness of this new approach, we analyzed utterances that led to dialogue\nbreakdowns and conducted automatic evaluation using LLMs, as well as human\nevaluation using evaluation criteria developed for the Murder Mystery game.\nExperimental results showed that the implementation of the next speaker\nselection mechanism significantly reduced dialogue breakdowns and improved the\nability of agents to share information and perform logical reasoning. The\nresults of this study demonstrate that the systematics of turn-taking in human\nconversation are also effective in controlling dialogue among AI agents, and\nprovide design guidelines for more advanced multi-agent dialogue systems."
                },
                "authors": [
                    {
                        "name": "Ryota Nonomura"
                    },
                    {
                        "name": "Hiroki Mori"
                    }
                ],
                "author_detail": {
                    "name": "Hiroki Mori"
                },
                "author": "Hiroki Mori",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04929v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04929v1",
                "updated": "2024-12-06T10:34:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    10,
                    34,
                    50,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T10:34:50Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    10,
                    34,
                    50,
                    4,
                    341,
                    0
                ],
                "title": "Continuous Video Process: Modeling Videos as Continuous\n  Multi-Dimensional Processes for Video Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous Video Process: Modeling Videos as Continuous\n  Multi-Dimensional Processes for Video Prediction"
                },
                "summary": "Diffusion models have made significant strides in image generation, mastering\ntasks such as unconditional image synthesis, text-image translation, and\nimage-to-image conversions. However, their capability falls short in the realm\nof video prediction, mainly because they treat videos as a collection of\nindependent images, relying on external constraints such as temporal attention\nmechanisms to enforce temporal coherence. In our paper, we introduce a novel\nmodel class, that treats video as a continuous multi-dimensional process rather\nthan a series of discrete frames. We also report a reduction of 75\\% sampling\nsteps required to sample a new frame thus making our framework more efficient\nduring the inference time. Through extensive experimentation, we establish\nstate-of-the-art performance in video prediction, validated on benchmark\ndatasets including KTH, BAIR, Human3.6M, and UCF101. Navigate to the project\npage https://www.cs.umd.edu/~gauravsh/cvp/supp/website.html for video results.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have made significant strides in image generation, mastering\ntasks such as unconditional image synthesis, text-image translation, and\nimage-to-image conversions. However, their capability falls short in the realm\nof video prediction, mainly because they treat videos as a collection of\nindependent images, relying on external constraints such as temporal attention\nmechanisms to enforce temporal coherence. In our paper, we introduce a novel\nmodel class, that treats video as a continuous multi-dimensional process rather\nthan a series of discrete frames. We also report a reduction of 75\\% sampling\nsteps required to sample a new frame thus making our framework more efficient\nduring the inference time. Through extensive experimentation, we establish\nstate-of-the-art performance in video prediction, validated on benchmark\ndatasets including KTH, BAIR, Human3.6M, and UCF101. Navigate to the project\npage https://www.cs.umd.edu/~gauravsh/cvp/supp/website.html for video results.}"
                },
                "authors": [
                    {
                        "name": "Gaurav Shrivastava"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Shrivastava"
                },
                "author": "Abhinav Shrivastava",
                "arxiv_comment": "Navigate to the project page\n  https://www.cs.umd.edu/~gauravsh/cvp/supp/website.html for video results.\n  Extended version of published CVPR paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04929v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01294v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01294v2",
                "updated": "2024-12-06T10:31:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    10,
                    31,
                    43,
                    4,
                    341,
                    0
                ],
                "published": "2024-10-02T07:40:56Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    7,
                    40,
                    56,
                    2,
                    276,
                    0
                ],
                "title": "Endless Jailbreaks with Bijection Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Endless Jailbreaks with Bijection Learning"
                },
                "summary": "Despite extensive safety measures, LLMs are vulnerable to adversarial inputs,\nor jailbreaks, which can elicit unsafe behaviors. In this work, we introduce\nbijection learning, a powerful attack algorithm which automatically fuzzes LLMs\nfor safety vulnerabilities using randomly-generated encodings whose complexity\ncan be tightly controlled. We leverage in-context learning to teach models\nbijective encodings, pass encoded queries to the model to bypass built-in\nsafety mechanisms, and finally decode responses back into English. Our attack\nis extremely effective on a wide range of frontier language models. Moreover,\nby controlling complexity parameters such as number of key-value mappings in\nthe encodings, we find a close relationship between the capability level of the\nattacked LLM and the average complexity of the most effective bijection\nattacks. Our work highlights that new vulnerabilities in frontier models can\nemerge with scale: more capable models are more severely jailbroken by\nbijection attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite extensive safety measures, LLMs are vulnerable to adversarial inputs,\nor jailbreaks, which can elicit unsafe behaviors. In this work, we introduce\nbijection learning, a powerful attack algorithm which automatically fuzzes LLMs\nfor safety vulnerabilities using randomly-generated encodings whose complexity\ncan be tightly controlled. We leverage in-context learning to teach models\nbijective encodings, pass encoded queries to the model to bypass built-in\nsafety mechanisms, and finally decode responses back into English. Our attack\nis extremely effective on a wide range of frontier language models. Moreover,\nby controlling complexity parameters such as number of key-value mappings in\nthe encodings, we find a close relationship between the capability level of the\nattacked LLM and the average complexity of the most effective bijection\nattacks. Our work highlights that new vulnerabilities in frontier models can\nemerge with scale: more capable models are more severely jailbroken by\nbijection attacks."
                },
                "authors": [
                    {
                        "name": "Brian R. Y. Huang"
                    },
                    {
                        "name": "Maximilian Li"
                    },
                    {
                        "name": "Leonard Tang"
                    }
                ],
                "author_detail": {
                    "name": "Leonard Tang"
                },
                "author": "Leonard Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01294v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01294v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00353v2",
                "updated": "2024-12-06T10:24:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    10,
                    24,
                    47,
                    4,
                    341,
                    0
                ],
                "published": "2024-11-30T04:22:00Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    4,
                    22,
                    0,
                    5,
                    335,
                    0
                ],
                "title": "Enhancing Zero-shot Chain of Thought Prompting via Uncertainty-Guided\n  Strategy Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Zero-shot Chain of Thought Prompting via Uncertainty-Guided\n  Strategy Selection"
                },
                "summary": "Chain-of-thought (CoT) prompting has significantly enhanced the capability of\nlarge language models (LLMs) by structuring their reasoning processes. However,\nexisting methods face critical limitations: handcrafted demonstrations require\nextensive human expertise, while trigger phrases are prone to inaccuracies. In\nthis paper, we propose the Zero-shot Uncertainty-based Selection (ZEUS) method,\na novel approach that improves CoT prompting by utilizing uncertainty estimates\nto select effective demonstrations without needing access to model parameters.\nUnlike traditional methods, ZEUS offers high sensitivity in distinguishing\nbetween helpful and ineffective questions, ensuring more precise and reliable\nselection. Our extensive evaluation shows that ZEUS consistently outperforms\nexisting CoT strategies across four challenging reasoning benchmarks,\ndemonstrating its robustness and scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) prompting has significantly enhanced the capability of\nlarge language models (LLMs) by structuring their reasoning processes. However,\nexisting methods face critical limitations: handcrafted demonstrations require\nextensive human expertise, while trigger phrases are prone to inaccuracies. In\nthis paper, we propose the Zero-shot Uncertainty-based Selection (ZEUS) method,\na novel approach that improves CoT prompting by utilizing uncertainty estimates\nto select effective demonstrations without needing access to model parameters.\nUnlike traditional methods, ZEUS offers high sensitivity in distinguishing\nbetween helpful and ineffective questions, ensuring more precise and reliable\nselection. Our extensive evaluation shows that ZEUS consistently outperforms\nexisting CoT strategies across four challenging reasoning benchmarks,\ndemonstrating its robustness and scalability."
                },
                "authors": [
                    {
                        "name": "Shanu Kumar"
                    },
                    {
                        "name": "Saish Mendke"
                    },
                    {
                        "name": "Karody Lubna Abdul Rahman"
                    },
                    {
                        "name": "Santosh Kurasa"
                    },
                    {
                        "name": "Parag Agrawal"
                    },
                    {
                        "name": "Sandipan Dandapat"
                    }
                ],
                "author_detail": {
                    "name": "Sandipan Dandapat"
                },
                "author": "Sandipan Dandapat",
                "arxiv_comment": "Accepted in COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04922v1",
                "updated": "2024-12-06T10:21:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    10,
                    21,
                    25,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T10:21:25Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    10,
                    21,
                    25,
                    4,
                    341,
                    0
                ],
                "title": "Large Language Models for Ingredient Substitution in Food Recipes using\n  Supervised Fine-tuning and Direct Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Ingredient Substitution in Food Recipes using\n  Supervised Fine-tuning and Direct Preference Optimization"
                },
                "summary": "In this paper, we address the challenge of recipe personalization through\ningredient substitution. We make use of Large Language Models (LLMs) to build\nan ingredient substitution system designed to predict plausible substitute\ningredients within a given recipe context. Given that the use of LLMs for this\ntask has been barely done, we carry out an extensive set of experiments to\ndetermine the best LLM, prompt, and the fine-tuning setups. We further\nexperiment with methods such as multi-task learning, two-stage fine-tuning, and\nDirect Preference Optimization (DPO). The experiments are conducted using the\npublicly available Recipe1MSub corpus. The best results are produced by the\nMistral7-Base LLM after fine-tuning and DPO. This result outperforms the strong\nbaseline available for the same corpus with a Hit@1 score of 22.04. Thus we\nbelieve that this research represents a significant step towards enabling\npersonalized and creative culinary experiences by utilizing LLM-based\ningredient substitution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we address the challenge of recipe personalization through\ningredient substitution. We make use of Large Language Models (LLMs) to build\nan ingredient substitution system designed to predict plausible substitute\ningredients within a given recipe context. Given that the use of LLMs for this\ntask has been barely done, we carry out an extensive set of experiments to\ndetermine the best LLM, prompt, and the fine-tuning setups. We further\nexperiment with methods such as multi-task learning, two-stage fine-tuning, and\nDirect Preference Optimization (DPO). The experiments are conducted using the\npublicly available Recipe1MSub corpus. The best results are produced by the\nMistral7-Base LLM after fine-tuning and DPO. This result outperforms the strong\nbaseline available for the same corpus with a Hit@1 score of 22.04. Thus we\nbelieve that this research represents a significant step towards enabling\npersonalized and creative culinary experiences by utilizing LLM-based\ningredient substitution."
                },
                "authors": [
                    {
                        "name": "Thevin Senath"
                    },
                    {
                        "name": "Kumuthu Athukorala"
                    },
                    {
                        "name": "Ransika Costa"
                    },
                    {
                        "name": "Surangika Ranathunga"
                    },
                    {
                        "name": "Rishemjit Kaur"
                    }
                ],
                "author_detail": {
                    "name": "Rishemjit Kaur"
                },
                "author": "Rishemjit Kaur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04921v1",
                "updated": "2024-12-06T10:19:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    10,
                    19,
                    45,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T10:19:45Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    10,
                    19,
                    45,
                    4,
                    341,
                    0
                ],
                "title": "Uniform characterisation of an ensemble of main-sequence benchmark\n  stars: effect of Gaia-based data on grid search models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uniform characterisation of an ensemble of main-sequence benchmark\n  stars: effect of Gaia-based data on grid search models"
                },
                "summary": "The inference of stellar parameters (such as radius and mass) through\nasteroseismic forward modelling depends on the number, accuracy, and precision\nof seismic and atmospheric constraints. ESA's Gaia space mission is providing\nprecise parallaxes which yield an additional constraint to be included in the\nmodel grid search. Using a handful of main-sequence benchmark stars, we perform\na uniform characterisation of these stars. We assess the accuracy and precision\nof stellar parameters inferred from grid-based searches when a Gaia-based\nluminosity is combined with different stellar constraints. We also examine the\nprecision needed for an interferometric radius (model-independent radius) to\nhave a significant contribution towards the determination of stellar mass in\nthe optimisation process. Our findings show that more precise stellar masses\nare inferred for some stars when seismic and spectroscopic constraints are\ncomplemented with a Gaia-based luminosity, with a scatter varying from 1.9 per\ncent to 0.8 per cent. However, the inferred stellar radii are underestimated\nwhen compared to the interferometric radii and yield a scatter of $\\sim$1.9 per\ncent. In addition, we demonstrate that a precisely measured interferometric\nradius ($\\lesssim$ 1 per cent) when applied in the optimisation process yields\na mass with a precision $\\lesssim$ 1.5 per cent. Finally, we find that when\nonly $l=0$ mode oscillation frequencies are available, robust masses and radii\nare still attainable. However, this requires precise and numerous $l=0$ mode\noscillations frequencies ($>$ 8) to be coupled with atmospheric constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference of stellar parameters (such as radius and mass) through\nasteroseismic forward modelling depends on the number, accuracy, and precision\nof seismic and atmospheric constraints. ESA's Gaia space mission is providing\nprecise parallaxes which yield an additional constraint to be included in the\nmodel grid search. Using a handful of main-sequence benchmark stars, we perform\na uniform characterisation of these stars. We assess the accuracy and precision\nof stellar parameters inferred from grid-based searches when a Gaia-based\nluminosity is combined with different stellar constraints. We also examine the\nprecision needed for an interferometric radius (model-independent radius) to\nhave a significant contribution towards the determination of stellar mass in\nthe optimisation process. Our findings show that more precise stellar masses\nare inferred for some stars when seismic and spectroscopic constraints are\ncomplemented with a Gaia-based luminosity, with a scatter varying from 1.9 per\ncent to 0.8 per cent. However, the inferred stellar radii are underestimated\nwhen compared to the interferometric radii and yield a scatter of $\\sim$1.9 per\ncent. In addition, we demonstrate that a precisely measured interferometric\nradius ($\\lesssim$ 1 per cent) when applied in the optimisation process yields\na mass with a precision $\\lesssim$ 1.5 per cent. Finally, we find that when\nonly $l=0$ mode oscillation frequencies are available, robust masses and radii\nare still attainable. However, this requires precise and numerous $l=0$ mode\noscillations frequencies ($>$ 8) to be coupled with atmospheric constraints."
                },
                "authors": [
                    {
                        "name": "Benard Nsamba"
                    },
                    {
                        "name": "Achim Weiss"
                    },
                    {
                        "name": "Juma Kamulali"
                    }
                ],
                "author_detail": {
                    "name": "Juma Kamulali"
                },
                "author": "Juma Kamulali",
                "arxiv_comment": "14 pages, 10 figures, Accepted for publication in Monthly Notices of\n  the Royal Astronomical Society Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04917v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04917v1",
                "updated": "2024-12-06T10:16:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    10,
                    16,
                    4,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T10:16:04Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    10,
                    16,
                    4,
                    4,
                    341,
                    0
                ],
                "title": "Continuous Speech Tokens Makes LLMs Robust Multi-Modality Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous Speech Tokens Makes LLMs Robust Multi-Modality Learners"
                },
                "summary": "Recent advances in GPT-4o like multi-modality models have demonstrated\nremarkable progress for direct speech-to-speech conversation, with real-time\nspeech interaction experience and strong speech understanding ability. However,\ncurrent research focuses on discrete speech tokens to align with discrete text\ntokens for language modelling, which depends on an audio codec with residual\nconnections or independent group tokens, such a codec usually leverages large\nscale and diverse datasets training to ensure that the discrete speech codes\nhave good representation for varied domain, noise, style data reconstruction as\nwell as a well-designed codec quantizer and encoder-decoder architecture for\ndiscrete token language modelling. This paper introduces Flow-Omni, a\ncontinuous speech token based GPT-4o like model, capable of real-time speech\ninteraction and low streaming latency. Specifically, first, instead of\ncross-entropy loss only, we combine flow matching loss with a pretrained\nautoregressive LLM and a small MLP network to predict the probability\ndistribution of the continuous-valued speech tokens from speech prompt. second,\nwe incorporated the continuous speech tokens to Flow-Omni multi-modality\ntraining, thereby achieving robust speech-to-speech performance with discrete\ntext tokens and continuous speech tokens together. Experiments demonstrate\nthat, compared to discrete text and speech multi-modality training and its\nvariants, the continuous speech tokens mitigate robustness issues by avoiding\nthe inherent flaws of discrete speech code's representation loss for LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in GPT-4o like multi-modality models have demonstrated\nremarkable progress for direct speech-to-speech conversation, with real-time\nspeech interaction experience and strong speech understanding ability. However,\ncurrent research focuses on discrete speech tokens to align with discrete text\ntokens for language modelling, which depends on an audio codec with residual\nconnections or independent group tokens, such a codec usually leverages large\nscale and diverse datasets training to ensure that the discrete speech codes\nhave good representation for varied domain, noise, style data reconstruction as\nwell as a well-designed codec quantizer and encoder-decoder architecture for\ndiscrete token language modelling. This paper introduces Flow-Omni, a\ncontinuous speech token based GPT-4o like model, capable of real-time speech\ninteraction and low streaming latency. Specifically, first, instead of\ncross-entropy loss only, we combine flow matching loss with a pretrained\nautoregressive LLM and a small MLP network to predict the probability\ndistribution of the continuous-valued speech tokens from speech prompt. second,\nwe incorporated the continuous speech tokens to Flow-Omni multi-modality\ntraining, thereby achieving robust speech-to-speech performance with discrete\ntext tokens and continuous speech tokens together. Experiments demonstrate\nthat, compared to discrete text and speech multi-modality training and its\nvariants, the continuous speech tokens mitigate robustness issues by avoiding\nthe inherent flaws of discrete speech code's representation loss for LLM."
                },
                "authors": [
                    {
                        "name": "Ze Yuan"
                    },
                    {
                        "name": "Yanqing Liu"
                    },
                    {
                        "name": "Shujie Liu"
                    },
                    {
                        "name": "Sheng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Zhao"
                },
                "author": "Sheng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04917v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04917v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.02278v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.02278v5",
                "updated": "2024-12-06T10:13:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    10,
                    13,
                    10,
                    4,
                    341,
                    0
                ],
                "published": "2023-04-05T07:50:16Z",
                "published_parsed": [
                    2023,
                    4,
                    5,
                    7,
                    50,
                    16,
                    2,
                    95,
                    0
                ],
                "title": "SCMM: Calibrating Cross-modal Representations for Text-Based Person\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCMM: Calibrating Cross-modal Representations for Text-Based Person\n  Search"
                },
                "summary": "Text-Based Person Search (TBPS) is a crucial task in the Internet of Things\n(IoT) domain that enables accurate retrieval of target individuals from\nlarge-scale galleries with only given textual caption. For cross-modal TBPS\ntasks, it is critical to obtain well-distributed representation in the common\nembedding space to reduce the inter-modal gap. Furthermore, learning detailed\nimage-text correspondences is essential to discriminate similar targets and\nenable fine-grained search. To address these challenges, we present a simple\nyet effective method named Sew Calibration and Masked Modeling (SCMM) that\ncalibrates cross-modal representations by learning compact and well-aligned\nembeddings. SCMM introduces two novel losses for fine-grained cross-modal\nrepresentations: Sew calibration loss that aligns image and text features based\non textual caption quality, and Masked Caption Modeling (MCM) loss that\nestablishes detailed relationships between textual and visual parts. This\ndual-pronged strategy enhances feature alignment and cross-modal\ncorrespondences, enabling accurate distinction of similar individuals while\nmaintaining a streamlined dual-encoder architecture for real-time inference,\nwhich is essential for resource-limited sensors and IoT systems. Extensive\nexperiments on three popular TBPS benchmarks demonstrate the superiority of\nSCMM, achieving 73.81%, 64.25%, and 57.35% Rank-1 accuracy on CUHK-PEDES,\nICFG-PEDES, and RSTPReID, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-Based Person Search (TBPS) is a crucial task in the Internet of Things\n(IoT) domain that enables accurate retrieval of target individuals from\nlarge-scale galleries with only given textual caption. For cross-modal TBPS\ntasks, it is critical to obtain well-distributed representation in the common\nembedding space to reduce the inter-modal gap. Furthermore, learning detailed\nimage-text correspondences is essential to discriminate similar targets and\nenable fine-grained search. To address these challenges, we present a simple\nyet effective method named Sew Calibration and Masked Modeling (SCMM) that\ncalibrates cross-modal representations by learning compact and well-aligned\nembeddings. SCMM introduces two novel losses for fine-grained cross-modal\nrepresentations: Sew calibration loss that aligns image and text features based\non textual caption quality, and Masked Caption Modeling (MCM) loss that\nestablishes detailed relationships between textual and visual parts. This\ndual-pronged strategy enhances feature alignment and cross-modal\ncorrespondences, enabling accurate distinction of similar individuals while\nmaintaining a streamlined dual-encoder architecture for real-time inference,\nwhich is essential for resource-limited sensors and IoT systems. Extensive\nexperiments on three popular TBPS benchmarks demonstrate the superiority of\nSCMM, achieving 73.81%, 64.25%, and 57.35% Rank-1 accuracy on CUHK-PEDES,\nICFG-PEDES, and RSTPReID, respectively."
                },
                "authors": [
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Donglai Wei"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Sipeng Zhang"
                    },
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "Victor C. M. Leung"
                    }
                ],
                "author_detail": {
                    "name": "Victor C. M. Leung"
                },
                "author": "Victor C. M. Leung",
                "arxiv_comment": "10 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.02278v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.02278v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04905v1",
                "updated": "2024-12-06T10:01:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    10,
                    1,
                    38,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T10:01:38Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    10,
                    1,
                    38,
                    4,
                    341,
                    0
                ],
                "title": "DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling"
                },
                "summary": "Large language models (LLMs) have made dialogue one of the central modes of\nhuman-machine interaction, leading to the accumulation of vast amounts of\nconversation logs and increasing demand for dialogue generation. A\nconversational life-cycle spans from the Prelude through the Interlocution to\nthe Epilogue, encompassing various elements. Despite the existence of numerous\ndialogue-related studies, there is a lack of benchmarks that encompass\ncomprehensive dialogue elements, hindering precise modeling and systematic\nevaluation. To bridge this gap, we introduce an innovative research task\n$\\textbf{D}$ialogue $\\textbf{E}$lement $\\textbf{MO}$deling, including\n$\\textit{Element Awareness}$ and $\\textit{Dialogue Agent Interaction}$, and\npropose a novel benchmark, $\\textbf{DEMO}$, designed for a comprehensive\ndialogue modeling and assessment. Inspired by imitation learning, we further\nbuild the agent which possesses the adept ability to model dialogue elements\nbased on the DEMO benchmark. Extensive experiments indicate that existing LLMs\nstill exhibit considerable potential for enhancement, and our DEMO agent has\nsuperior performance in both in-domain and out-of-domain tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made dialogue one of the central modes of\nhuman-machine interaction, leading to the accumulation of vast amounts of\nconversation logs and increasing demand for dialogue generation. A\nconversational life-cycle spans from the Prelude through the Interlocution to\nthe Epilogue, encompassing various elements. Despite the existence of numerous\ndialogue-related studies, there is a lack of benchmarks that encompass\ncomprehensive dialogue elements, hindering precise modeling and systematic\nevaluation. To bridge this gap, we introduce an innovative research task\n$\\textbf{D}$ialogue $\\textbf{E}$lement $\\textbf{MO}$deling, including\n$\\textit{Element Awareness}$ and $\\textit{Dialogue Agent Interaction}$, and\npropose a novel benchmark, $\\textbf{DEMO}$, designed for a comprehensive\ndialogue modeling and assessment. Inspired by imitation learning, we further\nbuild the agent which possesses the adept ability to model dialogue elements\nbased on the DEMO benchmark. Extensive experiments indicate that existing LLMs\nstill exhibit considerable potential for enhancement, and our DEMO agent has\nsuperior performance in both in-domain and out-of-domain tasks."
                },
                "authors": [
                    {
                        "name": "Minzheng Wang"
                    },
                    {
                        "name": "Xinghua Zhang"
                    },
                    {
                        "name": "Kun Chen"
                    },
                    {
                        "name": "Nan Xu"
                    },
                    {
                        "name": "Haiyang Yu"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Wenji Mao"
                    },
                    {
                        "name": "Yongbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Li"
                },
                "author": "Yongbin Li",
                "arxiv_comment": "We release the code and data at https://github.com/MozerWang/DEMO",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04903v1",
                "updated": "2024-12-06T09:59:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    9,
                    59,
                    47,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T09:59:47Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    9,
                    59,
                    47,
                    4,
                    341,
                    0
                ],
                "title": "EACO: Enhancing Alignment in Multimodal LLMs via Critical Observation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EACO: Enhancing Alignment in Multimodal LLMs via Critical Observation"
                },
                "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress on\nvarious visual question answering and reasoning tasks leveraging instruction\nfine-tuning specific datasets. They can also learn from preference data\nannotated by human to enhance their reasoning ability and mitigate\nhallucinations. Most of preference data is generated from the model itself.\nHowever, existing methods require high-quality critical labels, which are\ncostly and rely on human or proprietary models like GPT-4V. In this work, we\npropose Enhancing Alignment in MLLMs via Critical Observation (EACO), which\naligns MLLMs by self-generated preference data using only 5k images\neconomically. Our approach begins with collecting and refining a Scoring\nEvaluation Instruction-tuning dataset to train a critical evaluation model,\ntermed the Critic. This Critic observes model responses across multiple\ndimensions, selecting preferred and non-preferred outputs for refined Direct\nPreference Optimization (DPO) tuning. To further enhance model performance, we\nemploy an additional supervised fine-tuning stage after preference tuning. EACO\nreduces the overall hallucinations by 65.6% on HallusionBench and improves the\nreasoning ability by 21.8% on MME-Cognition. EACO achieves an 8.5% improvement\nover LLaVA-v1.6-Mistral-7B across multiple benchmarks. Remarkably, EACO also\nshows the potential critical ability in open-source MLLMs, demonstrating that\nEACO is a viable path to boost the competence of MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have achieved remarkable progress on\nvarious visual question answering and reasoning tasks leveraging instruction\nfine-tuning specific datasets. They can also learn from preference data\nannotated by human to enhance their reasoning ability and mitigate\nhallucinations. Most of preference data is generated from the model itself.\nHowever, existing methods require high-quality critical labels, which are\ncostly and rely on human or proprietary models like GPT-4V. In this work, we\npropose Enhancing Alignment in MLLMs via Critical Observation (EACO), which\naligns MLLMs by self-generated preference data using only 5k images\neconomically. Our approach begins with collecting and refining a Scoring\nEvaluation Instruction-tuning dataset to train a critical evaluation model,\ntermed the Critic. This Critic observes model responses across multiple\ndimensions, selecting preferred and non-preferred outputs for refined Direct\nPreference Optimization (DPO) tuning. To further enhance model performance, we\nemploy an additional supervised fine-tuning stage after preference tuning. EACO\nreduces the overall hallucinations by 65.6% on HallusionBench and improves the\nreasoning ability by 21.8% on MME-Cognition. EACO achieves an 8.5% improvement\nover LLaVA-v1.6-Mistral-7B across multiple benchmarks. Remarkably, EACO also\nshows the potential critical ability in open-source MLLMs, demonstrating that\nEACO is a viable path to boost the competence of MLLMs."
                },
                "authors": [
                    {
                        "name": "Yongxin Wang"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Haokun Lin"
                    },
                    {
                        "name": "Mingfei Han"
                    },
                    {
                        "name": "Liang Ma"
                    },
                    {
                        "name": "Jin Jiang"
                    },
                    {
                        "name": "Yuhao Cheng"
                    },
                    {
                        "name": "Xiaodan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Liang"
                },
                "author": "Xiaodan Liang",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2211.04700v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2211.04700v3",
                "updated": "2024-12-06T09:46:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    9,
                    46,
                    9,
                    4,
                    341,
                    0
                ],
                "published": "2022-11-09T06:18:18Z",
                "published_parsed": [
                    2022,
                    11,
                    9,
                    6,
                    18,
                    18,
                    2,
                    313,
                    0
                ],
                "title": "Noise Self-Regression: A New Learning Paradigm to Enhance Low-Light\n  Images Without Task-Related Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise Self-Regression: A New Learning Paradigm to Enhance Low-Light\n  Images Without Task-Related Data"
                },
                "summary": "Deep learning-based low-light image enhancement (LLIE) is a task of\nleveraging deep neural networks to enhance the image illumination while keeping\nthe image content unchanged. From the perspective of training data, existing\nmethods complete the LLIE task driven by one of the following three data types:\npaired data, unpaired data and zero-reference data. Each type of these\ndata-driven methods has its own advantages, e.g., zero-reference data-based\nmethods have very low requirements on training data and can meet the human\nneeds in many scenarios. In this paper, we leverage pure Gaussian noise to\ncomplete the LLIE task, which further reduces the requirements for training\ndata in LLIE tasks and can be used as another alternative in practical use.\nSpecifically, we propose Noise SElf-Regression (NoiSER) without access to any\ntask-related data, simply learns a convolutional neural network equipped with\nan instance-normalization layer by taking a random noise image,\n$\\mathcal{N}(0,\\sigma^2)$ for each pixel, as both input and output for each\ntraining pair, and then the low-light image is fed to the trained network for\npredicting the normal-light image. Technically, an intuitive explanation for\nits effectiveness is as follows: 1) the self-regression reconstructs the\ncontrast between adjacent pixels of the input image, 2) the\ninstance-normalization layer may naturally remediate the overall\nmagnitude/lighting of the input image, and 3) the $\\mathcal{N}(0,\\sigma^2)$\nassumption for each pixel enforces the output image to follow the well-known\ngray-world hypothesis when the image size is big enough. Compared to current\nstate-of-the-art LLIE methods with access to different task-related data,\nNoiSER is highly competitive in enhancement quality, yet with a much smaller\nmodel size, and much lower training and inference cost. Besides, NoiSER also\nexcels in mitigating overexposure and handling joint tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning-based low-light image enhancement (LLIE) is a task of\nleveraging deep neural networks to enhance the image illumination while keeping\nthe image content unchanged. From the perspective of training data, existing\nmethods complete the LLIE task driven by one of the following three data types:\npaired data, unpaired data and zero-reference data. Each type of these\ndata-driven methods has its own advantages, e.g., zero-reference data-based\nmethods have very low requirements on training data and can meet the human\nneeds in many scenarios. In this paper, we leverage pure Gaussian noise to\ncomplete the LLIE task, which further reduces the requirements for training\ndata in LLIE tasks and can be used as another alternative in practical use.\nSpecifically, we propose Noise SElf-Regression (NoiSER) without access to any\ntask-related data, simply learns a convolutional neural network equipped with\nan instance-normalization layer by taking a random noise image,\n$\\mathcal{N}(0,\\sigma^2)$ for each pixel, as both input and output for each\ntraining pair, and then the low-light image is fed to the trained network for\npredicting the normal-light image. Technically, an intuitive explanation for\nits effectiveness is as follows: 1) the self-regression reconstructs the\ncontrast between adjacent pixels of the input image, 2) the\ninstance-normalization layer may naturally remediate the overall\nmagnitude/lighting of the input image, and 3) the $\\mathcal{N}(0,\\sigma^2)$\nassumption for each pixel enforces the output image to follow the well-known\ngray-world hypothesis when the image size is big enough. Compared to current\nstate-of-the-art LLIE methods with access to different task-related data,\nNoiSER is highly competitive in enhancement quality, yet with a much smaller\nmodel size, and much lower training and inference cost. Besides, NoiSER also\nexcels in mitigating overexposure and handling joint tasks."
                },
                "authors": [
                    {
                        "name": "Zhao Zhang"
                    },
                    {
                        "name": "Suiyi Zhao"
                    },
                    {
                        "name": "Xiaojie Jin"
                    },
                    {
                        "name": "Mingliang Xu"
                    },
                    {
                        "name": "Yi Yang"
                    },
                    {
                        "name": "Shuicheng Yan"
                    },
                    {
                        "name": "Meng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Wang"
                },
                "author": "Meng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2211.04700v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2211.04700v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05526v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05526v3",
                "updated": "2024-12-06T09:34:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    9,
                    34,
                    43,
                    4,
                    341,
                    0
                ],
                "published": "2024-05-09T03:34:09Z",
                "published_parsed": [
                    2024,
                    5,
                    9,
                    3,
                    34,
                    9,
                    3,
                    130,
                    0
                ],
                "title": "Benchmarking Neural Radiance Fields for Autonomous Robots: An Overview",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Neural Radiance Fields for Autonomous Robots: An Overview"
                },
                "summary": "Neural Radiance Fields (NeRF) have emerged as a powerful paradigm for 3D\nscene representation, offering high-fidelity renderings and reconstructions\nfrom a set of sparse and unstructured sensor data. In the context of autonomous\nrobotics, where perception and understanding of the environment are pivotal,\nNeRF holds immense promise for improving performance. In this paper, we present\na comprehensive survey and analysis of the state-of-the-art techniques for\nutilizing NeRF to enhance the capabilities of autonomous robots. We especially\nfocus on the perception, localization and navigation, and decision-making\nmodules of autonomous robots and delve into tasks crucial for autonomous\noperation, including 3D reconstruction, segmentation, pose estimation,\nsimultaneous localization and mapping (SLAM), navigation and planning, and\ninteraction. Our survey meticulously benchmarks existing NeRF-based methods,\nproviding insights into their strengths and limitations. Moreover, we explore\npromising avenues for future research and development in this domain. Notably,\nwe discuss the integration of advanced techniques such as 3D Gaussian splatting\n(3DGS), large language models (LLM), and generative AIs, envisioning enhanced\nreconstruction efficiency, scene understanding, decision-making capabilities.\nThis survey serves as a roadmap for researchers seeking to leverage NeRFs to\nempower autonomous robots, paving the way for innovative solutions that can\nnavigate and interact seamlessly in complex environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Radiance Fields (NeRF) have emerged as a powerful paradigm for 3D\nscene representation, offering high-fidelity renderings and reconstructions\nfrom a set of sparse and unstructured sensor data. In the context of autonomous\nrobotics, where perception and understanding of the environment are pivotal,\nNeRF holds immense promise for improving performance. In this paper, we present\na comprehensive survey and analysis of the state-of-the-art techniques for\nutilizing NeRF to enhance the capabilities of autonomous robots. We especially\nfocus on the perception, localization and navigation, and decision-making\nmodules of autonomous robots and delve into tasks crucial for autonomous\noperation, including 3D reconstruction, segmentation, pose estimation,\nsimultaneous localization and mapping (SLAM), navigation and planning, and\ninteraction. Our survey meticulously benchmarks existing NeRF-based methods,\nproviding insights into their strengths and limitations. Moreover, we explore\npromising avenues for future research and development in this domain. Notably,\nwe discuss the integration of advanced techniques such as 3D Gaussian splatting\n(3DGS), large language models (LLM), and generative AIs, envisioning enhanced\nreconstruction efficiency, scene understanding, decision-making capabilities.\nThis survey serves as a roadmap for researchers seeking to leverage NeRFs to\nempower autonomous robots, paving the way for innovative solutions that can\nnavigate and interact seamlessly in complex environments."
                },
                "authors": [
                    {
                        "name": "Yuhang Ming"
                    },
                    {
                        "name": "Xingrui Yang"
                    },
                    {
                        "name": "Weihan Wang"
                    },
                    {
                        "name": "Zheng Chen"
                    },
                    {
                        "name": "Jinglun Feng"
                    },
                    {
                        "name": "Yifan Xing"
                    },
                    {
                        "name": "Guofeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Guofeng Zhang"
                },
                "author": "Guofeng Zhang",
                "arxiv_doi": "10.1016/j.engappai.2024.109685",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.engappai.2024.109685",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.05526v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05526v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "32 pages, 5 figures, 8 tables",
                "arxiv_journal_ref": "Engineering Applications of Artificial Intelligence, Volume 140,\n  15 January 2025, 109685",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20936v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20936v2",
                "updated": "2024-12-06T09:06:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    9,
                    6,
                    20,
                    4,
                    341,
                    0
                ],
                "published": "2024-10-28T11:37:39Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    11,
                    37,
                    39,
                    0,
                    302,
                    0
                ],
                "title": "Autoformalize Mathematical Statements by Symbolic Equivalence and\n  Semantic Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoformalize Mathematical Statements by Symbolic Equivalence and\n  Semantic Consistency"
                },
                "summary": "Autoformalization, the task of automatically translating natural language\ndescriptions into a formal language, poses a significant challenge across\nvarious domains, especially in mathematics. Recent advancements in large\nlanguage models (LLMs) have unveiled their promising capabilities to formalize\neven competition-level math problems. However, we observe a considerable\ndiscrepancy between pass@1 and pass@k accuracies in LLM-generated\nformalizations. To address this gap, we introduce a novel framework that scores\nand selects the best result from k autoformalization candidates based on two\ncomplementary self-consistency methods: symbolic equivalence and semantic\nconsistency. Elaborately, symbolic equivalence identifies the logical\nhomogeneity among autoformalization candidates using automated theorem provers,\nand semantic consistency evaluates the preservation of the original meaning by\ninformalizing the candidates and computing the similarity between the\nembeddings of the original and informalized texts. Our extensive experiments on\nthe MATH and miniF2F datasets demonstrate that our approach significantly\nenhances autoformalization accuracy, achieving up to 0.22-1.35x relative\nimprovements across various LLMs and baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoformalization, the task of automatically translating natural language\ndescriptions into a formal language, poses a significant challenge across\nvarious domains, especially in mathematics. Recent advancements in large\nlanguage models (LLMs) have unveiled their promising capabilities to formalize\neven competition-level math problems. However, we observe a considerable\ndiscrepancy between pass@1 and pass@k accuracies in LLM-generated\nformalizations. To address this gap, we introduce a novel framework that scores\nand selects the best result from k autoformalization candidates based on two\ncomplementary self-consistency methods: symbolic equivalence and semantic\nconsistency. Elaborately, symbolic equivalence identifies the logical\nhomogeneity among autoformalization candidates using automated theorem provers,\nand semantic consistency evaluates the preservation of the original meaning by\ninformalizing the candidates and computing the similarity between the\nembeddings of the original and informalized texts. Our extensive experiments on\nthe MATH and miniF2F datasets demonstrate that our approach significantly\nenhances autoformalization accuracy, achieving up to 0.22-1.35x relative\nimprovements across various LLMs and baseline methods."
                },
                "authors": [
                    {
                        "name": "Zenan Li"
                    },
                    {
                        "name": "Yifan Wu"
                    },
                    {
                        "name": "Zhaoyu Li"
                    },
                    {
                        "name": "Xinming Wei"
                    },
                    {
                        "name": "Xian Zhang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Xiaoxing Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxing Ma"
                },
                "author": "Xiaoxing Ma",
                "arxiv_comment": "Published as a conference paper at NeurIPS 2024. Code is available at\n  https://github.com/Miracle-Messi/Isa-AutoFormal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20936v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20936v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07623v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07623v2",
                "updated": "2024-12-06T09:04:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    9,
                    4,
                    55,
                    4,
                    341,
                    0
                ],
                "published": "2024-05-13T10:30:33Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    10,
                    30,
                    33,
                    0,
                    134,
                    0
                ],
                "title": "COBias and Debias: Minimizing Language Model Pairwise Accuracy Bias via\n  Nonlinear Integer Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COBias and Debias: Minimizing Language Model Pairwise Accuracy Bias via\n  Nonlinear Integer Programming"
                },
                "summary": "When performing classification tasks with language models, would you prefer\nhaving only one highly accurate class or having every class deliver reliable\nperformance? Obviously, a more balanced accuracy among classes better reflects\nthe expectations of the majority of users. Especially for large language models\n(LLMs), the fact that they achieve a fair overall accuracy by in-context\nlearning (ICL) obscures a large difference in individual class accuracies. In\nthis work, we uncover and tackle language models' imbalance in per-class\nprediction accuracy by reconceptualizing it as the Contextual Oddity Bias\n(COBias), and we are the first to engage nonlinear integer programming (NIP) to\ndebias it. Briefly, the proposed COBias metric measures accuracy differences\namong class pairs, with which we reveal the large per-class accuracy\ndifferences exhibited in LLMs of varied scales and families. Then we propose\nDebiasing as Nonlinear Integer Programming (DNIP) to correct ICL per-class\nprobabilities towards lower COBias and higher overall accuracy. Our\noptimization objective is directly based on the evaluation scores by COBias and\naccuracy metrics, which is non-differentiable and solved by the simulated\nannealing metaheuristic. Evaluations on three LLMs across seven NLP\nclassification tasks show that DNIP simultaneously achieves significant COBias\nreduction (-27%) and accuracy improvement (+12%) over the conventional ICL\napproach, suggesting that modeling pairwise class accuracy differences is a\ndirection in pushing forward more accurate, more reliable LLM predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When performing classification tasks with language models, would you prefer\nhaving only one highly accurate class or having every class deliver reliable\nperformance? Obviously, a more balanced accuracy among classes better reflects\nthe expectations of the majority of users. Especially for large language models\n(LLMs), the fact that they achieve a fair overall accuracy by in-context\nlearning (ICL) obscures a large difference in individual class accuracies. In\nthis work, we uncover and tackle language models' imbalance in per-class\nprediction accuracy by reconceptualizing it as the Contextual Oddity Bias\n(COBias), and we are the first to engage nonlinear integer programming (NIP) to\ndebias it. Briefly, the proposed COBias metric measures accuracy differences\namong class pairs, with which we reveal the large per-class accuracy\ndifferences exhibited in LLMs of varied scales and families. Then we propose\nDebiasing as Nonlinear Integer Programming (DNIP) to correct ICL per-class\nprobabilities towards lower COBias and higher overall accuracy. Our\noptimization objective is directly based on the evaluation scores by COBias and\naccuracy metrics, which is non-differentiable and solved by the simulated\nannealing metaheuristic. Evaluations on three LLMs across seven NLP\nclassification tasks show that DNIP simultaneously achieves significant COBias\nreduction (-27%) and accuracy improvement (+12%) over the conventional ICL\napproach, suggesting that modeling pairwise class accuracy differences is a\ndirection in pushing forward more accurate, more reliable LLM predictions."
                },
                "authors": [
                    {
                        "name": "Ruixi Lin"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07623v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07623v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04871v1",
                "updated": "2024-12-06T09:04:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    9,
                    4,
                    12,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T09:04:12Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    9,
                    4,
                    12,
                    4,
                    341,
                    0
                ],
                "title": "Building a Family of Data Augmentation Models for Low-cost LLM\n  Fine-tuning on the Cloud",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a Family of Data Augmentation Models for Low-cost LLM\n  Fine-tuning on the Cloud"
                },
                "summary": "Specializing LLMs in various domain-specific tasks has emerged as a critical\nstep towards achieving high performance. However, the construction and\nannotation of datasets in specific domains are always very costly. Apart from\nusing superior and expensive closed-source LLM APIs to construct datasets, some\nopen-source models have become strong enough to handle dataset construction in\nmany scenarios. Thus, we present a family of data augmentation models designed\nto significantly improve the efficiency for model fine-tuning. These models,\ntrained based on sufficiently small LLMs, support key functionalities with low\ninference costs: instruction expansion, instruction refinement, and\ninstruction-response pair expansion. To fulfill this goal, we first construct\nan automatic data collection system with seed datasets generated from both\npublic repositories and our in-house datasets. This system leverages powerful\nLLMs to expand, refine and re-write the instructions and responses,\nincorporating quality assessment techniques. Following this, we introduce the\ntraining process of our models, which effectively distills task-solving and\ntext synthesis abilities from teacher LLMs. Finally, we demonstrate how we\nintegrate these functionalities into a machine learning platform to support\nlow-cost LLM fine-tuning from both dataset preparation and training\nperspectives for users. Experiments and an application study prove the\neffectiveness of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Specializing LLMs in various domain-specific tasks has emerged as a critical\nstep towards achieving high performance. However, the construction and\nannotation of datasets in specific domains are always very costly. Apart from\nusing superior and expensive closed-source LLM APIs to construct datasets, some\nopen-source models have become strong enough to handle dataset construction in\nmany scenarios. Thus, we present a family of data augmentation models designed\nto significantly improve the efficiency for model fine-tuning. These models,\ntrained based on sufficiently small LLMs, support key functionalities with low\ninference costs: instruction expansion, instruction refinement, and\ninstruction-response pair expansion. To fulfill this goal, we first construct\nan automatic data collection system with seed datasets generated from both\npublic repositories and our in-house datasets. This system leverages powerful\nLLMs to expand, refine and re-write the instructions and responses,\nincorporating quality assessment techniques. Following this, we introduce the\ntraining process of our models, which effectively distills task-solving and\ntext synthesis abilities from teacher LLMs. Finally, we demonstrate how we\nintegrate these functionalities into a machine learning platform to support\nlow-cost LLM fine-tuning from both dataset preparation and training\nperspectives for users. Experiments and an application study prove the\neffectiveness of our approach."
                },
                "authors": [
                    {
                        "name": "Yuanhao Yue"
                    },
                    {
                        "name": "Chengyu Wang"
                    },
                    {
                        "name": "Jun Huang"
                    },
                    {
                        "name": "Peng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Wang"
                },
                "author": "Peng Wang",
                "arxiv_comment": "coling 2025 industry track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04086v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04086v2",
                "updated": "2024-12-06T09:00:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    9,
                    0,
                    39,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-05T11:48:54Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    11,
                    48,
                    54,
                    3,
                    340,
                    0
                ],
                "title": "BodyMetric: Evaluating the Realism of Human Bodies in Text-to-Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BodyMetric: Evaluating the Realism of Human Bodies in Text-to-Image\n  Generation"
                },
                "summary": "Accurately generating images of human bodies from text remains a challenging\nproblem for state of the art text-to-image models. Commonly observed\nbody-related artifacts include extra or missing limbs, unrealistic poses,\nblurred body parts, etc. Currently, evaluation of such artifacts relies heavily\non time-consuming human judgments, limiting the ability to benchmark models at\nscale. We address this by proposing BodyMetric, a learnable metric that\npredicts body realism in images. BodyMetric is trained on realism labels and\nmulti-modal signals including 3D body representations inferred from the input\nimage, and textual descriptions. In order to facilitate this approach, we\ndesign an annotation pipeline to collect expert ratings on human body realism\nleading to a new dataset for this task, namely, BodyRealism. Ablation studies\nsupport our architectural choices for BodyMetric and the importance of\nleveraging a 3D human body prior in capturing body-related artifacts in 2D\nimages. In comparison to concurrent metrics which evaluate general user\npreference in images, BodyMetric specifically reflects body-related artifacts.\nWe demonstrate the utility of BodyMetric through applications that were\npreviously infeasible at scale. In particular, we use BodyMetric to benchmark\nthe generation ability of text-to-image models to produce realistic human\nbodies. We also demonstrate the effectiveness of BodyMetric in ranking\ngenerated images based on the predicted realism scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately generating images of human bodies from text remains a challenging\nproblem for state of the art text-to-image models. Commonly observed\nbody-related artifacts include extra or missing limbs, unrealistic poses,\nblurred body parts, etc. Currently, evaluation of such artifacts relies heavily\non time-consuming human judgments, limiting the ability to benchmark models at\nscale. We address this by proposing BodyMetric, a learnable metric that\npredicts body realism in images. BodyMetric is trained on realism labels and\nmulti-modal signals including 3D body representations inferred from the input\nimage, and textual descriptions. In order to facilitate this approach, we\ndesign an annotation pipeline to collect expert ratings on human body realism\nleading to a new dataset for this task, namely, BodyRealism. Ablation studies\nsupport our architectural choices for BodyMetric and the importance of\nleveraging a 3D human body prior in capturing body-related artifacts in 2D\nimages. In comparison to concurrent metrics which evaluate general user\npreference in images, BodyMetric specifically reflects body-related artifacts.\nWe demonstrate the utility of BodyMetric through applications that were\npreviously infeasible at scale. In particular, we use BodyMetric to benchmark\nthe generation ability of text-to-image models to produce realistic human\nbodies. We also demonstrate the effectiveness of BodyMetric in ranking\ngenerated images based on the predicted realism scores."
                },
                "authors": [
                    {
                        "name": "Nefeli Andreou"
                    },
                    {
                        "name": "Varsha Vivek"
                    },
                    {
                        "name": "Ying Wang"
                    },
                    {
                        "name": "Alex Vorobiov"
                    },
                    {
                        "name": "Tiffany Deng"
                    },
                    {
                        "name": "Raja Bala"
                    },
                    {
                        "name": "Larry Davis"
                    },
                    {
                        "name": "Betty Mohler Tesch"
                    }
                ],
                "author_detail": {
                    "name": "Betty Mohler Tesch"
                },
                "author": "Betty Mohler Tesch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04086v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04086v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04865v1",
                "updated": "2024-12-06T08:58:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    8,
                    58,
                    19,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T08:58:19Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    8,
                    58,
                    19,
                    4,
                    341,
                    0
                ],
                "title": "Quantum-Enhanced Multi-Parameter Sensing in a Single Mode",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum-Enhanced Multi-Parameter Sensing in a Single Mode"
                },
                "summary": "Precision metrology underpins scientific and technological advancements.\nQuantum metrology offers a pathway to surpass classical sensing limits by\nleveraging quantum states and measurement strategies. However, measuring\nmultiple incompatible observables suffers from quantum backaction, where\nmeasurement of one observable pollutes a subsequent measurement of the other.\nThis is a manifestation of Heisenberg's uncertainty principle for two\nnon-commuting observables, such as position and momentum. Here, we demonstrate\nmeasurements of small changes in position and momentum where the uncertainties\nare simultaneously reduced below the standard quantum limit (SQL). We measure\n$\\textit{modular observables}$ using tailored, highly non-classical states that\nideally evade measurement backactions. The states are deterministically\nprepared in the single mode of the mechanical motion of a trapped ion using an\noptimal quantum control protocol. Our experiment uses grid states to measure\nsmall changes in position and momentum and shows a metrological gain of up to\n5.1(5)~dB over the simultaneous SQL. Using an adaptive-phase estimation\nalgorithm with Bayesian inference, we estimate these displacements with a\ncombined variance of 2.6(1.1)~dB below the SQL. Furthermore, we examine\nsimultaneously estimating $\\textit{number}$ and $\\textit{phase}$, which are the\npolar counterparts of position and momentum. This is performed by preparing a\nnovel quantum resource -- number-phase states -- and we demonstrate a\nmetrological gain over their SQL. The combination of quantum control and\nmulti-parameter quantum metrology marks a significant step towards\nunprecedented precision with applications ranging from fundamental physics to\nadvanced quantum technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precision metrology underpins scientific and technological advancements.\nQuantum metrology offers a pathway to surpass classical sensing limits by\nleveraging quantum states and measurement strategies. However, measuring\nmultiple incompatible observables suffers from quantum backaction, where\nmeasurement of one observable pollutes a subsequent measurement of the other.\nThis is a manifestation of Heisenberg's uncertainty principle for two\nnon-commuting observables, such as position and momentum. Here, we demonstrate\nmeasurements of small changes in position and momentum where the uncertainties\nare simultaneously reduced below the standard quantum limit (SQL). We measure\n$\\textit{modular observables}$ using tailored, highly non-classical states that\nideally evade measurement backactions. The states are deterministically\nprepared in the single mode of the mechanical motion of a trapped ion using an\noptimal quantum control protocol. Our experiment uses grid states to measure\nsmall changes in position and momentum and shows a metrological gain of up to\n5.1(5)~dB over the simultaneous SQL. Using an adaptive-phase estimation\nalgorithm with Bayesian inference, we estimate these displacements with a\ncombined variance of 2.6(1.1)~dB below the SQL. Furthermore, we examine\nsimultaneously estimating $\\textit{number}$ and $\\textit{phase}$, which are the\npolar counterparts of position and momentum. This is performed by preparing a\nnovel quantum resource -- number-phase states -- and we demonstrate a\nmetrological gain over their SQL. The combination of quantum control and\nmulti-parameter quantum metrology marks a significant step towards\nunprecedented precision with applications ranging from fundamental physics to\nadvanced quantum technologies."
                },
                "authors": [
                    {
                        "name": "Christophe H. Valahu"
                    },
                    {
                        "name": "Matthew P. Stafford"
                    },
                    {
                        "name": "Zixin Huang"
                    },
                    {
                        "name": "Vassili G. Matsos"
                    },
                    {
                        "name": "Maverick J. Millican"
                    },
                    {
                        "name": "Teerawat Chalermpusitarak"
                    },
                    {
                        "name": "Nicolas C. Menicucci"
                    },
                    {
                        "name": "Joshua Combes"
                    },
                    {
                        "name": "Ben Q. Baragiola"
                    },
                    {
                        "name": "Ting Rei Tan"
                    }
                ],
                "author_detail": {
                    "name": "Ting Rei Tan"
                },
                "author": "Ting Rei Tan",
                "arxiv_comment": "Main text: 8 pages, 4 figures; Supplemental material: 37 pages, 16\n  figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01497v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01497v2",
                "updated": "2024-12-06T08:53:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    8,
                    53,
                    43,
                    4,
                    341,
                    0
                ],
                "published": "2024-09-02T23:37:20Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    23,
                    37,
                    20,
                    0,
                    246,
                    0
                ],
                "title": "DiversityMedQA: Assessing Demographic Biases in Medical Diagnosis using\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiversityMedQA: Assessing Demographic Biases in Medical Diagnosis using\n  Large Language Models"
                },
                "summary": "As large language models (LLMs) gain traction in healthcare, concerns about\ntheir susceptibility to demographic biases are growing. We introduce\n{DiversityMedQA}, a novel benchmark designed to assess LLM responses to medical\nqueries across diverse patient demographics, such as gender and ethnicity. By\nperturbing questions from the MedQA dataset, which comprises medical board exam\nquestions, we created a benchmark that captures the nuanced differences in\nmedical diagnosis across varying patient profiles. Our findings reveal notable\ndiscrepancies in model performance when tested against these demographic\nvariations. Furthermore, to ensure the perturbations were accurate, we also\npropose a filtering strategy that validates each perturbation. By releasing\nDiversityMedQA, we provide a resource for evaluating and mitigating demographic\nbias in LLM medical diagnoses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) gain traction in healthcare, concerns about\ntheir susceptibility to demographic biases are growing. We introduce\n{DiversityMedQA}, a novel benchmark designed to assess LLM responses to medical\nqueries across diverse patient demographics, such as gender and ethnicity. By\nperturbing questions from the MedQA dataset, which comprises medical board exam\nquestions, we created a benchmark that captures the nuanced differences in\nmedical diagnosis across varying patient profiles. Our findings reveal notable\ndiscrepancies in model performance when tested against these demographic\nvariations. Furthermore, to ensure the perturbations were accurate, we also\npropose a filtering strategy that validates each perturbation. By releasing\nDiversityMedQA, we provide a resource for evaluating and mitigating demographic\nbias in LLM medical diagnoses."
                },
                "authors": [
                    {
                        "name": "Rajat Rawat"
                    },
                    {
                        "name": "Hudson McBride"
                    },
                    {
                        "name": "Dhiyaan Nirmal"
                    },
                    {
                        "name": "Rajarshi Ghosh"
                    },
                    {
                        "name": "Jong Moon"
                    },
                    {
                        "name": "Dhruv Alamuri"
                    },
                    {
                        "name": "Sean O'Brien"
                    },
                    {
                        "name": "Kevin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Zhu"
                },
                "author": "Kevin Zhu",
                "arxiv_comment": "Published in NLP4PI @ EMNLP 2024, Accepted to AIM-FM @ NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01497v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01497v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04859v1",
                "updated": "2024-12-06T08:52:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    8,
                    52,
                    30,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T08:52:30Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    8,
                    52,
                    30,
                    4,
                    341,
                    0
                ],
                "title": "Breaking Event Rumor Detection via Stance-Separated Multi-Agent Debate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Event Rumor Detection via Stance-Separated Multi-Agent Debate"
                },
                "summary": "The rapid spread of rumors on social media platforms during breaking events\nseverely hinders the dissemination of the truth. Previous studies reveal that\nthe lack of annotated resources hinders the direct detection of unforeseen\nbreaking events not covered in yesterday's news. Leveraging large language\nmodels (LLMs) for rumor detection holds significant promise. However, it is\nchallenging for LLMs to provide comprehensive responses to complex or\ncontroversial issues due to limited diversity. In this work, we propose the\nStance Separated Multi-Agent Debate (S2MAD) to address this issue.\nSpecifically, we firstly introduce Stance Separation, categorizing comments as\neither supporting or opposing the original claim. Subsequently, claims are\nclassified as subjective or objective, enabling agents to generate reasonable\ninitial viewpoints with different prompt strategies for each type of claim.\nDebaters then follow specific instructions through multiple rounds of debate to\nreach a consensus. If a consensus is not reached, a judge agent evaluates the\nopinions and delivers a final verdict on the claim's veracity. Extensive\nexperiments conducted on two real-world datasets demonstrate that our proposed\nmodel outperforms state-of-the-art methods in terms of performance and\neffectively improves the performance of LLMs in breaking event rumor detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid spread of rumors on social media platforms during breaking events\nseverely hinders the dissemination of the truth. Previous studies reveal that\nthe lack of annotated resources hinders the direct detection of unforeseen\nbreaking events not covered in yesterday's news. Leveraging large language\nmodels (LLMs) for rumor detection holds significant promise. However, it is\nchallenging for LLMs to provide comprehensive responses to complex or\ncontroversial issues due to limited diversity. In this work, we propose the\nStance Separated Multi-Agent Debate (S2MAD) to address this issue.\nSpecifically, we firstly introduce Stance Separation, categorizing comments as\neither supporting or opposing the original claim. Subsequently, claims are\nclassified as subjective or objective, enabling agents to generate reasonable\ninitial viewpoints with different prompt strategies for each type of claim.\nDebaters then follow specific instructions through multiple rounds of debate to\nreach a consensus. If a consensus is not reached, a judge agent evaluates the\nopinions and delivers a final verdict on the claim's veracity. Extensive\nexperiments conducted on two real-world datasets demonstrate that our proposed\nmodel outperforms state-of-the-art methods in terms of performance and\neffectively improves the performance of LLMs in breaking event rumor detection."
                },
                "authors": [
                    {
                        "name": "Mingqing Zhang"
                    },
                    {
                        "name": "Haisong Gong"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Shu Wu"
                    },
                    {
                        "name": "Liang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wang"
                },
                "author": "Liang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04857v1",
                "updated": "2024-12-06T08:49:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    8,
                    49,
                    49,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T08:49:49Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    8,
                    49,
                    49,
                    4,
                    341,
                    0
                ],
                "title": "Neuro-Symbolic Data Generation for Math Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuro-Symbolic Data Generation for Math Reasoning"
                },
                "summary": "A critical question about Large Language Models (LLMs) is whether their\napparent deficiency in mathematical reasoning is inherent, or merely a result\nof insufficient exposure to high-quality mathematical data. To explore this, we\ndeveloped an automated method for generating high-quality, supervised\nmathematical datasets. The method carefully mutates existing math problems,\nensuring both diversity and validity of the newly generated problems. This is\nachieved by a neuro-symbolic data generation framework combining the intuitive\ninformalization strengths of LLMs, and the precise symbolic reasoning of math\nsolvers along with projected Markov chain Monte Carlo sampling in the\nhighly-irregular symbolic space. Empirical experiments demonstrate the high\nquality of data generated by the proposed method, and that the LLMs,\nspecifically LLaMA-2 and Mistral, when realigned with the generated data,\nsurpass their state-of-the-art counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical question about Large Language Models (LLMs) is whether their\napparent deficiency in mathematical reasoning is inherent, or merely a result\nof insufficient exposure to high-quality mathematical data. To explore this, we\ndeveloped an automated method for generating high-quality, supervised\nmathematical datasets. The method carefully mutates existing math problems,\nensuring both diversity and validity of the newly generated problems. This is\nachieved by a neuro-symbolic data generation framework combining the intuitive\ninformalization strengths of LLMs, and the precise symbolic reasoning of math\nsolvers along with projected Markov chain Monte Carlo sampling in the\nhighly-irregular symbolic space. Empirical experiments demonstrate the high\nquality of data generated by the proposed method, and that the LLMs,\nspecifically LLaMA-2 and Mistral, when realigned with the generated data,\nsurpass their state-of-the-art counterparts."
                },
                "authors": [
                    {
                        "name": "Zenan Li"
                    },
                    {
                        "name": "Zhi Zhou"
                    },
                    {
                        "name": "Yuan Yao"
                    },
                    {
                        "name": "Yu-Feng Li"
                    },
                    {
                        "name": "Chun Cao"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Xian Zhang"
                    },
                    {
                        "name": "Xiaoxing Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxing Ma"
                },
                "author": "Xiaoxing Ma",
                "arxiv_comment": "Published as a conference paper at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04856v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04856v1",
                "updated": "2024-12-06T08:48:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    8,
                    48,
                    49,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T08:48:49Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    8,
                    48,
                    49,
                    4,
                    341,
                    0
                ],
                "title": "Can Large Language Models Effectively Process and Execute Financial\n  Trading Instructions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Effectively Process and Execute Financial\n  Trading Instructions?"
                },
                "summary": "The development of Large Language Models (LLMs) has created transformative\nopportunities for the financial industry, especially in the area of financial\ntrading. However, how to integrate LLMs with trading systems has become a\nchallenge. To address this problem, we propose an intelligent trade order\nrecognition pipeline that enables the conversion of trade orders into a\nstandard format in trade execution. The system improves the ability of human\ntraders to interact with trading platforms while addressing the problem of\nmisinformation acquisition in trade execution. In addition, we have created a\ntrade order dataset of 500 pieces of data to simulate real-world trading\nscenarios. Moreover, we designed several metrics to provide a comprehensive\nassessment of dataset reliability and the generative power of big models in\nfinance by experimenting with five state-of-the-art LLMs on our dataset. The\nresults indicate that while LLMs demonstrate high generation rates (87.50% to\n98.33%) and perfect follow-up rates, they face significant challenges in\naccuracy (5% to 10%) and completeness, with high missing rates (14.29% to\n67.29%). In addition, LLMs tend to over-interrogate, suggesting that large\nmodels tend to collect more information, carrying certain challenges for\ninformation security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of Large Language Models (LLMs) has created transformative\nopportunities for the financial industry, especially in the area of financial\ntrading. However, how to integrate LLMs with trading systems has become a\nchallenge. To address this problem, we propose an intelligent trade order\nrecognition pipeline that enables the conversion of trade orders into a\nstandard format in trade execution. The system improves the ability of human\ntraders to interact with trading platforms while addressing the problem of\nmisinformation acquisition in trade execution. In addition, we have created a\ntrade order dataset of 500 pieces of data to simulate real-world trading\nscenarios. Moreover, we designed several metrics to provide a comprehensive\nassessment of dataset reliability and the generative power of big models in\nfinance by experimenting with five state-of-the-art LLMs on our dataset. The\nresults indicate that while LLMs demonstrate high generation rates (87.50% to\n98.33%) and perfect follow-up rates, they face significant challenges in\naccuracy (5% to 10%) and completeness, with high missing rates (14.29% to\n67.29%). In addition, LLMs tend to over-interrogate, suggesting that large\nmodels tend to collect more information, carrying certain challenges for\ninformation security."
                },
                "authors": [
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Ge Wang"
                    },
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Yuda Wang"
                    },
                    {
                        "name": "Mingwen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Mingwen Liu"
                },
                "author": "Mingwen Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04856v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04856v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02580v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02580v2",
                "updated": "2024-12-06T08:41:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    8,
                    41,
                    1,
                    4,
                    341,
                    0
                ],
                "published": "2024-05-04T06:28:27Z",
                "published_parsed": [
                    2024,
                    5,
                    4,
                    6,
                    28,
                    27,
                    5,
                    125,
                    0
                ],
                "title": "PropertyGPT: LLM-driven Formal Verification of Smart Contracts through\n  Retrieval-Augmented Property Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PropertyGPT: LLM-driven Formal Verification of Smart Contracts through\n  Retrieval-Augmented Property Generation"
                },
                "summary": "With recent advances in large language models (LLMs), this paper explores the\npotential of leveraging state-of-the-art LLMs,such as GPT-4, to transfer\nexisting human-written properties (e.g.,those from Certora auditing reports)\nand automatically generate customized properties for unknown code. To this end,\nwe embed existing properties into a vector database and retrieve a reference\nproperty for LLM-based in-context learning to generate a new property for a\ngiven code. While this basic process is relatively straightforward, ensuring\nthat the generated properties are (i) compilable, (ii) appropriate, and (iii)\nverifiable presents challenges. To address (i), we use the compilation and\nstatic analysis feedback as an external oracle to guide LLMs in iteratively\nrevising the generated properties. For (ii), we consider multiple dimensions of\nsimilarity to rank the properties and employ a weighted algorithm to identify\nthe top-K properties as the final result. For (iii), we design a dedicated\nprover to formally verify the correctness of the generated properties. We have\nimplemented these strategies into a novel LLM-based property generation tool\ncalled PropertyGPT. Our experiments show that PropertyGPT can generate\ncomprehensive and high-quality properties, achieving an 80% recall compared to\nthe ground truth. It successfully detected 26 CVEs/attack incidents out of 37\ntested and also uncovered 12 zero-day vulnerabilities, leading to $8,256 in bug\nbounty rewards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With recent advances in large language models (LLMs), this paper explores the\npotential of leveraging state-of-the-art LLMs,such as GPT-4, to transfer\nexisting human-written properties (e.g.,those from Certora auditing reports)\nand automatically generate customized properties for unknown code. To this end,\nwe embed existing properties into a vector database and retrieve a reference\nproperty for LLM-based in-context learning to generate a new property for a\ngiven code. While this basic process is relatively straightforward, ensuring\nthat the generated properties are (i) compilable, (ii) appropriate, and (iii)\nverifiable presents challenges. To address (i), we use the compilation and\nstatic analysis feedback as an external oracle to guide LLMs in iteratively\nrevising the generated properties. For (ii), we consider multiple dimensions of\nsimilarity to rank the properties and employ a weighted algorithm to identify\nthe top-K properties as the final result. For (iii), we design a dedicated\nprover to formally verify the correctness of the generated properties. We have\nimplemented these strategies into a novel LLM-based property generation tool\ncalled PropertyGPT. Our experiments show that PropertyGPT can generate\ncomprehensive and high-quality properties, achieving an 80% recall compared to\nthe ground truth. It successfully detected 26 CVEs/attack incidents out of 37\ntested and also uncovered 12 zero-day vulnerabilities, leading to $8,256 in bug\nbounty rewards."
                },
                "authors": [
                    {
                        "name": "Ye Liu"
                    },
                    {
                        "name": "Yue Xue"
                    },
                    {
                        "name": "Daoyuan Wu"
                    },
                    {
                        "name": "Yuqiang Sun"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Miaolei Shi"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_doi": "10.14722/ndss.2025.241357",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14722/ndss.2025.241357",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.02580v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02580v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by NDSS Symposium 2025. Please cite the conference version\n  of this paper, e.g., \"Ye Liu, Yue Xue, Daoyuan Wu, Yuqiang Sun, Yi Li,\n  Miaolei Shi, Yang Liu. PropertyGPT: LLM-driven Formal Verification of Smart\n  Contracts through Retrieval-Augmented Property Generation. In 32nd Annual\n  Network and Distributed System Security Symposium (NDSS 2025).\"",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01084v2",
                "updated": "2024-12-06T08:39:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    8,
                    39,
                    13,
                    4,
                    341,
                    0
                ],
                "published": "2024-11-01T23:53:00Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    23,
                    53,
                    0,
                    4,
                    306,
                    0
                ],
                "title": "Plentiful Jailbreaks with String Compositions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plentiful Jailbreaks with String Compositions"
                },
                "summary": "Large language models (LLMs) remain vulnerable to a slew of adversarial\nattacks and jailbreaking methods. One common approach employed by white-hat\nattackers, or red-teamers, is to process model inputs and outputs using\nstring-level obfuscations, which can include leetspeak, rotary ciphers, Base64,\nASCII, and more. Our work extends these encoding-based attacks by unifying them\nin a framework of invertible string transformations. With invertibility, we can\ndevise arbitrary string compositions, defined as sequences of transformations,\nthat we can encode and decode end-to-end programmatically. We devise a\nautomated best-of-n attack that samples from a combinatorially large number of\nstring compositions. Our jailbreaks obtain competitive attack success rates on\nseveral leading frontier models when evaluated on HarmBench, highlighting that\nencoding-based attacks remain a persistent vulnerability even in advanced LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) remain vulnerable to a slew of adversarial\nattacks and jailbreaking methods. One common approach employed by white-hat\nattackers, or red-teamers, is to process model inputs and outputs using\nstring-level obfuscations, which can include leetspeak, rotary ciphers, Base64,\nASCII, and more. Our work extends these encoding-based attacks by unifying them\nin a framework of invertible string transformations. With invertibility, we can\ndevise arbitrary string compositions, defined as sequences of transformations,\nthat we can encode and decode end-to-end programmatically. We devise a\nautomated best-of-n attack that samples from a combinatorially large number of\nstring compositions. Our jailbreaks obtain competitive attack success rates on\nseveral leading frontier models when evaluated on HarmBench, highlighting that\nencoding-based attacks remain a persistent vulnerability even in advanced LLMs."
                },
                "authors": [
                    {
                        "name": "Brian R. Y. Huang"
                    }
                ],
                "author_detail": {
                    "name": "Brian R. Y. Huang"
                },
                "author": "Brian R. Y. Huang",
                "arxiv_comment": "NeurIPS SoLaR Workshop 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00641v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00641v2",
                "updated": "2024-12-06T08:35:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    8,
                    35,
                    27,
                    4,
                    341,
                    0
                ],
                "published": "2024-06-30T09:51:58Z",
                "published_parsed": [
                    2024,
                    6,
                    30,
                    9,
                    51,
                    58,
                    6,
                    182,
                    0
                ],
                "title": "NeuroNAS: A Framework for Energy-Efficient Neuromorphic\n  Compute-in-Memory Systems using Hardware-Aware Spiking Neural Architecture\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuroNAS: A Framework for Energy-Efficient Neuromorphic\n  Compute-in-Memory Systems using Hardware-Aware Spiking Neural Architecture\n  Search"
                },
                "summary": "Spiking Neural Networks (SNNs) have demonstrated capabilities for solving\ndiverse machine learning tasks with ultra-low power/energy consumption. To\nmaximize the performance and efficiency of SNN inference, the Compute-in-Memory\n(CIM) hardware accelerators with emerging device technologies (e.g., RRAM) have\nbeen employed. However, SNN architectures are typically developed without\nconsidering constraints from the application and the underlying CIM hardware,\nthereby hindering SNNs from reaching their full potential in accuracy and\nefficiency. To address this, we propose NeuroNAS, a novel framework for\ndeveloping energy-efficient neuromorphic CIM systems using a hardware-aware\nspiking neural architecture search (NAS), i.e., by quickly finding an SNN\narchitecture that offers high accuracy under the given constraints (e.g.,\nmemory, area, latency, and energy consumption). NeuroNAS employs the following\nkey steps: (1) optimizing SNN operations to enable efficient NAS, (2) employing\nquantization to minimize the memory footprint, (3) developing an SNN\narchitecture that facilitates an effective learning, and (4) devising a\nsystematic hardware-aware search algorithm to meet the constraints. Compared to\nthe state-of-the-art, NeuroNAS with 8bit weight precision quickly finds SNNs\nthat maintain high accuracy by up to 6.6x search time speed-ups, while\nachieving up to 92% area savings, 1.2x latency speed-ups, 84% energy savings\nacross CIFAR-10, CIFAR-100, and TinyImageNet-200 datasets; while the\nstate-of-the-art fail to meet all constraints at once. In this manner, NeuroNAS\nenables efficient design automation in developing energy-efficient neuromorphic\nCIM systems for diverse ML-based applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) have demonstrated capabilities for solving\ndiverse machine learning tasks with ultra-low power/energy consumption. To\nmaximize the performance and efficiency of SNN inference, the Compute-in-Memory\n(CIM) hardware accelerators with emerging device technologies (e.g., RRAM) have\nbeen employed. However, SNN architectures are typically developed without\nconsidering constraints from the application and the underlying CIM hardware,\nthereby hindering SNNs from reaching their full potential in accuracy and\nefficiency. To address this, we propose NeuroNAS, a novel framework for\ndeveloping energy-efficient neuromorphic CIM systems using a hardware-aware\nspiking neural architecture search (NAS), i.e., by quickly finding an SNN\narchitecture that offers high accuracy under the given constraints (e.g.,\nmemory, area, latency, and energy consumption). NeuroNAS employs the following\nkey steps: (1) optimizing SNN operations to enable efficient NAS, (2) employing\nquantization to minimize the memory footprint, (3) developing an SNN\narchitecture that facilitates an effective learning, and (4) devising a\nsystematic hardware-aware search algorithm to meet the constraints. Compared to\nthe state-of-the-art, NeuroNAS with 8bit weight precision quickly finds SNNs\nthat maintain high accuracy by up to 6.6x search time speed-ups, while\nachieving up to 92% area savings, 1.2x latency speed-ups, 84% energy savings\nacross CIFAR-10, CIFAR-100, and TinyImageNet-200 datasets; while the\nstate-of-the-art fail to meet all constraints at once. In this manner, NeuroNAS\nenables efficient design automation in developing energy-efficient neuromorphic\nCIM systems for diverse ML-based applications."
                },
                "authors": [
                    {
                        "name": "Rachmad Vidya Wicaksana Putra"
                    },
                    {
                        "name": "Muhammad Shafique"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Shafique"
                },
                "author": "Muhammad Shafique",
                "arxiv_comment": "7 pages, 13 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00641v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00641v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03205v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03205v2",
                "updated": "2024-12-06T08:29:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    8,
                    29,
                    43,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-04T10:44:50Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    44,
                    50,
                    2,
                    339,
                    0
                ],
                "title": "U-MATH: A University-Level Benchmark for Evaluating Mathematical Skills\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U-MATH: A University-Level Benchmark for Evaluating Mathematical Skills\n  in LLMs"
                },
                "summary": "The current evaluation of mathematical skills in LLMs is limited, as existing\nbenchmarks are either relatively small, primarily focus on elementary and\nhigh-school problems, or lack diversity in topics. Additionally, the inclusion\nof visual elements in tasks remains largely under-explored.\n  To address these gaps, we introduce U-MATH, a novel benchmark of 1,100\nunpublished open-ended university-level problems sourced from teaching\nmaterials. It is balanced across six core subjects, with 20% of multimodal\nproblems. Given the open-ended nature of U-MATH problems, we employ an LLM to\njudge the correctness of generated solutions. To this end, we release\n$\\mu$-MATH, a dataset to evaluate the LLMs' capabilities in judging solutions.\n  The evaluation of general domain, math-specific, and multimodal LLMs\nhighlights the challenges presented by U-MATH. Our findings reveal that LLMs\nachieve a maximum accuracy of only 63% on text-based tasks, with even lower 45%\non visual problems. The solution assessment proves challenging for LLMs, with\nthe best LLM judge having an F1-score of 80% on $\\mu$-MATH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current evaluation of mathematical skills in LLMs is limited, as existing\nbenchmarks are either relatively small, primarily focus on elementary and\nhigh-school problems, or lack diversity in topics. Additionally, the inclusion\nof visual elements in tasks remains largely under-explored.\n  To address these gaps, we introduce U-MATH, a novel benchmark of 1,100\nunpublished open-ended university-level problems sourced from teaching\nmaterials. It is balanced across six core subjects, with 20% of multimodal\nproblems. Given the open-ended nature of U-MATH problems, we employ an LLM to\njudge the correctness of generated solutions. To this end, we release\n$\\mu$-MATH, a dataset to evaluate the LLMs' capabilities in judging solutions.\n  The evaluation of general domain, math-specific, and multimodal LLMs\nhighlights the challenges presented by U-MATH. Our findings reveal that LLMs\nachieve a maximum accuracy of only 63% on text-based tasks, with even lower 45%\non visual problems. The solution assessment proves challenging for LLMs, with\nthe best LLM judge having an F1-score of 80% on $\\mu$-MATH."
                },
                "authors": [
                    {
                        "name": "Konstantin Chernyshev"
                    },
                    {
                        "name": "Vitaliy Polshkov"
                    },
                    {
                        "name": "Ekaterina Artemova"
                    },
                    {
                        "name": "Alex Myasnikov"
                    },
                    {
                        "name": "Vlad Stepanov"
                    },
                    {
                        "name": "Alexei Miasnikov"
                    },
                    {
                        "name": "Sergei Tilga"
                    }
                ],
                "author_detail": {
                    "name": "Sergei Tilga"
                },
                "author": "Sergei Tilga",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03205v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03205v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04831v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04831v1",
                "updated": "2024-12-06T07:54:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    7,
                    54,
                    34,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T07:54:34Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    7,
                    54,
                    34,
                    4,
                    341,
                    0
                ],
                "title": "Customized Generation Reimagined: Fidelity and Editability Harmonized",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customized Generation Reimagined: Fidelity and Editability Harmonized"
                },
                "summary": "Customized generation aims to incorporate a novel concept into a pre-trained\ntext-to-image model, enabling new generations of the concept in novel contexts\nguided by textual prompts. However, customized generation suffers from an\ninherent trade-off between concept fidelity and editability, i.e., between\nprecisely modeling the concept and faithfully adhering to the prompts. Previous\nmethods reluctantly seek a compromise and struggle to achieve both high concept\nfidelity and ideal prompt alignment simultaneously. In this paper, we propose a\nDivide, Conquer, then Integrate (DCI) framework, which performs a surgical\nadjustment in the early stage of denoising to liberate the fine-tuned model\nfrom the fidelity-editability trade-off at inference. The two conflicting\ncomponents in the trade-off are decoupled and individually conquered by two\ncollaborative branches, which are then selectively integrated to preserve high\nconcept fidelity while achieving faithful prompt adherence. To obtain a better\nfine-tuned model, we introduce an Image-specific Context Optimization} (ICO)\nstrategy for model customization. ICO replaces manual prompt templates with\nlearnable image-specific contexts, providing an adaptive and precise\nfine-tuning direction to promote the overall performance. Extensive experiments\ndemonstrate the effectiveness of our method in reconciling the\nfidelity-editability trade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customized generation aims to incorporate a novel concept into a pre-trained\ntext-to-image model, enabling new generations of the concept in novel contexts\nguided by textual prompts. However, customized generation suffers from an\ninherent trade-off between concept fidelity and editability, i.e., between\nprecisely modeling the concept and faithfully adhering to the prompts. Previous\nmethods reluctantly seek a compromise and struggle to achieve both high concept\nfidelity and ideal prompt alignment simultaneously. In this paper, we propose a\nDivide, Conquer, then Integrate (DCI) framework, which performs a surgical\nadjustment in the early stage of denoising to liberate the fine-tuned model\nfrom the fidelity-editability trade-off at inference. The two conflicting\ncomponents in the trade-off are decoupled and individually conquered by two\ncollaborative branches, which are then selectively integrated to preserve high\nconcept fidelity while achieving faithful prompt adherence. To obtain a better\nfine-tuned model, we introduce an Image-specific Context Optimization} (ICO)\nstrategy for model customization. ICO replaces manual prompt templates with\nlearnable image-specific contexts, providing an adaptive and precise\nfine-tuning direction to promote the overall performance. Extensive experiments\ndemonstrate the effectiveness of our method in reconciling the\nfidelity-editability trade-off."
                },
                "authors": [
                    {
                        "name": "Jian Jin"
                    },
                    {
                        "name": "Yang Shen"
                    },
                    {
                        "name": "Zhenyong Fu"
                    },
                    {
                        "name": "Jian Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Yang"
                },
                "author": "Jian Yang",
                "arxiv_comment": "18 pages, 12 figures, ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04831v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04831v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04346v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04346v3",
                "updated": "2024-12-06T07:43:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    7,
                    43,
                    34,
                    4,
                    341,
                    0
                ],
                "published": "2024-07-05T08:37:10Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    8,
                    37,
                    10,
                    4,
                    187,
                    0
                ],
                "title": "MobileFlow: A Multimodal LLM For Mobile GUI Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MobileFlow: A Multimodal LLM For Mobile GUI Agent"
                },
                "summary": "Currently, the integration of mobile Graphical User Interfaces (GUIs) is\nubiquitous in most people's daily lives. And the ongoing evolution of\nmultimodal large-scale models, such as GPT-4v, Qwen-VL-Max, has significantly\nbolstered the capabilities of GUI comprehension and user action analysis,\nshowcasing the potentiality of intelligent GUI assistants. However, current GUI\nAgents often need to access page layout information through calling system\nAPIs, which may pose privacy risks. Fixing GUI (such as mobile interfaces) to a\ncertain low resolution might result in the loss of fine-grained image details.\nAt the same time, the multimodal large models built for GUI Agents currently\nhave poor understanding and decision-making abilities for Chinese GUI\ninterfaces, making them difficult to apply to a large number of Chinese apps.\nThis paper introduces MobileFlow, a multimodal large language model\nmeticulously crafted for mobile GUI agents. Transforming from the open-source\nmodel Qwen-VL-Chat into GUI domain, MobileFlow contains approximately 21\nbillion parameters and is equipped with novel hybrid visual encoders, making it\npossible for variable resolutions of image inputs and good support for\nmultilingual GUI. By incorporating Mixture of Experts (MoE) expansions and\npioneering alignment training strategies, MobileFlow has the capacity to fully\ninterpret image data and comprehend user instructions for GUI interaction\ntasks. Finally, MobileFlow outperforms Qwen-VL-Max and GPT-4v in terms of task\nexecution by GUI agents on both public and our proposed evaluation metrics, and\nhas been successfully deployed in real-world business contexts, proving its\neffectiveness for practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Currently, the integration of mobile Graphical User Interfaces (GUIs) is\nubiquitous in most people's daily lives. And the ongoing evolution of\nmultimodal large-scale models, such as GPT-4v, Qwen-VL-Max, has significantly\nbolstered the capabilities of GUI comprehension and user action analysis,\nshowcasing the potentiality of intelligent GUI assistants. However, current GUI\nAgents often need to access page layout information through calling system\nAPIs, which may pose privacy risks. Fixing GUI (such as mobile interfaces) to a\ncertain low resolution might result in the loss of fine-grained image details.\nAt the same time, the multimodal large models built for GUI Agents currently\nhave poor understanding and decision-making abilities for Chinese GUI\ninterfaces, making them difficult to apply to a large number of Chinese apps.\nThis paper introduces MobileFlow, a multimodal large language model\nmeticulously crafted for mobile GUI agents. Transforming from the open-source\nmodel Qwen-VL-Chat into GUI domain, MobileFlow contains approximately 21\nbillion parameters and is equipped with novel hybrid visual encoders, making it\npossible for variable resolutions of image inputs and good support for\nmultilingual GUI. By incorporating Mixture of Experts (MoE) expansions and\npioneering alignment training strategies, MobileFlow has the capacity to fully\ninterpret image data and comprehend user instructions for GUI interaction\ntasks. Finally, MobileFlow outperforms Qwen-VL-Max and GPT-4v in terms of task\nexecution by GUI agents on both public and our proposed evaluation metrics, and\nhas been successfully deployed in real-world business contexts, proving its\neffectiveness for practical applications."
                },
                "authors": [
                    {
                        "name": "Songqin Nong"
                    },
                    {
                        "name": "Jiali Zhu"
                    },
                    {
                        "name": "Rui Wu"
                    },
                    {
                        "name": "Jiongchao Jin"
                    },
                    {
                        "name": "Shuo Shan"
                    },
                    {
                        "name": "Xiutian Huang"
                    },
                    {
                        "name": "Wenhao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wenhao Xu"
                },
                "author": "Wenhao Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04346v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04346v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19772v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19772v2",
                "updated": "2024-12-06T07:24:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    7,
                    24,
                    10,
                    4,
                    341,
                    0
                ],
                "published": "2024-11-29T15:18:06Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    15,
                    18,
                    6,
                    4,
                    334,
                    0
                ],
                "title": "LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware\n  Omni-Modal Perception of Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware\n  Omni-Modal Perception of Long Videos"
                },
                "summary": "Despite impressive advancements in video understanding, most efforts remain\nlimited to coarse-grained or visual-only video tasks. However, real-world\nvideos encompass omni-modal information (vision, audio, and speech) with a\nseries of events forming a cohesive storyline. The lack of multi-modal video\ndata with fine-grained event annotations and the high cost of manual labeling\nare major obstacles to comprehensive omni-modality video perception. To address\nthis gap, we propose an automatic pipeline consisting of high-quality\nmulti-modal video filtering, semantically coherent omni-modal event boundary\ndetection, and cross-modal correlation-aware event captioning. In this way, we\npresent LongVALE, the first-ever Vision-Audio-Language Event understanding\nbenchmark comprising 105K omni-modal events with precise temporal boundaries\nand detailed relation-aware captions within 8.4K high-quality long videos.\nFurther, we build a baseline that leverages LongVALE to enable video large\nlanguage models (LLMs) for omni-modality fine-grained temporal video\nunderstanding for the first time. Extensive experiments demonstrate the\neffectiveness and great potential of LongVALE in advancing comprehensive\nmulti-modal video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite impressive advancements in video understanding, most efforts remain\nlimited to coarse-grained or visual-only video tasks. However, real-world\nvideos encompass omni-modal information (vision, audio, and speech) with a\nseries of events forming a cohesive storyline. The lack of multi-modal video\ndata with fine-grained event annotations and the high cost of manual labeling\nare major obstacles to comprehensive omni-modality video perception. To address\nthis gap, we propose an automatic pipeline consisting of high-quality\nmulti-modal video filtering, semantically coherent omni-modal event boundary\ndetection, and cross-modal correlation-aware event captioning. In this way, we\npresent LongVALE, the first-ever Vision-Audio-Language Event understanding\nbenchmark comprising 105K omni-modal events with precise temporal boundaries\nand detailed relation-aware captions within 8.4K high-quality long videos.\nFurther, we build a baseline that leverages LongVALE to enable video large\nlanguage models (LLMs) for omni-modality fine-grained temporal video\nunderstanding for the first time. Extensive experiments demonstrate the\neffectiveness and great potential of LongVALE in advancing comprehensive\nmulti-modal video understanding."
                },
                "authors": [
                    {
                        "name": "Tiantian Geng"
                    },
                    {
                        "name": "Jinrui Zhang"
                    },
                    {
                        "name": "Qingni Wang"
                    },
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Jinming Duan"
                    },
                    {
                        "name": "Feng Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zheng"
                },
                "author": "Feng Zheng",
                "arxiv_comment": "18 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19772v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19772v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17276v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17276v3",
                "updated": "2024-12-06T07:13:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    7,
                    13,
                    53,
                    4,
                    341,
                    0
                ],
                "published": "2024-06-25T04:45:53Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    4,
                    45,
                    53,
                    1,
                    177,
                    0
                ],
                "title": "OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure"
                },
                "summary": "Autoregressive language models demonstrate excellent performance in various\nscenarios. However, the inference efficiency is limited by its\none-step-one-word generation mode, which has become a pressing problem recently\nas the models become increasingly larger. Speculative decoding employs a \"draft\nand then verify\" mechanism to allow multiple tokens to be generated in one\nstep, realizing lossless acceleration. Existing methods mainly adopt fixed\nheuristic draft structures, which fail to adapt to different situations to\nmaximize the acceptance length during verification. To alleviate this dilemma,\nwe proposed OPT-Tree, an algorithm to construct adaptive and scalable draft\ntrees. It searches the optimal tree structure that maximizes the mathematical\nexpectation of the acceptance length in each decoding step. Experimental\nresults reveal that OPT-Tree outperforms the existing draft structures and\nachieves a speed-up ratio of up to 3.2 compared with autoregressive decoding.\nIf the draft model is powerful enough and the node budget is sufficient, it can\ngenerate more than ten tokens in a single step. Our code is available at\nhttps://github.com/Jikai0Wang/OPT-Tree.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive language models demonstrate excellent performance in various\nscenarios. However, the inference efficiency is limited by its\none-step-one-word generation mode, which has become a pressing problem recently\nas the models become increasingly larger. Speculative decoding employs a \"draft\nand then verify\" mechanism to allow multiple tokens to be generated in one\nstep, realizing lossless acceleration. Existing methods mainly adopt fixed\nheuristic draft structures, which fail to adapt to different situations to\nmaximize the acceptance length during verification. To alleviate this dilemma,\nwe proposed OPT-Tree, an algorithm to construct adaptive and scalable draft\ntrees. It searches the optimal tree structure that maximizes the mathematical\nexpectation of the acceptance length in each decoding step. Experimental\nresults reveal that OPT-Tree outperforms the existing draft structures and\nachieves a speed-up ratio of up to 3.2 compared with autoregressive decoding.\nIf the draft model is powerful enough and the node budget is sufficient, it can\ngenerate more than ten tokens in a single step. Our code is available at\nhttps://github.com/Jikai0Wang/OPT-Tree."
                },
                "authors": [
                    {
                        "name": "Jikai Wang"
                    },
                    {
                        "name": "Yi Su"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Qingrong Xia"
                    },
                    {
                        "name": "Zi Ye"
                    },
                    {
                        "name": "Xinyu Duan"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Accepted at TACL; pre-MIT Press publication version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17276v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17276v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.07668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.07668v2",
                "updated": "2024-12-06T07:11:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    7,
                    11,
                    33,
                    4,
                    341,
                    0
                ],
                "published": "2023-09-14T12:30:48Z",
                "published_parsed": [
                    2023,
                    9,
                    14,
                    12,
                    30,
                    48,
                    3,
                    257,
                    0
                ],
                "title": "ChromaDistill: Colorizing Monochrome Radiance Fields with Knowledge\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChromaDistill: Colorizing Monochrome Radiance Fields with Knowledge\n  Distillation"
                },
                "summary": "Colorization is a well-explored problem in the domains of image and video\nprocessing. However, extending colorization to 3D scenes presents significant\nchallenges. Recent Neural Radiance Field (NeRF) and Gaussian-Splatting(3DGS)\nmethods enable high-quality novel-view synthesis for multi-view images.\nHowever, the question arises: How can we colorize these 3D representations?\nThis work presents a method for synthesizing colorized novel views from input\ngrayscale multi-view images. Using image or video colorization methods to\ncolorize novel views from these 3D representations naively will yield output\nwith severe inconsistencies. We introduce a novel method to use powerful image\ncolorization models for colorizing 3D representations. We propose a\ndistillation-based method that transfers color from these networks trained on\nnatural images to the target 3D representation. Notably, this strategy does not\nadd any additional weights or computational overhead to the original\nrepresentation during inference. Extensive experiments demonstrate that our\nmethod produces high-quality colorized views for indoor and outdoor scenes,\nshowcasing significant cross-view consistency advantages over baseline\napproaches. Our method is agnostic to the underlying 3D representation and\neasily generalizable to NeRF and 3DGS methods. Further, we validate the\nefficacy of our approach in several diverse applications: 1.) Infra-Red (IR)\nmulti-view images and 2.) Legacy grayscale multi-view image sequences. Project\nWebpage: https://val.cds.iisc.ac.in/chroma-distill.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Colorization is a well-explored problem in the domains of image and video\nprocessing. However, extending colorization to 3D scenes presents significant\nchallenges. Recent Neural Radiance Field (NeRF) and Gaussian-Splatting(3DGS)\nmethods enable high-quality novel-view synthesis for multi-view images.\nHowever, the question arises: How can we colorize these 3D representations?\nThis work presents a method for synthesizing colorized novel views from input\ngrayscale multi-view images. Using image or video colorization methods to\ncolorize novel views from these 3D representations naively will yield output\nwith severe inconsistencies. We introduce a novel method to use powerful image\ncolorization models for colorizing 3D representations. We propose a\ndistillation-based method that transfers color from these networks trained on\nnatural images to the target 3D representation. Notably, this strategy does not\nadd any additional weights or computational overhead to the original\nrepresentation during inference. Extensive experiments demonstrate that our\nmethod produces high-quality colorized views for indoor and outdoor scenes,\nshowcasing significant cross-view consistency advantages over baseline\napproaches. Our method is agnostic to the underlying 3D representation and\neasily generalizable to NeRF and 3DGS methods. Further, we validate the\nefficacy of our approach in several diverse applications: 1.) Infra-Red (IR)\nmulti-view images and 2.) Legacy grayscale multi-view image sequences. Project\nWebpage: https://val.cds.iisc.ac.in/chroma-distill.github.io/"
                },
                "authors": [
                    {
                        "name": "Ankit Dhiman"
                    },
                    {
                        "name": "R Srinath"
                    },
                    {
                        "name": "Srinjay Sarkar"
                    },
                    {
                        "name": "Lokesh R Boregowda"
                    },
                    {
                        "name": "R Venkatesh Babu"
                    }
                ],
                "author_detail": {
                    "name": "R Venkatesh Babu"
                },
                "author": "R Venkatesh Babu",
                "arxiv_comment": "WACV 2025, AI3DCC @ ICCV 2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.07668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.07668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06448v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06448v2",
                "updated": "2024-12-06T07:10:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    7,
                    10,
                    30,
                    4,
                    341,
                    0
                ],
                "published": "2024-04-09T16:50:30Z",
                "published_parsed": [
                    2024,
                    4,
                    9,
                    16,
                    50,
                    30,
                    1,
                    100,
                    0
                ],
                "title": "Automated Federated Pipeline for Parameter-Efficient Fine-Tuning of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Federated Pipeline for Parameter-Efficient Fine-Tuning of\n  Large Language Models"
                },
                "summary": "Recently, there has been a surge in the development of advanced intelligent\ngenerative content (AIGC), especially large language models (LLMs). However,\nfor many downstream tasks, it is necessary to fine-tune LLMs using private\ndata. While federated learning offers a promising privacy-preserving solution\nto LLM fine-tuning, the substantial size of an LLM, combined with high\ncomputational and communication demands, makes it hard to apply to downstream\ntasks. More importantly, private edge servers often possess varying computing\nand network resources in real-world scenarios, introducing additional\ncomplexities to LLM fine-tuning. To tackle these problems, we design and\nimplement an automated federated pipeline, named FedPipe, to fine-tune LLMs\nwith minimal training cost but without adding any inference latency. FedPipe\nfirstly identifies the weights to be fine-tuned based on their contributions to\nthe LLM training. It then configures a low-rank adapter for each selected\nweight to train local low-rank adapters on an edge server, and aggregate local\nadapters of all edge servers to fine-tune the whole LLM. Finally, it\nappropriately quantizes the parameters of LLM to reduce memory space according\nto the requirements of edge servers. Extensive experiments demonstrate that\nFedPipe expedites the model training and achieves higher accuracy than\nstate-of-the-art benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, there has been a surge in the development of advanced intelligent\ngenerative content (AIGC), especially large language models (LLMs). However,\nfor many downstream tasks, it is necessary to fine-tune LLMs using private\ndata. While federated learning offers a promising privacy-preserving solution\nto LLM fine-tuning, the substantial size of an LLM, combined with high\ncomputational and communication demands, makes it hard to apply to downstream\ntasks. More importantly, private edge servers often possess varying computing\nand network resources in real-world scenarios, introducing additional\ncomplexities to LLM fine-tuning. To tackle these problems, we design and\nimplement an automated federated pipeline, named FedPipe, to fine-tune LLMs\nwith minimal training cost but without adding any inference latency. FedPipe\nfirstly identifies the weights to be fine-tuned based on their contributions to\nthe LLM training. It then configures a low-rank adapter for each selected\nweight to train local low-rank adapters on an edge server, and aggregate local\nadapters of all edge servers to fine-tune the whole LLM. Finally, it\nappropriately quantizes the parameters of LLM to reduce memory space according\nto the requirements of edge servers. Extensive experiments demonstrate that\nFedPipe expedites the model training and achieves higher accuracy than\nstate-of-the-art benchmarks."
                },
                "authors": [
                    {
                        "name": "Zihan Fang"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Yue Gao"
                    },
                    {
                        "name": "Yuguang Fang"
                    }
                ],
                "author_detail": {
                    "name": "Yuguang Fang"
                },
                "author": "Yuguang Fang",
                "arxiv_comment": "15 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06448v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06448v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02565v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02565v2",
                "updated": "2024-12-06T07:08:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    7,
                    8,
                    56,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-03T16:53:58Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    53,
                    58,
                    1,
                    338,
                    0
                ],
                "title": "SJTU:Spatial judgments in multimodal models towards unified segmentation\n  through coordinate detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SJTU:Spatial judgments in multimodal models towards unified segmentation\n  through coordinate detection"
                },
                "summary": "Despite significant advances in vision-language understanding, implementing\nimage segmentation within multimodal architectures remains a fundamental\nchallenge in modern artificial intelligence systems. Existing vision-language\nmodels, which primarily rely on backbone architectures or CLIP-based embedding\nlearning, demonstrate inherent limitations in fine-grained spatial localization\nand operational capabilities. This paper introduces SJTU: Spatial Judgments in\nMultimodal Models - Towards Unified Segmentation through Coordinate Detection,\na framework that leverages spatial coordinate understanding to bridge\nvision-language interaction and precise segmentation, enabling accurate target\nidentification through natural language instructions. The framework presents an\napproach for integrating segmentation techniques with vision-language models\nthrough spatial inference in multimodal space. By utilizing normalized\ncoordinate detection for bounding boxes and transforming them into actionable\nsegmentation outputs, we establish a connection between spatial and language\nrepresentations in multimodal architectures. Experimental results demonstrate\nsuperior performance across benchmark datasets, achieving IoU scores of 0.5958\non COCO 2017 and 0.6758 on Pascal VOC. Testing on a single NVIDIA RTX 3090 GPU\nwith 512x512 resolution images yields an average inference time of 7 seconds\nper image, demonstrating the framework's effectiveness in both accuracy and\npractical deployability. The project code is available at\nhttps://github.com/jw-chae/SJTU",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant advances in vision-language understanding, implementing\nimage segmentation within multimodal architectures remains a fundamental\nchallenge in modern artificial intelligence systems. Existing vision-language\nmodels, which primarily rely on backbone architectures or CLIP-based embedding\nlearning, demonstrate inherent limitations in fine-grained spatial localization\nand operational capabilities. This paper introduces SJTU: Spatial Judgments in\nMultimodal Models - Towards Unified Segmentation through Coordinate Detection,\na framework that leverages spatial coordinate understanding to bridge\nvision-language interaction and precise segmentation, enabling accurate target\nidentification through natural language instructions. The framework presents an\napproach for integrating segmentation techniques with vision-language models\nthrough spatial inference in multimodal space. By utilizing normalized\ncoordinate detection for bounding boxes and transforming them into actionable\nsegmentation outputs, we establish a connection between spatial and language\nrepresentations in multimodal architectures. Experimental results demonstrate\nsuperior performance across benchmark datasets, achieving IoU scores of 0.5958\non COCO 2017 and 0.6758 on Pascal VOC. Testing on a single NVIDIA RTX 3090 GPU\nwith 512x512 resolution images yields an average inference time of 7 seconds\nper image, demonstrating the framework's effectiveness in both accuracy and\npractical deployability. The project code is available at\nhttps://github.com/jw-chae/SJTU"
                },
                "authors": [
                    {
                        "name": "Joongwon Chae"
                    },
                    {
                        "name": "Zhenyu Wang"
                    },
                    {
                        "name": "Peiwu Qin"
                    }
                ],
                "author_detail": {
                    "name": "Peiwu Qin"
                },
                "author": "Peiwu Qin",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02565v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02565v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05721v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05721v3",
                "updated": "2024-12-06T06:51:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    6,
                    51,
                    46,
                    4,
                    341,
                    0
                ],
                "published": "2024-07-08T08:25:56Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    8,
                    25,
                    56,
                    0,
                    190,
                    0
                ],
                "title": "PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation"
                },
                "summary": "Mental health has attracted substantial attention in recent years and LLM can\nbe an effective technology for alleviating this problem owing to its capability\nin text understanding and dialogue. However, existing research in this domain\noften suffers from limitations, such as training on datasets lacking crucial\nprior knowledge and evidence, and the absence of comprehensive evaluation\nmethods. In this paper, we propose a specialized psychological large language\nmodel (LLM), named PsycoLLM, trained on a proposed high-quality psychological\ndataset, including single-turn QA, multi-turn dialogues and knowledge-based QA.\nSpecifically, we construct multi-turn dialogues through a three-step pipeline\ncomprising multi-turn QA generation, evidence judgment, and dialogue\nrefinement. We augment this process with real-world psychological case\nbackgrounds extracted from online platforms, enhancing the relevance and\napplicability of the generated data. Additionally, to compare the performance\nof PsycoLLM with other LLMs, we develop a comprehensive psychological benchmark\nbased on authoritative psychological counseling examinations in China, which\nincludes assessments of professional ethics, theoretical proficiency, and case\nanalysis. The experimental results on the benchmark illustrate the\neffectiveness of PsycoLLM, which demonstrates superior performance compared to\nother LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mental health has attracted substantial attention in recent years and LLM can\nbe an effective technology for alleviating this problem owing to its capability\nin text understanding and dialogue. However, existing research in this domain\noften suffers from limitations, such as training on datasets lacking crucial\nprior knowledge and evidence, and the absence of comprehensive evaluation\nmethods. In this paper, we propose a specialized psychological large language\nmodel (LLM), named PsycoLLM, trained on a proposed high-quality psychological\ndataset, including single-turn QA, multi-turn dialogues and knowledge-based QA.\nSpecifically, we construct multi-turn dialogues through a three-step pipeline\ncomprising multi-turn QA generation, evidence judgment, and dialogue\nrefinement. We augment this process with real-world psychological case\nbackgrounds extracted from online platforms, enhancing the relevance and\napplicability of the generated data. Additionally, to compare the performance\nof PsycoLLM with other LLMs, we develop a comprehensive psychological benchmark\nbased on authoritative psychological counseling examinations in China, which\nincludes assessments of professional ethics, theoretical proficiency, and case\nanalysis. The experimental results on the benchmark illustrate the\neffectiveness of PsycoLLM, which demonstrates superior performance compared to\nother LLMs."
                },
                "authors": [
                    {
                        "name": "Jinpeng Hu"
                    },
                    {
                        "name": "Tengteng Dong"
                    },
                    {
                        "name": "Luo Gang"
                    },
                    {
                        "name": "Hui Ma"
                    },
                    {
                        "name": "Peng Zou"
                    },
                    {
                        "name": "Xiao Sun"
                    },
                    {
                        "name": "Dan Guo"
                    },
                    {
                        "name": "Xun Yang"
                    },
                    {
                        "name": "Meng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Wang"
                },
                "author": "Meng Wang",
                "arxiv_comment": "Accepted by IEEE Transactions on Computational Social Systems.\n  https://github.com/MACLAB-HFUT/PsycoLLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05721v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05721v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04232v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04232v2",
                "updated": "2024-12-06T06:35:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    6,
                    35,
                    41,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-05T15:09:21Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    9,
                    21,
                    3,
                    340,
                    0
                ],
                "title": "Intent-based Meta-Scheduling in Programmable Networks: A Research Agenda",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent-based Meta-Scheduling in Programmable Networks: A Research Agenda"
                },
                "summary": "The emergence and growth of 5G and beyond 5G (B5G) networks has brought about\nthe rise of so-called ''programmable'' networks, i.e., networks whose\noperational requirements are so stringent that they can only be met in an\nautomated manner, with minimal/no human involvement. Any requirements on such a\nnetwork would need to be formally specified via intents, which can represent\nuser requirements in a formal yet understandable manner. Meeting the user\nrequirements via intents would necessitate the rapid implementation of resource\nallocation and scheduling in the network. Also, given the expected size and\ngeographical distribution of programmable networks, multiple resource\nscheduling implementations would need to be implemented at the same time. This\nwould necessitate the use of a meta-scheduler that can coordinate the various\nschedulers and dynamically ensure optimal resource scheduling across the\nnetwork.\n  To that end, in this position paper, we propose a research agenda for\nmodeling, implementation, and inclusion of intent-based dynamic meta-scheduling\nin programmable networks. Our research agenda will be built on active\ninference, a type of causal inference. Active inference provides some level of\nautonomy to each scheduler while the meta-scheduler takes care of overall\nintent fulfillment. Our research agenda will comprise a strawman architecture\nfor meta-scheduling and a set of research questions that need to be addressed\nto make intent-based dynamic meta-scheduling a reality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence and growth of 5G and beyond 5G (B5G) networks has brought about\nthe rise of so-called ''programmable'' networks, i.e., networks whose\noperational requirements are so stringent that they can only be met in an\nautomated manner, with minimal/no human involvement. Any requirements on such a\nnetwork would need to be formally specified via intents, which can represent\nuser requirements in a formal yet understandable manner. Meeting the user\nrequirements via intents would necessitate the rapid implementation of resource\nallocation and scheduling in the network. Also, given the expected size and\ngeographical distribution of programmable networks, multiple resource\nscheduling implementations would need to be implemented at the same time. This\nwould necessitate the use of a meta-scheduler that can coordinate the various\nschedulers and dynamically ensure optimal resource scheduling across the\nnetwork.\n  To that end, in this position paper, we propose a research agenda for\nmodeling, implementation, and inclusion of intent-based dynamic meta-scheduling\nin programmable networks. Our research agenda will be built on active\ninference, a type of causal inference. Active inference provides some level of\nautonomy to each scheduler while the meta-scheduler takes care of overall\nintent fulfillment. Our research agenda will comprise a strawman architecture\nfor meta-scheduling and a set of research questions that need to be addressed\nto make intent-based dynamic meta-scheduling a reality."
                },
                "authors": [
                    {
                        "name": "Nanjangud C. Narendra"
                    },
                    {
                        "name": "Ronak Kanthaliya"
                    },
                    {
                        "name": "Venkatareddy Akumalla"
                    }
                ],
                "author_detail": {
                    "name": "Venkatareddy Akumalla"
                },
                "author": "Venkatareddy Akumalla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04232v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04232v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04806v1",
                "updated": "2024-12-06T06:32:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    6,
                    32,
                    47,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T06:32:47Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    6,
                    32,
                    47,
                    4,
                    341,
                    0
                ],
                "title": "Rethinking Time Series Forecasting with LLMs via Nearest Neighbor\n  Contrastive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Time Series Forecasting with LLMs via Nearest Neighbor\n  Contrastive Learning"
                },
                "summary": "Adapting Large Language Models (LLMs) that are extensively trained on\nabundant text data, and customizing the input prompt to enable time series\nforecasting has received considerable attention. While recent work has shown\ngreat potential for adapting the learned prior of LLMs, the formulation of the\nprompt to finetune LLMs remains challenging as prompt should be aligned with\ntime series data. Additionally, current approaches do not effectively leverage\nword token embeddings which embody the rich representation space learned by\nLLMs. This emphasizes the need for a robust approach to formulate the prompt\nwhich utilizes the word token embeddings while effectively representing the\ncharacteristics of the time series. To address these challenges, we propose\nNNCL-TLLM: Nearest Neighbor Contrastive Learning for Time series forecasting\nvia LLMs. First, we generate time series compatible text prototypes such that\neach text prototype represents both word token embeddings in its neighborhood\nand time series characteristics via end-to-end finetuning. Next, we draw\ninspiration from Nearest Neighbor Contrastive Learning to formulate the prompt\nwhile obtaining the top-$k$ nearest neighbor time series compatible text\nprototypes. We then fine-tune the layer normalization and positional embeddings\nof the LLM, keeping the other layers intact, reducing the trainable parameters\nand decreasing the computational cost. Our comprehensive experiments\ndemonstrate that NNCL-TLLM outperforms in few-shot forecasting while achieving\ncompetitive or superior performance over the state-of-the-art methods in\nlong-term and short-term forecasting tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Large Language Models (LLMs) that are extensively trained on\nabundant text data, and customizing the input prompt to enable time series\nforecasting has received considerable attention. While recent work has shown\ngreat potential for adapting the learned prior of LLMs, the formulation of the\nprompt to finetune LLMs remains challenging as prompt should be aligned with\ntime series data. Additionally, current approaches do not effectively leverage\nword token embeddings which embody the rich representation space learned by\nLLMs. This emphasizes the need for a robust approach to formulate the prompt\nwhich utilizes the word token embeddings while effectively representing the\ncharacteristics of the time series. To address these challenges, we propose\nNNCL-TLLM: Nearest Neighbor Contrastive Learning for Time series forecasting\nvia LLMs. First, we generate time series compatible text prototypes such that\neach text prototype represents both word token embeddings in its neighborhood\nand time series characteristics via end-to-end finetuning. Next, we draw\ninspiration from Nearest Neighbor Contrastive Learning to formulate the prompt\nwhile obtaining the top-$k$ nearest neighbor time series compatible text\nprototypes. We then fine-tune the layer normalization and positional embeddings\nof the LLM, keeping the other layers intact, reducing the trainable parameters\nand decreasing the computational cost. Our comprehensive experiments\ndemonstrate that NNCL-TLLM outperforms in few-shot forecasting while achieving\ncompetitive or superior performance over the state-of-the-art methods in\nlong-term and short-term forecasting tasks."
                },
                "authors": [
                    {
                        "name": "Jayanie Bogahawatte"
                    },
                    {
                        "name": "Sachith Seneviratne"
                    },
                    {
                        "name": "Maneesha Perera"
                    },
                    {
                        "name": "Saman Halgamuge"
                    }
                ],
                "author_detail": {
                    "name": "Saman Halgamuge"
                },
                "author": "Saman Halgamuge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01672v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01672v3",
                "updated": "2024-12-06T06:25:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    6,
                    25,
                    54,
                    4,
                    341,
                    0
                ],
                "published": "2024-10-02T15:41:22Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    41,
                    22,
                    2,
                    276,
                    0
                ],
                "title": "Practicing Stress Relief for the Everyday: Designing Social Simulation\n  Using VR, AR, and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practicing Stress Relief for the Everyday: Designing Social Simulation\n  Using VR, AR, and LLMs"
                },
                "summary": "Stress is an inevitable part of day-to-day life yet many find themselves\nunable to manage it themselves, particularly when professional or peer support\nare not always readily available. As self-care becomes increasingly vital for\nmental well-being, this paper explores the potential of social simulation as a\nsafe, virtual environment for practicing stress relief for everyday situations.\nLeveraging the immersive capabilities of VR, AR, and LLMs, we developed eight\ninteractive prototypes for various everyday stressful scenarios (e.g. public\nspeaking) then conducted prototype-driven semi-structured interviews with 19\nparticipants. We reveal that people currently lack effective means to support\nthemselves through everyday stress and found that social simulation fills a gap\nfor simulating real environments for training mental health practices. We\noutline key considerations for future development of simulation for self-care,\nincluding risks of trauma from hyper-realism, distrust of LLM-recommended\ntiming for mental health recommendations, and the value of accessibility for\nself-care interventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stress is an inevitable part of day-to-day life yet many find themselves\nunable to manage it themselves, particularly when professional or peer support\nare not always readily available. As self-care becomes increasingly vital for\nmental well-being, this paper explores the potential of social simulation as a\nsafe, virtual environment for practicing stress relief for everyday situations.\nLeveraging the immersive capabilities of VR, AR, and LLMs, we developed eight\ninteractive prototypes for various everyday stressful scenarios (e.g. public\nspeaking) then conducted prototype-driven semi-structured interviews with 19\nparticipants. We reveal that people currently lack effective means to support\nthemselves through everyday stress and found that social simulation fills a gap\nfor simulating real environments for training mental health practices. We\noutline key considerations for future development of simulation for self-care,\nincluding risks of trauma from hyper-realism, distrust of LLM-recommended\ntiming for mental health recommendations, and the value of accessibility for\nself-care interventions."
                },
                "authors": [
                    {
                        "name": "Anna Fang"
                    },
                    {
                        "name": "Hriday Chhabria"
                    },
                    {
                        "name": "Alekhya Maram"
                    },
                    {
                        "name": "Haiyi Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Haiyi Zhu"
                },
                "author": "Haiyi Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01672v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01672v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04802v1",
                "updated": "2024-12-06T06:22:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    6,
                    22,
                    43,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T06:22:43Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    6,
                    22,
                    43,
                    4,
                    341,
                    0
                ],
                "title": "Modality Decoupling is All You Need: A Simple Solution for Unsupervised\n  Hyperspectral Image Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modality Decoupling is All You Need: A Simple Solution for Unsupervised\n  Hyperspectral Image Fusion"
                },
                "summary": "Hyperspectral Image Fusion (HIF) aims to fuse low-resolution hyperspectral\nimages (LR-HSIs) and high-resolution multispectral images (HR-MSIs) to\nreconstruct high spatial and high spectral resolution images. Current methods\ntypically apply direct fusion from the two modalities without valid\nsupervision, failing to fully perceive the deep modality-complementary\ninformation and hence, resulting in a superficial understanding of\ninter-modality connections. To bridge this gap, we propose a simple and\neffective solution for unsupervised HIF with an assumption that modality\ndecoupling is essential for HIF. We introduce the modality clustering loss that\nensures clear guidance of the modality, decoupling towards modality-shared\nfeatures while steering clear of modality-complementary ones. Also, we propose\nan end-to-end Modality-Decoupled Spatial-Spectral Fusion (MossFuse) framework\nthat decouples shared and complementary information across modalities and\naggregates a concise representation of the LR-HSI and HR-MSI to reduce the\nmodality redundancy. Systematic experiments over multiple datasets demonstrate\nthat our simple and effective approach consistently outperforms the existing\nHIF methods while requiring considerably fewer parameters with reduced\ninference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyperspectral Image Fusion (HIF) aims to fuse low-resolution hyperspectral\nimages (LR-HSIs) and high-resolution multispectral images (HR-MSIs) to\nreconstruct high spatial and high spectral resolution images. Current methods\ntypically apply direct fusion from the two modalities without valid\nsupervision, failing to fully perceive the deep modality-complementary\ninformation and hence, resulting in a superficial understanding of\ninter-modality connections. To bridge this gap, we propose a simple and\neffective solution for unsupervised HIF with an assumption that modality\ndecoupling is essential for HIF. We introduce the modality clustering loss that\nensures clear guidance of the modality, decoupling towards modality-shared\nfeatures while steering clear of modality-complementary ones. Also, we propose\nan end-to-end Modality-Decoupled Spatial-Spectral Fusion (MossFuse) framework\nthat decouples shared and complementary information across modalities and\naggregates a concise representation of the LR-HSI and HR-MSI to reduce the\nmodality redundancy. Systematic experiments over multiple datasets demonstrate\nthat our simple and effective approach consistently outperforms the existing\nHIF methods while requiring considerably fewer parameters with reduced\ninference time."
                },
                "authors": [
                    {
                        "name": "Songcheng Du"
                    },
                    {
                        "name": "Yang Zou"
                    },
                    {
                        "name": "Zixu Wang"
                    },
                    {
                        "name": "Xingyuan Li"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Qiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Shen"
                },
                "author": "Qiang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04799v1",
                "updated": "2024-12-06T06:09:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    6,
                    9,
                    43,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T06:09:43Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    6,
                    9,
                    43,
                    4,
                    341,
                    0
                ],
                "title": "Estimating the treatment effect over time under general interference\n  through deep learner integrated TMLE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating the treatment effect over time under general interference\n  through deep learner integrated TMLE"
                },
                "summary": "Understanding the effects of quarantine policies in populations with\nunderlying social networks is crucial for public health, yet most causal\ninference methods fail here due to their assumption of independent individuals.\nWe introduce DeepNetTMLE, a deep-learning-enhanced Targeted Maximum Likelihood\nEstimation (TMLE) method designed to estimate time-sensitive treatment effects\nin observational data. DeepNetTMLE mitigates bias from time-varying confounders\nunder general interference by incorporating a temporal module and domain\nadversarial training to build intervention-invariant representations. This\nprocess removes associations between current treatments and historical\nvariables, while the targeting step maintains the bias-variance trade-off,\nenhancing the reliability of counterfactual predictions. Using simulations of a\n``Susceptible-Infected-Recovered'' model with varied quarantine coverages, we\nshow that DeepNetTMLE achieves lower bias and more precise confidence intervals\nin counterfactual estimates, enabling optimal quarantine recommendations within\nbudget constraints, surpassing state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the effects of quarantine policies in populations with\nunderlying social networks is crucial for public health, yet most causal\ninference methods fail here due to their assumption of independent individuals.\nWe introduce DeepNetTMLE, a deep-learning-enhanced Targeted Maximum Likelihood\nEstimation (TMLE) method designed to estimate time-sensitive treatment effects\nin observational data. DeepNetTMLE mitigates bias from time-varying confounders\nunder general interference by incorporating a temporal module and domain\nadversarial training to build intervention-invariant representations. This\nprocess removes associations between current treatments and historical\nvariables, while the targeting step maintains the bias-variance trade-off,\nenhancing the reliability of counterfactual predictions. Using simulations of a\n``Susceptible-Infected-Recovered'' model with varied quarantine coverages, we\nshow that DeepNetTMLE achieves lower bias and more precise confidence intervals\nin counterfactual estimates, enabling optimal quarantine recommendations within\nbudget constraints, surpassing state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Suhan Guo"
                    },
                    {
                        "name": "Furao Shen"
                    },
                    {
                        "name": "Ni Li"
                    }
                ],
                "author_detail": {
                    "name": "Ni Li"
                },
                "author": "Ni Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.05270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05270v1",
                "updated": "2024-12-06T18:55:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    55,
                    34,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T18:55:34Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    55,
                    34,
                    4,
                    341,
                    0
                ],
                "title": "APOLLO: SGD-like Memory, AdamW-level Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APOLLO: SGD-like Memory, AdamW-level Performance"
                },
                "summary": "Large language models (LLMs) are notoriously memory-intensive during\ntraining, particularly with the popular AdamW optimizer. This memory burden\nnecessitates using more or higher-end GPUs or reducing batch sizes, limiting\ntraining scalability and throughput. To address this, various memory-efficient\noptimizers have been proposed to reduce optimizer memory usage. However, they\nface critical challenges: (i) reliance on costly SVD operations; (ii)\nsignificant performance trade-offs compared to AdamW; and (iii) still\nsubstantial optimizer memory overhead to maintain competitive performance.\n  In this work, we identify that AdamW's learning rate adaptation rule can be\neffectively coarsened as a structured learning rate update. Based on this\ninsight, we propose Approximated Gradient Scaling for Memory-Efficient LLM\nOptimization (APOLLO), which approximates learning rate scaling using an\nauxiliary low-rank optimizer state based on pure random projection. This\nstructured learning rate update rule makes APOLLO highly tolerant to further\nmemory reductions while delivering comparable pre-training performance. Even\nits rank-1 variant, APOLLO-Mini, achieves superior pre-training performance\ncompared to AdamW with SGD-level memory costs.\n  Extensive experiments demonstrate that the APOLLO series performs on-par with\nor better than AdamW, while achieving greater memory savings by nearly\neliminating the optimization states of AdamW. These savings provide significant\nsystem-level benefits: (1) Enhanced Throughput: 3x throughput on an 8xA100-80GB\nsetup compared to AdamW by supporting 4x larger batch sizes. (2) Improved Model\nScalability: Pre-training LLaMA-13B with naive DDP on A100-80GB GPUs without\nsystem-level optimizations. (3) Low-End GPU Friendly Pre-training: Pre-training\nLLaMA-7B on a single GPU using less than 12 GB of memory with weight\nquantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are notoriously memory-intensive during\ntraining, particularly with the popular AdamW optimizer. This memory burden\nnecessitates using more or higher-end GPUs or reducing batch sizes, limiting\ntraining scalability and throughput. To address this, various memory-efficient\noptimizers have been proposed to reduce optimizer memory usage. However, they\nface critical challenges: (i) reliance on costly SVD operations; (ii)\nsignificant performance trade-offs compared to AdamW; and (iii) still\nsubstantial optimizer memory overhead to maintain competitive performance.\n  In this work, we identify that AdamW's learning rate adaptation rule can be\neffectively coarsened as a structured learning rate update. Based on this\ninsight, we propose Approximated Gradient Scaling for Memory-Efficient LLM\nOptimization (APOLLO), which approximates learning rate scaling using an\nauxiliary low-rank optimizer state based on pure random projection. This\nstructured learning rate update rule makes APOLLO highly tolerant to further\nmemory reductions while delivering comparable pre-training performance. Even\nits rank-1 variant, APOLLO-Mini, achieves superior pre-training performance\ncompared to AdamW with SGD-level memory costs.\n  Extensive experiments demonstrate that the APOLLO series performs on-par with\nor better than AdamW, while achieving greater memory savings by nearly\neliminating the optimization states of AdamW. These savings provide significant\nsystem-level benefits: (1) Enhanced Throughput: 3x throughput on an 8xA100-80GB\nsetup compared to AdamW by supporting 4x larger batch sizes. (2) Improved Model\nScalability: Pre-training LLaMA-13B with naive DDP on A100-80GB GPUs without\nsystem-level optimizations. (3) Low-End GPU Friendly Pre-training: Pre-training\nLLaMA-7B on a single GPU using less than 12 GB of memory with weight\nquantization."
                },
                "authors": [
                    {
                        "name": "Hanqing Zhu"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Wenyan Cong"
                    },
                    {
                        "name": "Xi Liu"
                    },
                    {
                        "name": "Sem Park"
                    },
                    {
                        "name": "Vikas Chandra"
                    },
                    {
                        "name": "Bo Long"
                    },
                    {
                        "name": "David Z. Pan"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Jinwon Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jinwon Lee"
                },
                "author": "Jinwon Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03459v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03459v3",
                "updated": "2024-12-06T18:31:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    31,
                    30,
                    4,
                    341,
                    0
                ],
                "published": "2024-08-06T22:11:00Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    22,
                    11,
                    0,
                    1,
                    219,
                    0
                ],
                "title": "On the Generalization of Preference Learning with DPO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Generalization of Preference Learning with DPO"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities but\noften struggle to align with human preferences, leading to harmful or\nundesirable outputs. Preference learning, which trains models to distinguish\nbetween preferred and non-preferred responses based on human feedback, has\nbecome a crucial component for ensuring that LLMs align with human values.\nDespite the widespread adoption in real-world systems, a thorough theoretical\nunderstanding of the generalization guarantees for these models remain lacking.\nThis paper bridges that gap by introducing a new theoretical framework to\nanalyze the generalization guarantees of models trained with direct preference\noptimization (DPO). While existing generalization theory often focuses on\noverparameterized models achieving near-optimal loss or models independent of\nthe training process, our framework rigorously assesses how well models\ngeneralize after a finite number of gradient steps, reflecting real-world LLM\ntraining practices. By analyzing the reward margin associated with each sample\nand its trajectory throughout training, we can effectively bound the\ngeneralization error. We derive learning guarantees showing that, under\nspecific conditions, models trained with DPO can correctly discern preferred\nresponses on unseen data with high probability. These insights are empirically\nvalidated on contemporary LLMs, underscoring the practical relevance of our\ntheoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities but\noften struggle to align with human preferences, leading to harmful or\nundesirable outputs. Preference learning, which trains models to distinguish\nbetween preferred and non-preferred responses based on human feedback, has\nbecome a crucial component for ensuring that LLMs align with human values.\nDespite the widespread adoption in real-world systems, a thorough theoretical\nunderstanding of the generalization guarantees for these models remain lacking.\nThis paper bridges that gap by introducing a new theoretical framework to\nanalyze the generalization guarantees of models trained with direct preference\noptimization (DPO). While existing generalization theory often focuses on\noverparameterized models achieving near-optimal loss or models independent of\nthe training process, our framework rigorously assesses how well models\ngeneralize after a finite number of gradient steps, reflecting real-world LLM\ntraining practices. By analyzing the reward margin associated with each sample\nand its trajectory throughout training, we can effectively bound the\ngeneralization error. We derive learning guarantees showing that, under\nspecific conditions, models trained with DPO can correctly discern preferred\nresponses on unseen data with high probability. These insights are empirically\nvalidated on contemporary LLMs, underscoring the practical relevance of our\ntheoretical findings."
                },
                "authors": [
                    {
                        "name": "Shawn Im"
                    },
                    {
                        "name": "Yixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Yixuan Li"
                },
                "author": "Yixuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03459v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03459v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05248v1",
                "updated": "2024-12-06T18:27:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    27,
                    15,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T18:27:15Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    27,
                    15,
                    4,
                    341,
                    0
                ],
                "title": "Enhancing FKG.in: automating Indian food composition analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing FKG.in: automating Indian food composition analysis"
                },
                "summary": "This paper presents a novel approach to compute food composition data for\nIndian recipes using a knowledge graph for Indian food (FKG.in) and LLMs. The\nprimary focus is to provide a broad overview of an automated food composition\nanalysis workflow and describe its core functionalities: nutrition data\naggregation, food composition analysis, and LLM-augmented information\nresolution. This workflow aims to complement FKG.in and iteratively supplement\nfood composition data from verified knowledge bases. Additionally, this paper\nhighlights the challenges of representing Indian food and accessing food\ncomposition data digitally. It also reviews three key sources of food\ncomposition data: the Indian Food Composition Tables, the Indian Nutrient\nDatabank, and the Nutritionix API. Furthermore, it briefly outlines how users\ncan interact with the workflow to obtain diet-based health recommendations and\ndetailed food composition information for numerous recipes. We then explore the\ncomplex challenges of analyzing Indian recipe information across dimensions\nsuch as structure, multilingualism, and uncertainty as well as present our\nongoing work on LLM-based solutions to address these issues. The methods\nproposed in this workshop paper for AI-driven knowledge curation and\ninformation resolution are application-agnostic, generalizable, and replicable\nfor any domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach to compute food composition data for\nIndian recipes using a knowledge graph for Indian food (FKG.in) and LLMs. The\nprimary focus is to provide a broad overview of an automated food composition\nanalysis workflow and describe its core functionalities: nutrition data\naggregation, food composition analysis, and LLM-augmented information\nresolution. This workflow aims to complement FKG.in and iteratively supplement\nfood composition data from verified knowledge bases. Additionally, this paper\nhighlights the challenges of representing Indian food and accessing food\ncomposition data digitally. It also reviews three key sources of food\ncomposition data: the Indian Food Composition Tables, the Indian Nutrient\nDatabank, and the Nutritionix API. Furthermore, it briefly outlines how users\ncan interact with the workflow to obtain diet-based health recommendations and\ndetailed food composition information for numerous recipes. We then explore the\ncomplex challenges of analyzing Indian recipe information across dimensions\nsuch as structure, multilingualism, and uncertainty as well as present our\nongoing work on LLM-based solutions to address these issues. The methods\nproposed in this workshop paper for AI-driven knowledge curation and\ninformation resolution are application-agnostic, generalizable, and replicable\nfor any domain."
                },
                "authors": [
                    {
                        "name": "Saransh Kumar Gupta"
                    },
                    {
                        "name": "Lipika Dey"
                    },
                    {
                        "name": "Partha Pratim Das"
                    },
                    {
                        "name": "Geeta Trilok-Kumar"
                    },
                    {
                        "name": "Ramesh Jain"
                    }
                ],
                "author_detail": {
                    "name": "Ramesh Jain"
                },
                "author": "Ramesh Jain",
                "arxiv_comment": "15 pages, 3 figures, 30 references, International Conference on\n  Pattern Recognition 2024 - Multimedia Assisted Dietary Management Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05243v1",
                "updated": "2024-12-06T18:22:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    22,
                    47,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T18:22:47Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    22,
                    47,
                    4,
                    341,
                    0
                ],
                "title": "CompCap: Improving Multimodal Large Language Models with Composite\n  Captions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompCap: Improving Multimodal Large Language Models with Composite\n  Captions"
                },
                "summary": "How well can Multimodal Large Language Models (MLLMs) understand composite\nimages? Composite images (CIs) are synthetic visuals created by merging\nmultiple visual elements, such as charts, posters, or screenshots, rather than\nbeing captured directly by a camera. While CIs are prevalent in real-world\napplications, recent MLLM developments have primarily focused on interpreting\nnatural images (NIs). Our research reveals that current MLLMs face significant\nchallenges in accurately understanding CIs, often struggling to extract\ninformation or perform complex reasoning based on these images. We find that\nexisting training data for CIs are mostly formatted for question-answer tasks\n(e.g., in datasets like ChartQA and ScienceQA), while high-quality\nimage-caption datasets, critical for robust vision-language alignment, are only\navailable for NIs. To bridge this gap, we introduce Composite Captions\n(CompCap), a flexible framework that leverages Large Language Models (LLMs) and\nautomation tools to synthesize CIs with accurate and detailed captions. Using\nCompCap, we curate CompCap-118K, a dataset containing 118K image-caption pairs\nacross six CI types. We validate the effectiveness of CompCap-118K by\nsupervised fine-tuning MLLMs of three sizes: xGen-MM-inst.-4B and\nLLaVA-NeXT-Vicuna-7B/13B. Empirical results show that CompCap-118K\nsignificantly enhances MLLMs' understanding of CIs, yielding average gains of\n1.7%, 2.0%, and 2.9% across eleven benchmarks, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How well can Multimodal Large Language Models (MLLMs) understand composite\nimages? Composite images (CIs) are synthetic visuals created by merging\nmultiple visual elements, such as charts, posters, or screenshots, rather than\nbeing captured directly by a camera. While CIs are prevalent in real-world\napplications, recent MLLM developments have primarily focused on interpreting\nnatural images (NIs). Our research reveals that current MLLMs face significant\nchallenges in accurately understanding CIs, often struggling to extract\ninformation or perform complex reasoning based on these images. We find that\nexisting training data for CIs are mostly formatted for question-answer tasks\n(e.g., in datasets like ChartQA and ScienceQA), while high-quality\nimage-caption datasets, critical for robust vision-language alignment, are only\navailable for NIs. To bridge this gap, we introduce Composite Captions\n(CompCap), a flexible framework that leverages Large Language Models (LLMs) and\nautomation tools to synthesize CIs with accurate and detailed captions. Using\nCompCap, we curate CompCap-118K, a dataset containing 118K image-caption pairs\nacross six CI types. We validate the effectiveness of CompCap-118K by\nsupervised fine-tuning MLLMs of three sizes: xGen-MM-inst.-4B and\nLLaVA-NeXT-Vicuna-7B/13B. Empirical results show that CompCap-118K\nsignificantly enhances MLLMs' understanding of CIs, yielding average gains of\n1.7%, 2.0%, and 2.9% across eleven benchmarks, respectively."
                },
                "authors": [
                    {
                        "name": "Xiaohui Chen"
                    },
                    {
                        "name": "Satya Narayan Shukla"
                    },
                    {
                        "name": "Mahmoud Azab"
                    },
                    {
                        "name": "Aashu Singh"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "David Yang"
                    },
                    {
                        "name": "ShengYun Peng"
                    },
                    {
                        "name": "Hanchao Yu"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Xuewen Zhang"
                    },
                    {
                        "name": "Baosheng He"
                    }
                ],
                "author_detail": {
                    "name": "Baosheng He"
                },
                "author": "Baosheng He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14471v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14471v2",
                "updated": "2024-12-06T18:22:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    22,
                    32,
                    4,
                    341,
                    0
                ],
                "published": "2024-08-26T17:59:01Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    59,
                    1,
                    0,
                    239,
                    0
                ],
                "title": "A Practitioner's Guide to Continual Multimodal Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Practitioner's Guide to Continual Multimodal Pretraining"
                },
                "summary": "Multimodal foundation models serve numerous applications at the intersection\nof vision and language. Still, despite being pretrained on extensive data, they\nbecome outdated over time. To keep models updated, research into continual\npretraining mainly explores scenarios with either (1) infrequent,\nindiscriminate updates on large-scale new data, or (2) frequent, sample-level\nupdates. However, practical model deployment often operates in the gap between\nthese two limit cases, as real-world applications often demand adaptation to\nspecific subdomains, tasks or concepts -- spread over the entire, varying life\ncycle of a model. In this work, we complement current perspectives on continual\npretraining through a research test bed as well as provide comprehensive\nguidance for effective continual model updates in such scenarios. We first\nintroduce FoMo-in-Flux, a continual multimodal pretraining benchmark with\nrealistic compute constraints and practical deployment requirements,\nconstructed over 63 datasets with diverse visual and semantic coverage. Using\nFoMo-in-Flux, we explore the complex landscape of practical continual\npretraining through multiple perspectives: (1) A data-centric investigation of\ndata mixtures and stream orderings that emulate real-world deployment\nsituations, (2) a method-centric investigation ranging from simple fine-tuning\nand traditional continual learning strategies to parameter-efficient updates\nand model merging, (3) meta learning rate schedules and mechanistic design\nchoices, and (4) the influence of model and compute scaling. Together, our\ninsights provide a practitioner's guide to continual multimodal pretraining for\nreal-world deployment. Our benchmark and code is here:\nhttps://github.com/ExplainableML/fomo_in_flux.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal foundation models serve numerous applications at the intersection\nof vision and language. Still, despite being pretrained on extensive data, they\nbecome outdated over time. To keep models updated, research into continual\npretraining mainly explores scenarios with either (1) infrequent,\nindiscriminate updates on large-scale new data, or (2) frequent, sample-level\nupdates. However, practical model deployment often operates in the gap between\nthese two limit cases, as real-world applications often demand adaptation to\nspecific subdomains, tasks or concepts -- spread over the entire, varying life\ncycle of a model. In this work, we complement current perspectives on continual\npretraining through a research test bed as well as provide comprehensive\nguidance for effective continual model updates in such scenarios. We first\nintroduce FoMo-in-Flux, a continual multimodal pretraining benchmark with\nrealistic compute constraints and practical deployment requirements,\nconstructed over 63 datasets with diverse visual and semantic coverage. Using\nFoMo-in-Flux, we explore the complex landscape of practical continual\npretraining through multiple perspectives: (1) A data-centric investigation of\ndata mixtures and stream orderings that emulate real-world deployment\nsituations, (2) a method-centric investigation ranging from simple fine-tuning\nand traditional continual learning strategies to parameter-efficient updates\nand model merging, (3) meta learning rate schedules and mechanistic design\nchoices, and (4) the influence of model and compute scaling. Together, our\ninsights provide a practitioner's guide to continual multimodal pretraining for\nreal-world deployment. Our benchmark and code is here:\nhttps://github.com/ExplainableML/fomo_in_flux."
                },
                "authors": [
                    {
                        "name": "Karsten Roth"
                    },
                    {
                        "name": "Vishaal Udandarao"
                    },
                    {
                        "name": "Sebastian Dziadzio"
                    },
                    {
                        "name": "Ameya Prabhu"
                    },
                    {
                        "name": "Mehdi Cherti"
                    },
                    {
                        "name": "Oriol Vinyals"
                    },
                    {
                        "name": "Olivier Hénaff"
                    },
                    {
                        "name": "Samuel Albanie"
                    },
                    {
                        "name": "Matthias Bethge"
                    },
                    {
                        "name": "Zeynep Akata"
                    }
                ],
                "author_detail": {
                    "name": "Zeynep Akata"
                },
                "author": "Zeynep Akata",
                "arxiv_comment": "Technical Report. 52 pages. Shorter version published at the NeurIPS\n  2024 Dataset & Benchmarks track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14471v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14471v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05232v1",
                "updated": "2024-12-06T18:02:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    2,
                    59,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T18:02:59Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    2,
                    59,
                    4,
                    341,
                    0
                ],
                "title": "LIAR: Leveraging Alignment (Best-of-N) to Jailbreak LLMs in Seconds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LIAR: Leveraging Alignment (Best-of-N) to Jailbreak LLMs in Seconds"
                },
                "summary": "Many existing jailbreak techniques rely on solving discrete combinatorial\noptimization, while more recent approaches involve training LLMs to generate\nmultiple adversarial prompts. However, both approaches require significant\ncomputational resources to produce even a single adversarial prompt. We\nhypothesize that the inefficiency of current approaches stems from an\ninadequate characterization of the jailbreak problem. To address this gap, we\nformulate the jailbreak problem in terms of alignment. By starting from an\navailable safety-aligned model, we leverage an unsafe reward to guide the safe\nmodel towards generating unsafe outputs using alignment techniques (e.g.,\nreinforcement learning from human feedback), effectively performing\njailbreaking via alignment. We propose a novel jailbreak method called LIAR\n(LeveragIng Alignment to jailbReak). To demonstrate the simplicity and\neffectiveness of our approach, we employ a best-of-N method to solve the\nalignment problem. LIAR offers significant advantages: lower computational\nrequirements without additional training, fully black-box operation,\ncompetitive attack success rates, and more human-readable prompts. We provide\ntheoretical insights into the possibility of jailbreaking a safety-aligned\nmodel, revealing inherent vulnerabilities in current alignment strategies for\nLLMs. We also provide sub-optimality guarantees for the proposed \\algo.\nExperimentally, we achieve ASR comparable to the SoTA with a 10x improvement to\nperplexity and a Time-to-Attack measured in seconds rather than tens of hours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many existing jailbreak techniques rely on solving discrete combinatorial\noptimization, while more recent approaches involve training LLMs to generate\nmultiple adversarial prompts. However, both approaches require significant\ncomputational resources to produce even a single adversarial prompt. We\nhypothesize that the inefficiency of current approaches stems from an\ninadequate characterization of the jailbreak problem. To address this gap, we\nformulate the jailbreak problem in terms of alignment. By starting from an\navailable safety-aligned model, we leverage an unsafe reward to guide the safe\nmodel towards generating unsafe outputs using alignment techniques (e.g.,\nreinforcement learning from human feedback), effectively performing\njailbreaking via alignment. We propose a novel jailbreak method called LIAR\n(LeveragIng Alignment to jailbReak). To demonstrate the simplicity and\neffectiveness of our approach, we employ a best-of-N method to solve the\nalignment problem. LIAR offers significant advantages: lower computational\nrequirements without additional training, fully black-box operation,\ncompetitive attack success rates, and more human-readable prompts. We provide\ntheoretical insights into the possibility of jailbreaking a safety-aligned\nmodel, revealing inherent vulnerabilities in current alignment strategies for\nLLMs. We also provide sub-optimality guarantees for the proposed \\algo.\nExperimentally, we achieve ASR comparable to the SoTA with a 10x improvement to\nperplexity and a Time-to-Attack measured in seconds rather than tens of hours."
                },
                "authors": [
                    {
                        "name": "James Beetham"
                    },
                    {
                        "name": "Souradip Chakraborty"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Amrit Singh Bedi"
                    },
                    {
                        "name": "Mubarak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Mubarak Shah"
                },
                "author": "Mubarak Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05225v1",
                "updated": "2024-12-06T17:58:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    14,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:58:14Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    14,
                    4,
                    341,
                    0
                ],
                "title": "BEExformer: A Fast Inferencing Transformer Architecture via Binarization\n  with Multiple Early Exits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEExformer: A Fast Inferencing Transformer Architecture via Binarization\n  with Multiple Early Exits"
                },
                "summary": "Large Language Models (LLMs) based on transformers achieve cutting-edge\nresults on a variety of applications. However, their enormous size and\nprocessing requirements make deployment on devices with constrained resources\nextremely difficult. Among various efficiency considerations, model\nbinarization and Early Exit (EE) are common effective solutions. However,\nbinarization may lead to performance loss due to reduced precision affecting\ngradient estimation and parameter updates. Besides, the present early-exit\nmechanisms are still in the nascent stages of research. To ameliorate these\nissues, we propose Binarized Early Exit Transformer (BEExformer), the\nfirst-ever selective learning transformer architecture to combine early exit\nwith binarization for textual inference. It improves the binarization process\nthrough a differentiable second-order approximation to the impulse function.\nThis enables gradient computation concerning both the sign as well as the\nmagnitude of the weights. In contrast to absolute threshold-based EE, the\nproposed EE mechanism hinges on fractional reduction in entropy among\nintermediate transformer blocks with soft-routing loss estimation. While\nbinarization results in 18.44 times reduction in model size, early exit reduces\nthe FLOPs during inference by 54.85% and even improves accuracy by 5.98%\nthrough resolving the \"overthinking\" problem inherent in deep networks.\nMoreover, the proposed BEExformer simplifies training by not requiring\nknowledge distillation from a full-precision LLM. Extensive evaluation on the\nGLUE dataset and comparison with the SOTA works showcase its pareto-optimal\nperformance-efficiency trade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) based on transformers achieve cutting-edge\nresults on a variety of applications. However, their enormous size and\nprocessing requirements make deployment on devices with constrained resources\nextremely difficult. Among various efficiency considerations, model\nbinarization and Early Exit (EE) are common effective solutions. However,\nbinarization may lead to performance loss due to reduced precision affecting\ngradient estimation and parameter updates. Besides, the present early-exit\nmechanisms are still in the nascent stages of research. To ameliorate these\nissues, we propose Binarized Early Exit Transformer (BEExformer), the\nfirst-ever selective learning transformer architecture to combine early exit\nwith binarization for textual inference. It improves the binarization process\nthrough a differentiable second-order approximation to the impulse function.\nThis enables gradient computation concerning both the sign as well as the\nmagnitude of the weights. In contrast to absolute threshold-based EE, the\nproposed EE mechanism hinges on fractional reduction in entropy among\nintermediate transformer blocks with soft-routing loss estimation. While\nbinarization results in 18.44 times reduction in model size, early exit reduces\nthe FLOPs during inference by 54.85% and even improves accuracy by 5.98%\nthrough resolving the \"overthinking\" problem inherent in deep networks.\nMoreover, the proposed BEExformer simplifies training by not requiring\nknowledge distillation from a full-precision LLM. Extensive evaluation on the\nGLUE dataset and comparison with the SOTA works showcase its pareto-optimal\nperformance-efficiency trade-off."
                },
                "authors": [
                    {
                        "name": "Wazib Ansar"
                    },
                    {
                        "name": "Saptarsi Goswami"
                    },
                    {
                        "name": "Amlan Chakrabarti"
                    }
                ],
                "author_detail": {
                    "name": "Amlan Chakrabarti"
                },
                "author": "Amlan Chakrabarti",
                "arxiv_comment": "15 pages, 15 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05223v1",
                "updated": "2024-12-06T17:54:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    54,
                    54,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:54:54Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    54,
                    54,
                    4,
                    341,
                    0
                ],
                "title": "100% Hallucination Elimination Using Acurai",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "100% Hallucination Elimination Using Acurai"
                },
                "summary": "The issue of hallucinations in large language models (LLMs) remains a\ncritical barrier to the adoption of AI in enterprise and other high-stakes\napplications. Despite advancements in retrieval-augmented generation (RAG)\nsystems, current state-of-the-art methods fail to achieve more than 80%\naccuracy in generating faithful and factually correct outputs, even when\nprovided with relevant and accurate context. In this work, we introduce Acurai,\na novel systematic approach that achieves 100% hallucination-free responses in\nLLMs by reformatting queries and context data prior to input. Leveraging a deep\nunderstanding of LLM internal representations, the importance of noun-phrase\ndominance, and the role of discrete functional units (DFUs), Acurai ensures\nalignment between input context and generated output. We validate this method\nusing the RAGTruth corpus, demonstrating its ability to eliminate 100%\nhallucinations for both GPT-4 and GPT-3.5 Turbo. Acurai sets a new standard for\nachieving consistent, accurate, and faithful AI responses, marking a\nsignificant step forward in the development of trustworthy AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The issue of hallucinations in large language models (LLMs) remains a\ncritical barrier to the adoption of AI in enterprise and other high-stakes\napplications. Despite advancements in retrieval-augmented generation (RAG)\nsystems, current state-of-the-art methods fail to achieve more than 80%\naccuracy in generating faithful and factually correct outputs, even when\nprovided with relevant and accurate context. In this work, we introduce Acurai,\na novel systematic approach that achieves 100% hallucination-free responses in\nLLMs by reformatting queries and context data prior to input. Leveraging a deep\nunderstanding of LLM internal representations, the importance of noun-phrase\ndominance, and the role of discrete functional units (DFUs), Acurai ensures\nalignment between input context and generated output. We validate this method\nusing the RAGTruth corpus, demonstrating its ability to eliminate 100%\nhallucinations for both GPT-4 and GPT-3.5 Turbo. Acurai sets a new standard for\nachieving consistent, accurate, and faithful AI responses, marking a\nsignificant step forward in the development of trustworthy AI systems."
                },
                "authors": [
                    {
                        "name": "Michael C. Wood"
                    },
                    {
                        "name": "Adam A. Forbes"
                    }
                ],
                "author_detail": {
                    "name": "Adam A. Forbes"
                },
                "author": "Adam A. Forbes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05210v1",
                "updated": "2024-12-06T17:40:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    40,
                    38,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:40:38Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    40,
                    38,
                    4,
                    341,
                    0
                ],
                "title": "Evaluating and Aligning CodeLLMs on Human Preference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating and Aligning CodeLLMs on Human Preference"
                },
                "summary": "Code large language models (codeLLMs) have made significant strides in code\ngeneration. Most previous code-related benchmarks, which consist of various\nprogramming exercises along with the corresponding test cases, are used as a\ncommon measure to evaluate the performance and capabilities of code LLMs.\nHowever, the current code LLMs focus on synthesizing the correct code snippet,\nignoring the alignment with human preferences, where the query should be\nsampled from the practical application scenarios and the model-generated\nresponses should satisfy the human preference. To bridge the gap between the\nmodel-generated response and human preference, we present a rigorous\nhuman-curated benchmark CodeArena to emulate the complexity and diversity of\nreal-world coding tasks, where 397 high-quality samples spanning 40 categories\nand 44 programming languages, carefully curated from user queries. Further, we\npropose a diverse synthetic instruction corpus SynCode-Instruct (nearly 20B\ntokens) by scaling instructions from the website to verify the effectiveness of\nthe large-scale synthetic instruction fine-tuning, where Qwen2.5-SynCoder\ntotally trained on synthetic instruction data can achieve top-tier performance\nof open-source code LLMs. The results find performance differences between\nexecution-based benchmarks and CodeArena. Our systematic experiments of\nCodeArena on 40+ LLMs reveal a notable performance gap between open SOTA code\nLLMs (e.g. Qwen2.5-Coder) and proprietary LLMs (e.g., OpenAI o1), underscoring\nthe importance of the human preference\nalignment.\\footnote{\\url{https://codearenaeval.github.io/ }}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code large language models (codeLLMs) have made significant strides in code\ngeneration. Most previous code-related benchmarks, which consist of various\nprogramming exercises along with the corresponding test cases, are used as a\ncommon measure to evaluate the performance and capabilities of code LLMs.\nHowever, the current code LLMs focus on synthesizing the correct code snippet,\nignoring the alignment with human preferences, where the query should be\nsampled from the practical application scenarios and the model-generated\nresponses should satisfy the human preference. To bridge the gap between the\nmodel-generated response and human preference, we present a rigorous\nhuman-curated benchmark CodeArena to emulate the complexity and diversity of\nreal-world coding tasks, where 397 high-quality samples spanning 40 categories\nand 44 programming languages, carefully curated from user queries. Further, we\npropose a diverse synthetic instruction corpus SynCode-Instruct (nearly 20B\ntokens) by scaling instructions from the website to verify the effectiveness of\nthe large-scale synthetic instruction fine-tuning, where Qwen2.5-SynCoder\ntotally trained on synthetic instruction data can achieve top-tier performance\nof open-source code LLMs. The results find performance differences between\nexecution-based benchmarks and CodeArena. Our systematic experiments of\nCodeArena on 40+ LLMs reveal a notable performance gap between open SOTA code\nLLMs (e.g. Qwen2.5-Coder) and proprietary LLMs (e.g., OpenAI o1), underscoring\nthe importance of the human preference\nalignment.\\footnote{\\url{https://codearenaeval.github.io/ }}"
                },
                "authors": [
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Jiaxi Yang"
                    },
                    {
                        "name": "Ke Jin"
                    },
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Liqun Yang"
                    },
                    {
                        "name": "Zeyu Cui"
                    },
                    {
                        "name": "Yichang Zhang"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05208v1",
                "updated": "2024-12-06T17:36:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    36,
                    28,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:36:28Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    36,
                    28,
                    4,
                    341,
                    0
                ],
                "title": "A Survey of Large Language Model-Based Generative AI for Text-to-SQL:\n  Benchmarks, Applications, Use Cases, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Large Language Model-Based Generative AI for Text-to-SQL:\n  Benchmarks, Applications, Use Cases, and Challenges"
                },
                "summary": "Text-to-SQL systems facilitate smooth interaction with databases by\ntranslating natural language queries into Structured Query Language (SQL),\nbridging the gap between non-technical users and complex database management\nsystems. This survey provides a comprehensive overview of the evolution of\nAI-driven text-to-SQL systems, highlighting their foundational components,\nadvancements in large language model (LLM) architectures, and the critical role\nof datasets such as Spider, WikiSQL, and CoSQL in driving progress. We examine\nthe applications of text-to-SQL in domains like healthcare, education, and\nfinance, emphasizing their transformative potential for improving data\naccessibility. Additionally, we analyze persistent challenges, including domain\ngeneralization, query optimization, support for multi-turn conversational\ninteractions, and the limited availability of datasets tailored for NoSQL\ndatabases and dynamic real-world scenarios. To address these challenges, we\noutline future research directions, such as extending text-to-SQL capabilities\nto support NoSQL databases, designing datasets for dynamic multi-turn\ninteractions, and optimizing systems for real-world scalability and robustness.\nBy surveying current advancements and identifying key gaps, this paper aims to\nguide the next generation of research and applications in LLM-based text-to-SQL\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL systems facilitate smooth interaction with databases by\ntranslating natural language queries into Structured Query Language (SQL),\nbridging the gap between non-technical users and complex database management\nsystems. This survey provides a comprehensive overview of the evolution of\nAI-driven text-to-SQL systems, highlighting their foundational components,\nadvancements in large language model (LLM) architectures, and the critical role\nof datasets such as Spider, WikiSQL, and CoSQL in driving progress. We examine\nthe applications of text-to-SQL in domains like healthcare, education, and\nfinance, emphasizing their transformative potential for improving data\naccessibility. Additionally, we analyze persistent challenges, including domain\ngeneralization, query optimization, support for multi-turn conversational\ninteractions, and the limited availability of datasets tailored for NoSQL\ndatabases and dynamic real-world scenarios. To address these challenges, we\noutline future research directions, such as extending text-to-SQL capabilities\nto support NoSQL databases, designing datasets for dynamic multi-turn\ninteractions, and optimizing systems for real-world scalability and robustness.\nBy surveying current advancements and identifying key gaps, this paper aims to\nguide the next generation of research and applications in LLM-based text-to-SQL\nsystems."
                },
                "authors": [
                    {
                        "name": "Aditi Singh"
                    },
                    {
                        "name": "Akash Shetty"
                    },
                    {
                        "name": "Abul Ehtesham"
                    },
                    {
                        "name": "Saket Kumar"
                    },
                    {
                        "name": "Tala Talaei Khoei"
                    }
                ],
                "author_detail": {
                    "name": "Tala Talaei Khoei"
                },
                "author": "Tala Talaei Khoei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05206v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05206v1",
                "updated": "2024-12-06T17:35:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    35,
                    52,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:35:52Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    35,
                    52,
                    4,
                    341,
                    0
                ],
                "title": "ConQRet: Benchmarking Fine-Grained Evaluation of Retrieval Augmented\n  Argumentation with LLM Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConQRet: Benchmarking Fine-Grained Evaluation of Retrieval Augmented\n  Argumentation with LLM Judges"
                },
                "summary": "Computational argumentation, which involves generating answers or summaries\nfor controversial topics like abortion bans and vaccination, has become\nincreasingly important in today's polarized environment. Sophisticated LLM\ncapabilities offer the potential to provide nuanced, evidence-based answers to\nsuch questions through Retrieval-Augmented Argumentation (RAArg), leveraging\nreal-world evidence for high-quality, grounded arguments. However, evaluating\nRAArg remains challenging, as human evaluation is costly and difficult for\ncomplex, lengthy answers on complicated topics. At the same time, re-using\nexisting argumentation datasets is no longer sufficient, as they lack long,\ncomplex arguments and realistic evidence from potentially misleading sources,\nlimiting holistic evaluation of retrieval effectiveness and argument quality.\nTo address these gaps, we investigate automated evaluation methods using\nmultiple fine-grained LLM judges, providing better and more interpretable\nassessments than traditional single-score metrics and even previously reported\nhuman crowdsourcing. To validate the proposed techniques, we introduce ConQRet,\na new benchmark featuring long and complex human-authored arguments on debated\ntopics, grounded in real-world websites, allowing an exhaustive evaluation\nacross retrieval effectiveness, argument quality, and groundedness. We validate\nour LLM Judges on a prior dataset and the new ConQRet benchmark. Our proposed\nLLM Judges and the ConQRet benchmark can enable rapid progress in computational\nargumentation and can be naturally extended to other complex\nretrieval-augmented generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational argumentation, which involves generating answers or summaries\nfor controversial topics like abortion bans and vaccination, has become\nincreasingly important in today's polarized environment. Sophisticated LLM\ncapabilities offer the potential to provide nuanced, evidence-based answers to\nsuch questions through Retrieval-Augmented Argumentation (RAArg), leveraging\nreal-world evidence for high-quality, grounded arguments. However, evaluating\nRAArg remains challenging, as human evaluation is costly and difficult for\ncomplex, lengthy answers on complicated topics. At the same time, re-using\nexisting argumentation datasets is no longer sufficient, as they lack long,\ncomplex arguments and realistic evidence from potentially misleading sources,\nlimiting holistic evaluation of retrieval effectiveness and argument quality.\nTo address these gaps, we investigate automated evaluation methods using\nmultiple fine-grained LLM judges, providing better and more interpretable\nassessments than traditional single-score metrics and even previously reported\nhuman crowdsourcing. To validate the proposed techniques, we introduce ConQRet,\na new benchmark featuring long and complex human-authored arguments on debated\ntopics, grounded in real-world websites, allowing an exhaustive evaluation\nacross retrieval effectiveness, argument quality, and groundedness. We validate\nour LLM Judges on a prior dataset and the new ConQRet benchmark. Our proposed\nLLM Judges and the ConQRet benchmark can enable rapid progress in computational\nargumentation and can be naturally extended to other complex\nretrieval-augmented generation tasks."
                },
                "authors": [
                    {
                        "name": "Kaustubh D. Dhole"
                    },
                    {
                        "name": "Kai Shu"
                    },
                    {
                        "name": "Eugene Agichtein"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Agichtein"
                },
                "author": "Eugene Agichtein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05206v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05200v1",
                "updated": "2024-12-06T17:28:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    28,
                    43,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:28:43Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    28,
                    43,
                    4,
                    341,
                    0
                ],
                "title": "Are Frontier Large Language Models Suitable for Q&A in Science Centres?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Frontier Large Language Models Suitable for Q&A in Science Centres?"
                },
                "summary": "This paper investigates the suitability of frontier Large Language Models\n(LLMs) for Q&A interactions in science centres, with the aim of boosting\nvisitor engagement while maintaining factual accuracy. Using a dataset of\nquestions collected from the National Space Centre in Leicester (UK), we\nevaluated responses generated by three leading models: OpenAI's GPT-4, Claude\n3.5 Sonnet, and Google Gemini 1.5. Each model was prompted for both standard\nand creative responses tailored to an 8-year-old audience, and these responses\nwere assessed by space science experts based on accuracy, engagement, clarity,\nnovelty, and deviation from expected answers. The results revealed a trade-off\nbetween creativity and accuracy, with Claude outperforming GPT and Gemini in\nboth maintaining clarity and engaging young audiences, even when asked to\ngenerate more creative responses. Nonetheless, experts observed that higher\nnovelty was generally associated with reduced factual reliability across all\nmodels. This study highlights the potential of LLMs in educational settings,\nemphasizing the need for careful prompt engineering to balance engagement with\nscientific rigor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the suitability of frontier Large Language Models\n(LLMs) for Q&A interactions in science centres, with the aim of boosting\nvisitor engagement while maintaining factual accuracy. Using a dataset of\nquestions collected from the National Space Centre in Leicester (UK), we\nevaluated responses generated by three leading models: OpenAI's GPT-4, Claude\n3.5 Sonnet, and Google Gemini 1.5. Each model was prompted for both standard\nand creative responses tailored to an 8-year-old audience, and these responses\nwere assessed by space science experts based on accuracy, engagement, clarity,\nnovelty, and deviation from expected answers. The results revealed a trade-off\nbetween creativity and accuracy, with Claude outperforming GPT and Gemini in\nboth maintaining clarity and engaging young audiences, even when asked to\ngenerate more creative responses. Nonetheless, experts observed that higher\nnovelty was generally associated with reduced factual reliability across all\nmodels. This study highlights the potential of LLMs in educational settings,\nemphasizing the need for careful prompt engineering to balance engagement with\nscientific rigor."
                },
                "authors": [
                    {
                        "name": "Jacob Watson"
                    },
                    {
                        "name": "Fabrício Góes"
                    },
                    {
                        "name": "Marco Volpe"
                    },
                    {
                        "name": "Talles Medeiros"
                    }
                ],
                "author_detail": {
                    "name": "Talles Medeiros"
                },
                "author": "Talles Medeiros",
                "arxiv_comment": "19 pages, 2 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03019v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03019v2",
                "updated": "2024-12-06T17:23:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    23,
                    53,
                    4,
                    341,
                    0
                ],
                "published": "2024-10-03T22:05:06Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    22,
                    5,
                    6,
                    3,
                    277,
                    0
                ],
                "title": "Is Your Paper Being Reviewed by an LLM? Investigating AI Text\n  Detectability in Peer Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Your Paper Being Reviewed by an LLM? Investigating AI Text\n  Detectability in Peer Review"
                },
                "summary": "Peer review is a critical process for ensuring the integrity of published\nscientific research. Confidence in this process is predicated on the assumption\nthat experts in the relevant domain give careful consideration to the merits of\nmanuscripts which are submitted for publication. With the recent rapid\nadvancements in the linguistic capabilities of large language models (LLMs), a\nnew potential risk to the peer review process is that negligent reviewers will\nrely on LLMs to perform the often time consuming process of reviewing a paper.\nIn this study, we investigate the ability of existing AI text detection\nalgorithms to distinguish between peer reviews written by humans and different\nstate-of-the-art LLMs. Our analysis shows that existing approaches fail to\nidentify many GPT-4o written reviews without also producing a high number of\nfalse positive classifications. To address this deficiency, we propose a new\ndetection approach which surpasses existing methods in the identification of\nGPT-4o written peer reviews at low levels of false positive classifications.\nOur work reveals the difficulty of accurately identifying AI-generated text at\nthe individual review level, highlighting the urgent need for new tools and\nmethods to detect this type of unethical application of generative AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peer review is a critical process for ensuring the integrity of published\nscientific research. Confidence in this process is predicated on the assumption\nthat experts in the relevant domain give careful consideration to the merits of\nmanuscripts which are submitted for publication. With the recent rapid\nadvancements in the linguistic capabilities of large language models (LLMs), a\nnew potential risk to the peer review process is that negligent reviewers will\nrely on LLMs to perform the often time consuming process of reviewing a paper.\nIn this study, we investigate the ability of existing AI text detection\nalgorithms to distinguish between peer reviews written by humans and different\nstate-of-the-art LLMs. Our analysis shows that existing approaches fail to\nidentify many GPT-4o written reviews without also producing a high number of\nfalse positive classifications. To address this deficiency, we propose a new\ndetection approach which surpasses existing methods in the identification of\nGPT-4o written peer reviews at low levels of false positive classifications.\nOur work reveals the difficulty of accurately identifying AI-generated text at\nthe individual review level, highlighting the urgent need for new tools and\nmethods to detect this type of unethical application of generative AI."
                },
                "authors": [
                    {
                        "name": "Sungduk Yu"
                    },
                    {
                        "name": "Man Luo"
                    },
                    {
                        "name": "Avinash Madasu"
                    },
                    {
                        "name": "Vasudev Lal"
                    },
                    {
                        "name": "Phillip Howard"
                    }
                ],
                "author_detail": {
                    "name": "Phillip Howard"
                },
                "author": "Phillip Howard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03019v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03019v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05187v1",
                "updated": "2024-12-06T17:07:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    7,
                    27,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:07:27Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    7,
                    27,
                    4,
                    341,
                    0
                ],
                "title": "SurgBox: Agent-Driven Operating Room Sandbox with Surgery Copilot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SurgBox: Agent-Driven Operating Room Sandbox with Surgery Copilot"
                },
                "summary": "Surgical interventions, particularly in neurology, represent complex and\nhigh-stakes scenarios that impose substantial cognitive burdens on surgical\nteams. Although deliberate education and practice can enhance cognitive\ncapabilities, surgical training opportunities remain limited due to patient\nsafety concerns. To address these cognitive challenges in surgical training and\noperation, we propose SurgBox, an agent-driven sandbox framework to\nsystematically enhance the cognitive capabilities of surgeons in immersive\nsurgical simulations. Specifically, our SurgBox leverages large language models\n(LLMs) with tailored Retrieval-Augmented Generation (RAG) to authentically\nreplicate various surgical roles, enabling realistic training environments for\ndeliberate practice. In particular, we devise Surgery Copilot, an AI-driven\nassistant to actively coordinate the surgical information stream and support\nclinical decision-making, thereby diminishing the cognitive workload of\nsurgical teams during surgery. By incorporating a novel Long-Short Memory\nmechanism, our Surgery Copilot can effectively balance immediate procedural\nassistance with comprehensive surgical knowledge. Extensive experiments using\nreal neurosurgical procedure records validate our SurgBox framework in both\nenhancing surgical cognitive capabilities and supporting clinical\ndecision-making. By providing an integrated solution for training and\noperational support to address cognitive challenges, our SurgBox framework\nadvances surgical education and practice, potentially transforming surgical\noutcomes and healthcare quality. The code is available at\nhttps://github.com/franciszchen/SurgBox.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical interventions, particularly in neurology, represent complex and\nhigh-stakes scenarios that impose substantial cognitive burdens on surgical\nteams. Although deliberate education and practice can enhance cognitive\ncapabilities, surgical training opportunities remain limited due to patient\nsafety concerns. To address these cognitive challenges in surgical training and\noperation, we propose SurgBox, an agent-driven sandbox framework to\nsystematically enhance the cognitive capabilities of surgeons in immersive\nsurgical simulations. Specifically, our SurgBox leverages large language models\n(LLMs) with tailored Retrieval-Augmented Generation (RAG) to authentically\nreplicate various surgical roles, enabling realistic training environments for\ndeliberate practice. In particular, we devise Surgery Copilot, an AI-driven\nassistant to actively coordinate the surgical information stream and support\nclinical decision-making, thereby diminishing the cognitive workload of\nsurgical teams during surgery. By incorporating a novel Long-Short Memory\nmechanism, our Surgery Copilot can effectively balance immediate procedural\nassistance with comprehensive surgical knowledge. Extensive experiments using\nreal neurosurgical procedure records validate our SurgBox framework in both\nenhancing surgical cognitive capabilities and supporting clinical\ndecision-making. By providing an integrated solution for training and\noperational support to address cognitive challenges, our SurgBox framework\nadvances surgical education and practice, potentially transforming surgical\noutcomes and healthcare quality. The code is available at\nhttps://github.com/franciszchen/SurgBox."
                },
                "authors": [
                    {
                        "name": "Jinlin Wu"
                    },
                    {
                        "name": "Xusheng Liang"
                    },
                    {
                        "name": "Xuexue Bai"
                    },
                    {
                        "name": "Zhen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Chen"
                },
                "author": "Zhen Chen",
                "arxiv_comment": "This work is accepted by IEEE Big Data 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05185v1",
                "updated": "2024-12-06T17:04:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    4,
                    42,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:04:42Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    4,
                    42,
                    4,
                    341,
                    0
                ],
                "title": "LinVT: Empower Your Image-level Large Language Model to Understand\n  Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LinVT: Empower Your Image-level Large Language Model to Understand\n  Videos"
                },
                "summary": "Large Language Models (LLMs) have been widely used in various tasks,\nmotivating us to develop an LLM-based assistant for videos. Instead of training\nfrom scratch, we propose a module to transform arbitrary well-trained\nimage-based LLMs into video-LLMs (after being trained on video data). To better\nadapt image-LLMs for processing videos, we introduce two design principles:\nlinear transformation to preserve the original visual-language alignment and\nrepresentative information condensation from redundant video content. Guided by\nthese principles, we propose a plug-and-play Linear Video Tokenizer(LinVT),\nwhich enables existing image-LLMs to understand videos. We benchmark LinVT with\nsix recent visual LLMs: Aquila, Blip-3, InternVL2, Mipha, Molmo and Qwen2-VL,\nshowcasing the high compatibility of LinVT. LinVT-based LLMs achieve\nstate-of-the-art performance across various video benchmarks, illustrating the\neffectiveness of LinVT in multi-modal video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely used in various tasks,\nmotivating us to develop an LLM-based assistant for videos. Instead of training\nfrom scratch, we propose a module to transform arbitrary well-trained\nimage-based LLMs into video-LLMs (after being trained on video data). To better\nadapt image-LLMs for processing videos, we introduce two design principles:\nlinear transformation to preserve the original visual-language alignment and\nrepresentative information condensation from redundant video content. Guided by\nthese principles, we propose a plug-and-play Linear Video Tokenizer(LinVT),\nwhich enables existing image-LLMs to understand videos. We benchmark LinVT with\nsix recent visual LLMs: Aquila, Blip-3, InternVL2, Mipha, Molmo and Qwen2-VL,\nshowcasing the high compatibility of LinVT. LinVT-based LLMs achieve\nstate-of-the-art performance across various video benchmarks, illustrating the\neffectiveness of LinVT in multi-modal video understanding."
                },
                "authors": [
                    {
                        "name": "Lishuai Gao"
                    },
                    {
                        "name": "Yujie Zhong"
                    },
                    {
                        "name": "Yingsen Zeng"
                    },
                    {
                        "name": "Haoxian Tan"
                    },
                    {
                        "name": "Dengjie Li"
                    },
                    {
                        "name": "Zheng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Zhao"
                },
                "author": "Zheng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05184v1",
                "updated": "2024-12-06T17:04:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    4,
                    21,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:04:21Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    4,
                    21,
                    4,
                    341,
                    0
                ],
                "title": "QueEn: A Large Language Model for Quechua-English Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QueEn: A Large Language Model for Quechua-English Translation"
                },
                "summary": "Recent studies show that large language models (LLMs) are powerful tools for\nworking with natural language, bringing advances in many areas of computational\nlinguistics. However, these models face challenges when applied to low-resource\nlanguages due to limited training data and difficulty in understanding cultural\nnuances. In this paper, we propose QueEn, a novel approach for Quechua-English\ntranslation that combines Retrieval-Augmented Generation (RAG) with\nparameter-efficient fine-tuning techniques. Our method leverages external\nlinguistic resources through RAG and uses Low-Rank Adaptation (LoRA) for\nefficient model adaptation. Experimental results show that our approach\nsubstantially exceeds baseline models, with a BLEU score of 17.6 compared to\n1.5 for standard GPT models. The integration of RAG with fine-tuning allows our\nsystem to address the challenges of low-resource language translation while\nmaintaining computational efficiency. This work contributes to the broader goal\nof preserving endangered languages through advanced language technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies show that large language models (LLMs) are powerful tools for\nworking with natural language, bringing advances in many areas of computational\nlinguistics. However, these models face challenges when applied to low-resource\nlanguages due to limited training data and difficulty in understanding cultural\nnuances. In this paper, we propose QueEn, a novel approach for Quechua-English\ntranslation that combines Retrieval-Augmented Generation (RAG) with\nparameter-efficient fine-tuning techniques. Our method leverages external\nlinguistic resources through RAG and uses Low-Rank Adaptation (LoRA) for\nefficient model adaptation. Experimental results show that our approach\nsubstantially exceeds baseline models, with a BLEU score of 17.6 compared to\n1.5 for standard GPT models. The integration of RAG with fine-tuning allows our\nsystem to address the challenges of low-resource language translation while\nmaintaining computational efficiency. This work contributes to the broader goal\nof preserving endangered languages through advanced language technologies."
                },
                "authors": [
                    {
                        "name": "Junhao Chen"
                    },
                    {
                        "name": "Peng Shu"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Huaqin Zhao"
                    },
                    {
                        "name": "Hanqi Jiang"
                    },
                    {
                        "name": "Yi Pan"
                    },
                    {
                        "name": "Yifan Zhou"
                    },
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Lewis C Howe"
                    },
                    {
                        "name": "Tianming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tianming Liu"
                },
                "author": "Tianming Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06467v2",
                "updated": "2024-12-06T16:50:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    16,
                    50,
                    13,
                    4,
                    341,
                    0
                ],
                "published": "2024-10-09T01:41:14Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    1,
                    41,
                    14,
                    2,
                    283,
                    0
                ],
                "title": "WAPITI: A Watermark for Finetuned Open-Source LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WAPITI: A Watermark for Finetuned Open-Source LLMs"
                },
                "summary": "Watermarking of large language models (LLMs) generation embeds an\nimperceptible statistical pattern within texts, making it algorithmically\ndetectable. Watermarking is a promising method for addressing potential harm\nand biases from LLMs, as it enables traceability, accountability, and detection\nof manipulated content, helping to mitigate unintended consequences. However,\nfor open-source models, watermarking faces two major challenges: (i)\nincompatibility with fine-tuned models, and (ii) vulnerability to fine-tuning\nattacks. In this work, we propose WAPITI, a new method that transfers\nwatermarking from base models to fine-tuned models through parameter\nintegration. To the best of our knowledge, we propose the first watermark for\nfine-tuned open-source LLMs that preserves their fine-tuned capabilities.\nFurthermore, our approach offers an effective defense against fine-tuning\nattacks. We test our method on various model architectures and watermarking\nstrategies. Results demonstrate that our method can successfully inject\nwatermarks and is highly compatible with fine-tuned models. Additionally, we\noffer an in-depth analysis of how parameter editing influences the watermark\nstrength and overall capabilities of the resulting models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking of large language models (LLMs) generation embeds an\nimperceptible statistical pattern within texts, making it algorithmically\ndetectable. Watermarking is a promising method for addressing potential harm\nand biases from LLMs, as it enables traceability, accountability, and detection\nof manipulated content, helping to mitigate unintended consequences. However,\nfor open-source models, watermarking faces two major challenges: (i)\nincompatibility with fine-tuned models, and (ii) vulnerability to fine-tuning\nattacks. In this work, we propose WAPITI, a new method that transfers\nwatermarking from base models to fine-tuned models through parameter\nintegration. To the best of our knowledge, we propose the first watermark for\nfine-tuned open-source LLMs that preserves their fine-tuned capabilities.\nFurthermore, our approach offers an effective defense against fine-tuning\nattacks. We test our method on various model architectures and watermarking\nstrategies. Results demonstrate that our method can successfully inject\nwatermarks and is highly compatible with fine-tuned models. Additionally, we\noffer an in-depth analysis of how parameter editing influences the watermark\nstrength and overall capabilities of the resulting models."
                },
                "authors": [
                    {
                        "name": "Lingjie Chen"
                    },
                    {
                        "name": "Ruizhong Qiu"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Zhining Liu"
                    },
                    {
                        "name": "Tianxin Wei"
                    },
                    {
                        "name": "Hyunsik Yoo"
                    },
                    {
                        "name": "Zhichen Zeng"
                    },
                    {
                        "name": "Deqing Yang"
                    },
                    {
                        "name": "Hanghang Tong"
                    }
                ],
                "author_detail": {
                    "name": "Hanghang Tong"
                },
                "author": "Hanghang Tong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02263v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02263v2",
                "updated": "2024-12-06T16:43:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    16,
                    43,
                    58,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-03T08:35:51Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    35,
                    51,
                    1,
                    338,
                    0
                ],
                "title": "Connecting Large Language Models with Blockchain: Advancing the\n  Evolution of Smart Contracts from Automation to Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Connecting Large Language Models with Blockchain: Advancing the\n  Evolution of Smart Contracts from Automation to Intelligence"
                },
                "summary": "Blockchain smart contracts have catalyzed the development of decentralized\napplications across various domains, including decentralized finance. However,\ndue to constraints in computational resources and the prevalence of data silos,\ncurrent smart contracts face significant challenges in fully leveraging the\npowerful capabilities of Large Language Models (LLMs) for tasks such as\nintelligent analysis and reasoning. To address this gap, this paper proposes\nand implements a universal framework for integrating LLMs with blockchain data,\n{\\sysname}, effectively overcoming the interoperability barriers between\nblockchain and LLMs. By combining semantic relatedness with truth discovery\nmethods, we introduce an innovative data aggregation approach, {\\funcname},\nwhich significantly enhances the accuracy and trustworthiness of data generated\nby LLMs. To validate the framework's effectiveness, we construct a dataset\nconsisting of three types of questions, capturing Q\\&A interactions between 10\noracle nodes and 5 LLM models. Experimental results demonstrate that, even with\n40\\% malicious nodes, the proposed solution improves data accuracy by an\naverage of 17.74\\% compared to the optimal baseline. This research not only\nprovides an innovative solution for the intelligent enhancement of smart\ncontracts but also highlights the potential for deep integration between LLMs\nand blockchain technology, paving the way for more intelligent and complex\napplications of smart contracts in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blockchain smart contracts have catalyzed the development of decentralized\napplications across various domains, including decentralized finance. However,\ndue to constraints in computational resources and the prevalence of data silos,\ncurrent smart contracts face significant challenges in fully leveraging the\npowerful capabilities of Large Language Models (LLMs) for tasks such as\nintelligent analysis and reasoning. To address this gap, this paper proposes\nand implements a universal framework for integrating LLMs with blockchain data,\n{\\sysname}, effectively overcoming the interoperability barriers between\nblockchain and LLMs. By combining semantic relatedness with truth discovery\nmethods, we introduce an innovative data aggregation approach, {\\funcname},\nwhich significantly enhances the accuracy and trustworthiness of data generated\nby LLMs. To validate the framework's effectiveness, we construct a dataset\nconsisting of three types of questions, capturing Q\\&A interactions between 10\noracle nodes and 5 LLM models. Experimental results demonstrate that, even with\n40\\% malicious nodes, the proposed solution improves data accuracy by an\naverage of 17.74\\% compared to the optimal baseline. This research not only\nprovides an innovative solution for the intelligent enhancement of smart\ncontracts but also highlights the potential for deep integration between LLMs\nand blockchain technology, paving the way for more intelligent and complex\napplications of smart contracts in the future."
                },
                "authors": [
                    {
                        "name": "Youquan Xian"
                    },
                    {
                        "name": "Xueying Zeng"
                    },
                    {
                        "name": "Duancheng Xuan"
                    },
                    {
                        "name": "Danping Yang"
                    },
                    {
                        "name": "Chunpei Li"
                    },
                    {
                        "name": "Peng Fan"
                    },
                    {
                        "name": "Peng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Peng Liu"
                },
                "author": "Peng Liu",
                "arxiv_comment": "10 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02263v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02263v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05153v1",
                "updated": "2024-12-06T16:10:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    16,
                    10,
                    40,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T16:10:40Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    16,
                    10,
                    40,
                    4,
                    341,
                    0
                ],
                "title": "A text-to-tabular approach to generate synthetic patient data using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A text-to-tabular approach to generate synthetic patient data using LLMs"
                },
                "summary": "Access to large-scale high-quality healthcare databases is key to accelerate\nmedical research and make insightful discoveries about diseases. However,\naccess to such data is often limited by patient privacy concerns, data sharing\nrestrictions and high costs. To overcome these limitations, synthetic patient\ndata has emerged as an alternative. However, synthetic data generation (SDG)\nmethods typically rely on machine learning (ML) models trained on original\ndata, leading back to the data scarcity problem. We propose an approach to\ngenerate synthetic tabular patient data that does not require access to the\noriginal data, but only a description of the desired database. We leverage\nprior medical knowledge and in-context learning capabilities of large language\nmodels (LLMs) to generate realistic patient data, even in a low-resource\nsetting. We quantitatively evaluate our approach against state-of-the-art SDG\nmodels, using fidelity, privacy, and utility metrics. Our results show that\nwhile LLMs may not match the performance of state-of-the-art models trained on\nthe original data, they effectively generate realistic patient data with\nwell-preserved clinical correlations. An ablation study highlights key elements\nof our prompt contributing to high-quality synthetic patient data generation.\nThis approach, which is easy to use and does not require original data or\nadvanced ML skills, is particularly valuable for quickly generating\ncustom-designed patient data, supporting project implementation and providing\neducational resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Access to large-scale high-quality healthcare databases is key to accelerate\nmedical research and make insightful discoveries about diseases. However,\naccess to such data is often limited by patient privacy concerns, data sharing\nrestrictions and high costs. To overcome these limitations, synthetic patient\ndata has emerged as an alternative. However, synthetic data generation (SDG)\nmethods typically rely on machine learning (ML) models trained on original\ndata, leading back to the data scarcity problem. We propose an approach to\ngenerate synthetic tabular patient data that does not require access to the\noriginal data, but only a description of the desired database. We leverage\nprior medical knowledge and in-context learning capabilities of large language\nmodels (LLMs) to generate realistic patient data, even in a low-resource\nsetting. We quantitatively evaluate our approach against state-of-the-art SDG\nmodels, using fidelity, privacy, and utility metrics. Our results show that\nwhile LLMs may not match the performance of state-of-the-art models trained on\nthe original data, they effectively generate realistic patient data with\nwell-preserved clinical correlations. An ablation study highlights key elements\nof our prompt contributing to high-quality synthetic patient data generation.\nThis approach, which is easy to use and does not require original data or\nadvanced ML skills, is particularly valuable for quickly generating\ncustom-designed patient data, supporting project implementation and providing\neducational resources."
                },
                "authors": [
                    {
                        "name": "Margaux Tornqvist"
                    },
                    {
                        "name": "Jean-Daniel Zucker"
                    },
                    {
                        "name": "Tristan Fauvel"
                    },
                    {
                        "name": "Nicolas Lambert"
                    },
                    {
                        "name": "Mathilde Berthelot"
                    },
                    {
                        "name": "Antoine Movschin"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Movschin"
                },
                "author": "Antoine Movschin",
                "arxiv_comment": "12 pages, 2 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05145v1",
                "updated": "2024-12-06T16:01:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    16,
                    1,
                    30,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T16:01:30Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    16,
                    1,
                    30,
                    4,
                    341,
                    0
                ],
                "title": "Explingo: Explaining AI Predictions using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explingo: Explaining AI Predictions using Large Language Models"
                },
                "summary": "Explanations of machine learning (ML) model predictions generated by\nExplainable AI (XAI) techniques such as SHAP are essential for people using ML\noutputs for decision-making. We explore the potential of Large Language Models\n(LLMs) to transform these explanations into human-readable, narrative formats\nthat align with natural communication. We address two key research questions:\n(1) Can LLMs reliably transform traditional explanations into high-quality\nnarratives? and (2) How can we effectively evaluate the quality of narrative\nexplanations? To answer these questions, we introduce Explingo, which consists\nof two LLM-based subsystems, a Narrator and Grader. The Narrator takes in ML\nexplanations and transforms them into natural-language descriptions. The Grader\nscores these narratives on a set of metrics including accuracy, completeness,\nfluency, and conciseness.\n  Our experiments demonstrate that LLMs can generate high-quality narratives\nthat achieve high scores across all metrics, particularly when guided by a\nsmall number of human-labeled and bootstrapped examples. We also identified\nareas that remain challenging, in particular for effectively scoring narratives\nin complex domains. The findings from this work have been integrated into an\nopen-source tool that makes narrative explanations available for further\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explanations of machine learning (ML) model predictions generated by\nExplainable AI (XAI) techniques such as SHAP are essential for people using ML\noutputs for decision-making. We explore the potential of Large Language Models\n(LLMs) to transform these explanations into human-readable, narrative formats\nthat align with natural communication. We address two key research questions:\n(1) Can LLMs reliably transform traditional explanations into high-quality\nnarratives? and (2) How can we effectively evaluate the quality of narrative\nexplanations? To answer these questions, we introduce Explingo, which consists\nof two LLM-based subsystems, a Narrator and Grader. The Narrator takes in ML\nexplanations and transforms them into natural-language descriptions. The Grader\nscores these narratives on a set of metrics including accuracy, completeness,\nfluency, and conciseness.\n  Our experiments demonstrate that LLMs can generate high-quality narratives\nthat achieve high scores across all metrics, particularly when guided by a\nsmall number of human-labeled and bootstrapped examples. We also identified\nareas that remain challenging, in particular for effectively scoring narratives\nin complex domains. The findings from this work have been integrated into an\nopen-source tool that makes narrative explanations available for further\napplications."
                },
                "authors": [
                    {
                        "name": "Alexandra Zytek"
                    },
                    {
                        "name": "Sara Pido"
                    },
                    {
                        "name": "Sarah Alnegheimish"
                    },
                    {
                        "name": "Laure Berti-Equille"
                    },
                    {
                        "name": "Kalyan Veeramachaneni"
                    }
                ],
                "author_detail": {
                    "name": "Kalyan Veeramachaneni"
                },
                "author": "Kalyan Veeramachaneni",
                "arxiv_comment": "To be presented in the 2024 IEEE International Conference on Big Data\n  (IEEE BigData)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05137v1",
                "updated": "2024-12-06T15:51:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    15,
                    51,
                    22,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T15:51:22Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    15,
                    51,
                    22,
                    4,
                    341,
                    0
                ],
                "title": "Can Large Language Models Serve as Effective Classifiers for\n  Hierarchical Multi-Label Classification of Scientific Documents at Industrial\n  Scale?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Serve as Effective Classifiers for\n  Hierarchical Multi-Label Classification of Scientific Documents at Industrial\n  Scale?"
                },
                "summary": "We address the task of hierarchical multi-label classification (HMC) of\nscientific documents at an industrial scale, where hundreds of thousands of\ndocuments must be classified across thousands of dynamic labels. The rapid\ngrowth of scientific publications necessitates scalable and efficient methods\nfor classification, further complicated by the evolving nature of\ntaxonomies--where new categories are introduced, existing ones are merged, and\noutdated ones are deprecated. Traditional machine learning approaches, which\nrequire costly retraining with each taxonomy update, become impractical due to\nthe high overhead of labelled data collection and model adaptation. Large\nLanguage Models (LLMs) have demonstrated great potential in complex tasks such\nas multi-label classification. However, applying them to large and dynamic\ntaxonomies presents unique challenges as the vast number of labels can exceed\nLLMs' input limits. In this paper, we present novel methods that combine the\nstrengths of LLMs with dense retrieval techniques to overcome these challenges.\nOur approach avoids retraining by leveraging zero-shot HMC for real-time label\nassignment. We evaluate the effectiveness of our methods on SSRN, a large\nrepository of preprints spanning multiple disciplines, and demonstrate\nsignificant improvements in both classification accuracy and cost-efficiency.\nBy developing a tailored evaluation framework for dynamic taxonomies and\npublicly releasing our code, this research provides critical insights into\napplying LLMs for document classification, where the number of classes\ncorresponds to the number of nodes in a large taxonomy, at an industrial scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the task of hierarchical multi-label classification (HMC) of\nscientific documents at an industrial scale, where hundreds of thousands of\ndocuments must be classified across thousands of dynamic labels. The rapid\ngrowth of scientific publications necessitates scalable and efficient methods\nfor classification, further complicated by the evolving nature of\ntaxonomies--where new categories are introduced, existing ones are merged, and\noutdated ones are deprecated. Traditional machine learning approaches, which\nrequire costly retraining with each taxonomy update, become impractical due to\nthe high overhead of labelled data collection and model adaptation. Large\nLanguage Models (LLMs) have demonstrated great potential in complex tasks such\nas multi-label classification. However, applying them to large and dynamic\ntaxonomies presents unique challenges as the vast number of labels can exceed\nLLMs' input limits. In this paper, we present novel methods that combine the\nstrengths of LLMs with dense retrieval techniques to overcome these challenges.\nOur approach avoids retraining by leveraging zero-shot HMC for real-time label\nassignment. We evaluate the effectiveness of our methods on SSRN, a large\nrepository of preprints spanning multiple disciplines, and demonstrate\nsignificant improvements in both classification accuracy and cost-efficiency.\nBy developing a tailored evaluation framework for dynamic taxonomies and\npublicly releasing our code, this research provides critical insights into\napplying LLMs for document classification, where the number of classes\ncorresponds to the number of nodes in a large taxonomy, at an industrial scale."
                },
                "authors": [
                    {
                        "name": "Seyed Amin Tabatabaei"
                    },
                    {
                        "name": "Sarah Fancher"
                    },
                    {
                        "name": "Michael Parsons"
                    },
                    {
                        "name": "Arian Askari"
                    }
                ],
                "author_detail": {
                    "name": "Arian Askari"
                },
                "author": "Arian Askari",
                "arxiv_comment": "This paper has been accepted at COLING 2025 (Industry Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04517v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04517v2",
                "updated": "2024-12-06T15:42:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    15,
                    42,
                    7,
                    4,
                    341,
                    0
                ],
                "published": "2024-05-07T17:50:21Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    17,
                    50,
                    21,
                    1,
                    128,
                    0
                ],
                "title": "xLSTM: Extended Long Short-Term Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xLSTM: Extended Long Short-Term Memory"
                },
                "summary": "In the 1990s, the constant error carousel and gating were introduced as the\ncentral ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have\nstood the test of time and contributed to numerous deep learning success\nstories, in particular they constituted the first Large Language Models (LLMs).\nHowever, the advent of the Transformer technology with parallelizable\nself-attention at its core marked the dawn of a new era, outpacing LSTMs at\nscale. We now raise a simple question: How far do we get in language modeling\nwhen scaling LSTMs to billions of parameters, leveraging the latest techniques\nfrom modern LLMs, but mitigating known limitations of LSTMs? Firstly, we\nintroduce exponential gating with appropriate normalization and stabilization\ntechniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM\nwith a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that\nis fully parallelizable with a matrix memory and a covariance update rule.\nIntegrating these LSTM extensions into residual block backbones yields xLSTM\nblocks that are then residually stacked into xLSTM architectures. Exponential\ngating and modified memory structures boost xLSTM capabilities to perform\nfavorably when compared to state-of-the-art Transformers and State Space\nModels, both in performance and scaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the 1990s, the constant error carousel and gating were introduced as the\ncentral ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have\nstood the test of time and contributed to numerous deep learning success\nstories, in particular they constituted the first Large Language Models (LLMs).\nHowever, the advent of the Transformer technology with parallelizable\nself-attention at its core marked the dawn of a new era, outpacing LSTMs at\nscale. We now raise a simple question: How far do we get in language modeling\nwhen scaling LSTMs to billions of parameters, leveraging the latest techniques\nfrom modern LLMs, but mitigating known limitations of LSTMs? Firstly, we\nintroduce exponential gating with appropriate normalization and stabilization\ntechniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM\nwith a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that\nis fully parallelizable with a matrix memory and a covariance update rule.\nIntegrating these LSTM extensions into residual block backbones yields xLSTM\nblocks that are then residually stacked into xLSTM architectures. Exponential\ngating and modified memory structures boost xLSTM capabilities to perform\nfavorably when compared to state-of-the-art Transformers and State Space\nModels, both in performance and scaling."
                },
                "authors": [
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Korbinian Pöppel"
                    },
                    {
                        "name": "Markus Spanring"
                    },
                    {
                        "name": "Andreas Auer"
                    },
                    {
                        "name": "Oleksandra Prudnikova"
                    },
                    {
                        "name": "Michael Kopp"
                    },
                    {
                        "name": "Günter Klambauer"
                    },
                    {
                        "name": "Johannes Brandstetter"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "arxiv_comment": "Code available at https://github.com/NX-AI/xlstm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04517v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04517v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05127v1",
                "updated": "2024-12-06T15:35:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    15,
                    35,
                    18,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T15:35:18Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    15,
                    35,
                    18,
                    4,
                    341,
                    0
                ],
                "title": "The Prompt Canvas: A Literature-Based Practitioner Guide for Creating\n  Effective Prompts in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Prompt Canvas: A Literature-Based Practitioner Guide for Creating\n  Effective Prompts in Large Language Models"
                },
                "summary": "The rise of large language models (LLMs) has highlighted the importance of\nprompt engineering as a crucial technique for optimizing model outputs. While\nexperimentation with various prompting methods, such as Few-shot,\nChain-of-Thought, and role-based techniques, has yielded promising results,\nthese advancements remain fragmented across academic papers, blog posts and\nanecdotal experimentation. The lack of a single, unified resource to\nconsolidate the field's knowledge impedes the progress of both research and\npractical application. This paper argues for the creation of an overarching\nframework that synthesizes existing methodologies into a cohesive overview for\npractitioners. Using a design-based research approach, we present the Prompt\nCanvas, a structured framework resulting from an extensive literature review on\nprompt engineering that captures current knowledge and expertise. By combining\nthe conceptual foundations and practical strategies identified in prompt\nengineering, the Prompt Canvas provides a practical approach for leveraging the\npotential of Large Language Models. It is primarily designed as a learning\nresource for pupils, students and employees, offering a structured introduction\nto prompt engineering. This work aims to contribute to the growing discourse on\nprompt engineering by establishing a unified methodology for researchers and\nproviding guidance for practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large language models (LLMs) has highlighted the importance of\nprompt engineering as a crucial technique for optimizing model outputs. While\nexperimentation with various prompting methods, such as Few-shot,\nChain-of-Thought, and role-based techniques, has yielded promising results,\nthese advancements remain fragmented across academic papers, blog posts and\nanecdotal experimentation. The lack of a single, unified resource to\nconsolidate the field's knowledge impedes the progress of both research and\npractical application. This paper argues for the creation of an overarching\nframework that synthesizes existing methodologies into a cohesive overview for\npractitioners. Using a design-based research approach, we present the Prompt\nCanvas, a structured framework resulting from an extensive literature review on\nprompt engineering that captures current knowledge and expertise. By combining\nthe conceptual foundations and practical strategies identified in prompt\nengineering, the Prompt Canvas provides a practical approach for leveraging the\npotential of Large Language Models. It is primarily designed as a learning\nresource for pupils, students and employees, offering a structured introduction\nto prompt engineering. This work aims to contribute to the growing discourse on\nprompt engineering by establishing a unified methodology for researchers and\nproviding guidance for practitioners."
                },
                "authors": [
                    {
                        "name": "Michael Hewing"
                    },
                    {
                        "name": "Vincent Leinhos"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Leinhos"
                },
                "author": "Vincent Leinhos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05098v1",
                "updated": "2024-12-06T14:54:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    14,
                    54,
                    21,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T14:54:21Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    14,
                    54,
                    21,
                    4,
                    341,
                    0
                ],
                "title": "From Defects to Demands: A Unified, Iterative, and Heuristically Guided\n  LLM-Based Framework for Automated Software Repair and Requirement Realization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Defects to Demands: A Unified, Iterative, and Heuristically Guided\n  LLM-Based Framework for Automated Software Repair and Requirement Realization"
                },
                "summary": "This manuscript signals a new era in the integration of artificial\nintelligence with software engineering, placing machines at the pinnacle of\ncoding capability. We present a formalized, iterative methodology proving that\nAI can fully replace human programmers in all aspects of code creation and\nrefinement. Our approach, combining large language models with formal\nverification, test-driven development, and incremental architectural guidance,\nachieves a 38.6% improvement over the current top performer's 48.33% accuracy\non the SWE-bench benchmark. This surpasses previously assumed limits, signaling\nthe end of human-exclusive coding and the rise of autonomous AI-driven software\ninnovation. More than a technical advance, our work challenges centuries-old\nassumptions about human creativity. We provide robust evidence of AI\nsuperiority, demonstrating tangible gains in practical engineering contexts and\nlaying the foundation for a future in which computational creativity outpaces\nhuman ingenuity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This manuscript signals a new era in the integration of artificial\nintelligence with software engineering, placing machines at the pinnacle of\ncoding capability. We present a formalized, iterative methodology proving that\nAI can fully replace human programmers in all aspects of code creation and\nrefinement. Our approach, combining large language models with formal\nverification, test-driven development, and incremental architectural guidance,\nachieves a 38.6% improvement over the current top performer's 48.33% accuracy\non the SWE-bench benchmark. This surpasses previously assumed limits, signaling\nthe end of human-exclusive coding and the rise of autonomous AI-driven software\ninnovation. More than a technical advance, our work challenges centuries-old\nassumptions about human creativity. We provide robust evidence of AI\nsuperiority, demonstrating tangible gains in practical engineering contexts and\nlaying the foundation for a future in which computational creativity outpaces\nhuman ingenuity."
                },
                "authors": [
                    {
                        "name": "Alex"
                    },
                    {
                        "name": "Liu"
                    },
                    {
                        "name": "Vivian"
                    },
                    {
                        "name": "Chi"
                    }
                ],
                "author_detail": {
                    "name": "Chi"
                },
                "arxiv_affiliation": "Zirong",
                "author": "Chi",
                "arxiv_comment": "21 pages,1 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05093v1",
                "updated": "2024-12-06T14:50:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    14,
                    50,
                    1,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T14:50:01Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    14,
                    50,
                    1,
                    4,
                    341,
                    0
                ],
                "title": "Sense and Sensitivity: Evaluating the simulation of social dynamics via\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sense and Sensitivity: Evaluating the simulation of social dynamics via\n  Large Language Models"
                },
                "summary": "Large language models have increasingly been proposed as a powerful\nreplacement for classical agent-based models (ABMs) to simulate social\ndynamics. By using LLMs as a proxy for human behavior, the hope of this new\napproach is to be able to simulate significantly more complex dynamics than\nwith classical ABMs and gain new insights in fields such as social science,\npolitical science, and economics. However, due to the black box nature of LLMs,\nit is unclear whether LLM agents actually execute the intended semantics that\nare encoded in their natural language instructions and, if the resulting\ndynamics of interactions are meaningful. To study this question, we propose a\nnew evaluation framework that grounds LLM simulations within the dynamics of\nestablished reference models of social science. By treating LLMs as a black-box\nfunction, we evaluate their input-output behavior relative to this reference\nmodel, which allows us to evaluate detailed aspects of their behavior. Our\nresults show that, while it is possible to engineer prompts that approximate\nthe intended dynamics, the quality of these simulations is highly sensitive to\nthe particular choice of prompts. Importantly, simulations are even sensitive\nto arbitrary variations such as minor wording changes and whitespace. This puts\ninto question the usefulness of current versions of LLMs for meaningful\nsimulations, as without a reference model, it is impossible to determine a\npriori what impact seemingly meaningless changes in prompt will have on the\nsimulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have increasingly been proposed as a powerful\nreplacement for classical agent-based models (ABMs) to simulate social\ndynamics. By using LLMs as a proxy for human behavior, the hope of this new\napproach is to be able to simulate significantly more complex dynamics than\nwith classical ABMs and gain new insights in fields such as social science,\npolitical science, and economics. However, due to the black box nature of LLMs,\nit is unclear whether LLM agents actually execute the intended semantics that\nare encoded in their natural language instructions and, if the resulting\ndynamics of interactions are meaningful. To study this question, we propose a\nnew evaluation framework that grounds LLM simulations within the dynamics of\nestablished reference models of social science. By treating LLMs as a black-box\nfunction, we evaluate their input-output behavior relative to this reference\nmodel, which allows us to evaluate detailed aspects of their behavior. Our\nresults show that, while it is possible to engineer prompts that approximate\nthe intended dynamics, the quality of these simulations is highly sensitive to\nthe particular choice of prompts. Importantly, simulations are even sensitive\nto arbitrary variations such as minor wording changes and whitespace. This puts\ninto question the usefulness of current versions of LLMs for meaningful\nsimulations, as without a reference model, it is impossible to determine a\npriori what impact seemingly meaningless changes in prompt will have on the\nsimulation."
                },
                "authors": [
                    {
                        "name": "Da Ju"
                    },
                    {
                        "name": "Adina Williams"
                    },
                    {
                        "name": "Brian Karrer"
                    },
                    {
                        "name": "Maximilian Nickel"
                    }
                ],
                "author_detail": {
                    "name": "Maximilian Nickel"
                },
                "author": "Maximilian Nickel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05075v1",
                "updated": "2024-12-06T14:33:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    14,
                    33,
                    34,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T14:33:34Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    14,
                    33,
                    34,
                    4,
                    341,
                    0
                ],
                "title": "Towards the interoperability of low-code platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards the interoperability of low-code platforms"
                },
                "summary": "With the promise of accelerating software development, low-code platforms\n(LCPs) are becoming popular across various industries. Nevertheless, there are\nstill barriers hindering their adoption. Among them, vendor lock-in is a major\nconcern, especially considering the lack of interoperability between these\nplatforms. Typically, after modeling an application in one LCP, migrating to\nanother requires starting from scratch remodeling everything (the data model,\nthe graphical user interface, workflows, etc.), in the new platform.\n  To overcome this situation, this work proposes an approach to improve the\ninteroperability of LCPs by (semi)automatically migrating models specified in\none platform to another one. The concrete migration path depends on the\ncapabilities of the source and target tools. We first analyze popular LCPs,\ncharacterize their import and export alternatives and define transformations\nbetween those data formats when available. This is then complemented with an\nLLM-based solution, where image recognition features of large language models\nare employed to migrate models based on a simple image export of the model at\nhand. The full pipelines are implemented on top of the BESSER modeling\nframework that acts as a pivot representation between the tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the promise of accelerating software development, low-code platforms\n(LCPs) are becoming popular across various industries. Nevertheless, there are\nstill barriers hindering their adoption. Among them, vendor lock-in is a major\nconcern, especially considering the lack of interoperability between these\nplatforms. Typically, after modeling an application in one LCP, migrating to\nanother requires starting from scratch remodeling everything (the data model,\nthe graphical user interface, workflows, etc.), in the new platform.\n  To overcome this situation, this work proposes an approach to improve the\ninteroperability of LCPs by (semi)automatically migrating models specified in\none platform to another one. The concrete migration path depends on the\ncapabilities of the source and target tools. We first analyze popular LCPs,\ncharacterize their import and export alternatives and define transformations\nbetween those data formats when available. This is then complemented with an\nLLM-based solution, where image recognition features of large language models\nare employed to migrate models based on a simple image export of the model at\nhand. The full pipelines are implemented on top of the BESSER modeling\nframework that acts as a pivot representation between the tools."
                },
                "authors": [
                    {
                        "name": "Iván Alfonso"
                    },
                    {
                        "name": "Aaron Conrardy"
                    },
                    {
                        "name": "Jordi Cabot"
                    }
                ],
                "author_detail": {
                    "name": "Jordi Cabot"
                },
                "author": "Jordi Cabot",
                "arxiv_comment": "Submitted to International Conference on Advanced Information Systems\n  Engineering (CAiSE25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68-04",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.12",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07057v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07057v2",
                "updated": "2024-12-06T14:21:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    14,
                    21,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-06-11T08:38:13Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    8,
                    38,
                    13,
                    1,
                    163,
                    0
                ],
                "title": "MultiTrust: A Comprehensive Benchmark Towards Trustworthy Multimodal\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiTrust: A Comprehensive Benchmark Towards Trustworthy Multimodal\n  Large Language Models"
                },
                "summary": "Despite the superior capabilities of Multimodal Large Language Models (MLLMs)\nacross diverse tasks, they still face significant trustworthiness challenges.\nYet, current literature on the assessment of trustworthy MLLMs remains limited,\nlacking a holistic evaluation to offer thorough insights into future\nimprovements. In this work, we establish MultiTrust, the first comprehensive\nand unified benchmark on the trustworthiness of MLLMs across five primary\naspects: truthfulness, safety, robustness, fairness, and privacy. Our benchmark\nemploys a rigorous evaluation strategy that addresses both multimodal risks and\ncross-modal impacts, encompassing 32 diverse tasks with self-curated datasets.\nExtensive experiments with 21 modern MLLMs reveal some previously unexplored\ntrustworthiness issues and risks, highlighting the complexities introduced by\nthe multimodality and underscoring the necessity for advanced methodologies to\nenhance their reliability. For instance, typical proprietary models still\nstruggle with the perception of visually confusing images and are vulnerable to\nmultimodal jailbreaking and adversarial attacks; MLLMs are more inclined to\ndisclose privacy in text and reveal ideological and cultural biases even when\npaired with irrelevant images in inference, indicating that the multimodality\namplifies the internal risks from base LLMs. Additionally, we release a\nscalable toolbox for standardized trustworthiness research, aiming to\nfacilitate future advancements in this important field. Code and resources are\npublicly available at: https://multi-trust.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the superior capabilities of Multimodal Large Language Models (MLLMs)\nacross diverse tasks, they still face significant trustworthiness challenges.\nYet, current literature on the assessment of trustworthy MLLMs remains limited,\nlacking a holistic evaluation to offer thorough insights into future\nimprovements. In this work, we establish MultiTrust, the first comprehensive\nand unified benchmark on the trustworthiness of MLLMs across five primary\naspects: truthfulness, safety, robustness, fairness, and privacy. Our benchmark\nemploys a rigorous evaluation strategy that addresses both multimodal risks and\ncross-modal impacts, encompassing 32 diverse tasks with self-curated datasets.\nExtensive experiments with 21 modern MLLMs reveal some previously unexplored\ntrustworthiness issues and risks, highlighting the complexities introduced by\nthe multimodality and underscoring the necessity for advanced methodologies to\nenhance their reliability. For instance, typical proprietary models still\nstruggle with the perception of visually confusing images and are vulnerable to\nmultimodal jailbreaking and adversarial attacks; MLLMs are more inclined to\ndisclose privacy in text and reveal ideological and cultural biases even when\npaired with irrelevant images in inference, indicating that the multimodality\namplifies the internal risks from base LLMs. Additionally, we release a\nscalable toolbox for standardized trustworthiness research, aiming to\nfacilitate future advancements in this important field. Code and resources are\npublicly available at: https://multi-trust.github.io/."
                },
                "authors": [
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Yao Huang"
                    },
                    {
                        "name": "Yitong Sun"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Zhengwei Fang"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Huanran Chen"
                    },
                    {
                        "name": "Xiao Yang"
                    },
                    {
                        "name": "Xingxing Wei"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Yinpeng Dong"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "arxiv_comment": "100 pages, 84 figures, 33 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07057v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07057v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05039v1",
                "updated": "2024-12-06T13:45:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    13,
                    45,
                    27,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T13:45:27Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    13,
                    45,
                    27,
                    4,
                    341,
                    0
                ],
                "title": "Level Up or Game Over: Exploring How Dark Patterns Shape Mobile Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Level Up or Game Over: Exploring How Dark Patterns Shape Mobile Games"
                },
                "summary": "This study explores the prevalence of dark patterns in mobile games that\nexploit players through temporal, monetary, social, and psychological means.\nRecognizing the ethical concerns and potential harm surrounding these\nmanipulative strategies, we analyze user-generated data of 1496 games to\nidentify relationships between the deployment of dark patterns within \"dark\"\nand \"healthy\" games. Our findings reveal that dark patterns are not only\nwidespread in games typically seen as problematic but are also present in games\nthat may be perceived as benign. This research contributes needed quantitative\nsupport to the broader understanding of dark patterns in games. With an\nemphasis on ethical design, our study highlights current problems of revenue\nmodels that can be particularly harmful to vulnerable populations. To this end,\nwe discuss the relevance of community-based approaches to surface harmful\ndesign and the necessity for collaboration among players/users and\npractitioners to promote healthier gaming experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the prevalence of dark patterns in mobile games that\nexploit players through temporal, monetary, social, and psychological means.\nRecognizing the ethical concerns and potential harm surrounding these\nmanipulative strategies, we analyze user-generated data of 1496 games to\nidentify relationships between the deployment of dark patterns within \"dark\"\nand \"healthy\" games. Our findings reveal that dark patterns are not only\nwidespread in games typically seen as problematic but are also present in games\nthat may be perceived as benign. This research contributes needed quantitative\nsupport to the broader understanding of dark patterns in games. With an\nemphasis on ethical design, our study highlights current problems of revenue\nmodels that can be particularly harmful to vulnerable populations. To this end,\nwe discuss the relevance of community-based approaches to surface harmful\ndesign and the necessity for collaboration among players/users and\npractitioners to promote healthier gaming experiences."
                },
                "authors": [
                    {
                        "name": "Sam Niknejad"
                    },
                    {
                        "name": "Thomas Mildner"
                    },
                    {
                        "name": "Nima Zargham"
                    },
                    {
                        "name": "Susanne Putze"
                    },
                    {
                        "name": "Rainer Malaka"
                    }
                ],
                "author_detail": {
                    "name": "Rainer Malaka"
                },
                "author": "Rainer Malaka",
                "arxiv_doi": "10.1145/3701571.3701604",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701571.3701604",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.05039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "International Conference on Mobile and Ubiquitous Multimedia (MUM\n  '24), December 1--4, 2024, Stockholm, Sweden",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.1.2; K.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19832v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19832v2",
                "updated": "2024-12-06T13:41:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    13,
                    41,
                    53,
                    4,
                    341,
                    0
                ],
                "published": "2024-11-29T16:44:02Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    44,
                    2,
                    4,
                    334,
                    0
                ],
                "title": "Sensitive Content Classification in Social Media: A Holistic Resource\n  and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensitive Content Classification in Social Media: A Holistic Resource\n  and Evaluation"
                },
                "summary": "The detection of sensitive content in large datasets is crucial for ensuring\nthat shared and analysed data is free from harmful material. However, current\nmoderation tools, such as external APIs, suffer from limitations in\ncustomisation, accuracy across diverse sensitive categories, and privacy\nconcerns. Additionally, existing datasets and open-source models focus\npredominantly on toxic language, leaving gaps in detecting other sensitive\ncategories such as substance abuse or self-harm. In this paper, we put forward\na unified dataset tailored for social media content moderation across six\nsensitive categories: conflictual language, profanity, sexually explicit\nmaterial, drug-related content, self-harm, and spam. By collecting and\nannotating data with consistent retrieval strategies and guidelines, we address\nthe shortcomings of previous focalised research. Our analysis demonstrates that\nfine-tuning large language models (LLMs) on this novel dataset yields\nsignificant improvements in detection performance compared to open\noff-the-shelf models such as LLaMA, and even proprietary OpenAI models, which\nunderperform by 10-15% overall. This limitation is even more pronounced on\npopular moderation APIs, which cannot be easily tailored to specific sensitive\ncontent categories, among others.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detection of sensitive content in large datasets is crucial for ensuring\nthat shared and analysed data is free from harmful material. However, current\nmoderation tools, such as external APIs, suffer from limitations in\ncustomisation, accuracy across diverse sensitive categories, and privacy\nconcerns. Additionally, existing datasets and open-source models focus\npredominantly on toxic language, leaving gaps in detecting other sensitive\ncategories such as substance abuse or self-harm. In this paper, we put forward\na unified dataset tailored for social media content moderation across six\nsensitive categories: conflictual language, profanity, sexually explicit\nmaterial, drug-related content, self-harm, and spam. By collecting and\nannotating data with consistent retrieval strategies and guidelines, we address\nthe shortcomings of previous focalised research. Our analysis demonstrates that\nfine-tuning large language models (LLMs) on this novel dataset yields\nsignificant improvements in detection performance compared to open\noff-the-shelf models such as LLaMA, and even proprietary OpenAI models, which\nunderperform by 10-15% overall. This limitation is even more pronounced on\npopular moderation APIs, which cannot be easily tailored to specific sensitive\ncontent categories, among others."
                },
                "authors": [
                    {
                        "name": "Dimosthenis Antypas"
                    },
                    {
                        "name": "Indira Sen"
                    },
                    {
                        "name": "Carla Perez-Almendros"
                    },
                    {
                        "name": "Jose Camacho-Collados"
                    },
                    {
                        "name": "Francesco Barbieri"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Barbieri"
                },
                "author": "Francesco Barbieri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19832v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19832v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18506v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18506v3",
                "updated": "2024-12-06T13:35:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    13,
                    35,
                    45,
                    4,
                    341,
                    0
                ],
                "published": "2024-11-27T16:48:24Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    16,
                    48,
                    24,
                    2,
                    332,
                    0
                ],
                "title": "LLM-ABBA: Understanding time series via symbolic approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-ABBA: Understanding time series via symbolic approximation"
                },
                "summary": "The success of large language models (LLMs) for time series has been\ndemonstrated in previous work. Utilizing a symbolic time series representation,\none can efficiently bridge the gap between LLMs and time series. However, the\nremaining challenge is to exploit the semantic information hidden in time\nseries by using symbols or existing tokens of LLMs, while aligning the\nembedding space of LLMs according to the hidden information of time series. The\nsymbolic time series approximation (STSA) method called adaptive Brownian\nbridge-based symbolic aggregation (ABBA) shows outstanding efficacy in\npreserving salient time series features by modeling time series patterns in\nterms of amplitude and period while using existing tokens of LLMs.\n  In this paper, we introduce a method, called LLM-ABBA, that integrates ABBA\ninto large language models for various downstream time series tasks. By\nsymbolizing time series, LLM-ABBA compares favorably to the recent\nstate-of-the-art (SOTA) in UCR and three medical time series classification\ntasks. Meanwhile, a fixed-polygonal chain trick in ABBA is introduced to\n\\kc{avoid obvious drifting} during prediction tasks by significantly mitigating\nthe effects of cumulative error arising from misused symbols during the\ntransition from symbols to numerical values. In time series regression tasks,\nLLM-ABBA achieves the new SOTA on Time Series Extrinsic Regression (TSER)\nbenchmarks. LLM-ABBA also shows competitive prediction capability compared to\nrecent SOTA time series prediction results. We believe this framework can also\nseamlessly extend to other time series tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of large language models (LLMs) for time series has been\ndemonstrated in previous work. Utilizing a symbolic time series representation,\none can efficiently bridge the gap between LLMs and time series. However, the\nremaining challenge is to exploit the semantic information hidden in time\nseries by using symbols or existing tokens of LLMs, while aligning the\nembedding space of LLMs according to the hidden information of time series. The\nsymbolic time series approximation (STSA) method called adaptive Brownian\nbridge-based symbolic aggregation (ABBA) shows outstanding efficacy in\npreserving salient time series features by modeling time series patterns in\nterms of amplitude and period while using existing tokens of LLMs.\n  In this paper, we introduce a method, called LLM-ABBA, that integrates ABBA\ninto large language models for various downstream time series tasks. By\nsymbolizing time series, LLM-ABBA compares favorably to the recent\nstate-of-the-art (SOTA) in UCR and three medical time series classification\ntasks. Meanwhile, a fixed-polygonal chain trick in ABBA is introduced to\n\\kc{avoid obvious drifting} during prediction tasks by significantly mitigating\nthe effects of cumulative error arising from misused symbols during the\ntransition from symbols to numerical values. In time series regression tasks,\nLLM-ABBA achieves the new SOTA on Time Series Extrinsic Regression (TSER)\nbenchmarks. LLM-ABBA also shows competitive prediction capability compared to\nrecent SOTA time series prediction results. We believe this framework can also\nseamlessly extend to other time series tasks."
                },
                "authors": [
                    {
                        "name": "Erin Carson"
                    },
                    {
                        "name": "Xinye Chen"
                    },
                    {
                        "name": "Cheng Kang"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Kang"
                },
                "author": "Cheng Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18506v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18506v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05023v1",
                "updated": "2024-12-06T13:20:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    13,
                    20,
                    57,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T13:20:57Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    13,
                    20,
                    57,
                    4,
                    341,
                    0
                ],
                "title": "Steps are all you need: Rethinking STEM Education with Prompt\n  Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steps are all you need: Rethinking STEM Education with Prompt\n  Engineering"
                },
                "summary": "Few shot and Chain-of-Thought prompting have shown promise when applied to\nPhysics Question Answering Tasks, but are limited by the lack of mathematical\nability inherent to LLMs, and are prone to hallucination. By utilizing a\nMixture of Experts (MoE) Model, along with analogical prompting, we are able to\nshow improved model performance when compared to the baseline on standard LLMs.\nWe also survey the limits of these prompting techniques and the effects they\nhave on model performance. Additionally, we propose Analogical CoT prompting, a\nprompting technique designed to allow smaller, open source models to leverage\nAnalogical prompting, something they have struggled with, possibly due to a\nlack of specialist training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few shot and Chain-of-Thought prompting have shown promise when applied to\nPhysics Question Answering Tasks, but are limited by the lack of mathematical\nability inherent to LLMs, and are prone to hallucination. By utilizing a\nMixture of Experts (MoE) Model, along with analogical prompting, we are able to\nshow improved model performance when compared to the baseline on standard LLMs.\nWe also survey the limits of these prompting techniques and the effects they\nhave on model performance. Additionally, we propose Analogical CoT prompting, a\nprompting technique designed to allow smaller, open source models to leverage\nAnalogical prompting, something they have struggled with, possibly due to a\nlack of specialist training data."
                },
                "authors": [
                    {
                        "name": "Krishnasai Addala"
                    },
                    {
                        "name": "Kabir Dev Paul Baghel"
                    },
                    {
                        "name": "Chhavi Kirtani"
                    },
                    {
                        "name": "Avinash Anand"
                    },
                    {
                        "name": "Rajiv Ratn Shah"
                    }
                ],
                "author_detail": {
                    "name": "Rajiv Ratn Shah"
                },
                "author": "Rajiv Ratn Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05250v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05250v3",
                "updated": "2024-12-06T12:40:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    12,
                    40,
                    53,
                    4,
                    341,
                    0
                ],
                "published": "2024-06-07T20:22:36Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    20,
                    22,
                    36,
                    4,
                    159,
                    0
                ],
                "title": "LLM-Enhanced Bayesian Optimization for Efficient Analog Layout\n  Constraint Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Enhanced Bayesian Optimization for Efficient Analog Layout\n  Constraint Generation"
                },
                "summary": "Analog layout synthesis faces significant challenges due to its dependence on\nmanual processes, considerable time requirements, and performance instability.\nCurrent Bayesian Optimization (BO)-based techniques for analog layout\nsynthesis, despite their potential for automation, suffer from slow convergence\nand extensive data needs, limiting their practical application. This paper\npresents the \\texttt{LLANA} framework, a novel approach that leverages Large\nLanguage Models (LLMs) to enhance BO by exploiting the few-shot learning\nabilities of LLMs for more efficient generation of analog design-dependent\nparameter constraints. Experimental results demonstrate that \\texttt{LLANA} not\nonly achieves performance comparable to state-of-the-art (SOTA) BO methods but\nalso enables a more effective exploration of the analog circuit design space,\nthanks to LLM's superior contextual understanding and learning efficiency. The\ncode is available at https://github.com/dekura/LLANA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog layout synthesis faces significant challenges due to its dependence on\nmanual processes, considerable time requirements, and performance instability.\nCurrent Bayesian Optimization (BO)-based techniques for analog layout\nsynthesis, despite their potential for automation, suffer from slow convergence\nand extensive data needs, limiting their practical application. This paper\npresents the \\texttt{LLANA} framework, a novel approach that leverages Large\nLanguage Models (LLMs) to enhance BO by exploiting the few-shot learning\nabilities of LLMs for more efficient generation of analog design-dependent\nparameter constraints. Experimental results demonstrate that \\texttt{LLANA} not\nonly achieves performance comparable to state-of-the-art (SOTA) BO methods but\nalso enables a more effective exploration of the analog circuit design space,\nthanks to LLM's superior contextual understanding and learning efficiency. The\ncode is available at https://github.com/dekura/LLANA."
                },
                "authors": [
                    {
                        "name": "Guojin Chen"
                    },
                    {
                        "name": "Keren Zhu"
                    },
                    {
                        "name": "Seunggeun Kim"
                    },
                    {
                        "name": "Hanqing Zhu"
                    },
                    {
                        "name": "Yao Lai"
                    },
                    {
                        "name": "Bei Yu"
                    },
                    {
                        "name": "David Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "David Z. Pan"
                },
                "author": "David Z. Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05250v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05250v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02976v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02976v2",
                "updated": "2024-12-06T12:39:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    12,
                    39,
                    0,
                    4,
                    341,
                    0
                ],
                "published": "2024-09-04T13:59:38Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    59,
                    38,
                    2,
                    248,
                    0
                ],
                "title": "Hallucination Detection in LLMs: Fast and Memory-Efficient Fine-Tuned\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination Detection in LLMs: Fast and Memory-Efficient Fine-Tuned\n  Models"
                },
                "summary": "Uncertainty estimation is a necessary component when implementing AI in\nhigh-risk settings, such as autonomous cars, medicine, or insurances. Large\nLanguage Models (LLMs) have seen a surge in popularity in recent years, but\nthey are subject to hallucinations, which may cause serious harm in high-risk\nsettings. Despite their success, LLMs are expensive to train and run: they need\na large amount of computations and memory, preventing the use of ensembling\nmethods in practice. In this work, we present a novel method that allows for\nfast and memory-friendly training of LLM ensembles. We show that the resulting\nensembles can detect hallucinations and are a viable approach in practice as\nonly one GPU is needed for training and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty estimation is a necessary component when implementing AI in\nhigh-risk settings, such as autonomous cars, medicine, or insurances. Large\nLanguage Models (LLMs) have seen a surge in popularity in recent years, but\nthey are subject to hallucinations, which may cause serious harm in high-risk\nsettings. Despite their success, LLMs are expensive to train and run: they need\na large amount of computations and memory, preventing the use of ensembling\nmethods in practice. In this work, we present a novel method that allows for\nfast and memory-friendly training of LLM ensembles. We show that the resulting\nensembles can detect hallucinations and are a viable approach in practice as\nonly one GPU is needed for training and inference."
                },
                "authors": [
                    {
                        "name": "Gabriel Y. Arteaga"
                    },
                    {
                        "name": "Thomas B. Schön"
                    },
                    {
                        "name": "Nicolas Pielawski"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Pielawski"
                },
                "author": "Nicolas Pielawski",
                "arxiv_comment": "6 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02976v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02976v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04986v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04986v1",
                "updated": "2024-12-06T12:15:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    12,
                    15,
                    11,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T12:15:11Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    12,
                    15,
                    11,
                    4,
                    341,
                    0
                ],
                "title": "Power Plant Detection for Energy Estimation using GIS with Remote\n  Sensing, CNN & Vision Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Power Plant Detection for Energy Estimation using GIS with Remote\n  Sensing, CNN & Vision Transformers"
                },
                "summary": "In this research, we propose a hybrid model for power plant detection to\nassist energy estimation applications, by pipelining GIS (Geographical\nInformation Systems) having Remote Sensing capabilities with CNN (Convolutional\nNeural Networks) and ViT (Vision Transformers). Our proposed approach enables\nreal-time analysis with multiple data types on a common map via the GIS,\nentails feature-extraction abilities due to the CNN, and captures long-range\ndependencies through the ViT. This hybrid approach is found to enhance\nclassification, thus helping in the monitoring and operational management of\npower plants; hence assisting energy estimation and sustainable energy planning\nin the future. It exemplifies adequate deployment of machine learning methods\nin conjunction with domain-specific approaches to enhance performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this research, we propose a hybrid model for power plant detection to\nassist energy estimation applications, by pipelining GIS (Geographical\nInformation Systems) having Remote Sensing capabilities with CNN (Convolutional\nNeural Networks) and ViT (Vision Transformers). Our proposed approach enables\nreal-time analysis with multiple data types on a common map via the GIS,\nentails feature-extraction abilities due to the CNN, and captures long-range\ndependencies through the ViT. This hybrid approach is found to enhance\nclassification, thus helping in the monitoring and operational management of\npower plants; hence assisting energy estimation and sustainable energy planning\nin the future. It exemplifies adequate deployment of machine learning methods\nin conjunction with domain-specific approaches to enhance performance."
                },
                "authors": [
                    {
                        "name": "Blessing Austin-Gabriel"
                    },
                    {
                        "name": "Cristian Noriega Monsalve"
                    },
                    {
                        "name": "Aparna S. Varde"
                    }
                ],
                "author_detail": {
                    "name": "Aparna S. Varde"
                },
                "author": "Aparna S. Varde",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04986v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.m; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09439v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09439v2",
                "updated": "2024-12-06T12:09:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    12,
                    9,
                    15,
                    4,
                    341,
                    0
                ],
                "published": "2024-08-18T11:07:38Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    11,
                    7,
                    38,
                    6,
                    231,
                    0
                ],
                "title": "Towards Boosting LLMs-driven Relevance Modeling with Progressive\n  Retrieved Behavior-augmented Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Boosting LLMs-driven Relevance Modeling with Progressive\n  Retrieved Behavior-augmented Prompting"
                },
                "summary": "Relevance modeling is a critical component for enhancing user experience in\nsearch engines, with the primary objective of identifying items that align with\nusers' queries. Traditional models only rely on the semantic congruence between\nqueries and items to ascertain relevance. However, this approach represents\nmerely one aspect of the relevance judgement, and is insufficient in isolation.\nEven powerful Large Language Models (LLMs) still cannot accurately judge the\nrelevance of a query and an item from a semantic perspective. To augment\nLLMs-driven relevance modeling, this study proposes leveraging user\ninteractions recorded in search logs to yield insights into users' implicit\nsearch intentions. The challenge lies in the effective prompting of LLMs to\ncapture dynamic search intentions, which poses several obstacles in real-world\nrelevance scenarios, i.e., the absence of domain-specific knowledge, the\ninadequacy of an isolated prompt, and the prohibitive costs associated with\ndeploying LLMs. In response, we propose ProRBP, a novel Progressive Retrieved\nBehavior-augmented Prompting framework for integrating search scenario-oriented\nknowledge with LLMs effectively. Specifically, we perform the user-driven\nbehavior neighbors retrieval from the daily search logs to obtain\ndomain-specific knowledge in time, retrieving candidates that users consider to\nmeet their expectations. Then, we guide LLMs for relevance modeling by\nemploying advanced prompting techniques that progressively improve the outputs\nof the LLMs, followed by a progressive aggregation with comprehensive\nconsideration of diverse aspects. For online serving, we have developed an\nindustrial application framework tailored for the deployment of LLMs in\nrelevance modeling. Experiments on real-world industry data and online A/B\ntesting demonstrate our proposal achieves promising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relevance modeling is a critical component for enhancing user experience in\nsearch engines, with the primary objective of identifying items that align with\nusers' queries. Traditional models only rely on the semantic congruence between\nqueries and items to ascertain relevance. However, this approach represents\nmerely one aspect of the relevance judgement, and is insufficient in isolation.\nEven powerful Large Language Models (LLMs) still cannot accurately judge the\nrelevance of a query and an item from a semantic perspective. To augment\nLLMs-driven relevance modeling, this study proposes leveraging user\ninteractions recorded in search logs to yield insights into users' implicit\nsearch intentions. The challenge lies in the effective prompting of LLMs to\ncapture dynamic search intentions, which poses several obstacles in real-world\nrelevance scenarios, i.e., the absence of domain-specific knowledge, the\ninadequacy of an isolated prompt, and the prohibitive costs associated with\ndeploying LLMs. In response, we propose ProRBP, a novel Progressive Retrieved\nBehavior-augmented Prompting framework for integrating search scenario-oriented\nknowledge with LLMs effectively. Specifically, we perform the user-driven\nbehavior neighbors retrieval from the daily search logs to obtain\ndomain-specific knowledge in time, retrieving candidates that users consider to\nmeet their expectations. Then, we guide LLMs for relevance modeling by\nemploying advanced prompting techniques that progressively improve the outputs\nof the LLMs, followed by a progressive aggregation with comprehensive\nconsideration of diverse aspects. For online serving, we have developed an\nindustrial application framework tailored for the deployment of LLMs in\nrelevance modeling. Experiments on real-world industry data and online A/B\ntesting demonstrate our proposal achieves promising performance."
                },
                "authors": [
                    {
                        "name": "Zeyuan Chen"
                    },
                    {
                        "name": "Haiyan Wu"
                    },
                    {
                        "name": "Kaixin Wu"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Mingjie Zhong"
                    },
                    {
                        "name": "Jia Xu"
                    },
                    {
                        "name": "Zhongyi Liu"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "Accepted By COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09439v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09439v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01349v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01349v3",
                "updated": "2024-12-06T11:54:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    54,
                    40,
                    4,
                    341,
                    0
                ],
                "published": "2024-02-02T12:07:00Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    12,
                    7,
                    0,
                    4,
                    33,
                    0
                ],
                "title": "LLMs May Perform MCQA by Selecting the Least Incorrect Option",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs May Perform MCQA by Selecting the Least Incorrect Option"
                },
                "summary": "In the field of NLP, Large Language Models (LLMs) have markedly enhanced\nperformance across a variety of tasks. However, the comprehensive evaluation of\nLLMs remains an inevitable challenge for the community. Recently, the adoption\nof Multiple Choice Question Answering (MCQA) as a benchmark for assessing LLMs\nhas gained considerable traction. However, concerns regarding the robustness of\nthis evaluative method persist. Building upon previous discussions on the issue\nof \\textit{variability}, we reveal an additional dimension of concern: LLMs may\nperform MCQA by selecting the least incorrect option rather than distinctly\ncorrect. This observation suggests that LLMs might regard multiple options as\ncorrect, which could undermine the reliability of MCQA as a metric for\nevaluating LLMs. To address this challenge, we introduce an enhanced dataset\naugmentation method for MCQA, termed MCQA+, to provide a more accurate\nreflection of the model performance, thereby highlighting the necessity for\nmore sophisticated evaluation mechanisms in the assessment of LLM capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of NLP, Large Language Models (LLMs) have markedly enhanced\nperformance across a variety of tasks. However, the comprehensive evaluation of\nLLMs remains an inevitable challenge for the community. Recently, the adoption\nof Multiple Choice Question Answering (MCQA) as a benchmark for assessing LLMs\nhas gained considerable traction. However, concerns regarding the robustness of\nthis evaluative method persist. Building upon previous discussions on the issue\nof \\textit{variability}, we reveal an additional dimension of concern: LLMs may\nperform MCQA by selecting the least incorrect option rather than distinctly\ncorrect. This observation suggests that LLMs might regard multiple options as\ncorrect, which could undermine the reliability of MCQA as a metric for\nevaluating LLMs. To address this challenge, we introduce an enhanced dataset\naugmentation method for MCQA, termed MCQA+, to provide a more accurate\nreflection of the model performance, thereby highlighting the necessity for\nmore sophisticated evaluation mechanisms in the assessment of LLM capabilities."
                },
                "authors": [
                    {
                        "name": "Haochun Wang"
                    },
                    {
                        "name": "Sendong Zhao"
                    },
                    {
                        "name": "Zewen Qiang"
                    },
                    {
                        "name": "Nuwa Xi"
                    },
                    {
                        "name": "Bing Qin"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "arxiv_comment": "COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01349v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01349v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04977v1",
                "updated": "2024-12-06T11:53:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    53,
                    57,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T11:53:57Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    53,
                    57,
                    4,
                    341,
                    0
                ],
                "title": "ORKG ASK: a Neuro-symbolic Scholarly Search and Exploration System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORKG ASK: a Neuro-symbolic Scholarly Search and Exploration System"
                },
                "summary": "Purpose: Finding scholarly articles is a time-consuming and cumbersome\nactivity, yet crucial for conducting science. Due to the growing number of\nscholarly articles, new scholarly search systems are needed to effectively\nassist researchers in finding relevant literature.\n  Methodology: We take a neuro-symbolic approach to scholarly search and\nexploration by leveraging state-of-the-art components, including semantic\nsearch, Large Language Models (LLMs), and Knowledge Graphs (KGs). The semantic\nsearch component composes a set of relevant articles. From this set of\narticles, information is extracted and presented to the user.\n  Findings: The presented system, called ORKG ASK (Assistant for Scientific\nKnowledge), provides a production-ready search and exploration system. Our\npreliminary evaluation indicates that our proposed approach is indeed suitable\nfor the task of scholarly information retrieval.\n  Value: With ORKG ASK, we present a next-generation scholarly search and\nexploration system and make it available online. Additionally, the system\ncomponents are open source with a permissive license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: Finding scholarly articles is a time-consuming and cumbersome\nactivity, yet crucial for conducting science. Due to the growing number of\nscholarly articles, new scholarly search systems are needed to effectively\nassist researchers in finding relevant literature.\n  Methodology: We take a neuro-symbolic approach to scholarly search and\nexploration by leveraging state-of-the-art components, including semantic\nsearch, Large Language Models (LLMs), and Knowledge Graphs (KGs). The semantic\nsearch component composes a set of relevant articles. From this set of\narticles, information is extracted and presented to the user.\n  Findings: The presented system, called ORKG ASK (Assistant for Scientific\nKnowledge), provides a production-ready search and exploration system. Our\npreliminary evaluation indicates that our proposed approach is indeed suitable\nfor the task of scholarly information retrieval.\n  Value: With ORKG ASK, we present a next-generation scholarly search and\nexploration system and make it available online. Additionally, the system\ncomponents are open source with a permissive license."
                },
                "authors": [
                    {
                        "name": "Allard Oelen"
                    },
                    {
                        "name": "Mohamad Yaser Jaradeh"
                    },
                    {
                        "name": "Sören Auer"
                    }
                ],
                "author_detail": {
                    "name": "Sören Auer"
                },
                "author": "Sören Auer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14974v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14974v2",
                "updated": "2024-12-06T11:39:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    39,
                    35,
                    4,
                    341,
                    0
                ],
                "published": "2024-10-19T04:37:01Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    4,
                    37,
                    1,
                    5,
                    293,
                    0
                ],
                "title": "CAGE: Causal Attention Enables Data-Efficient Generalizable Robotic\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAGE: Causal Attention Enables Data-Efficient Generalizable Robotic\n  Manipulation"
                },
                "summary": "Generalization in robotic manipulation remains a critical challenge,\nparticularly when scaling to new environments with limited demonstrations. This\npaper introduces CAGE, a novel robotic manipulation policy designed to overcome\nthese generalization barriers by integrating a causal attention mechanism. CAGE\nutilizes the powerful feature extraction capabilities of the vision foundation\nmodel DINOv2, combined with LoRA fine-tuning for robust environment\nunderstanding. The policy further employs a causal Perceiver for effective\ntoken compression and a diffusion-based action prediction head with attention\nmechanisms to enhance task-specific fine-grained conditioning. With as few as\n50 demonstrations from a single training environment, CAGE achieves robust\ngeneralization across diverse visual changes in objects, backgrounds, and\nviewpoints. Extensive experiments validate that CAGE significantly outperforms\nexisting state-of-the-art RGB/RGB-D approaches in various manipulation tasks,\nespecially under large distribution shifts. In similar environments, CAGE\noffers an average of 42% increase in task completion rate. While all baselines\nfail to execute the task in unseen environments, CAGE manages to obtain a 43%\ncompletion rate and a 51% success rate in average, making a huge step towards\npractical deployment of robots in real-world settings. Project website:\ncage-policy.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalization in robotic manipulation remains a critical challenge,\nparticularly when scaling to new environments with limited demonstrations. This\npaper introduces CAGE, a novel robotic manipulation policy designed to overcome\nthese generalization barriers by integrating a causal attention mechanism. CAGE\nutilizes the powerful feature extraction capabilities of the vision foundation\nmodel DINOv2, combined with LoRA fine-tuning for robust environment\nunderstanding. The policy further employs a causal Perceiver for effective\ntoken compression and a diffusion-based action prediction head with attention\nmechanisms to enhance task-specific fine-grained conditioning. With as few as\n50 demonstrations from a single training environment, CAGE achieves robust\ngeneralization across diverse visual changes in objects, backgrounds, and\nviewpoints. Extensive experiments validate that CAGE significantly outperforms\nexisting state-of-the-art RGB/RGB-D approaches in various manipulation tasks,\nespecially under large distribution shifts. In similar environments, CAGE\noffers an average of 42% increase in task completion rate. While all baselines\nfail to execute the task in unseen environments, CAGE manages to obtain a 43%\ncompletion rate and a 51% success rate in average, making a huge step towards\npractical deployment of robots in real-world settings. Project website:\ncage-policy.github.io."
                },
                "authors": [
                    {
                        "name": "Shangning Xia"
                    },
                    {
                        "name": "Hongjie Fang"
                    },
                    {
                        "name": "Cewu Lu"
                    },
                    {
                        "name": "Hao-Shu Fang"
                    }
                ],
                "author_detail": {
                    "name": "Hao-Shu Fang"
                },
                "author": "Hao-Shu Fang",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14974v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14974v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04315v2",
                "updated": "2024-12-06T11:39:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    39,
                    27,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-05T16:31:13Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    31,
                    13,
                    3,
                    340,
                    0
                ],
                "title": "Densing Law of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Densing Law of LLMs"
                },
                "summary": "Large Language Models (LLMs) have emerged as a milestone in artificial\nintelligence, and their performance can improve as the model size increases.\nHowever, this scaling brings great challenges to training and inference\nefficiency, particularly for deploying LLMs in resource-constrained\nenvironments, and the scaling trend is becoming increasingly unsustainable.\nThis paper introduces the concept of ``\\textit{capacity density}'' as a new\nmetric to evaluate the quality of the LLMs across different scales and\ndescribes the trend of LLMs in terms of both effectiveness and efficiency. To\ncalculate the capacity density of a given target LLM, we first introduce a set\nof reference models and develop a scaling law to predict the downstream\nperformance of these reference models based on their parameter sizes. We then\ndefine the \\textit{effective parameter size} of the target LLM as the parameter\nsize required by a reference model to achieve equivalent performance, and\nformalize the capacity density as the ratio of the effective parameter size to\nthe actual parameter size of the target LLM. Capacity density provides a\nunified framework for assessing both model effectiveness and efficiency. Our\nfurther analysis of recent open-source base LLMs reveals an empirical law (the\ndensing law)that the capacity density of LLMs grows exponentially over time.\nMore specifically, using some widely used benchmarks for evaluation, the\ncapacity density of LLMs doubles approximately every three months. The law\nprovides new perspectives to guide future LLM development, emphasizing the\nimportance of improving capacity density to achieve optimal results with\nminimal computational overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as a milestone in artificial\nintelligence, and their performance can improve as the model size increases.\nHowever, this scaling brings great challenges to training and inference\nefficiency, particularly for deploying LLMs in resource-constrained\nenvironments, and the scaling trend is becoming increasingly unsustainable.\nThis paper introduces the concept of ``\\textit{capacity density}'' as a new\nmetric to evaluate the quality of the LLMs across different scales and\ndescribes the trend of LLMs in terms of both effectiveness and efficiency. To\ncalculate the capacity density of a given target LLM, we first introduce a set\nof reference models and develop a scaling law to predict the downstream\nperformance of these reference models based on their parameter sizes. We then\ndefine the \\textit{effective parameter size} of the target LLM as the parameter\nsize required by a reference model to achieve equivalent performance, and\nformalize the capacity density as the ratio of the effective parameter size to\nthe actual parameter size of the target LLM. Capacity density provides a\nunified framework for assessing both model effectiveness and efficiency. Our\nfurther analysis of recent open-source base LLMs reveals an empirical law (the\ndensing law)that the capacity density of LLMs grows exponentially over time.\nMore specifically, using some widely used benchmarks for evaluation, the\ncapacity density of LLMs doubles approximately every three months. The law\nprovides new perspectives to guide future LLM development, emphasizing the\nimportance of improving capacity density to achieve optimal results with\nminimal computational overhead."
                },
                "authors": [
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Jie Cai"
                    },
                    {
                        "name": "Weilin Zhao"
                    },
                    {
                        "name": "Guoyang Zeng"
                    },
                    {
                        "name": "Biyuan Lin"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Zhi Zheng"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04964v1",
                "updated": "2024-12-06T11:29:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    29,
                    32,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T11:29:32Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    29,
                    32,
                    4,
                    341,
                    0
                ],
                "title": "Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast\n  Large Language Model Inference"
                },
                "summary": "The ever-increasing sizes of large language models necessitate distributed\nsolutions for fast inference that exploit multi-dimensional parallelism, where\ncomputational loads are split across various accelerators such as GPU clusters.\nHowever, this approach often introduces significant communication overhead,\nespecially on devices with limited bandwidth. In this paper, we introduce\n\\emph{Flash Communication}, a novel low-bit compression technique designed to\nalleviate the tensor-parallelism communication bottleneck during inference. Our\nmethod substantially boosts intra-node communication speed by more than 3x and\nreduces the \\emph{time-to-first-token} by 2x, with nearly no sacrifice in model\naccuracy. Extensive experiments on various up-to-date LLMs demonstrate the\neffectiveness of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ever-increasing sizes of large language models necessitate distributed\nsolutions for fast inference that exploit multi-dimensional parallelism, where\ncomputational loads are split across various accelerators such as GPU clusters.\nHowever, this approach often introduces significant communication overhead,\nespecially on devices with limited bandwidth. In this paper, we introduce\n\\emph{Flash Communication}, a novel low-bit compression technique designed to\nalleviate the tensor-parallelism communication bottleneck during inference. Our\nmethod substantially boosts intra-node communication speed by more than 3x and\nreduces the \\emph{time-to-first-token} by 2x, with nearly no sacrifice in model\naccuracy. Extensive experiments on various up-to-date LLMs demonstrate the\neffectiveness of our approach."
                },
                "authors": [
                    {
                        "name": "Qingyuan Li"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Liang Ye"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Yerui Sun"
                    },
                    {
                        "name": "Lin Ma"
                    },
                    {
                        "name": "Yuchen Xie"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Xie"
                },
                "author": "Yuchen Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02030v2",
                "updated": "2024-12-06T11:22:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    22,
                    17,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-02T23:20:35Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    23,
                    20,
                    35,
                    0,
                    337,
                    0
                ],
                "title": "NitroFusion: High-Fidelity Single-Step Diffusion through Dynamic\n  Adversarial Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NitroFusion: High-Fidelity Single-Step Diffusion through Dynamic\n  Adversarial Training"
                },
                "summary": "We introduce NitroFusion, a fundamentally different approach to single-step\ndiffusion that achieves high-quality generation through a dynamic adversarial\nframework. While one-step methods offer dramatic speed advantages, they\ntypically suffer from quality degradation compared to their multi-step\ncounterparts. Just as a panel of art critics provides comprehensive feedback by\nspecializing in different aspects like composition, color, and technique, our\napproach maintains a large pool of specialized discriminator heads that\ncollectively guide the generation process. Each discriminator group develops\nexpertise in specific quality aspects at different noise levels, providing\ndiverse feedback that enables high-fidelity one-step generation. Our framework\ncombines: (i) a dynamic discriminator pool with specialized discriminator\ngroups to improve generation quality, (ii) strategic refresh mechanisms to\nprevent discriminator overfitting, and (iii) global-local discriminator heads\nfor multi-scale quality assessment, and unconditional/conditional training for\nbalanced generation. Additionally, our framework uniquely supports flexible\ndeployment through bottom-up refinement, allowing users to dynamically choose\nbetween 1-4 denoising steps with the same model for direct quality-speed\ntrade-offs. Through comprehensive experiments, we demonstrate that NitroFusion\nsignificantly outperforms existing single-step methods across multiple\nevaluation metrics, particularly excelling in preserving fine details and\nglobal consistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce NitroFusion, a fundamentally different approach to single-step\ndiffusion that achieves high-quality generation through a dynamic adversarial\nframework. While one-step methods offer dramatic speed advantages, they\ntypically suffer from quality degradation compared to their multi-step\ncounterparts. Just as a panel of art critics provides comprehensive feedback by\nspecializing in different aspects like composition, color, and technique, our\napproach maintains a large pool of specialized discriminator heads that\ncollectively guide the generation process. Each discriminator group develops\nexpertise in specific quality aspects at different noise levels, providing\ndiverse feedback that enables high-fidelity one-step generation. Our framework\ncombines: (i) a dynamic discriminator pool with specialized discriminator\ngroups to improve generation quality, (ii) strategic refresh mechanisms to\nprevent discriminator overfitting, and (iii) global-local discriminator heads\nfor multi-scale quality assessment, and unconditional/conditional training for\nbalanced generation. Additionally, our framework uniquely supports flexible\ndeployment through bottom-up refinement, allowing users to dynamically choose\nbetween 1-4 denoising steps with the same model for direct quality-speed\ntrade-offs. Through comprehensive experiments, we demonstrate that NitroFusion\nsignificantly outperforms existing single-step methods across multiple\nevaluation metrics, particularly excelling in preserving fine details and\nglobal consistency."
                },
                "authors": [
                    {
                        "name": "Dar-Yen Chen"
                    },
                    {
                        "name": "Hmrishav Bandyopadhyay"
                    },
                    {
                        "name": "Kai Zou"
                    },
                    {
                        "name": "Yi-Zhe Song"
                    }
                ],
                "author_detail": {
                    "name": "Yi-Zhe Song"
                },
                "author": "Yi-Zhe Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04954v1",
                "updated": "2024-12-06T11:14:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    14,
                    3,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T11:14:03Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    14,
                    3,
                    4,
                    341,
                    0
                ],
                "title": "Gla-AI4BioMed at RRG24: Visual Instruction-tuned Adaptation for\n  Radiology Report Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gla-AI4BioMed at RRG24: Visual Instruction-tuned Adaptation for\n  Radiology Report Generation"
                },
                "summary": "We introduce a radiology-focused visual language model designed to generate\nradiology reports from chest X-rays. Building on previous findings that large\nlanguage models (LLMs) can acquire multimodal capabilities when aligned with\npretrained vision encoders, we demonstrate similar potential with chest X-ray\nimages. This integration enhances the ability of model to understand and\ndescribe chest X-ray images. Our model combines an image encoder with a\nfine-tuned LLM based on the Vicuna-7B architecture, enabling it to generate\ndifferent sections of a radiology report with notable accuracy. The training\nprocess involves a two-stage approach: (i) initial alignment of chest X-ray\nfeatures with the LLM (ii) followed by fine-tuning for radiology report\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a radiology-focused visual language model designed to generate\nradiology reports from chest X-rays. Building on previous findings that large\nlanguage models (LLMs) can acquire multimodal capabilities when aligned with\npretrained vision encoders, we demonstrate similar potential with chest X-ray\nimages. This integration enhances the ability of model to understand and\ndescribe chest X-ray images. Our model combines an image encoder with a\nfine-tuned LLM based on the Vicuna-7B architecture, enabling it to generate\ndifferent sections of a radiology report with notable accuracy. The training\nprocess involves a two-stage approach: (i) initial alignment of chest X-ray\nfeatures with the LLM (ii) followed by fine-tuning for radiology report\ngeneration."
                },
                "authors": [
                    {
                        "name": "Xi Zhang"
                    },
                    {
                        "name": "Zaiqiao Meng"
                    },
                    {
                        "name": "Jake Lever"
                    },
                    {
                        "name": "Edmond S. L. Ho"
                    }
                ],
                "author_detail": {
                    "name": "Edmond S. L. Ho"
                },
                "author": "Edmond S. L. Ho",
                "arxiv_doi": "10.18653/v1/2024.bionlp-1.54",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.bionlp-1.54",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.04954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by BioNLP@ACL 2024",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04948v1",
                "updated": "2024-12-06T11:08:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    8,
                    24,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T11:08:24Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    8,
                    24,
                    4,
                    341,
                    0
                ],
                "title": "KaLM: Knowledge-aligned Autoregressive Language Modeling via Dual-view\n  Knowledge Graph Contrastive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KaLM: Knowledge-aligned Autoregressive Language Modeling via Dual-view\n  Knowledge Graph Contrastive Learning"
                },
                "summary": "Autoregressive large language models (LLMs) pre-trained by next token\nprediction are inherently proficient in generative tasks. However, their\nperformance on knowledge-driven tasks such as factual knowledge querying\nremains unsatisfactory. Knowledge graphs (KGs), as high-quality structured\nknowledge bases, can provide reliable knowledge for LLMs, potentially\ncompensating for their knowledge deficiencies. Aligning LLMs with explicit,\nstructured knowledge from KGs has been a challenge; previous attempts either\nfailed to effectively align knowledge representations or compromised the\ngenerative capabilities of LLMs, leading to less-than-optimal outcomes. This\npaper proposes \\textbf{KaLM}, a \\textit{Knowledge-aligned Language Modeling}\napproach, which fine-tunes autoregressive LLMs to align with KG knowledge via\nthe joint objective of explicit knowledge alignment and implicit knowledge\nalignment. The explicit knowledge alignment objective aims to directly optimize\nthe knowledge representation of LLMs through dual-view knowledge graph\ncontrastive learning. The implicit knowledge alignment objective focuses on\nincorporating textual patterns of knowledge into LLMs through triple completion\nlanguage modeling. Notably, our method achieves a significant performance boost\nin evaluations of knowledge-driven tasks, specifically embedding-based\nknowledge graph completion and generation-based knowledge graph question\nanswering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive large language models (LLMs) pre-trained by next token\nprediction are inherently proficient in generative tasks. However, their\nperformance on knowledge-driven tasks such as factual knowledge querying\nremains unsatisfactory. Knowledge graphs (KGs), as high-quality structured\nknowledge bases, can provide reliable knowledge for LLMs, potentially\ncompensating for their knowledge deficiencies. Aligning LLMs with explicit,\nstructured knowledge from KGs has been a challenge; previous attempts either\nfailed to effectively align knowledge representations or compromised the\ngenerative capabilities of LLMs, leading to less-than-optimal outcomes. This\npaper proposes \\textbf{KaLM}, a \\textit{Knowledge-aligned Language Modeling}\napproach, which fine-tunes autoregressive LLMs to align with KG knowledge via\nthe joint objective of explicit knowledge alignment and implicit knowledge\nalignment. The explicit knowledge alignment objective aims to directly optimize\nthe knowledge representation of LLMs through dual-view knowledge graph\ncontrastive learning. The implicit knowledge alignment objective focuses on\nincorporating textual patterns of knowledge into LLMs through triple completion\nlanguage modeling. Notably, our method achieves a significant performance boost\nin evaluations of knowledge-driven tasks, specifically embedding-based\nknowledge graph completion and generation-based knowledge graph question\nanswering."
                },
                "authors": [
                    {
                        "name": "Peng Yu"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Beiya Dai"
                    },
                    {
                        "name": "Xinbing Wang"
                    },
                    {
                        "name": "Ying Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wen"
                },
                "author": "Ying Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04947v1",
                "updated": "2024-12-06T11:07:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    7,
                    44,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T11:07:44Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    7,
                    44,
                    4,
                    341,
                    0
                ],
                "title": "C$^2$LEVA: Toward Comprehensive and Contamination-Free Language Model\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C$^2$LEVA: Toward Comprehensive and Contamination-Free Language Model\n  Evaluation"
                },
                "summary": "Recent advances in large language models (LLMs) have shown significant\npromise, yet their evaluation raises concerns, particularly regarding data\ncontamination due to the lack of access to proprietary training data. To\naddress this issue, we present C$^2$LEVA, a comprehensive bilingual benchmark\nfeaturing systematic contamination prevention. C$^2$LEVA firstly offers a\nholistic evaluation encompassing 22 tasks, each targeting a specific\napplication or ability of LLMs, and secondly a trustworthy assessment due to\nour contamination-free tasks, ensured by a systematic contamination prevention\nstrategy that fully automates test data renewal and enforces data protection\nduring benchmark data release. Our large-scale evaluation of 15 open-source and\nproprietary models demonstrates the effectiveness of C$^2$LEVA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have shown significant\npromise, yet their evaluation raises concerns, particularly regarding data\ncontamination due to the lack of access to proprietary training data. To\naddress this issue, we present C$^2$LEVA, a comprehensive bilingual benchmark\nfeaturing systematic contamination prevention. C$^2$LEVA firstly offers a\nholistic evaluation encompassing 22 tasks, each targeting a specific\napplication or ability of LLMs, and secondly a trustworthy assessment due to\nour contamination-free tasks, ensured by a systematic contamination prevention\nstrategy that fully automates test data renewal and enforces data protection\nduring benchmark data release. Our large-scale evaluation of 15 open-source and\nproprietary models demonstrates the effectiveness of C$^2$LEVA."
                },
                "authors": [
                    {
                        "name": "Yanyang Li"
                    },
                    {
                        "name": "Tin Long Wong"
                    },
                    {
                        "name": "Cheung To Hung"
                    },
                    {
                        "name": "Jianqiao Zhao"
                    },
                    {
                        "name": "Duo Zheng"
                    },
                    {
                        "name": "Ka Wai Liu"
                    },
                    {
                        "name": "Michael R. Lyu"
                    },
                    {
                        "name": "Liwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liwei Wang"
                },
                "author": "Liwei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19050v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19050v2",
                "updated": "2024-12-06T10:58:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    10,
                    58,
                    53,
                    4,
                    341,
                    0
                ],
                "published": "2024-11-28T10:55:09Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    10,
                    55,
                    9,
                    3,
                    333,
                    0
                ],
                "title": "I Dream My Painting: Connecting MLLMs and Diffusion Models via Prompt\n  Generation for Text-Guided Multi-Mask Inpainting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Dream My Painting: Connecting MLLMs and Diffusion Models via Prompt\n  Generation for Text-Guided Multi-Mask Inpainting"
                },
                "summary": "Inpainting focuses on filling missing or corrupted regions of an image to\nblend seamlessly with its surrounding content and style. While conditional\ndiffusion models have proven effective for text-guided inpainting, we introduce\nthe novel task of multi-mask inpainting, where multiple regions are\nsimultaneously inpainted using distinct prompts. Furthermore, we design a\nfine-tuning procedure for multimodal LLMs, such as LLaVA, to generate\nmulti-mask prompts automatically using corrupted images as inputs. These models\ncan generate helpful and detailed prompt suggestions for filling the masked\nregions. The generated prompts are then fed to Stable Diffusion, which is\nfine-tuned for the multi-mask inpainting problem using rectified\ncross-attention, enforcing prompts onto their designated regions for filling.\nExperiments on digitized paintings from WikiArt and the Densely Captioned\nImages dataset demonstrate that our pipeline delivers creative and accurate\ninpainting results. Our code, data, and trained models are available at\nhttps://cilabuniba.github.io/i-dream-my-painting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inpainting focuses on filling missing or corrupted regions of an image to\nblend seamlessly with its surrounding content and style. While conditional\ndiffusion models have proven effective for text-guided inpainting, we introduce\nthe novel task of multi-mask inpainting, where multiple regions are\nsimultaneously inpainted using distinct prompts. Furthermore, we design a\nfine-tuning procedure for multimodal LLMs, such as LLaVA, to generate\nmulti-mask prompts automatically using corrupted images as inputs. These models\ncan generate helpful and detailed prompt suggestions for filling the masked\nregions. The generated prompts are then fed to Stable Diffusion, which is\nfine-tuned for the multi-mask inpainting problem using rectified\ncross-attention, enforcing prompts onto their designated regions for filling.\nExperiments on digitized paintings from WikiArt and the Densely Captioned\nImages dataset demonstrate that our pipeline delivers creative and accurate\ninpainting results. Our code, data, and trained models are available at\nhttps://cilabuniba.github.io/i-dream-my-painting."
                },
                "authors": [
                    {
                        "name": "Nicola Fanelli"
                    },
                    {
                        "name": "Gennaro Vessio"
                    },
                    {
                        "name": "Giovanna Castellano"
                    }
                ],
                "author_detail": {
                    "name": "Giovanna Castellano"
                },
                "author": "Giovanna Castellano",
                "arxiv_comment": "Accepted at WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19050v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19050v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04937v1",
                "updated": "2024-12-06T10:45:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    10,
                    45,
                    54,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T10:45:54Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    10,
                    45,
                    54,
                    4,
                    341,
                    0
                ],
                "title": "Who Speaks Next? Multi-party AI Discussion Leveraging the Systematics of\n  Turn-taking in Murder Mystery Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who Speaks Next? Multi-party AI Discussion Leveraging the Systematics of\n  Turn-taking in Murder Mystery Games"
                },
                "summary": "Multi-agent systems utilizing large language models (LLMs) have shown great\npromise in achieving natural dialogue. However, smooth dialogue control and\nautonomous decision making among agents still remain challenges. In this study,\nwe focus on conversational norms such as adjacency pairs and turn-taking found\nin conversation analysis and propose a new framework called \"Murder Mystery\nAgents\" that applies these norms to AI agents' dialogue control. As an\nevaluation target, we employed the \"Murder Mystery\" game, a reasoning-type\ntable-top role-playing game that requires complex social reasoning and\ninformation manipulation. In this game, players need to unravel the truth of\nthe case based on fragmentary information through cooperation and bargaining.\nThe proposed framework integrates next speaker selection based on adjacency\npairs and a self-selection mechanism that takes agents' internal states into\naccount to achieve more natural and strategic dialogue. To verify the\neffectiveness of this new approach, we analyzed utterances that led to dialogue\nbreakdowns and conducted automatic evaluation using LLMs, as well as human\nevaluation using evaluation criteria developed for the Murder Mystery game.\nExperimental results showed that the implementation of the next speaker\nselection mechanism significantly reduced dialogue breakdowns and improved the\nability of agents to share information and perform logical reasoning. The\nresults of this study demonstrate that the systematics of turn-taking in human\nconversation are also effective in controlling dialogue among AI agents, and\nprovide design guidelines for more advanced multi-agent dialogue systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems utilizing large language models (LLMs) have shown great\npromise in achieving natural dialogue. However, smooth dialogue control and\nautonomous decision making among agents still remain challenges. In this study,\nwe focus on conversational norms such as adjacency pairs and turn-taking found\nin conversation analysis and propose a new framework called \"Murder Mystery\nAgents\" that applies these norms to AI agents' dialogue control. As an\nevaluation target, we employed the \"Murder Mystery\" game, a reasoning-type\ntable-top role-playing game that requires complex social reasoning and\ninformation manipulation. In this game, players need to unravel the truth of\nthe case based on fragmentary information through cooperation and bargaining.\nThe proposed framework integrates next speaker selection based on adjacency\npairs and a self-selection mechanism that takes agents' internal states into\naccount to achieve more natural and strategic dialogue. To verify the\neffectiveness of this new approach, we analyzed utterances that led to dialogue\nbreakdowns and conducted automatic evaluation using LLMs, as well as human\nevaluation using evaluation criteria developed for the Murder Mystery game.\nExperimental results showed that the implementation of the next speaker\nselection mechanism significantly reduced dialogue breakdowns and improved the\nability of agents to share information and perform logical reasoning. The\nresults of this study demonstrate that the systematics of turn-taking in human\nconversation are also effective in controlling dialogue among AI agents, and\nprovide design guidelines for more advanced multi-agent dialogue systems."
                },
                "authors": [
                    {
                        "name": "Ryota Nonomura"
                    },
                    {
                        "name": "Hiroki Mori"
                    }
                ],
                "author_detail": {
                    "name": "Hiroki Mori"
                },
                "author": "Hiroki Mori",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01294v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01294v2",
                "updated": "2024-12-06T10:31:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    10,
                    31,
                    43,
                    4,
                    341,
                    0
                ],
                "published": "2024-10-02T07:40:56Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    7,
                    40,
                    56,
                    2,
                    276,
                    0
                ],
                "title": "Endless Jailbreaks with Bijection Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Endless Jailbreaks with Bijection Learning"
                },
                "summary": "Despite extensive safety measures, LLMs are vulnerable to adversarial inputs,\nor jailbreaks, which can elicit unsafe behaviors. In this work, we introduce\nbijection learning, a powerful attack algorithm which automatically fuzzes LLMs\nfor safety vulnerabilities using randomly-generated encodings whose complexity\ncan be tightly controlled. We leverage in-context learning to teach models\nbijective encodings, pass encoded queries to the model to bypass built-in\nsafety mechanisms, and finally decode responses back into English. Our attack\nis extremely effective on a wide range of frontier language models. Moreover,\nby controlling complexity parameters such as number of key-value mappings in\nthe encodings, we find a close relationship between the capability level of the\nattacked LLM and the average complexity of the most effective bijection\nattacks. Our work highlights that new vulnerabilities in frontier models can\nemerge with scale: more capable models are more severely jailbroken by\nbijection attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite extensive safety measures, LLMs are vulnerable to adversarial inputs,\nor jailbreaks, which can elicit unsafe behaviors. In this work, we introduce\nbijection learning, a powerful attack algorithm which automatically fuzzes LLMs\nfor safety vulnerabilities using randomly-generated encodings whose complexity\ncan be tightly controlled. We leverage in-context learning to teach models\nbijective encodings, pass encoded queries to the model to bypass built-in\nsafety mechanisms, and finally decode responses back into English. Our attack\nis extremely effective on a wide range of frontier language models. Moreover,\nby controlling complexity parameters such as number of key-value mappings in\nthe encodings, we find a close relationship between the capability level of the\nattacked LLM and the average complexity of the most effective bijection\nattacks. Our work highlights that new vulnerabilities in frontier models can\nemerge with scale: more capable models are more severely jailbroken by\nbijection attacks."
                },
                "authors": [
                    {
                        "name": "Brian R. Y. Huang"
                    },
                    {
                        "name": "Maximilian Li"
                    },
                    {
                        "name": "Leonard Tang"
                    }
                ],
                "author_detail": {
                    "name": "Leonard Tang"
                },
                "author": "Leonard Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01294v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01294v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00353v2",
                "updated": "2024-12-06T10:24:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    10,
                    24,
                    47,
                    4,
                    341,
                    0
                ],
                "published": "2024-11-30T04:22:00Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    4,
                    22,
                    0,
                    5,
                    335,
                    0
                ],
                "title": "Enhancing Zero-shot Chain of Thought Prompting via Uncertainty-Guided\n  Strategy Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Zero-shot Chain of Thought Prompting via Uncertainty-Guided\n  Strategy Selection"
                },
                "summary": "Chain-of-thought (CoT) prompting has significantly enhanced the capability of\nlarge language models (LLMs) by structuring their reasoning processes. However,\nexisting methods face critical limitations: handcrafted demonstrations require\nextensive human expertise, while trigger phrases are prone to inaccuracies. In\nthis paper, we propose the Zero-shot Uncertainty-based Selection (ZEUS) method,\na novel approach that improves CoT prompting by utilizing uncertainty estimates\nto select effective demonstrations without needing access to model parameters.\nUnlike traditional methods, ZEUS offers high sensitivity in distinguishing\nbetween helpful and ineffective questions, ensuring more precise and reliable\nselection. Our extensive evaluation shows that ZEUS consistently outperforms\nexisting CoT strategies across four challenging reasoning benchmarks,\ndemonstrating its robustness and scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) prompting has significantly enhanced the capability of\nlarge language models (LLMs) by structuring their reasoning processes. However,\nexisting methods face critical limitations: handcrafted demonstrations require\nextensive human expertise, while trigger phrases are prone to inaccuracies. In\nthis paper, we propose the Zero-shot Uncertainty-based Selection (ZEUS) method,\na novel approach that improves CoT prompting by utilizing uncertainty estimates\nto select effective demonstrations without needing access to model parameters.\nUnlike traditional methods, ZEUS offers high sensitivity in distinguishing\nbetween helpful and ineffective questions, ensuring more precise and reliable\nselection. Our extensive evaluation shows that ZEUS consistently outperforms\nexisting CoT strategies across four challenging reasoning benchmarks,\ndemonstrating its robustness and scalability."
                },
                "authors": [
                    {
                        "name": "Shanu Kumar"
                    },
                    {
                        "name": "Saish Mendke"
                    },
                    {
                        "name": "Karody Lubna Abdul Rahman"
                    },
                    {
                        "name": "Santosh Kurasa"
                    },
                    {
                        "name": "Parag Agrawal"
                    },
                    {
                        "name": "Sandipan Dandapat"
                    }
                ],
                "author_detail": {
                    "name": "Sandipan Dandapat"
                },
                "author": "Sandipan Dandapat",
                "arxiv_comment": "Accepted in COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04922v1",
                "updated": "2024-12-06T10:21:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    10,
                    21,
                    25,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T10:21:25Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    10,
                    21,
                    25,
                    4,
                    341,
                    0
                ],
                "title": "Large Language Models for Ingredient Substitution in Food Recipes using\n  Supervised Fine-tuning and Direct Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Ingredient Substitution in Food Recipes using\n  Supervised Fine-tuning and Direct Preference Optimization"
                },
                "summary": "In this paper, we address the challenge of recipe personalization through\ningredient substitution. We make use of Large Language Models (LLMs) to build\nan ingredient substitution system designed to predict plausible substitute\ningredients within a given recipe context. Given that the use of LLMs for this\ntask has been barely done, we carry out an extensive set of experiments to\ndetermine the best LLM, prompt, and the fine-tuning setups. We further\nexperiment with methods such as multi-task learning, two-stage fine-tuning, and\nDirect Preference Optimization (DPO). The experiments are conducted using the\npublicly available Recipe1MSub corpus. The best results are produced by the\nMistral7-Base LLM after fine-tuning and DPO. This result outperforms the strong\nbaseline available for the same corpus with a Hit@1 score of 22.04. Thus we\nbelieve that this research represents a significant step towards enabling\npersonalized and creative culinary experiences by utilizing LLM-based\ningredient substitution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we address the challenge of recipe personalization through\ningredient substitution. We make use of Large Language Models (LLMs) to build\nan ingredient substitution system designed to predict plausible substitute\ningredients within a given recipe context. Given that the use of LLMs for this\ntask has been barely done, we carry out an extensive set of experiments to\ndetermine the best LLM, prompt, and the fine-tuning setups. We further\nexperiment with methods such as multi-task learning, two-stage fine-tuning, and\nDirect Preference Optimization (DPO). The experiments are conducted using the\npublicly available Recipe1MSub corpus. The best results are produced by the\nMistral7-Base LLM after fine-tuning and DPO. This result outperforms the strong\nbaseline available for the same corpus with a Hit@1 score of 22.04. Thus we\nbelieve that this research represents a significant step towards enabling\npersonalized and creative culinary experiences by utilizing LLM-based\ningredient substitution."
                },
                "authors": [
                    {
                        "name": "Thevin Senath"
                    },
                    {
                        "name": "Kumuthu Athukorala"
                    },
                    {
                        "name": "Ransika Costa"
                    },
                    {
                        "name": "Surangika Ranathunga"
                    },
                    {
                        "name": "Rishemjit Kaur"
                    }
                ],
                "author_detail": {
                    "name": "Rishemjit Kaur"
                },
                "author": "Rishemjit Kaur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04917v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04917v1",
                "updated": "2024-12-06T10:16:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    10,
                    16,
                    4,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T10:16:04Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    10,
                    16,
                    4,
                    4,
                    341,
                    0
                ],
                "title": "Continuous Speech Tokens Makes LLMs Robust Multi-Modality Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous Speech Tokens Makes LLMs Robust Multi-Modality Learners"
                },
                "summary": "Recent advances in GPT-4o like multi-modality models have demonstrated\nremarkable progress for direct speech-to-speech conversation, with real-time\nspeech interaction experience and strong speech understanding ability. However,\ncurrent research focuses on discrete speech tokens to align with discrete text\ntokens for language modelling, which depends on an audio codec with residual\nconnections or independent group tokens, such a codec usually leverages large\nscale and diverse datasets training to ensure that the discrete speech codes\nhave good representation for varied domain, noise, style data reconstruction as\nwell as a well-designed codec quantizer and encoder-decoder architecture for\ndiscrete token language modelling. This paper introduces Flow-Omni, a\ncontinuous speech token based GPT-4o like model, capable of real-time speech\ninteraction and low streaming latency. Specifically, first, instead of\ncross-entropy loss only, we combine flow matching loss with a pretrained\nautoregressive LLM and a small MLP network to predict the probability\ndistribution of the continuous-valued speech tokens from speech prompt. second,\nwe incorporated the continuous speech tokens to Flow-Omni multi-modality\ntraining, thereby achieving robust speech-to-speech performance with discrete\ntext tokens and continuous speech tokens together. Experiments demonstrate\nthat, compared to discrete text and speech multi-modality training and its\nvariants, the continuous speech tokens mitigate robustness issues by avoiding\nthe inherent flaws of discrete speech code's representation loss for LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in GPT-4o like multi-modality models have demonstrated\nremarkable progress for direct speech-to-speech conversation, with real-time\nspeech interaction experience and strong speech understanding ability. However,\ncurrent research focuses on discrete speech tokens to align with discrete text\ntokens for language modelling, which depends on an audio codec with residual\nconnections or independent group tokens, such a codec usually leverages large\nscale and diverse datasets training to ensure that the discrete speech codes\nhave good representation for varied domain, noise, style data reconstruction as\nwell as a well-designed codec quantizer and encoder-decoder architecture for\ndiscrete token language modelling. This paper introduces Flow-Omni, a\ncontinuous speech token based GPT-4o like model, capable of real-time speech\ninteraction and low streaming latency. Specifically, first, instead of\ncross-entropy loss only, we combine flow matching loss with a pretrained\nautoregressive LLM and a small MLP network to predict the probability\ndistribution of the continuous-valued speech tokens from speech prompt. second,\nwe incorporated the continuous speech tokens to Flow-Omni multi-modality\ntraining, thereby achieving robust speech-to-speech performance with discrete\ntext tokens and continuous speech tokens together. Experiments demonstrate\nthat, compared to discrete text and speech multi-modality training and its\nvariants, the continuous speech tokens mitigate robustness issues by avoiding\nthe inherent flaws of discrete speech code's representation loss for LLM."
                },
                "authors": [
                    {
                        "name": "Ze Yuan"
                    },
                    {
                        "name": "Yanqing Liu"
                    },
                    {
                        "name": "Shujie Liu"
                    },
                    {
                        "name": "Sheng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Zhao"
                },
                "author": "Sheng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04917v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04917v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04905v1",
                "updated": "2024-12-06T10:01:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    10,
                    1,
                    38,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T10:01:38Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    10,
                    1,
                    38,
                    4,
                    341,
                    0
                ],
                "title": "DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling"
                },
                "summary": "Large language models (LLMs) have made dialogue one of the central modes of\nhuman-machine interaction, leading to the accumulation of vast amounts of\nconversation logs and increasing demand for dialogue generation. A\nconversational life-cycle spans from the Prelude through the Interlocution to\nthe Epilogue, encompassing various elements. Despite the existence of numerous\ndialogue-related studies, there is a lack of benchmarks that encompass\ncomprehensive dialogue elements, hindering precise modeling and systematic\nevaluation. To bridge this gap, we introduce an innovative research task\n$\\textbf{D}$ialogue $\\textbf{E}$lement $\\textbf{MO}$deling, including\n$\\textit{Element Awareness}$ and $\\textit{Dialogue Agent Interaction}$, and\npropose a novel benchmark, $\\textbf{DEMO}$, designed for a comprehensive\ndialogue modeling and assessment. Inspired by imitation learning, we further\nbuild the agent which possesses the adept ability to model dialogue elements\nbased on the DEMO benchmark. Extensive experiments indicate that existing LLMs\nstill exhibit considerable potential for enhancement, and our DEMO agent has\nsuperior performance in both in-domain and out-of-domain tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made dialogue one of the central modes of\nhuman-machine interaction, leading to the accumulation of vast amounts of\nconversation logs and increasing demand for dialogue generation. A\nconversational life-cycle spans from the Prelude through the Interlocution to\nthe Epilogue, encompassing various elements. Despite the existence of numerous\ndialogue-related studies, there is a lack of benchmarks that encompass\ncomprehensive dialogue elements, hindering precise modeling and systematic\nevaluation. To bridge this gap, we introduce an innovative research task\n$\\textbf{D}$ialogue $\\textbf{E}$lement $\\textbf{MO}$deling, including\n$\\textit{Element Awareness}$ and $\\textit{Dialogue Agent Interaction}$, and\npropose a novel benchmark, $\\textbf{DEMO}$, designed for a comprehensive\ndialogue modeling and assessment. Inspired by imitation learning, we further\nbuild the agent which possesses the adept ability to model dialogue elements\nbased on the DEMO benchmark. Extensive experiments indicate that existing LLMs\nstill exhibit considerable potential for enhancement, and our DEMO agent has\nsuperior performance in both in-domain and out-of-domain tasks."
                },
                "authors": [
                    {
                        "name": "Minzheng Wang"
                    },
                    {
                        "name": "Xinghua Zhang"
                    },
                    {
                        "name": "Kun Chen"
                    },
                    {
                        "name": "Nan Xu"
                    },
                    {
                        "name": "Haiyang Yu"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Wenji Mao"
                    },
                    {
                        "name": "Yongbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Li"
                },
                "author": "Yongbin Li",
                "arxiv_comment": "We release the code and data at https://github.com/MozerWang/DEMO",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04903v1",
                "updated": "2024-12-06T09:59:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    9,
                    59,
                    47,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T09:59:47Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    9,
                    59,
                    47,
                    4,
                    341,
                    0
                ],
                "title": "EACO: Enhancing Alignment in Multimodal LLMs via Critical Observation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EACO: Enhancing Alignment in Multimodal LLMs via Critical Observation"
                },
                "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress on\nvarious visual question answering and reasoning tasks leveraging instruction\nfine-tuning specific datasets. They can also learn from preference data\nannotated by human to enhance their reasoning ability and mitigate\nhallucinations. Most of preference data is generated from the model itself.\nHowever, existing methods require high-quality critical labels, which are\ncostly and rely on human or proprietary models like GPT-4V. In this work, we\npropose Enhancing Alignment in MLLMs via Critical Observation (EACO), which\naligns MLLMs by self-generated preference data using only 5k images\neconomically. Our approach begins with collecting and refining a Scoring\nEvaluation Instruction-tuning dataset to train a critical evaluation model,\ntermed the Critic. This Critic observes model responses across multiple\ndimensions, selecting preferred and non-preferred outputs for refined Direct\nPreference Optimization (DPO) tuning. To further enhance model performance, we\nemploy an additional supervised fine-tuning stage after preference tuning. EACO\nreduces the overall hallucinations by 65.6% on HallusionBench and improves the\nreasoning ability by 21.8% on MME-Cognition. EACO achieves an 8.5% improvement\nover LLaVA-v1.6-Mistral-7B across multiple benchmarks. Remarkably, EACO also\nshows the potential critical ability in open-source MLLMs, demonstrating that\nEACO is a viable path to boost the competence of MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have achieved remarkable progress on\nvarious visual question answering and reasoning tasks leveraging instruction\nfine-tuning specific datasets. They can also learn from preference data\nannotated by human to enhance their reasoning ability and mitigate\nhallucinations. Most of preference data is generated from the model itself.\nHowever, existing methods require high-quality critical labels, which are\ncostly and rely on human or proprietary models like GPT-4V. In this work, we\npropose Enhancing Alignment in MLLMs via Critical Observation (EACO), which\naligns MLLMs by self-generated preference data using only 5k images\neconomically. Our approach begins with collecting and refining a Scoring\nEvaluation Instruction-tuning dataset to train a critical evaluation model,\ntermed the Critic. This Critic observes model responses across multiple\ndimensions, selecting preferred and non-preferred outputs for refined Direct\nPreference Optimization (DPO) tuning. To further enhance model performance, we\nemploy an additional supervised fine-tuning stage after preference tuning. EACO\nreduces the overall hallucinations by 65.6% on HallusionBench and improves the\nreasoning ability by 21.8% on MME-Cognition. EACO achieves an 8.5% improvement\nover LLaVA-v1.6-Mistral-7B across multiple benchmarks. Remarkably, EACO also\nshows the potential critical ability in open-source MLLMs, demonstrating that\nEACO is a viable path to boost the competence of MLLMs."
                },
                "authors": [
                    {
                        "name": "Yongxin Wang"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Haokun Lin"
                    },
                    {
                        "name": "Mingfei Han"
                    },
                    {
                        "name": "Liang Ma"
                    },
                    {
                        "name": "Jin Jiang"
                    },
                    {
                        "name": "Yuhao Cheng"
                    },
                    {
                        "name": "Xiaodan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Liang"
                },
                "author": "Xiaodan Liang",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05526v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05526v3",
                "updated": "2024-12-06T09:34:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    9,
                    34,
                    43,
                    4,
                    341,
                    0
                ],
                "published": "2024-05-09T03:34:09Z",
                "published_parsed": [
                    2024,
                    5,
                    9,
                    3,
                    34,
                    9,
                    3,
                    130,
                    0
                ],
                "title": "Benchmarking Neural Radiance Fields for Autonomous Robots: An Overview",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Neural Radiance Fields for Autonomous Robots: An Overview"
                },
                "summary": "Neural Radiance Fields (NeRF) have emerged as a powerful paradigm for 3D\nscene representation, offering high-fidelity renderings and reconstructions\nfrom a set of sparse and unstructured sensor data. In the context of autonomous\nrobotics, where perception and understanding of the environment are pivotal,\nNeRF holds immense promise for improving performance. In this paper, we present\na comprehensive survey and analysis of the state-of-the-art techniques for\nutilizing NeRF to enhance the capabilities of autonomous robots. We especially\nfocus on the perception, localization and navigation, and decision-making\nmodules of autonomous robots and delve into tasks crucial for autonomous\noperation, including 3D reconstruction, segmentation, pose estimation,\nsimultaneous localization and mapping (SLAM), navigation and planning, and\ninteraction. Our survey meticulously benchmarks existing NeRF-based methods,\nproviding insights into their strengths and limitations. Moreover, we explore\npromising avenues for future research and development in this domain. Notably,\nwe discuss the integration of advanced techniques such as 3D Gaussian splatting\n(3DGS), large language models (LLM), and generative AIs, envisioning enhanced\nreconstruction efficiency, scene understanding, decision-making capabilities.\nThis survey serves as a roadmap for researchers seeking to leverage NeRFs to\nempower autonomous robots, paving the way for innovative solutions that can\nnavigate and interact seamlessly in complex environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Radiance Fields (NeRF) have emerged as a powerful paradigm for 3D\nscene representation, offering high-fidelity renderings and reconstructions\nfrom a set of sparse and unstructured sensor data. In the context of autonomous\nrobotics, where perception and understanding of the environment are pivotal,\nNeRF holds immense promise for improving performance. In this paper, we present\na comprehensive survey and analysis of the state-of-the-art techniques for\nutilizing NeRF to enhance the capabilities of autonomous robots. We especially\nfocus on the perception, localization and navigation, and decision-making\nmodules of autonomous robots and delve into tasks crucial for autonomous\noperation, including 3D reconstruction, segmentation, pose estimation,\nsimultaneous localization and mapping (SLAM), navigation and planning, and\ninteraction. Our survey meticulously benchmarks existing NeRF-based methods,\nproviding insights into their strengths and limitations. Moreover, we explore\npromising avenues for future research and development in this domain. Notably,\nwe discuss the integration of advanced techniques such as 3D Gaussian splatting\n(3DGS), large language models (LLM), and generative AIs, envisioning enhanced\nreconstruction efficiency, scene understanding, decision-making capabilities.\nThis survey serves as a roadmap for researchers seeking to leverage NeRFs to\nempower autonomous robots, paving the way for innovative solutions that can\nnavigate and interact seamlessly in complex environments."
                },
                "authors": [
                    {
                        "name": "Yuhang Ming"
                    },
                    {
                        "name": "Xingrui Yang"
                    },
                    {
                        "name": "Weihan Wang"
                    },
                    {
                        "name": "Zheng Chen"
                    },
                    {
                        "name": "Jinglun Feng"
                    },
                    {
                        "name": "Yifan Xing"
                    },
                    {
                        "name": "Guofeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Guofeng Zhang"
                },
                "author": "Guofeng Zhang",
                "arxiv_doi": "10.1016/j.engappai.2024.109685",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.engappai.2024.109685",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.05526v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05526v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "32 pages, 5 figures, 8 tables",
                "arxiv_journal_ref": "Engineering Applications of Artificial Intelligence, Volume 140,\n  15 January 2025, 109685",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20936v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20936v2",
                "updated": "2024-12-06T09:06:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    9,
                    6,
                    20,
                    4,
                    341,
                    0
                ],
                "published": "2024-10-28T11:37:39Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    11,
                    37,
                    39,
                    0,
                    302,
                    0
                ],
                "title": "Autoformalize Mathematical Statements by Symbolic Equivalence and\n  Semantic Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoformalize Mathematical Statements by Symbolic Equivalence and\n  Semantic Consistency"
                },
                "summary": "Autoformalization, the task of automatically translating natural language\ndescriptions into a formal language, poses a significant challenge across\nvarious domains, especially in mathematics. Recent advancements in large\nlanguage models (LLMs) have unveiled their promising capabilities to formalize\neven competition-level math problems. However, we observe a considerable\ndiscrepancy between pass@1 and pass@k accuracies in LLM-generated\nformalizations. To address this gap, we introduce a novel framework that scores\nand selects the best result from k autoformalization candidates based on two\ncomplementary self-consistency methods: symbolic equivalence and semantic\nconsistency. Elaborately, symbolic equivalence identifies the logical\nhomogeneity among autoformalization candidates using automated theorem provers,\nand semantic consistency evaluates the preservation of the original meaning by\ninformalizing the candidates and computing the similarity between the\nembeddings of the original and informalized texts. Our extensive experiments on\nthe MATH and miniF2F datasets demonstrate that our approach significantly\nenhances autoformalization accuracy, achieving up to 0.22-1.35x relative\nimprovements across various LLMs and baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoformalization, the task of automatically translating natural language\ndescriptions into a formal language, poses a significant challenge across\nvarious domains, especially in mathematics. Recent advancements in large\nlanguage models (LLMs) have unveiled their promising capabilities to formalize\neven competition-level math problems. However, we observe a considerable\ndiscrepancy between pass@1 and pass@k accuracies in LLM-generated\nformalizations. To address this gap, we introduce a novel framework that scores\nand selects the best result from k autoformalization candidates based on two\ncomplementary self-consistency methods: symbolic equivalence and semantic\nconsistency. Elaborately, symbolic equivalence identifies the logical\nhomogeneity among autoformalization candidates using automated theorem provers,\nand semantic consistency evaluates the preservation of the original meaning by\ninformalizing the candidates and computing the similarity between the\nembeddings of the original and informalized texts. Our extensive experiments on\nthe MATH and miniF2F datasets demonstrate that our approach significantly\nenhances autoformalization accuracy, achieving up to 0.22-1.35x relative\nimprovements across various LLMs and baseline methods."
                },
                "authors": [
                    {
                        "name": "Zenan Li"
                    },
                    {
                        "name": "Yifan Wu"
                    },
                    {
                        "name": "Zhaoyu Li"
                    },
                    {
                        "name": "Xinming Wei"
                    },
                    {
                        "name": "Xian Zhang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Xiaoxing Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxing Ma"
                },
                "author": "Xiaoxing Ma",
                "arxiv_comment": "Published as a conference paper at NeurIPS 2024. Code is available at\n  https://github.com/Miracle-Messi/Isa-AutoFormal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20936v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20936v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07623v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07623v2",
                "updated": "2024-12-06T09:04:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    9,
                    4,
                    55,
                    4,
                    341,
                    0
                ],
                "published": "2024-05-13T10:30:33Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    10,
                    30,
                    33,
                    0,
                    134,
                    0
                ],
                "title": "COBias and Debias: Minimizing Language Model Pairwise Accuracy Bias via\n  Nonlinear Integer Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COBias and Debias: Minimizing Language Model Pairwise Accuracy Bias via\n  Nonlinear Integer Programming"
                },
                "summary": "When performing classification tasks with language models, would you prefer\nhaving only one highly accurate class or having every class deliver reliable\nperformance? Obviously, a more balanced accuracy among classes better reflects\nthe expectations of the majority of users. Especially for large language models\n(LLMs), the fact that they achieve a fair overall accuracy by in-context\nlearning (ICL) obscures a large difference in individual class accuracies. In\nthis work, we uncover and tackle language models' imbalance in per-class\nprediction accuracy by reconceptualizing it as the Contextual Oddity Bias\n(COBias), and we are the first to engage nonlinear integer programming (NIP) to\ndebias it. Briefly, the proposed COBias metric measures accuracy differences\namong class pairs, with which we reveal the large per-class accuracy\ndifferences exhibited in LLMs of varied scales and families. Then we propose\nDebiasing as Nonlinear Integer Programming (DNIP) to correct ICL per-class\nprobabilities towards lower COBias and higher overall accuracy. Our\noptimization objective is directly based on the evaluation scores by COBias and\naccuracy metrics, which is non-differentiable and solved by the simulated\nannealing metaheuristic. Evaluations on three LLMs across seven NLP\nclassification tasks show that DNIP simultaneously achieves significant COBias\nreduction (-27%) and accuracy improvement (+12%) over the conventional ICL\napproach, suggesting that modeling pairwise class accuracy differences is a\ndirection in pushing forward more accurate, more reliable LLM predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When performing classification tasks with language models, would you prefer\nhaving only one highly accurate class or having every class deliver reliable\nperformance? Obviously, a more balanced accuracy among classes better reflects\nthe expectations of the majority of users. Especially for large language models\n(LLMs), the fact that they achieve a fair overall accuracy by in-context\nlearning (ICL) obscures a large difference in individual class accuracies. In\nthis work, we uncover and tackle language models' imbalance in per-class\nprediction accuracy by reconceptualizing it as the Contextual Oddity Bias\n(COBias), and we are the first to engage nonlinear integer programming (NIP) to\ndebias it. Briefly, the proposed COBias metric measures accuracy differences\namong class pairs, with which we reveal the large per-class accuracy\ndifferences exhibited in LLMs of varied scales and families. Then we propose\nDebiasing as Nonlinear Integer Programming (DNIP) to correct ICL per-class\nprobabilities towards lower COBias and higher overall accuracy. Our\noptimization objective is directly based on the evaluation scores by COBias and\naccuracy metrics, which is non-differentiable and solved by the simulated\nannealing metaheuristic. Evaluations on three LLMs across seven NLP\nclassification tasks show that DNIP simultaneously achieves significant COBias\nreduction (-27%) and accuracy improvement (+12%) over the conventional ICL\napproach, suggesting that modeling pairwise class accuracy differences is a\ndirection in pushing forward more accurate, more reliable LLM predictions."
                },
                "authors": [
                    {
                        "name": "Ruixi Lin"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07623v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07623v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04871v1",
                "updated": "2024-12-06T09:04:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    9,
                    4,
                    12,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T09:04:12Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    9,
                    4,
                    12,
                    4,
                    341,
                    0
                ],
                "title": "Building a Family of Data Augmentation Models for Low-cost LLM\n  Fine-tuning on the Cloud",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a Family of Data Augmentation Models for Low-cost LLM\n  Fine-tuning on the Cloud"
                },
                "summary": "Specializing LLMs in various domain-specific tasks has emerged as a critical\nstep towards achieving high performance. However, the construction and\nannotation of datasets in specific domains are always very costly. Apart from\nusing superior and expensive closed-source LLM APIs to construct datasets, some\nopen-source models have become strong enough to handle dataset construction in\nmany scenarios. Thus, we present a family of data augmentation models designed\nto significantly improve the efficiency for model fine-tuning. These models,\ntrained based on sufficiently small LLMs, support key functionalities with low\ninference costs: instruction expansion, instruction refinement, and\ninstruction-response pair expansion. To fulfill this goal, we first construct\nan automatic data collection system with seed datasets generated from both\npublic repositories and our in-house datasets. This system leverages powerful\nLLMs to expand, refine and re-write the instructions and responses,\nincorporating quality assessment techniques. Following this, we introduce the\ntraining process of our models, which effectively distills task-solving and\ntext synthesis abilities from teacher LLMs. Finally, we demonstrate how we\nintegrate these functionalities into a machine learning platform to support\nlow-cost LLM fine-tuning from both dataset preparation and training\nperspectives for users. Experiments and an application study prove the\neffectiveness of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Specializing LLMs in various domain-specific tasks has emerged as a critical\nstep towards achieving high performance. However, the construction and\nannotation of datasets in specific domains are always very costly. Apart from\nusing superior and expensive closed-source LLM APIs to construct datasets, some\nopen-source models have become strong enough to handle dataset construction in\nmany scenarios. Thus, we present a family of data augmentation models designed\nto significantly improve the efficiency for model fine-tuning. These models,\ntrained based on sufficiently small LLMs, support key functionalities with low\ninference costs: instruction expansion, instruction refinement, and\ninstruction-response pair expansion. To fulfill this goal, we first construct\nan automatic data collection system with seed datasets generated from both\npublic repositories and our in-house datasets. This system leverages powerful\nLLMs to expand, refine and re-write the instructions and responses,\nincorporating quality assessment techniques. Following this, we introduce the\ntraining process of our models, which effectively distills task-solving and\ntext synthesis abilities from teacher LLMs. Finally, we demonstrate how we\nintegrate these functionalities into a machine learning platform to support\nlow-cost LLM fine-tuning from both dataset preparation and training\nperspectives for users. Experiments and an application study prove the\neffectiveness of our approach."
                },
                "authors": [
                    {
                        "name": "Yuanhao Yue"
                    },
                    {
                        "name": "Chengyu Wang"
                    },
                    {
                        "name": "Jun Huang"
                    },
                    {
                        "name": "Peng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Wang"
                },
                "author": "Peng Wang",
                "arxiv_comment": "coling 2025 industry track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01497v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01497v2",
                "updated": "2024-12-06T08:53:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    8,
                    53,
                    43,
                    4,
                    341,
                    0
                ],
                "published": "2024-09-02T23:37:20Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    23,
                    37,
                    20,
                    0,
                    246,
                    0
                ],
                "title": "DiversityMedQA: Assessing Demographic Biases in Medical Diagnosis using\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiversityMedQA: Assessing Demographic Biases in Medical Diagnosis using\n  Large Language Models"
                },
                "summary": "As large language models (LLMs) gain traction in healthcare, concerns about\ntheir susceptibility to demographic biases are growing. We introduce\n{DiversityMedQA}, a novel benchmark designed to assess LLM responses to medical\nqueries across diverse patient demographics, such as gender and ethnicity. By\nperturbing questions from the MedQA dataset, which comprises medical board exam\nquestions, we created a benchmark that captures the nuanced differences in\nmedical diagnosis across varying patient profiles. Our findings reveal notable\ndiscrepancies in model performance when tested against these demographic\nvariations. Furthermore, to ensure the perturbations were accurate, we also\npropose a filtering strategy that validates each perturbation. By releasing\nDiversityMedQA, we provide a resource for evaluating and mitigating demographic\nbias in LLM medical diagnoses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) gain traction in healthcare, concerns about\ntheir susceptibility to demographic biases are growing. We introduce\n{DiversityMedQA}, a novel benchmark designed to assess LLM responses to medical\nqueries across diverse patient demographics, such as gender and ethnicity. By\nperturbing questions from the MedQA dataset, which comprises medical board exam\nquestions, we created a benchmark that captures the nuanced differences in\nmedical diagnosis across varying patient profiles. Our findings reveal notable\ndiscrepancies in model performance when tested against these demographic\nvariations. Furthermore, to ensure the perturbations were accurate, we also\npropose a filtering strategy that validates each perturbation. By releasing\nDiversityMedQA, we provide a resource for evaluating and mitigating demographic\nbias in LLM medical diagnoses."
                },
                "authors": [
                    {
                        "name": "Rajat Rawat"
                    },
                    {
                        "name": "Hudson McBride"
                    },
                    {
                        "name": "Dhiyaan Nirmal"
                    },
                    {
                        "name": "Rajarshi Ghosh"
                    },
                    {
                        "name": "Jong Moon"
                    },
                    {
                        "name": "Dhruv Alamuri"
                    },
                    {
                        "name": "Sean O'Brien"
                    },
                    {
                        "name": "Kevin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Zhu"
                },
                "author": "Kevin Zhu",
                "arxiv_comment": "Published in NLP4PI @ EMNLP 2024, Accepted to AIM-FM @ NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01497v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01497v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04859v1",
                "updated": "2024-12-06T08:52:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    8,
                    52,
                    30,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T08:52:30Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    8,
                    52,
                    30,
                    4,
                    341,
                    0
                ],
                "title": "Breaking Event Rumor Detection via Stance-Separated Multi-Agent Debate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Event Rumor Detection via Stance-Separated Multi-Agent Debate"
                },
                "summary": "The rapid spread of rumors on social media platforms during breaking events\nseverely hinders the dissemination of the truth. Previous studies reveal that\nthe lack of annotated resources hinders the direct detection of unforeseen\nbreaking events not covered in yesterday's news. Leveraging large language\nmodels (LLMs) for rumor detection holds significant promise. However, it is\nchallenging for LLMs to provide comprehensive responses to complex or\ncontroversial issues due to limited diversity. In this work, we propose the\nStance Separated Multi-Agent Debate (S2MAD) to address this issue.\nSpecifically, we firstly introduce Stance Separation, categorizing comments as\neither supporting or opposing the original claim. Subsequently, claims are\nclassified as subjective or objective, enabling agents to generate reasonable\ninitial viewpoints with different prompt strategies for each type of claim.\nDebaters then follow specific instructions through multiple rounds of debate to\nreach a consensus. If a consensus is not reached, a judge agent evaluates the\nopinions and delivers a final verdict on the claim's veracity. Extensive\nexperiments conducted on two real-world datasets demonstrate that our proposed\nmodel outperforms state-of-the-art methods in terms of performance and\neffectively improves the performance of LLMs in breaking event rumor detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid spread of rumors on social media platforms during breaking events\nseverely hinders the dissemination of the truth. Previous studies reveal that\nthe lack of annotated resources hinders the direct detection of unforeseen\nbreaking events not covered in yesterday's news. Leveraging large language\nmodels (LLMs) for rumor detection holds significant promise. However, it is\nchallenging for LLMs to provide comprehensive responses to complex or\ncontroversial issues due to limited diversity. In this work, we propose the\nStance Separated Multi-Agent Debate (S2MAD) to address this issue.\nSpecifically, we firstly introduce Stance Separation, categorizing comments as\neither supporting or opposing the original claim. Subsequently, claims are\nclassified as subjective or objective, enabling agents to generate reasonable\ninitial viewpoints with different prompt strategies for each type of claim.\nDebaters then follow specific instructions through multiple rounds of debate to\nreach a consensus. If a consensus is not reached, a judge agent evaluates the\nopinions and delivers a final verdict on the claim's veracity. Extensive\nexperiments conducted on two real-world datasets demonstrate that our proposed\nmodel outperforms state-of-the-art methods in terms of performance and\neffectively improves the performance of LLMs in breaking event rumor detection."
                },
                "authors": [
                    {
                        "name": "Mingqing Zhang"
                    },
                    {
                        "name": "Haisong Gong"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Shu Wu"
                    },
                    {
                        "name": "Liang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wang"
                },
                "author": "Liang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04857v1",
                "updated": "2024-12-06T08:49:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    8,
                    49,
                    49,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T08:49:49Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    8,
                    49,
                    49,
                    4,
                    341,
                    0
                ],
                "title": "Neuro-Symbolic Data Generation for Math Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuro-Symbolic Data Generation for Math Reasoning"
                },
                "summary": "A critical question about Large Language Models (LLMs) is whether their\napparent deficiency in mathematical reasoning is inherent, or merely a result\nof insufficient exposure to high-quality mathematical data. To explore this, we\ndeveloped an automated method for generating high-quality, supervised\nmathematical datasets. The method carefully mutates existing math problems,\nensuring both diversity and validity of the newly generated problems. This is\nachieved by a neuro-symbolic data generation framework combining the intuitive\ninformalization strengths of LLMs, and the precise symbolic reasoning of math\nsolvers along with projected Markov chain Monte Carlo sampling in the\nhighly-irregular symbolic space. Empirical experiments demonstrate the high\nquality of data generated by the proposed method, and that the LLMs,\nspecifically LLaMA-2 and Mistral, when realigned with the generated data,\nsurpass their state-of-the-art counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical question about Large Language Models (LLMs) is whether their\napparent deficiency in mathematical reasoning is inherent, or merely a result\nof insufficient exposure to high-quality mathematical data. To explore this, we\ndeveloped an automated method for generating high-quality, supervised\nmathematical datasets. The method carefully mutates existing math problems,\nensuring both diversity and validity of the newly generated problems. This is\nachieved by a neuro-symbolic data generation framework combining the intuitive\ninformalization strengths of LLMs, and the precise symbolic reasoning of math\nsolvers along with projected Markov chain Monte Carlo sampling in the\nhighly-irregular symbolic space. Empirical experiments demonstrate the high\nquality of data generated by the proposed method, and that the LLMs,\nspecifically LLaMA-2 and Mistral, when realigned with the generated data,\nsurpass their state-of-the-art counterparts."
                },
                "authors": [
                    {
                        "name": "Zenan Li"
                    },
                    {
                        "name": "Zhi Zhou"
                    },
                    {
                        "name": "Yuan Yao"
                    },
                    {
                        "name": "Yu-Feng Li"
                    },
                    {
                        "name": "Chun Cao"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Xian Zhang"
                    },
                    {
                        "name": "Xiaoxing Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxing Ma"
                },
                "author": "Xiaoxing Ma",
                "arxiv_comment": "Published as a conference paper at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04856v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04856v1",
                "updated": "2024-12-06T08:48:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    8,
                    48,
                    49,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T08:48:49Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    8,
                    48,
                    49,
                    4,
                    341,
                    0
                ],
                "title": "Can Large Language Models Effectively Process and Execute Financial\n  Trading Instructions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Effectively Process and Execute Financial\n  Trading Instructions?"
                },
                "summary": "The development of Large Language Models (LLMs) has created transformative\nopportunities for the financial industry, especially in the area of financial\ntrading. However, how to integrate LLMs with trading systems has become a\nchallenge. To address this problem, we propose an intelligent trade order\nrecognition pipeline that enables the conversion of trade orders into a\nstandard format in trade execution. The system improves the ability of human\ntraders to interact with trading platforms while addressing the problem of\nmisinformation acquisition in trade execution. In addition, we have created a\ntrade order dataset of 500 pieces of data to simulate real-world trading\nscenarios. Moreover, we designed several metrics to provide a comprehensive\nassessment of dataset reliability and the generative power of big models in\nfinance by experimenting with five state-of-the-art LLMs on our dataset. The\nresults indicate that while LLMs demonstrate high generation rates (87.50% to\n98.33%) and perfect follow-up rates, they face significant challenges in\naccuracy (5% to 10%) and completeness, with high missing rates (14.29% to\n67.29%). In addition, LLMs tend to over-interrogate, suggesting that large\nmodels tend to collect more information, carrying certain challenges for\ninformation security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of Large Language Models (LLMs) has created transformative\nopportunities for the financial industry, especially in the area of financial\ntrading. However, how to integrate LLMs with trading systems has become a\nchallenge. To address this problem, we propose an intelligent trade order\nrecognition pipeline that enables the conversion of trade orders into a\nstandard format in trade execution. The system improves the ability of human\ntraders to interact with trading platforms while addressing the problem of\nmisinformation acquisition in trade execution. In addition, we have created a\ntrade order dataset of 500 pieces of data to simulate real-world trading\nscenarios. Moreover, we designed several metrics to provide a comprehensive\nassessment of dataset reliability and the generative power of big models in\nfinance by experimenting with five state-of-the-art LLMs on our dataset. The\nresults indicate that while LLMs demonstrate high generation rates (87.50% to\n98.33%) and perfect follow-up rates, they face significant challenges in\naccuracy (5% to 10%) and completeness, with high missing rates (14.29% to\n67.29%). In addition, LLMs tend to over-interrogate, suggesting that large\nmodels tend to collect more information, carrying certain challenges for\ninformation security."
                },
                "authors": [
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Ge Wang"
                    },
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Yuda Wang"
                    },
                    {
                        "name": "Mingwen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Mingwen Liu"
                },
                "author": "Mingwen Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04856v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04856v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02580v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02580v2",
                "updated": "2024-12-06T08:41:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    8,
                    41,
                    1,
                    4,
                    341,
                    0
                ],
                "published": "2024-05-04T06:28:27Z",
                "published_parsed": [
                    2024,
                    5,
                    4,
                    6,
                    28,
                    27,
                    5,
                    125,
                    0
                ],
                "title": "PropertyGPT: LLM-driven Formal Verification of Smart Contracts through\n  Retrieval-Augmented Property Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PropertyGPT: LLM-driven Formal Verification of Smart Contracts through\n  Retrieval-Augmented Property Generation"
                },
                "summary": "With recent advances in large language models (LLMs), this paper explores the\npotential of leveraging state-of-the-art LLMs,such as GPT-4, to transfer\nexisting human-written properties (e.g.,those from Certora auditing reports)\nand automatically generate customized properties for unknown code. To this end,\nwe embed existing properties into a vector database and retrieve a reference\nproperty for LLM-based in-context learning to generate a new property for a\ngiven code. While this basic process is relatively straightforward, ensuring\nthat the generated properties are (i) compilable, (ii) appropriate, and (iii)\nverifiable presents challenges. To address (i), we use the compilation and\nstatic analysis feedback as an external oracle to guide LLMs in iteratively\nrevising the generated properties. For (ii), we consider multiple dimensions of\nsimilarity to rank the properties and employ a weighted algorithm to identify\nthe top-K properties as the final result. For (iii), we design a dedicated\nprover to formally verify the correctness of the generated properties. We have\nimplemented these strategies into a novel LLM-based property generation tool\ncalled PropertyGPT. Our experiments show that PropertyGPT can generate\ncomprehensive and high-quality properties, achieving an 80% recall compared to\nthe ground truth. It successfully detected 26 CVEs/attack incidents out of 37\ntested and also uncovered 12 zero-day vulnerabilities, leading to $8,256 in bug\nbounty rewards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With recent advances in large language models (LLMs), this paper explores the\npotential of leveraging state-of-the-art LLMs,such as GPT-4, to transfer\nexisting human-written properties (e.g.,those from Certora auditing reports)\nand automatically generate customized properties for unknown code. To this end,\nwe embed existing properties into a vector database and retrieve a reference\nproperty for LLM-based in-context learning to generate a new property for a\ngiven code. While this basic process is relatively straightforward, ensuring\nthat the generated properties are (i) compilable, (ii) appropriate, and (iii)\nverifiable presents challenges. To address (i), we use the compilation and\nstatic analysis feedback as an external oracle to guide LLMs in iteratively\nrevising the generated properties. For (ii), we consider multiple dimensions of\nsimilarity to rank the properties and employ a weighted algorithm to identify\nthe top-K properties as the final result. For (iii), we design a dedicated\nprover to formally verify the correctness of the generated properties. We have\nimplemented these strategies into a novel LLM-based property generation tool\ncalled PropertyGPT. Our experiments show that PropertyGPT can generate\ncomprehensive and high-quality properties, achieving an 80% recall compared to\nthe ground truth. It successfully detected 26 CVEs/attack incidents out of 37\ntested and also uncovered 12 zero-day vulnerabilities, leading to $8,256 in bug\nbounty rewards."
                },
                "authors": [
                    {
                        "name": "Ye Liu"
                    },
                    {
                        "name": "Yue Xue"
                    },
                    {
                        "name": "Daoyuan Wu"
                    },
                    {
                        "name": "Yuqiang Sun"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Miaolei Shi"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_doi": "10.14722/ndss.2025.241357",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14722/ndss.2025.241357",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.02580v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02580v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by NDSS Symposium 2025. Please cite the conference version\n  of this paper, e.g., \"Ye Liu, Yue Xue, Daoyuan Wu, Yuqiang Sun, Yi Li,\n  Miaolei Shi, Yang Liu. PropertyGPT: LLM-driven Formal Verification of Smart\n  Contracts through Retrieval-Augmented Property Generation. In 32nd Annual\n  Network and Distributed System Security Symposium (NDSS 2025).\"",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01084v2",
                "updated": "2024-12-06T08:39:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    8,
                    39,
                    13,
                    4,
                    341,
                    0
                ],
                "published": "2024-11-01T23:53:00Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    23,
                    53,
                    0,
                    4,
                    306,
                    0
                ],
                "title": "Plentiful Jailbreaks with String Compositions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plentiful Jailbreaks with String Compositions"
                },
                "summary": "Large language models (LLMs) remain vulnerable to a slew of adversarial\nattacks and jailbreaking methods. One common approach employed by white-hat\nattackers, or red-teamers, is to process model inputs and outputs using\nstring-level obfuscations, which can include leetspeak, rotary ciphers, Base64,\nASCII, and more. Our work extends these encoding-based attacks by unifying them\nin a framework of invertible string transformations. With invertibility, we can\ndevise arbitrary string compositions, defined as sequences of transformations,\nthat we can encode and decode end-to-end programmatically. We devise a\nautomated best-of-n attack that samples from a combinatorially large number of\nstring compositions. Our jailbreaks obtain competitive attack success rates on\nseveral leading frontier models when evaluated on HarmBench, highlighting that\nencoding-based attacks remain a persistent vulnerability even in advanced LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) remain vulnerable to a slew of adversarial\nattacks and jailbreaking methods. One common approach employed by white-hat\nattackers, or red-teamers, is to process model inputs and outputs using\nstring-level obfuscations, which can include leetspeak, rotary ciphers, Base64,\nASCII, and more. Our work extends these encoding-based attacks by unifying them\nin a framework of invertible string transformations. With invertibility, we can\ndevise arbitrary string compositions, defined as sequences of transformations,\nthat we can encode and decode end-to-end programmatically. We devise a\nautomated best-of-n attack that samples from a combinatorially large number of\nstring compositions. Our jailbreaks obtain competitive attack success rates on\nseveral leading frontier models when evaluated on HarmBench, highlighting that\nencoding-based attacks remain a persistent vulnerability even in advanced LLMs."
                },
                "authors": [
                    {
                        "name": "Brian R. Y. Huang"
                    }
                ],
                "author_detail": {
                    "name": "Brian R. Y. Huang"
                },
                "author": "Brian R. Y. Huang",
                "arxiv_comment": "NeurIPS SoLaR Workshop 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03205v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03205v2",
                "updated": "2024-12-06T08:29:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    8,
                    29,
                    43,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-04T10:44:50Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    44,
                    50,
                    2,
                    339,
                    0
                ],
                "title": "U-MATH: A University-Level Benchmark for Evaluating Mathematical Skills\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U-MATH: A University-Level Benchmark for Evaluating Mathematical Skills\n  in LLMs"
                },
                "summary": "The current evaluation of mathematical skills in LLMs is limited, as existing\nbenchmarks are either relatively small, primarily focus on elementary and\nhigh-school problems, or lack diversity in topics. Additionally, the inclusion\nof visual elements in tasks remains largely under-explored.\n  To address these gaps, we introduce U-MATH, a novel benchmark of 1,100\nunpublished open-ended university-level problems sourced from teaching\nmaterials. It is balanced across six core subjects, with 20% of multimodal\nproblems. Given the open-ended nature of U-MATH problems, we employ an LLM to\njudge the correctness of generated solutions. To this end, we release\n$\\mu$-MATH, a dataset to evaluate the LLMs' capabilities in judging solutions.\n  The evaluation of general domain, math-specific, and multimodal LLMs\nhighlights the challenges presented by U-MATH. Our findings reveal that LLMs\nachieve a maximum accuracy of only 63% on text-based tasks, with even lower 45%\non visual problems. The solution assessment proves challenging for LLMs, with\nthe best LLM judge having an F1-score of 80% on $\\mu$-MATH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current evaluation of mathematical skills in LLMs is limited, as existing\nbenchmarks are either relatively small, primarily focus on elementary and\nhigh-school problems, or lack diversity in topics. Additionally, the inclusion\nof visual elements in tasks remains largely under-explored.\n  To address these gaps, we introduce U-MATH, a novel benchmark of 1,100\nunpublished open-ended university-level problems sourced from teaching\nmaterials. It is balanced across six core subjects, with 20% of multimodal\nproblems. Given the open-ended nature of U-MATH problems, we employ an LLM to\njudge the correctness of generated solutions. To this end, we release\n$\\mu$-MATH, a dataset to evaluate the LLMs' capabilities in judging solutions.\n  The evaluation of general domain, math-specific, and multimodal LLMs\nhighlights the challenges presented by U-MATH. Our findings reveal that LLMs\nachieve a maximum accuracy of only 63% on text-based tasks, with even lower 45%\non visual problems. The solution assessment proves challenging for LLMs, with\nthe best LLM judge having an F1-score of 80% on $\\mu$-MATH."
                },
                "authors": [
                    {
                        "name": "Konstantin Chernyshev"
                    },
                    {
                        "name": "Vitaliy Polshkov"
                    },
                    {
                        "name": "Ekaterina Artemova"
                    },
                    {
                        "name": "Alex Myasnikov"
                    },
                    {
                        "name": "Vlad Stepanov"
                    },
                    {
                        "name": "Alexei Miasnikov"
                    },
                    {
                        "name": "Sergei Tilga"
                    }
                ],
                "author_detail": {
                    "name": "Sergei Tilga"
                },
                "author": "Sergei Tilga",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03205v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03205v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17341v2",
                "updated": "2024-12-06T08:07:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    8,
                    7,
                    23,
                    4,
                    341,
                    0
                ],
                "published": "2024-06-25T07:54:32Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    7,
                    54,
                    32,
                    1,
                    177,
                    0
                ],
                "title": "Generative Modelling of Structurally Constrained Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Modelling of Structurally Constrained Graphs"
                },
                "summary": "Graph diffusion models have emerged as state-of-the-art techniques in graph\ngeneration; yet, integrating domain knowledge into these models remains\nchallenging. Domain knowledge is particularly important in real-world\nscenarios, where invalid generated graphs hinder deployment in practical\napplications. Unconstrained and conditioned graph diffusion models fail to\nguarantee such domain-specific structural properties. We present ConStruct, a\nnovel framework that enables graph diffusion models to incorporate hard\nconstraints on specific properties, such as planarity or acyclicity. Our\napproach ensures that the sampled graphs remain within the domain of graphs\nthat satisfy the specified property throughout the entire trajectory in both\nthe forward and reverse processes. This is achieved by introducing an\nedge-absorbing noise model and a new projector operator. ConStruct demonstrates\nversatility across several structural and edge-deletion invariant constraints\nand achieves state-of-the-art performance for both synthetic benchmarks and\nattributed real-world datasets. For example, by incorporating planarity\nconstraints in digital pathology graph datasets, the proposed method\noutperforms existing baselines, improving data validity by up to 71.1\npercentage points.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph diffusion models have emerged as state-of-the-art techniques in graph\ngeneration; yet, integrating domain knowledge into these models remains\nchallenging. Domain knowledge is particularly important in real-world\nscenarios, where invalid generated graphs hinder deployment in practical\napplications. Unconstrained and conditioned graph diffusion models fail to\nguarantee such domain-specific structural properties. We present ConStruct, a\nnovel framework that enables graph diffusion models to incorporate hard\nconstraints on specific properties, such as planarity or acyclicity. Our\napproach ensures that the sampled graphs remain within the domain of graphs\nthat satisfy the specified property throughout the entire trajectory in both\nthe forward and reverse processes. This is achieved by introducing an\nedge-absorbing noise model and a new projector operator. ConStruct demonstrates\nversatility across several structural and edge-deletion invariant constraints\nand achieves state-of-the-art performance for both synthetic benchmarks and\nattributed real-world datasets. For example, by incorporating planarity\nconstraints in digital pathology graph datasets, the proposed method\noutperforms existing baselines, improving data validity by up to 71.1\npercentage points."
                },
                "authors": [
                    {
                        "name": "Manuel Madeira"
                    },
                    {
                        "name": "Clement Vignac"
                    },
                    {
                        "name": "Dorina Thanou"
                    },
                    {
                        "name": "Pascal Frossard"
                    }
                ],
                "author_detail": {
                    "name": "Pascal Frossard"
                },
                "author": "Pascal Frossard",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04832v1",
                "updated": "2024-12-06T07:56:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    7,
                    56,
                    14,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T07:56:14Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    7,
                    56,
                    14,
                    4,
                    341,
                    0
                ],
                "title": "WRF-GS: Wireless Radiation Field Reconstruction with 3D Gaussian\n  Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WRF-GS: Wireless Radiation Field Reconstruction with 3D Gaussian\n  Splatting"
                },
                "summary": "Wireless channel modeling plays a pivotal role in designing, analyzing, and\noptimizing wireless communication systems. Nevertheless, developing an\neffective channel modeling approach has been a longstanding challenge. This\nissue has been escalated due to the denser network deployment, larger antenna\narrays, and wider bandwidth in 5G and beyond networks. To address this\nchallenge, we put forth WRF-GS, a novel framework for channel modeling based on\nwireless radiation field (WRF) reconstruction using 3D Gaussian splatting.\nWRF-GS employs 3D Gaussian primitives and neural networks to capture the\ninteractions between the environment and radio signals, enabling efficient WRF\nreconstruction and visualization of the propagation characteristics. The\nreconstructed WRF can then be used to synthesize the spatial spectrum for\ncomprehensive wireless channel characterization. Notably, with a small number\nof measurements, WRF-GS can synthesize new spatial spectra within milliseconds\nfor a given scene, thereby enabling latency-sensitive applications.\nExperimental results demonstrate that WRF-GS outperforms existing methods for\nspatial spectrum synthesis, such as ray tracing and other deep-learning\napproaches. Moreover, WRF-GS achieves superior performance in the channel state\ninformation prediction task, surpassing existing methods by a significant\nmargin of more than 2.43 dB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless channel modeling plays a pivotal role in designing, analyzing, and\noptimizing wireless communication systems. Nevertheless, developing an\neffective channel modeling approach has been a longstanding challenge. This\nissue has been escalated due to the denser network deployment, larger antenna\narrays, and wider bandwidth in 5G and beyond networks. To address this\nchallenge, we put forth WRF-GS, a novel framework for channel modeling based on\nwireless radiation field (WRF) reconstruction using 3D Gaussian splatting.\nWRF-GS employs 3D Gaussian primitives and neural networks to capture the\ninteractions between the environment and radio signals, enabling efficient WRF\nreconstruction and visualization of the propagation characteristics. The\nreconstructed WRF can then be used to synthesize the spatial spectrum for\ncomprehensive wireless channel characterization. Notably, with a small number\nof measurements, WRF-GS can synthesize new spatial spectra within milliseconds\nfor a given scene, thereby enabling latency-sensitive applications.\nExperimental results demonstrate that WRF-GS outperforms existing methods for\nspatial spectrum synthesis, such as ray tracing and other deep-learning\napproaches. Moreover, WRF-GS achieves superior performance in the channel state\ninformation prediction task, surpassing existing methods by a significant\nmargin of more than 2.43 dB."
                },
                "authors": [
                    {
                        "name": "Chaozheng Wen"
                    },
                    {
                        "name": "Jingwen Tong"
                    },
                    {
                        "name": "Yingdong Hu"
                    },
                    {
                        "name": "Zehong Lin"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "accepted to the IEEE International Conference on Computer\n  Communications (INFOCOM 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04346v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04346v3",
                "updated": "2024-12-06T07:43:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    7,
                    43,
                    34,
                    4,
                    341,
                    0
                ],
                "published": "2024-07-05T08:37:10Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    8,
                    37,
                    10,
                    4,
                    187,
                    0
                ],
                "title": "MobileFlow: A Multimodal LLM For Mobile GUI Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MobileFlow: A Multimodal LLM For Mobile GUI Agent"
                },
                "summary": "Currently, the integration of mobile Graphical User Interfaces (GUIs) is\nubiquitous in most people's daily lives. And the ongoing evolution of\nmultimodal large-scale models, such as GPT-4v, Qwen-VL-Max, has significantly\nbolstered the capabilities of GUI comprehension and user action analysis,\nshowcasing the potentiality of intelligent GUI assistants. However, current GUI\nAgents often need to access page layout information through calling system\nAPIs, which may pose privacy risks. Fixing GUI (such as mobile interfaces) to a\ncertain low resolution might result in the loss of fine-grained image details.\nAt the same time, the multimodal large models built for GUI Agents currently\nhave poor understanding and decision-making abilities for Chinese GUI\ninterfaces, making them difficult to apply to a large number of Chinese apps.\nThis paper introduces MobileFlow, a multimodal large language model\nmeticulously crafted for mobile GUI agents. Transforming from the open-source\nmodel Qwen-VL-Chat into GUI domain, MobileFlow contains approximately 21\nbillion parameters and is equipped with novel hybrid visual encoders, making it\npossible for variable resolutions of image inputs and good support for\nmultilingual GUI. By incorporating Mixture of Experts (MoE) expansions and\npioneering alignment training strategies, MobileFlow has the capacity to fully\ninterpret image data and comprehend user instructions for GUI interaction\ntasks. Finally, MobileFlow outperforms Qwen-VL-Max and GPT-4v in terms of task\nexecution by GUI agents on both public and our proposed evaluation metrics, and\nhas been successfully deployed in real-world business contexts, proving its\neffectiveness for practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Currently, the integration of mobile Graphical User Interfaces (GUIs) is\nubiquitous in most people's daily lives. And the ongoing evolution of\nmultimodal large-scale models, such as GPT-4v, Qwen-VL-Max, has significantly\nbolstered the capabilities of GUI comprehension and user action analysis,\nshowcasing the potentiality of intelligent GUI assistants. However, current GUI\nAgents often need to access page layout information through calling system\nAPIs, which may pose privacy risks. Fixing GUI (such as mobile interfaces) to a\ncertain low resolution might result in the loss of fine-grained image details.\nAt the same time, the multimodal large models built for GUI Agents currently\nhave poor understanding and decision-making abilities for Chinese GUI\ninterfaces, making them difficult to apply to a large number of Chinese apps.\nThis paper introduces MobileFlow, a multimodal large language model\nmeticulously crafted for mobile GUI agents. Transforming from the open-source\nmodel Qwen-VL-Chat into GUI domain, MobileFlow contains approximately 21\nbillion parameters and is equipped with novel hybrid visual encoders, making it\npossible for variable resolutions of image inputs and good support for\nmultilingual GUI. By incorporating Mixture of Experts (MoE) expansions and\npioneering alignment training strategies, MobileFlow has the capacity to fully\ninterpret image data and comprehend user instructions for GUI interaction\ntasks. Finally, MobileFlow outperforms Qwen-VL-Max and GPT-4v in terms of task\nexecution by GUI agents on both public and our proposed evaluation metrics, and\nhas been successfully deployed in real-world business contexts, proving its\neffectiveness for practical applications."
                },
                "authors": [
                    {
                        "name": "Songqin Nong"
                    },
                    {
                        "name": "Jiali Zhu"
                    },
                    {
                        "name": "Rui Wu"
                    },
                    {
                        "name": "Jiongchao Jin"
                    },
                    {
                        "name": "Shuo Shan"
                    },
                    {
                        "name": "Xiutian Huang"
                    },
                    {
                        "name": "Wenhao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wenhao Xu"
                },
                "author": "Wenhao Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04346v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04346v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04821v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04821v1",
                "updated": "2024-12-06T07:29:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    7,
                    29,
                    34,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T07:29:34Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    7,
                    29,
                    34,
                    4,
                    341,
                    0
                ],
                "title": "CCS: Continuous Learning for Customized Incremental Wireless Sensing\n  Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CCS: Continuous Learning for Customized Incremental Wireless Sensing\n  Services"
                },
                "summary": "Wireless sensing has made significant progress in tasks ranging from action\nrecognition, vital sign estimation, pose estimation, etc. After over a decade\nof work, wireless sensing currently stands at the tipping point transitioning\nfrom proof-of-concept systems to the large-scale deployment. We envision a\nfuture service scenario where wireless sensing service providers distribute\nsensing models to users. During usage, users might request new sensing\ncapabilities. For example, if someone is away from home on a business trip or\nvacation for an extended period, they may want a new sensing capability that\ncan detect falls in elderly parents or grandparents and promptly alert them. In\nthis paper, we propose CCS (continuous customized service), enabling model\nupdates on users' local computing resources without data transmission to the\nservice providers. To address the issue of catastrophic forgetting in model\nupdates where updating model parameters to implement new capabilities leads to\nthe loss of existing capabilities we design knowledge distillation and weight\nalignment modules. These modules enable the sensing model to acquire new\ncapabilities while retaining the existing ones. We conducted extensive\nexperiments on the large-scale XRF55 dataset across Wi-Fi, millimeter-wave\nradar, and RFID modalities to simulate scenarios where four users sequentially\nintroduced new customized demands. The results affirm that CCS excels in\ncontinuous model services across all the above wireless modalities,\nsignificantly outperforming existing approaches like OneFi.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless sensing has made significant progress in tasks ranging from action\nrecognition, vital sign estimation, pose estimation, etc. After over a decade\nof work, wireless sensing currently stands at the tipping point transitioning\nfrom proof-of-concept systems to the large-scale deployment. We envision a\nfuture service scenario where wireless sensing service providers distribute\nsensing models to users. During usage, users might request new sensing\ncapabilities. For example, if someone is away from home on a business trip or\nvacation for an extended period, they may want a new sensing capability that\ncan detect falls in elderly parents or grandparents and promptly alert them. In\nthis paper, we propose CCS (continuous customized service), enabling model\nupdates on users' local computing resources without data transmission to the\nservice providers. To address the issue of catastrophic forgetting in model\nupdates where updating model parameters to implement new capabilities leads to\nthe loss of existing capabilities we design knowledge distillation and weight\nalignment modules. These modules enable the sensing model to acquire new\ncapabilities while retaining the existing ones. We conducted extensive\nexperiments on the large-scale XRF55 dataset across Wi-Fi, millimeter-wave\nradar, and RFID modalities to simulate scenarios where four users sequentially\nintroduced new customized demands. The results affirm that CCS excels in\ncontinuous model services across all the above wireless modalities,\nsignificantly outperforming existing approaches like OneFi."
                },
                "authors": [
                    {
                        "name": "Qunhang Fu"
                    },
                    {
                        "name": "Fei Wang"
                    },
                    {
                        "name": "Mengdie Zhu"
                    },
                    {
                        "name": "Han Ding"
                    },
                    {
                        "name": "Jinsong Han"
                    },
                    {
                        "name": "Tony Xiao Han"
                    }
                ],
                "author_detail": {
                    "name": "Tony Xiao Han"
                },
                "author": "Tony Xiao Han",
                "arxiv_comment": "9 pages,8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04821v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04821v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19772v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19772v2",
                "updated": "2024-12-06T07:24:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    7,
                    24,
                    10,
                    4,
                    341,
                    0
                ],
                "published": "2024-11-29T15:18:06Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    15,
                    18,
                    6,
                    4,
                    334,
                    0
                ],
                "title": "LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware\n  Omni-Modal Perception of Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware\n  Omni-Modal Perception of Long Videos"
                },
                "summary": "Despite impressive advancements in video understanding, most efforts remain\nlimited to coarse-grained or visual-only video tasks. However, real-world\nvideos encompass omni-modal information (vision, audio, and speech) with a\nseries of events forming a cohesive storyline. The lack of multi-modal video\ndata with fine-grained event annotations and the high cost of manual labeling\nare major obstacles to comprehensive omni-modality video perception. To address\nthis gap, we propose an automatic pipeline consisting of high-quality\nmulti-modal video filtering, semantically coherent omni-modal event boundary\ndetection, and cross-modal correlation-aware event captioning. In this way, we\npresent LongVALE, the first-ever Vision-Audio-Language Event understanding\nbenchmark comprising 105K omni-modal events with precise temporal boundaries\nand detailed relation-aware captions within 8.4K high-quality long videos.\nFurther, we build a baseline that leverages LongVALE to enable video large\nlanguage models (LLMs) for omni-modality fine-grained temporal video\nunderstanding for the first time. Extensive experiments demonstrate the\neffectiveness and great potential of LongVALE in advancing comprehensive\nmulti-modal video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite impressive advancements in video understanding, most efforts remain\nlimited to coarse-grained or visual-only video tasks. However, real-world\nvideos encompass omni-modal information (vision, audio, and speech) with a\nseries of events forming a cohesive storyline. The lack of multi-modal video\ndata with fine-grained event annotations and the high cost of manual labeling\nare major obstacles to comprehensive omni-modality video perception. To address\nthis gap, we propose an automatic pipeline consisting of high-quality\nmulti-modal video filtering, semantically coherent omni-modal event boundary\ndetection, and cross-modal correlation-aware event captioning. In this way, we\npresent LongVALE, the first-ever Vision-Audio-Language Event understanding\nbenchmark comprising 105K omni-modal events with precise temporal boundaries\nand detailed relation-aware captions within 8.4K high-quality long videos.\nFurther, we build a baseline that leverages LongVALE to enable video large\nlanguage models (LLMs) for omni-modality fine-grained temporal video\nunderstanding for the first time. Extensive experiments demonstrate the\neffectiveness and great potential of LongVALE in advancing comprehensive\nmulti-modal video understanding."
                },
                "authors": [
                    {
                        "name": "Tiantian Geng"
                    },
                    {
                        "name": "Jinrui Zhang"
                    },
                    {
                        "name": "Qingni Wang"
                    },
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Jinming Duan"
                    },
                    {
                        "name": "Feng Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zheng"
                },
                "author": "Feng Zheng",
                "arxiv_comment": "18 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19772v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19772v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06448v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06448v2",
                "updated": "2024-12-06T07:10:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    7,
                    10,
                    30,
                    4,
                    341,
                    0
                ],
                "published": "2024-04-09T16:50:30Z",
                "published_parsed": [
                    2024,
                    4,
                    9,
                    16,
                    50,
                    30,
                    1,
                    100,
                    0
                ],
                "title": "Automated Federated Pipeline for Parameter-Efficient Fine-Tuning of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Federated Pipeline for Parameter-Efficient Fine-Tuning of\n  Large Language Models"
                },
                "summary": "Recently, there has been a surge in the development of advanced intelligent\ngenerative content (AIGC), especially large language models (LLMs). However,\nfor many downstream tasks, it is necessary to fine-tune LLMs using private\ndata. While federated learning offers a promising privacy-preserving solution\nto LLM fine-tuning, the substantial size of an LLM, combined with high\ncomputational and communication demands, makes it hard to apply to downstream\ntasks. More importantly, private edge servers often possess varying computing\nand network resources in real-world scenarios, introducing additional\ncomplexities to LLM fine-tuning. To tackle these problems, we design and\nimplement an automated federated pipeline, named FedPipe, to fine-tune LLMs\nwith minimal training cost but without adding any inference latency. FedPipe\nfirstly identifies the weights to be fine-tuned based on their contributions to\nthe LLM training. It then configures a low-rank adapter for each selected\nweight to train local low-rank adapters on an edge server, and aggregate local\nadapters of all edge servers to fine-tune the whole LLM. Finally, it\nappropriately quantizes the parameters of LLM to reduce memory space according\nto the requirements of edge servers. Extensive experiments demonstrate that\nFedPipe expedites the model training and achieves higher accuracy than\nstate-of-the-art benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, there has been a surge in the development of advanced intelligent\ngenerative content (AIGC), especially large language models (LLMs). However,\nfor many downstream tasks, it is necessary to fine-tune LLMs using private\ndata. While federated learning offers a promising privacy-preserving solution\nto LLM fine-tuning, the substantial size of an LLM, combined with high\ncomputational and communication demands, makes it hard to apply to downstream\ntasks. More importantly, private edge servers often possess varying computing\nand network resources in real-world scenarios, introducing additional\ncomplexities to LLM fine-tuning. To tackle these problems, we design and\nimplement an automated federated pipeline, named FedPipe, to fine-tune LLMs\nwith minimal training cost but without adding any inference latency. FedPipe\nfirstly identifies the weights to be fine-tuned based on their contributions to\nthe LLM training. It then configures a low-rank adapter for each selected\nweight to train local low-rank adapters on an edge server, and aggregate local\nadapters of all edge servers to fine-tune the whole LLM. Finally, it\nappropriately quantizes the parameters of LLM to reduce memory space according\nto the requirements of edge servers. Extensive experiments demonstrate that\nFedPipe expedites the model training and achieves higher accuracy than\nstate-of-the-art benchmarks."
                },
                "authors": [
                    {
                        "name": "Zihan Fang"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Yue Gao"
                    },
                    {
                        "name": "Yuguang Fang"
                    }
                ],
                "author_detail": {
                    "name": "Yuguang Fang"
                },
                "author": "Yuguang Fang",
                "arxiv_comment": "15 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06448v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06448v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02565v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02565v2",
                "updated": "2024-12-06T07:08:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    7,
                    8,
                    56,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-03T16:53:58Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    53,
                    58,
                    1,
                    338,
                    0
                ],
                "title": "SJTU:Spatial judgments in multimodal models towards unified segmentation\n  through coordinate detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SJTU:Spatial judgments in multimodal models towards unified segmentation\n  through coordinate detection"
                },
                "summary": "Despite significant advances in vision-language understanding, implementing\nimage segmentation within multimodal architectures remains a fundamental\nchallenge in modern artificial intelligence systems. Existing vision-language\nmodels, which primarily rely on backbone architectures or CLIP-based embedding\nlearning, demonstrate inherent limitations in fine-grained spatial localization\nand operational capabilities. This paper introduces SJTU: Spatial Judgments in\nMultimodal Models - Towards Unified Segmentation through Coordinate Detection,\na framework that leverages spatial coordinate understanding to bridge\nvision-language interaction and precise segmentation, enabling accurate target\nidentification through natural language instructions. The framework presents an\napproach for integrating segmentation techniques with vision-language models\nthrough spatial inference in multimodal space. By utilizing normalized\ncoordinate detection for bounding boxes and transforming them into actionable\nsegmentation outputs, we establish a connection between spatial and language\nrepresentations in multimodal architectures. Experimental results demonstrate\nsuperior performance across benchmark datasets, achieving IoU scores of 0.5958\non COCO 2017 and 0.6758 on Pascal VOC. Testing on a single NVIDIA RTX 3090 GPU\nwith 512x512 resolution images yields an average inference time of 7 seconds\nper image, demonstrating the framework's effectiveness in both accuracy and\npractical deployability. The project code is available at\nhttps://github.com/jw-chae/SJTU",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant advances in vision-language understanding, implementing\nimage segmentation within multimodal architectures remains a fundamental\nchallenge in modern artificial intelligence systems. Existing vision-language\nmodels, which primarily rely on backbone architectures or CLIP-based embedding\nlearning, demonstrate inherent limitations in fine-grained spatial localization\nand operational capabilities. This paper introduces SJTU: Spatial Judgments in\nMultimodal Models - Towards Unified Segmentation through Coordinate Detection,\na framework that leverages spatial coordinate understanding to bridge\nvision-language interaction and precise segmentation, enabling accurate target\nidentification through natural language instructions. The framework presents an\napproach for integrating segmentation techniques with vision-language models\nthrough spatial inference in multimodal space. By utilizing normalized\ncoordinate detection for bounding boxes and transforming them into actionable\nsegmentation outputs, we establish a connection between spatial and language\nrepresentations in multimodal architectures. Experimental results demonstrate\nsuperior performance across benchmark datasets, achieving IoU scores of 0.5958\non COCO 2017 and 0.6758 on Pascal VOC. Testing on a single NVIDIA RTX 3090 GPU\nwith 512x512 resolution images yields an average inference time of 7 seconds\nper image, demonstrating the framework's effectiveness in both accuracy and\npractical deployability. The project code is available at\nhttps://github.com/jw-chae/SJTU"
                },
                "authors": [
                    {
                        "name": "Joongwon Chae"
                    },
                    {
                        "name": "Zhenyu Wang"
                    },
                    {
                        "name": "Peiwu Qin"
                    }
                ],
                "author_detail": {
                    "name": "Peiwu Qin"
                },
                "author": "Peiwu Qin",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02565v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02565v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03795v2",
                "updated": "2024-12-06T06:59:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    6,
                    59,
                    9,
                    4,
                    341,
                    0
                ],
                "published": "2024-10-04T02:50:58Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    2,
                    50,
                    58,
                    4,
                    278,
                    0
                ],
                "title": "Deep Learning and Machine Learning: Advancing Big Data Analytics and\n  Management with Design Patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning and Machine Learning: Advancing Big Data Analytics and\n  Management with Design Patterns"
                },
                "summary": "This book, Design Patterns in Machine Learning and Deep Learning: Advancing\nBig Data Analytics Management, presents a comprehensive study of essential\ndesign patterns tailored for large-scale machine learning and deep learning\napplications. The book explores the application of classical software\nengineering patterns, Creational, Structural, Behavioral, and Concurrency\nPatterns, to optimize the development, maintenance, and scalability of big data\nanalytics systems. Through practical examples and detailed Python\nimplementations, it bridges the gap between traditional object-oriented design\npatterns and the unique demands of modern data analytics environments. Key\ndesign patterns such as Singleton, Factory, Observer, and Strategy are analyzed\nfor their impact on model management, deployment strategies, and team\ncollaboration, providing invaluable insights into the engineering of efficient,\nreusable, and flexible systems. This volume is an essential resource for\ndevelopers, researchers, and engineers aiming to enhance their technical\nexpertise in both machine learning and software design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This book, Design Patterns in Machine Learning and Deep Learning: Advancing\nBig Data Analytics Management, presents a comprehensive study of essential\ndesign patterns tailored for large-scale machine learning and deep learning\napplications. The book explores the application of classical software\nengineering patterns, Creational, Structural, Behavioral, and Concurrency\nPatterns, to optimize the development, maintenance, and scalability of big data\nanalytics systems. Through practical examples and detailed Python\nimplementations, it bridges the gap between traditional object-oriented design\npatterns and the unique demands of modern data analytics environments. Key\ndesign patterns such as Singleton, Factory, Observer, and Strategy are analyzed\nfor their impact on model management, deployment strategies, and team\ncollaboration, providing invaluable insights into the engineering of efficient,\nreusable, and flexible systems. This volume is an essential resource for\ndevelopers, researchers, and engineers aiming to enhance their technical\nexpertise in both machine learning and software design."
                },
                "authors": [
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Ziqian Bi"
                    },
                    {
                        "name": "Tianyang Wang"
                    },
                    {
                        "name": "Yizhu Wen"
                    },
                    {
                        "name": "Pohsun Feng"
                    },
                    {
                        "name": "Qian Niu"
                    },
                    {
                        "name": "Junyu Liu"
                    },
                    {
                        "name": "Benji Peng"
                    },
                    {
                        "name": "Sen Zhang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Xuanhe Pan"
                    },
                    {
                        "name": "Jiawei Xu"
                    },
                    {
                        "name": "Jinlang Wang"
                    },
                    {
                        "name": "Ming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ming Liu"
                },
                "author": "Ming Liu",
                "arxiv_comment": "138pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05721v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05721v3",
                "updated": "2024-12-06T06:51:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    6,
                    51,
                    46,
                    4,
                    341,
                    0
                ],
                "published": "2024-07-08T08:25:56Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    8,
                    25,
                    56,
                    0,
                    190,
                    0
                ],
                "title": "PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation"
                },
                "summary": "Mental health has attracted substantial attention in recent years and LLM can\nbe an effective technology for alleviating this problem owing to its capability\nin text understanding and dialogue. However, existing research in this domain\noften suffers from limitations, such as training on datasets lacking crucial\nprior knowledge and evidence, and the absence of comprehensive evaluation\nmethods. In this paper, we propose a specialized psychological large language\nmodel (LLM), named PsycoLLM, trained on a proposed high-quality psychological\ndataset, including single-turn QA, multi-turn dialogues and knowledge-based QA.\nSpecifically, we construct multi-turn dialogues through a three-step pipeline\ncomprising multi-turn QA generation, evidence judgment, and dialogue\nrefinement. We augment this process with real-world psychological case\nbackgrounds extracted from online platforms, enhancing the relevance and\napplicability of the generated data. Additionally, to compare the performance\nof PsycoLLM with other LLMs, we develop a comprehensive psychological benchmark\nbased on authoritative psychological counseling examinations in China, which\nincludes assessments of professional ethics, theoretical proficiency, and case\nanalysis. The experimental results on the benchmark illustrate the\neffectiveness of PsycoLLM, which demonstrates superior performance compared to\nother LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mental health has attracted substantial attention in recent years and LLM can\nbe an effective technology for alleviating this problem owing to its capability\nin text understanding and dialogue. However, existing research in this domain\noften suffers from limitations, such as training on datasets lacking crucial\nprior knowledge and evidence, and the absence of comprehensive evaluation\nmethods. In this paper, we propose a specialized psychological large language\nmodel (LLM), named PsycoLLM, trained on a proposed high-quality psychological\ndataset, including single-turn QA, multi-turn dialogues and knowledge-based QA.\nSpecifically, we construct multi-turn dialogues through a three-step pipeline\ncomprising multi-turn QA generation, evidence judgment, and dialogue\nrefinement. We augment this process with real-world psychological case\nbackgrounds extracted from online platforms, enhancing the relevance and\napplicability of the generated data. Additionally, to compare the performance\nof PsycoLLM with other LLMs, we develop a comprehensive psychological benchmark\nbased on authoritative psychological counseling examinations in China, which\nincludes assessments of professional ethics, theoretical proficiency, and case\nanalysis. The experimental results on the benchmark illustrate the\neffectiveness of PsycoLLM, which demonstrates superior performance compared to\nother LLMs."
                },
                "authors": [
                    {
                        "name": "Jinpeng Hu"
                    },
                    {
                        "name": "Tengteng Dong"
                    },
                    {
                        "name": "Luo Gang"
                    },
                    {
                        "name": "Hui Ma"
                    },
                    {
                        "name": "Peng Zou"
                    },
                    {
                        "name": "Xiao Sun"
                    },
                    {
                        "name": "Dan Guo"
                    },
                    {
                        "name": "Xun Yang"
                    },
                    {
                        "name": "Meng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Wang"
                },
                "author": "Meng Wang",
                "arxiv_comment": "Accepted by IEEE Transactions on Computational Social Systems.\n  https://github.com/MACLAB-HFUT/PsycoLLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05721v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05721v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04806v1",
                "updated": "2024-12-06T06:32:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    6,
                    32,
                    47,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T06:32:47Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    6,
                    32,
                    47,
                    4,
                    341,
                    0
                ],
                "title": "Rethinking Time Series Forecasting with LLMs via Nearest Neighbor\n  Contrastive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Time Series Forecasting with LLMs via Nearest Neighbor\n  Contrastive Learning"
                },
                "summary": "Adapting Large Language Models (LLMs) that are extensively trained on\nabundant text data, and customizing the input prompt to enable time series\nforecasting has received considerable attention. While recent work has shown\ngreat potential for adapting the learned prior of LLMs, the formulation of the\nprompt to finetune LLMs remains challenging as prompt should be aligned with\ntime series data. Additionally, current approaches do not effectively leverage\nword token embeddings which embody the rich representation space learned by\nLLMs. This emphasizes the need for a robust approach to formulate the prompt\nwhich utilizes the word token embeddings while effectively representing the\ncharacteristics of the time series. To address these challenges, we propose\nNNCL-TLLM: Nearest Neighbor Contrastive Learning for Time series forecasting\nvia LLMs. First, we generate time series compatible text prototypes such that\neach text prototype represents both word token embeddings in its neighborhood\nand time series characteristics via end-to-end finetuning. Next, we draw\ninspiration from Nearest Neighbor Contrastive Learning to formulate the prompt\nwhile obtaining the top-$k$ nearest neighbor time series compatible text\nprototypes. We then fine-tune the layer normalization and positional embeddings\nof the LLM, keeping the other layers intact, reducing the trainable parameters\nand decreasing the computational cost. Our comprehensive experiments\ndemonstrate that NNCL-TLLM outperforms in few-shot forecasting while achieving\ncompetitive or superior performance over the state-of-the-art methods in\nlong-term and short-term forecasting tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Large Language Models (LLMs) that are extensively trained on\nabundant text data, and customizing the input prompt to enable time series\nforecasting has received considerable attention. While recent work has shown\ngreat potential for adapting the learned prior of LLMs, the formulation of the\nprompt to finetune LLMs remains challenging as prompt should be aligned with\ntime series data. Additionally, current approaches do not effectively leverage\nword token embeddings which embody the rich representation space learned by\nLLMs. This emphasizes the need for a robust approach to formulate the prompt\nwhich utilizes the word token embeddings while effectively representing the\ncharacteristics of the time series. To address these challenges, we propose\nNNCL-TLLM: Nearest Neighbor Contrastive Learning for Time series forecasting\nvia LLMs. First, we generate time series compatible text prototypes such that\neach text prototype represents both word token embeddings in its neighborhood\nand time series characteristics via end-to-end finetuning. Next, we draw\ninspiration from Nearest Neighbor Contrastive Learning to formulate the prompt\nwhile obtaining the top-$k$ nearest neighbor time series compatible text\nprototypes. We then fine-tune the layer normalization and positional embeddings\nof the LLM, keeping the other layers intact, reducing the trainable parameters\nand decreasing the computational cost. Our comprehensive experiments\ndemonstrate that NNCL-TLLM outperforms in few-shot forecasting while achieving\ncompetitive or superior performance over the state-of-the-art methods in\nlong-term and short-term forecasting tasks."
                },
                "authors": [
                    {
                        "name": "Jayanie Bogahawatte"
                    },
                    {
                        "name": "Sachith Seneviratne"
                    },
                    {
                        "name": "Maneesha Perera"
                    },
                    {
                        "name": "Saman Halgamuge"
                    }
                ],
                "author_detail": {
                    "name": "Saman Halgamuge"
                },
                "author": "Saman Halgamuge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01672v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01672v3",
                "updated": "2024-12-06T06:25:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    6,
                    25,
                    54,
                    4,
                    341,
                    0
                ],
                "published": "2024-10-02T15:41:22Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    41,
                    22,
                    2,
                    276,
                    0
                ],
                "title": "Practicing Stress Relief for the Everyday: Designing Social Simulation\n  Using VR, AR, and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practicing Stress Relief for the Everyday: Designing Social Simulation\n  Using VR, AR, and LLMs"
                },
                "summary": "Stress is an inevitable part of day-to-day life yet many find themselves\nunable to manage it themselves, particularly when professional or peer support\nare not always readily available. As self-care becomes increasingly vital for\nmental well-being, this paper explores the potential of social simulation as a\nsafe, virtual environment for practicing stress relief for everyday situations.\nLeveraging the immersive capabilities of VR, AR, and LLMs, we developed eight\ninteractive prototypes for various everyday stressful scenarios (e.g. public\nspeaking) then conducted prototype-driven semi-structured interviews with 19\nparticipants. We reveal that people currently lack effective means to support\nthemselves through everyday stress and found that social simulation fills a gap\nfor simulating real environments for training mental health practices. We\noutline key considerations for future development of simulation for self-care,\nincluding risks of trauma from hyper-realism, distrust of LLM-recommended\ntiming for mental health recommendations, and the value of accessibility for\nself-care interventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stress is an inevitable part of day-to-day life yet many find themselves\nunable to manage it themselves, particularly when professional or peer support\nare not always readily available. As self-care becomes increasingly vital for\nmental well-being, this paper explores the potential of social simulation as a\nsafe, virtual environment for practicing stress relief for everyday situations.\nLeveraging the immersive capabilities of VR, AR, and LLMs, we developed eight\ninteractive prototypes for various everyday stressful scenarios (e.g. public\nspeaking) then conducted prototype-driven semi-structured interviews with 19\nparticipants. We reveal that people currently lack effective means to support\nthemselves through everyday stress and found that social simulation fills a gap\nfor simulating real environments for training mental health practices. We\noutline key considerations for future development of simulation for self-care,\nincluding risks of trauma from hyper-realism, distrust of LLM-recommended\ntiming for mental health recommendations, and the value of accessibility for\nself-care interventions."
                },
                "authors": [
                    {
                        "name": "Anna Fang"
                    },
                    {
                        "name": "Hriday Chhabria"
                    },
                    {
                        "name": "Alekhya Maram"
                    },
                    {
                        "name": "Haiyi Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Haiyi Zhu"
                },
                "author": "Haiyi Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01672v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01672v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00573v2",
                "updated": "2024-12-06T06:05:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    6,
                    5,
                    59,
                    4,
                    341,
                    0
                ],
                "published": "2024-11-30T20:00:41Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    20,
                    0,
                    41,
                    5,
                    335,
                    0
                ],
                "title": "Opus: A Large Work Model for Complex Workflow Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Opus: A Large Work Model for Complex Workflow Generation"
                },
                "summary": "This paper introduces Opus, a novel framework for generating and optimizing\nWorkflows tailored to complex Business Process Outsourcing (BPO) use cases,\nfocusing on cost reduction and quality enhancement while adhering to\nestablished industry processes and operational constraints. Our approach\ngenerates executable Workflows from Intention, defined as the alignment of\nClient Input, Client Output, and Process Context. These Workflows are\nrepresented as Directed Acyclic Graphs (DAGs), with nodes as Tasks consisting\nof sequences of executable Instructions, including tools and human expert\nreviews. We adopt a two-phase methodology: Workflow Generation and Workflow\nOptimization. In the Generation phase, Workflows are generated using a Large\nWork Model (LWM) informed by a Work Knowledge Graph (WKG) that encodes\ndomain-specific procedural and operational knowledge. In the Optimization\nphase, Workflows are transformed into Workflow Graphs (WFGs), where optimal\nWorkflows are determined through path optimization. Our experiments demonstrate\nthat state-of-the-art Large Language Models (LLMs) face challenges in reliably\nretrieving detailed process data as well as generating industry-compliant\nworkflows. The key contributions of this paper include integrating a Work\nKnowledge Graph (WKG) into a Large Work Model (LWM) to enable the generation of\ncontext-aware, semantically aligned, structured and auditable Workflows. It\nfurther introduces a two-phase approach that combines Workflow Generation from\nIntention with graph-based Workflow Optimization. Finally, we present Opus\nAlpha 1 Large and Opus Alpha 1 Small that outperform state-of-the-art LLMs by\n38% and 29% respectively in Workflow Generation for a Medical Coding use case.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Opus, a novel framework for generating and optimizing\nWorkflows tailored to complex Business Process Outsourcing (BPO) use cases,\nfocusing on cost reduction and quality enhancement while adhering to\nestablished industry processes and operational constraints. Our approach\ngenerates executable Workflows from Intention, defined as the alignment of\nClient Input, Client Output, and Process Context. These Workflows are\nrepresented as Directed Acyclic Graphs (DAGs), with nodes as Tasks consisting\nof sequences of executable Instructions, including tools and human expert\nreviews. We adopt a two-phase methodology: Workflow Generation and Workflow\nOptimization. In the Generation phase, Workflows are generated using a Large\nWork Model (LWM) informed by a Work Knowledge Graph (WKG) that encodes\ndomain-specific procedural and operational knowledge. In the Optimization\nphase, Workflows are transformed into Workflow Graphs (WFGs), where optimal\nWorkflows are determined through path optimization. Our experiments demonstrate\nthat state-of-the-art Large Language Models (LLMs) face challenges in reliably\nretrieving detailed process data as well as generating industry-compliant\nworkflows. The key contributions of this paper include integrating a Work\nKnowledge Graph (WKG) into a Large Work Model (LWM) to enable the generation of\ncontext-aware, semantically aligned, structured and auditable Workflows. It\nfurther introduces a two-phase approach that combines Workflow Generation from\nIntention with graph-based Workflow Optimization. Finally, we present Opus\nAlpha 1 Large and Opus Alpha 1 Small that outperform state-of-the-art LLMs by\n38% and 29% respectively in Workflow Generation for a Medical Coding use case."
                },
                "authors": [
                    {
                        "name": "Théo Fagnoni"
                    },
                    {
                        "name": "Bellinda Mesbah"
                    },
                    {
                        "name": "Mahsun Altin"
                    },
                    {
                        "name": "Phillip Kingston"
                    }
                ],
                "author_detail": {
                    "name": "Phillip Kingston"
                },
                "author": "Phillip Kingston",
                "arxiv_comment": "25 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04475v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04475v2",
                "updated": "2024-12-06T05:51:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    5,
                    51,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-09-05T13:45:42Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    13,
                    45,
                    42,
                    3,
                    249,
                    0
                ],
                "title": "Revolutionizing Database Q&A with Large Language Models: Comprehensive\n  Benchmark and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revolutionizing Database Q&A with Large Language Models: Comprehensive\n  Benchmark and Evaluation"
                },
                "summary": "The development of Large Language Models (LLMs) has revolutionized QA across\nvarious industries, including the database domain. However, there is still a\nlack of a comprehensive benchmark to evaluate the capabilities of different\nLLMs and their modular components in database QA. To this end, we introduce\nDQABench, the first comprehensive database QA benchmark for LLMs. DQABench\nfeatures an innovative LLM-based method to automate the generation, cleaning,\nand rewriting of evaluation dataset, resulting in over 200,000 QA pairs in\nEnglish and Chinese, separately. These QA pairs cover a wide range of\ndatabase-related knowledge extracted from manuals, online communities, and\ndatabase instances. This inclusion allows for an additional assessment of LLMs'\nRetrieval-Augmented Generation (RAG) and Tool Invocation Generation (TIG)\ncapabilities in the database QA task. Furthermore, we propose a comprehensive\nLLM-based database QA testbed DQATestbed. This testbed is highly modular and\nscalable, with basic and advanced components such as Question Classification\nRouting (QCR), RAG, TIG, and Prompt Template Engineering (PTE). Moreover,\nDQABench provides a comprehensive evaluation pipeline that computes various\nmetrics throughout a standardized evaluation process to ensure the accuracy and\nfairness of the evaluation. We use DQABench to evaluate the database QA\ncapabilities under the proposed testbed comprehensively. The evaluation reveals\nfindings like (i) the strengths and limitations of nine LLM-based QA bots and\n(ii) the performance impact and potential improvements of various service\ncomponents (e.g., QCR, RAG, TIG). Our benchmark and findings will guide the\nfuture development of LLM-based database QA research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of Large Language Models (LLMs) has revolutionized QA across\nvarious industries, including the database domain. However, there is still a\nlack of a comprehensive benchmark to evaluate the capabilities of different\nLLMs and their modular components in database QA. To this end, we introduce\nDQABench, the first comprehensive database QA benchmark for LLMs. DQABench\nfeatures an innovative LLM-based method to automate the generation, cleaning,\nand rewriting of evaluation dataset, resulting in over 200,000 QA pairs in\nEnglish and Chinese, separately. These QA pairs cover a wide range of\ndatabase-related knowledge extracted from manuals, online communities, and\ndatabase instances. This inclusion allows for an additional assessment of LLMs'\nRetrieval-Augmented Generation (RAG) and Tool Invocation Generation (TIG)\ncapabilities in the database QA task. Furthermore, we propose a comprehensive\nLLM-based database QA testbed DQATestbed. This testbed is highly modular and\nscalable, with basic and advanced components such as Question Classification\nRouting (QCR), RAG, TIG, and Prompt Template Engineering (PTE). Moreover,\nDQABench provides a comprehensive evaluation pipeline that computes various\nmetrics throughout a standardized evaluation process to ensure the accuracy and\nfairness of the evaluation. We use DQABench to evaluate the database QA\ncapabilities under the proposed testbed comprehensively. The evaluation reveals\nfindings like (i) the strengths and limitations of nine LLM-based QA bots and\n(ii) the performance impact and potential improvements of various service\ncomponents (e.g., QCR, RAG, TIG). Our benchmark and findings will guide the\nfuture development of LLM-based database QA research."
                },
                "authors": [
                    {
                        "name": "Yihang Zheng"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Zhenghao Lin"
                    },
                    {
                        "name": "Yi Luo"
                    },
                    {
                        "name": "Xuanhe Zhou"
                    },
                    {
                        "name": "Chen Lin"
                    },
                    {
                        "name": "Jinsong Su"
                    },
                    {
                        "name": "Guoliang Li"
                    },
                    {
                        "name": "Shifu Li"
                    }
                ],
                "author_detail": {
                    "name": "Shifu Li"
                },
                "author": "Shifu Li",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04475v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04475v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04788v1",
                "updated": "2024-12-06T05:46:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    5,
                    46,
                    43,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T05:46:43Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    5,
                    46,
                    43,
                    4,
                    341,
                    0
                ],
                "title": "GUIDE: A Global Unified Inference Engine for Deploying Large Language\n  Models in Heterogeneous Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUIDE: A Global Unified Inference Engine for Deploying Large Language\n  Models in Heterogeneous Environments"
                },
                "summary": "Efficiently deploying large language models (LLMs) in real-world scenarios\nremains a critical challenge, primarily due to hardware heterogeneity,\ninference framework limitations, and workload complexities.Efficiently\ndeploying large language models (LLMs) in real-world scenarios remains a\ncritical challenge, primarily due to hardware heterogeneity, inference\nframework limitations, and workload complexities. These challenges often lead\nto inefficiencies in memory utilization, latency, and throughput, hindering the\neffective deployment of LLMs, especially for non-experts. Through extensive\nexperiments, we identify key performance bottlenecks, including sudden drops in\nmemory utilization, latency fluctuations with varying batch sizes, and\ninefficiencies in multi-GPU configurations. These insights reveal a vast\noptimization space shaped by the intricate interplay of hardware, frameworks,\nand workload parameters. This underscores the need for a systematic approach to\noptimize LLM inference, motivating the design of our framework, GUIDE. GUIDE\nleverages dynamic modeling and simulation-based optimization to address these\nissues, achieving prediction errors between 25% and 55% for key metrics such as\nbatch latency, TTFT, and decode throughput. By effectively bridging the gap\nbetween theoretical performance and practical deployment, our framework\nempowers practitioners, particularly non-specialists, to make data-driven\ndecisions and unlock the full potential of LLMs in heterogeneous environments\ncheaply.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently deploying large language models (LLMs) in real-world scenarios\nremains a critical challenge, primarily due to hardware heterogeneity,\ninference framework limitations, and workload complexities.Efficiently\ndeploying large language models (LLMs) in real-world scenarios remains a\ncritical challenge, primarily due to hardware heterogeneity, inference\nframework limitations, and workload complexities. These challenges often lead\nto inefficiencies in memory utilization, latency, and throughput, hindering the\neffective deployment of LLMs, especially for non-experts. Through extensive\nexperiments, we identify key performance bottlenecks, including sudden drops in\nmemory utilization, latency fluctuations with varying batch sizes, and\ninefficiencies in multi-GPU configurations. These insights reveal a vast\noptimization space shaped by the intricate interplay of hardware, frameworks,\nand workload parameters. This underscores the need for a systematic approach to\noptimize LLM inference, motivating the design of our framework, GUIDE. GUIDE\nleverages dynamic modeling and simulation-based optimization to address these\nissues, achieving prediction errors between 25% and 55% for key metrics such as\nbatch latency, TTFT, and decode throughput. By effectively bridging the gap\nbetween theoretical performance and practical deployment, our framework\nempowers practitioners, particularly non-specialists, to make data-driven\ndecisions and unlock the full potential of LLMs in heterogeneous environments\ncheaply."
                },
                "authors": [
                    {
                        "name": "Yanyu Chen"
                    },
                    {
                        "name": "Ganhong Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ganhong Huang"
                },
                "author": "Ganhong Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04787v1",
                "updated": "2024-12-06T05:41:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    5,
                    41,
                    11,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T05:41:11Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    5,
                    41,
                    11,
                    4,
                    341,
                    0
                ],
                "title": "Direct Quantized Training of Language Models with Stochastic Rounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Quantized Training of Language Models with Stochastic Rounding"
                },
                "summary": "Although recent quantized Large Language Models (LLMs), such as BitNet, have\npaved the way for significant reduction in memory usage during deployment with\nbinary or ternary weights, training these models still demands substantial\nmemory footprints. This is partly because high-precision (i.e., unquantized)\nweight matrices required for straight-through estimation must be maintained\nthroughout the whole training process. To address this, we explore the\npotential of directly updating the quantized low-precision weight matrices\nwithout relying on the straight-through estimator during backpropagation,\nthereby saving memory usage during training. Specifically, we employ a\nstochastic rounding technique to minimize information loss caused by the use of\nlow-bit weights throughout training. Experimental results on our\nLLaMA-structured models indicate that (1) training with only low-precision\nweights is feasible even when they are constrained to ternary values, (2)\nextending the bit width to 8 bits results in only a 5% loss degradation\ncompared to BitNet b1.58 while offering the potential for reduced memory usage\nduring training, and (3) our models can also perform inference using ternary\nweights, showcasing their flexibility in deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although recent quantized Large Language Models (LLMs), such as BitNet, have\npaved the way for significant reduction in memory usage during deployment with\nbinary or ternary weights, training these models still demands substantial\nmemory footprints. This is partly because high-precision (i.e., unquantized)\nweight matrices required for straight-through estimation must be maintained\nthroughout the whole training process. To address this, we explore the\npotential of directly updating the quantized low-precision weight matrices\nwithout relying on the straight-through estimator during backpropagation,\nthereby saving memory usage during training. Specifically, we employ a\nstochastic rounding technique to minimize information loss caused by the use of\nlow-bit weights throughout training. Experimental results on our\nLLaMA-structured models indicate that (1) training with only low-precision\nweights is feasible even when they are constrained to ternary values, (2)\nextending the bit width to 8 bits results in only a 5% loss degradation\ncompared to BitNet b1.58 while offering the potential for reduced memory usage\nduring training, and (3) our models can also perform inference using ternary\nweights, showcasing their flexibility in deployment."
                },
                "authors": [
                    {
                        "name": "Kaiyan Zhao"
                    },
                    {
                        "name": "Tsuguchika Tabaru"
                    },
                    {
                        "name": "Kenichi Kobayashi"
                    },
                    {
                        "name": "Takumi Honda"
                    },
                    {
                        "name": "Masafumi Yamazaki"
                    },
                    {
                        "name": "Yoshimasa Tsuruoka"
                    }
                ],
                "author_detail": {
                    "name": "Yoshimasa Tsuruoka"
                },
                "author": "Yoshimasa Tsuruoka",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04782v1",
                "updated": "2024-12-06T05:20:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    5,
                    20,
                    4,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T05:20:04Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    5,
                    20,
                    4,
                    4,
                    341,
                    0
                ],
                "title": "A Survey of Sustainability in Large Language Models: Applications,\n  Economics, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Sustainability in Large Language Models: Applications,\n  Economics, and Challenges"
                },
                "summary": "Large Language Models (LLMs) have transformed numerous domains by providing\nadvanced capabilities in natural language understanding, generation, and\nreasoning. Despite their groundbreaking applications across industries such as\nresearch, healthcare, and creative media, their rapid adoption raises critical\nconcerns regarding sustainability. This survey paper comprehensively examines\nthe environmental, economic, and computational challenges associated with LLMs,\nfocusing on energy consumption, carbon emissions, and resource utilization in\ndata centers. By synthesizing insights from existing literature, this work\nexplores strategies such as resource-efficient training, sustainable deployment\npractices, and lifecycle assessments to mitigate the environmental impacts of\nLLMs. Key areas of emphasis include energy optimization, renewable energy\nintegration, and balancing performance with sustainability. The findings aim to\nguide researchers, practitioners, and policymakers in developing actionable\nstrategies for sustainable AI systems, fostering a responsible and\nenvironmentally conscious future for artificial intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have transformed numerous domains by providing\nadvanced capabilities in natural language understanding, generation, and\nreasoning. Despite their groundbreaking applications across industries such as\nresearch, healthcare, and creative media, their rapid adoption raises critical\nconcerns regarding sustainability. This survey paper comprehensively examines\nthe environmental, economic, and computational challenges associated with LLMs,\nfocusing on energy consumption, carbon emissions, and resource utilization in\ndata centers. By synthesizing insights from existing literature, this work\nexplores strategies such as resource-efficient training, sustainable deployment\npractices, and lifecycle assessments to mitigate the environmental impacts of\nLLMs. Key areas of emphasis include energy optimization, renewable energy\nintegration, and balancing performance with sustainability. The findings aim to\nguide researchers, practitioners, and policymakers in developing actionable\nstrategies for sustainable AI systems, fostering a responsible and\nenvironmentally conscious future for artificial intelligence."
                },
                "authors": [
                    {
                        "name": "Aditi Singh"
                    },
                    {
                        "name": "Nirmal Prakashbhai Patel"
                    },
                    {
                        "name": "Abul Ehtesham"
                    },
                    {
                        "name": "Saket Kumar"
                    },
                    {
                        "name": "Tala Talaei Khoei"
                    }
                ],
                "author_detail": {
                    "name": "Tala Talaei Khoei"
                },
                "author": "Tala Talaei Khoei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04237v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04237v2",
                "updated": "2024-12-06T05:16:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    5,
                    16,
                    57,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-05T15:17:06Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    17,
                    6,
                    3,
                    340,
                    0
                ],
                "title": "VASCAR: Content-Aware Layout Generation via Visual-Aware Self-Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VASCAR: Content-Aware Layout Generation via Visual-Aware Self-Correction"
                },
                "summary": "Large language models (LLMs) have proven effective for layout generation due\nto their ability to produce structure-description languages, such as HTML or\nJSON, even without access to visual information. Recently, LLM providers have\nevolved these models into large vision-language models (LVLM), which shows\nprominent multi-modal understanding capabilities. Then, how can we leverage\nthis multi-modal power for layout generation? To answer this, we propose\nVisual-Aware Self-Correction LAyout GeneRation (VASCAR) for LVLM-based\ncontent-aware layout generation. In our method, LVLMs iteratively refine their\noutputs with reference to rendered layout images, which are visualized as\ncolored bounding boxes on poster backgrounds. In experiments, we demonstrate\nthat our method combined with the Gemini. Without any additional training,\nVASCAR achieves state-of-the-art (SOTA) layout generation quality outperforming\nboth existing layout-specific generative models and other LLM-based methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have proven effective for layout generation due\nto their ability to produce structure-description languages, such as HTML or\nJSON, even without access to visual information. Recently, LLM providers have\nevolved these models into large vision-language models (LVLM), which shows\nprominent multi-modal understanding capabilities. Then, how can we leverage\nthis multi-modal power for layout generation? To answer this, we propose\nVisual-Aware Self-Correction LAyout GeneRation (VASCAR) for LVLM-based\ncontent-aware layout generation. In our method, LVLMs iteratively refine their\noutputs with reference to rendered layout images, which are visualized as\ncolored bounding boxes on poster backgrounds. In experiments, we demonstrate\nthat our method combined with the Gemini. Without any additional training,\nVASCAR achieves state-of-the-art (SOTA) layout generation quality outperforming\nboth existing layout-specific generative models and other LLM-based methods."
                },
                "authors": [
                    {
                        "name": "Jiahao Zhang"
                    },
                    {
                        "name": "Ryota Yoshihashi"
                    },
                    {
                        "name": "Shunsuke Kitada"
                    },
                    {
                        "name": "Atsuki Osanai"
                    },
                    {
                        "name": "Yuta Nakashima"
                    }
                ],
                "author_detail": {
                    "name": "Yuta Nakashima"
                },
                "author": "Yuta Nakashima",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04237v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04237v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04774v1",
                "updated": "2024-12-06T04:34:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    4,
                    34,
                    45,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T04:34:45Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    4,
                    34,
                    45,
                    4,
                    341,
                    0
                ],
                "title": "Foundation Models for Low-Resource Language Education (Vision Paper)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Models for Low-Resource Language Education (Vision Paper)"
                },
                "summary": "Recent studies show that large language models (LLMs) are powerful tools for\nworking with natural language, bringing advances in many areas of computational\nlinguistics. However, these models face challenges when applied to low-resource\nlanguages due to limited training data and difficulty in understanding cultural\nnuances. Research is now focusing on multilingual models to improve LLM\nperformance for these languages. Education in these languages also struggles\nwith a lack of resources and qualified teachers, particularly in underdeveloped\nregions. Here, LLMs can be transformative, supporting innovative methods like\ncommunity-driven learning and digital platforms. This paper discusses how LLMs\ncould enhance education for low-resource languages, emphasizing practical\napplications and benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies show that large language models (LLMs) are powerful tools for\nworking with natural language, bringing advances in many areas of computational\nlinguistics. However, these models face challenges when applied to low-resource\nlanguages due to limited training data and difficulty in understanding cultural\nnuances. Research is now focusing on multilingual models to improve LLM\nperformance for these languages. Education in these languages also struggles\nwith a lack of resources and qualified teachers, particularly in underdeveloped\nregions. Here, LLMs can be transformative, supporting innovative methods like\ncommunity-driven learning and digital platforms. This paper discusses how LLMs\ncould enhance education for low-resource languages, emphasizing practical\napplications and benefits."
                },
                "authors": [
                    {
                        "name": "Zhaojun Ding"
                    },
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Hanqi Jiang"
                    },
                    {
                        "name": "Yizhu Gao"
                    },
                    {
                        "name": "Xiaoming Zhai"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Ninghao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ninghao Liu"
                },
                "author": "Ninghao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17993v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17993v3",
                "updated": "2024-12-06T04:08:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    4,
                    8,
                    34,
                    4,
                    341,
                    0
                ],
                "published": "2024-11-27T02:20:44Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    2,
                    20,
                    44,
                    2,
                    332,
                    0
                ],
                "title": "DRS: Deep Question Reformulation With Structured Output",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRS: Deep Question Reformulation With Structured Output"
                },
                "summary": "Question answering represents a core capability of large language models\n(LLMs). However, when individuals encounter unfamiliar knowledge in texts, they\noften formulate questions that the text itself cannot answer due to\ninsufficient understanding of the underlying information. Recent studies reveal\nthat while LLMs can detect unanswerable questions, they struggle to assist\nusers in reformulating these questions. Even advanced models like GPT-3.5\ndemonstrate limited effectiveness in this regard. To address this limitation,\nwe propose DRS: Deep Question Reformulation with Structured Output, a novel\nzero-shot method aimed at enhancing LLMs ability to assist users in\nreformulating questions to extract relevant information from new documents. DRS\ncombines the strengths of LLMs with a DFS-based algorithm to iteratively\nexplore potential entity combinations and constrain outputs using predefined\nentities. This structured approach significantly enhances the reformulation\ncapabilities of LLMs. Comprehensive experimental evaluations demonstrate that\nDRS improves the reformulation accuracy of GPT-3.5 from 23.03% to 70.42%, while\nalso enhancing the performance of open-source models, such as Gemma2-9B, from\n26.35% to 56.75%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question answering represents a core capability of large language models\n(LLMs). However, when individuals encounter unfamiliar knowledge in texts, they\noften formulate questions that the text itself cannot answer due to\ninsufficient understanding of the underlying information. Recent studies reveal\nthat while LLMs can detect unanswerable questions, they struggle to assist\nusers in reformulating these questions. Even advanced models like GPT-3.5\ndemonstrate limited effectiveness in this regard. To address this limitation,\nwe propose DRS: Deep Question Reformulation with Structured Output, a novel\nzero-shot method aimed at enhancing LLMs ability to assist users in\nreformulating questions to extract relevant information from new documents. DRS\ncombines the strengths of LLMs with a DFS-based algorithm to iteratively\nexplore potential entity combinations and constrain outputs using predefined\nentities. This structured approach significantly enhances the reformulation\ncapabilities of LLMs. Comprehensive experimental evaluations demonstrate that\nDRS improves the reformulation accuracy of GPT-3.5 from 23.03% to 70.42%, while\nalso enhancing the performance of open-source models, such as Gemma2-9B, from\n26.35% to 56.75%."
                },
                "authors": [
                    {
                        "name": "Zhecheng Li"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Nanyun Peng"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Wei Chang"
                },
                "author": "Kai-Wei Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17993v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17993v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04757v1",
                "updated": "2024-12-06T03:46:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    46,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T03:46:06Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    46,
                    6,
                    4,
                    341,
                    0
                ],
                "title": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free\n  Dynamic Triangular Attention Pattern",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free\n  Dynamic Triangular Attention Pattern"
                },
                "summary": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference."
                },
                "authors": [
                    {
                        "name": "Hongyin Tang"
                    },
                    {
                        "name": "Di Xiu"
                    },
                    {
                        "name": "Lanrui Wang"
                    },
                    {
                        "name": "Xiurui Geng"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xunliang Cai"
                },
                "author": "Xunliang Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04756v1",
                "updated": "2024-12-06T03:45:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    45,
                    49,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T03:45:49Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    45,
                    49,
                    4,
                    341,
                    0
                ],
                "title": "ChatNVD: Advancing Cybersecurity Vulnerability Assessment with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatNVD: Advancing Cybersecurity Vulnerability Assessment with Large\n  Language Models"
                },
                "summary": "The increasing frequency and sophistication of cybersecurity vulnerabilities\nin software systems underscore the urgent need for robust and effective methods\nof vulnerability assessment. However, existing approaches often rely on highly\ntechnical and abstract frameworks, which hinders understanding and increases\nthe likelihood of exploitation, resulting in severe cyberattacks. Given the\ngrowing adoption of Large Language Models (LLMs) across diverse domains, this\npaper explores their potential application in cybersecurity, specifically for\nenhancing the assessment of software vulnerabilities. We propose ChatNVD, an\nLLM-based cybersecurity vulnerability assessment tool leveraging the National\nVulnerability Database (NVD) to provide context-rich insights and streamline\nvulnerability analysis for cybersecurity professionals, developers, and\nnon-technical users. We develop three variants of ChatNVD, utilizing three\nprominent LLMs: GPT-4o mini by OpenAI, Llama 3 by Meta, and Gemini 1.5 Pro by\nGoogle. To evaluate their efficacy, we conduct a comparative analysis of these\nmodels using a comprehensive questionnaire comprising common security\nvulnerability questions, assessing their accuracy in identifying and analyzing\nsoftware vulnerabilities. This study provides valuable insights into the\npotential of LLMs to address critical challenges in understanding and\nmitigation of software vulnerabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing frequency and sophistication of cybersecurity vulnerabilities\nin software systems underscore the urgent need for robust and effective methods\nof vulnerability assessment. However, existing approaches often rely on highly\ntechnical and abstract frameworks, which hinders understanding and increases\nthe likelihood of exploitation, resulting in severe cyberattacks. Given the\ngrowing adoption of Large Language Models (LLMs) across diverse domains, this\npaper explores their potential application in cybersecurity, specifically for\nenhancing the assessment of software vulnerabilities. We propose ChatNVD, an\nLLM-based cybersecurity vulnerability assessment tool leveraging the National\nVulnerability Database (NVD) to provide context-rich insights and streamline\nvulnerability analysis for cybersecurity professionals, developers, and\nnon-technical users. We develop three variants of ChatNVD, utilizing three\nprominent LLMs: GPT-4o mini by OpenAI, Llama 3 by Meta, and Gemini 1.5 Pro by\nGoogle. To evaluate their efficacy, we conduct a comparative analysis of these\nmodels using a comprehensive questionnaire comprising common security\nvulnerability questions, assessing their accuracy in identifying and analyzing\nsoftware vulnerabilities. This study provides valuable insights into the\npotential of LLMs to address critical challenges in understanding and\nmitigation of software vulnerabilities."
                },
                "authors": [
                    {
                        "name": "Shivansh Chopra"
                    },
                    {
                        "name": "Hussain Ahmad"
                    },
                    {
                        "name": "Diksha Goel"
                    },
                    {
                        "name": "Claudia Szabo"
                    }
                ],
                "author_detail": {
                    "name": "Claudia Szabo"
                },
                "author": "Claudia Szabo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04747v1",
                "updated": "2024-12-06T03:20:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    20,
                    3,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T03:20:03Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    20,
                    3,
                    4,
                    341,
                    0
                ],
                "title": "Code generation and runtime techniques for enabling data-efficient deep\n  learning training on GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation and runtime techniques for enabling data-efficient deep\n  learning training on GPUs"
                },
                "summary": "As deep learning models scale, their training cost has surged significantly.\nDue to both hardware advancements and limitations in current software stacks,\nthe need for data efficiency has risen. Data efficiency refers to the effective\nhiding of data access latency and the avoidance of unnecessary data movements.\nMajor challenges arise from the growing disparity between GPU memory bandwidth\nand computational throughput, imminent GPU memory capacity limitations, and\ninefficiencies in the PyTorch software stack, including a lack of\ndevice-specific PCIe transfer optimizations and high-level domain-specific\nabstractions. To effectively mitigate these data inefficiencies for deep\nlearning training, this dissertation analyzes data inefficiency in\nrepresentative deep training tasks, specifically in graph neural networks\n(GNNs) and large language models (LLMs). It then proposes novel runtime and\ncode generation techniques to mitigate these challenges and implements these\noptimizations seamlessly within the PyTorch stack while maintaining strong\nprogrammability and interoperability. First, PyTorch-Direct is devised to\nincorporate the GPU-centric PCIe data transfer paradigm in PyTorch for GNN\ntraining. Next, Hector intermediate representation (IR) and its code generator\nare proposed to introduce domain-specific high-level abstraction and\nsystematically address memory-intensive performance challenges for relational\nGNNs. Finally, in LLM training, the throughput has been increasingly\nconstrained by GPU memory capacity. To mitigate this, the SSDTrain offloading\nframework is designed and implemented. Together, these contributions show that\ncode generation and runtime techniques can systematically mitigate the data\nmanagement bottlenecks in deep learning training, which stem from the\ndata-intensive nature of workloads and the oversimplification inherent in the\ndeep learning training software stack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As deep learning models scale, their training cost has surged significantly.\nDue to both hardware advancements and limitations in current software stacks,\nthe need for data efficiency has risen. Data efficiency refers to the effective\nhiding of data access latency and the avoidance of unnecessary data movements.\nMajor challenges arise from the growing disparity between GPU memory bandwidth\nand computational throughput, imminent GPU memory capacity limitations, and\ninefficiencies in the PyTorch software stack, including a lack of\ndevice-specific PCIe transfer optimizations and high-level domain-specific\nabstractions. To effectively mitigate these data inefficiencies for deep\nlearning training, this dissertation analyzes data inefficiency in\nrepresentative deep training tasks, specifically in graph neural networks\n(GNNs) and large language models (LLMs). It then proposes novel runtime and\ncode generation techniques to mitigate these challenges and implements these\noptimizations seamlessly within the PyTorch stack while maintaining strong\nprogrammability and interoperability. First, PyTorch-Direct is devised to\nincorporate the GPU-centric PCIe data transfer paradigm in PyTorch for GNN\ntraining. Next, Hector intermediate representation (IR) and its code generator\nare proposed to introduce domain-specific high-level abstraction and\nsystematically address memory-intensive performance challenges for relational\nGNNs. Finally, in LLM training, the throughput has been increasingly\nconstrained by GPU memory capacity. To mitigate this, the SSDTrain offloading\nframework is designed and implemented. Together, these contributions show that\ncode generation and runtime techniques can systematically mitigate the data\nmanagement bottlenecks in deep learning training, which stem from the\ndata-intensive nature of workloads and the oversimplification inherent in the\ndeep learning training software stack."
                },
                "authors": [
                    {
                        "name": "Kun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kun Wu"
                },
                "author": "Kun Wu",
                "arxiv_comment": "Ph.D. Thesis, University of Illinois Urbana-Champaign, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04746v1",
                "updated": "2024-12-06T03:18:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    18,
                    18,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T03:18:18Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    18,
                    18,
                    4,
                    341,
                    0
                ],
                "title": "Diff4Steer: Steerable Diffusion Prior for Generative Music Retrieval\n  with Semantic Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diff4Steer: Steerable Diffusion Prior for Generative Music Retrieval\n  with Semantic Guidance"
                },
                "summary": "Modern music retrieval systems often rely on fixed representations of user\npreferences, limiting their ability to capture users' diverse and uncertain\nretrieval needs. To address this limitation, we introduce Diff4Steer, a novel\ngenerative retrieval framework that employs lightweight diffusion models to\nsynthesize diverse seed embeddings from user queries that represent potential\ndirections for music exploration. Unlike deterministic methods that map user\nquery to a single point in embedding space, Diff4Steer provides a statistical\nprior on the target modality (audio) for retrieval, effectively capturing the\nuncertainty and multi-faceted nature of user preferences. Furthermore,\nDiff4Steer can be steered by image or text inputs, enabling more flexible and\ncontrollable music discovery combined with nearest neighbor search. Our\nframework outperforms deterministic regression methods and LLM-based generative\nretrieval baseline in terms of retrieval and ranking metrics, demonstrating its\neffectiveness in capturing user preferences, leading to more diverse and\nrelevant recommendations. Listening examples are available at\ntinyurl.com/diff4steer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern music retrieval systems often rely on fixed representations of user\npreferences, limiting their ability to capture users' diverse and uncertain\nretrieval needs. To address this limitation, we introduce Diff4Steer, a novel\ngenerative retrieval framework that employs lightweight diffusion models to\nsynthesize diverse seed embeddings from user queries that represent potential\ndirections for music exploration. Unlike deterministic methods that map user\nquery to a single point in embedding space, Diff4Steer provides a statistical\nprior on the target modality (audio) for retrieval, effectively capturing the\nuncertainty and multi-faceted nature of user preferences. Furthermore,\nDiff4Steer can be steered by image or text inputs, enabling more flexible and\ncontrollable music discovery combined with nearest neighbor search. Our\nframework outperforms deterministic regression methods and LLM-based generative\nretrieval baseline in terms of retrieval and ranking metrics, demonstrating its\neffectiveness in capturing user preferences, leading to more diverse and\nrelevant recommendations. Listening examples are available at\ntinyurl.com/diff4steer."
                },
                "authors": [
                    {
                        "name": "Xuchan Bao"
                    },
                    {
                        "name": "Judith Yue Li"
                    },
                    {
                        "name": "Zhong Yi Wan"
                    },
                    {
                        "name": "Kun Su"
                    },
                    {
                        "name": "Timo Denk"
                    },
                    {
                        "name": "Joonseok Lee"
                    },
                    {
                        "name": "Dima Kuzmin"
                    },
                    {
                        "name": "Fei Sha"
                    }
                ],
                "author_detail": {
                    "name": "Fei Sha"
                },
                "author": "Fei Sha",
                "arxiv_comment": "NeurIPS 2024 Creative AI Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.08918v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.08918v3",
                "updated": "2024-12-06T02:46:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    2,
                    46,
                    38,
                    4,
                    341,
                    0
                ],
                "published": "2024-02-14T03:16:13Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    3,
                    16,
                    13,
                    2,
                    45,
                    0
                ],
                "title": "SimMLP: Training MLPs on Graphs without Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimMLP: Training MLPs on Graphs without Supervision"
                },
                "summary": "Graph Neural Networks (GNNs) have demonstrated their effectiveness in various\ngraph learning tasks, yet their reliance on neighborhood aggregation during\ninference poses challenges for deployment in latency-sensitive applications,\nsuch as real-time financial fraud detection. To address this limitation, recent\nstudies have proposed distilling knowledge from teacher GNNs into student\nMulti-Layer Perceptrons (MLPs) trained on node content, aiming to accelerate\ninference. However, these approaches often inadequately explore structural\ninformation when inferring unseen nodes. To this end, we introduce SimMLP, a\nSelf-supervised framework for learning MLPs on graphs, designed to fully\nintegrate rich structural information into MLPs. Notably, SimMLP is the first\nMLP-learning method that can achieve equivalence to GNNs in the optimal case.\nThe key idea is to employ self-supervised learning to align the representations\nencoded by graph context-aware GNNs and neighborhood dependency-free MLPs,\nthereby fully integrating the structural information into MLPs. We provide a\ncomprehensive theoretical analysis, demonstrating the equivalence between\nSimMLP and GNNs based on mutual information and inductive bias, highlighting\nSimMLP's advanced structural learning capabilities. Additionally, we conduct\nextensive experiments on 20 benchmark datasets, covering node classification,\nlink prediction, and graph classification, to showcase SimMLP's superiority\nover state-of-the-art baselines, particularly in scenarios involving unseen\nnodes (e.g., inductive and cold-start node classification) where structural\ninsights are crucial. Our codes are available at:\nhttps://github.com/Zehong-Wang/SimMLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have demonstrated their effectiveness in various\ngraph learning tasks, yet their reliance on neighborhood aggregation during\ninference poses challenges for deployment in latency-sensitive applications,\nsuch as real-time financial fraud detection. To address this limitation, recent\nstudies have proposed distilling knowledge from teacher GNNs into student\nMulti-Layer Perceptrons (MLPs) trained on node content, aiming to accelerate\ninference. However, these approaches often inadequately explore structural\ninformation when inferring unseen nodes. To this end, we introduce SimMLP, a\nSelf-supervised framework for learning MLPs on graphs, designed to fully\nintegrate rich structural information into MLPs. Notably, SimMLP is the first\nMLP-learning method that can achieve equivalence to GNNs in the optimal case.\nThe key idea is to employ self-supervised learning to align the representations\nencoded by graph context-aware GNNs and neighborhood dependency-free MLPs,\nthereby fully integrating the structural information into MLPs. We provide a\ncomprehensive theoretical analysis, demonstrating the equivalence between\nSimMLP and GNNs based on mutual information and inductive bias, highlighting\nSimMLP's advanced structural learning capabilities. Additionally, we conduct\nextensive experiments on 20 benchmark datasets, covering node classification,\nlink prediction, and graph classification, to showcase SimMLP's superiority\nover state-of-the-art baselines, particularly in scenarios involving unseen\nnodes (e.g., inductive and cold-start node classification) where structural\ninsights are crucial. Our codes are available at:\nhttps://github.com/Zehong-Wang/SimMLP."
                },
                "authors": [
                    {
                        "name": "Zehong Wang"
                    },
                    {
                        "name": "Zheyuan Zhang"
                    },
                    {
                        "name": "Chuxu Zhang"
                    },
                    {
                        "name": "Yanfang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Yanfang Ye"
                },
                "author": "Yanfang Ye",
                "arxiv_comment": "New Version: arXiv:2412.03864",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.08918v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.08918v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04726v1",
                "updated": "2024-12-06T02:34:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    2,
                    34,
                    40,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T02:34:40Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    2,
                    34,
                    40,
                    4,
                    341,
                    0
                ],
                "title": "BESSTIE: A Benchmark for Sentiment and Sarcasm Classification for\n  Varieties of English",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BESSTIE: A Benchmark for Sentiment and Sarcasm Classification for\n  Varieties of English"
                },
                "summary": "Despite large language models (LLMs) being known to exhibit bias against\nnon-mainstream varieties, there are no known labeled datasets for sentiment\nanalysis of English. To address this gap, we introduce BESSTIE, a benchmark for\nsentiment and sarcasm classification for three varieties of English: Australian\n(en-AU), Indian (en-IN), and British (en-UK). Using web-based content from two\ndomains, namely, Google Place reviews and Reddit comments, we collect datasets\nfor these language varieties using two methods: location-based and topic-based\nfiltering. Native speakers of the language varieties manually annotate the\ndatasets with sentiment and sarcasm labels. Subsequently, we fine-tune nine\nlarge language models (LLMs) (representing a range of encoder/decoder and\nmono/multilingual models) on these datasets, and evaluate their performance on\nthe two tasks. Our results reveal that the models consistently perform better\non inner-circle varieties (i.e., en-AU and en-UK), with significant performance\ndrops for en-IN, particularly in sarcasm detection. We also report challenges\nin cross-variety generalisation, highlighting the need for language\nvariety-specific datasets such as ours. BESSTIE promises to be a useful\nevaluative benchmark for future research in equitable LLMs, specifically in\nterms of language varieties. The BESSTIE datasets, code, and models are\ncurrently available on request, while the paper is under review. Please email\naditya.joshi@unsw.edu.au.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite large language models (LLMs) being known to exhibit bias against\nnon-mainstream varieties, there are no known labeled datasets for sentiment\nanalysis of English. To address this gap, we introduce BESSTIE, a benchmark for\nsentiment and sarcasm classification for three varieties of English: Australian\n(en-AU), Indian (en-IN), and British (en-UK). Using web-based content from two\ndomains, namely, Google Place reviews and Reddit comments, we collect datasets\nfor these language varieties using two methods: location-based and topic-based\nfiltering. Native speakers of the language varieties manually annotate the\ndatasets with sentiment and sarcasm labels. Subsequently, we fine-tune nine\nlarge language models (LLMs) (representing a range of encoder/decoder and\nmono/multilingual models) on these datasets, and evaluate their performance on\nthe two tasks. Our results reveal that the models consistently perform better\non inner-circle varieties (i.e., en-AU and en-UK), with significant performance\ndrops for en-IN, particularly in sarcasm detection. We also report challenges\nin cross-variety generalisation, highlighting the need for language\nvariety-specific datasets such as ours. BESSTIE promises to be a useful\nevaluative benchmark for future research in equitable LLMs, specifically in\nterms of language varieties. The BESSTIE datasets, code, and models are\ncurrently available on request, while the paper is under review. Please email\naditya.joshi@unsw.edu.au."
                },
                "authors": [
                    {
                        "name": "Dipankar Srirag"
                    },
                    {
                        "name": "Aditya Joshi"
                    },
                    {
                        "name": "Jordan Painter"
                    },
                    {
                        "name": "Diptesh Kanojia"
                    }
                ],
                "author_detail": {
                    "name": "Diptesh Kanojia"
                },
                "author": "Diptesh Kanojia",
                "arxiv_comment": "10 pages, 7 figures, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04718v1",
                "updated": "2024-12-06T02:17:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    2,
                    17,
                    30,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T02:17:30Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    2,
                    17,
                    30,
                    4,
                    341,
                    0
                ],
                "title": "Adaptive Optimization for Enhanced Efficiency in Large-Scale Language\n  Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Optimization for Enhanced Efficiency in Large-Scale Language\n  Model Training"
                },
                "summary": "With the rapid development of natural language processing technology,\nlarge-scale language models (LLM) have achieved remarkable results in a variety\nof tasks. However, how to effectively train these huge models and improve their\nperformance and computational efficiency remains an important challenge. This\npaper proposes an improved method based on adaptive optimization algorithm,\naiming to improve the training efficiency and final performance of LLM. Through\ncomparative experiments on the SQuAD and GLUE data sets, the experimental\nresults show that compared with traditional optimization algorithms (such as\nSGD, Momentum, AdaGrad, RMSProp and Adam), the adaptive optimization algorithm\nwe proposed has better accuracy and F1 score. Both have achieved significant\nimprovements, especially showed stronger training capabilities when processed\nlarge-scale texts and complex tasks. The research results verify the advantages\nof adaptive optimization algorithms in large-scale language model training and\nprovide new ideas and directions for future optimization methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of natural language processing technology,\nlarge-scale language models (LLM) have achieved remarkable results in a variety\nof tasks. However, how to effectively train these huge models and improve their\nperformance and computational efficiency remains an important challenge. This\npaper proposes an improved method based on adaptive optimization algorithm,\naiming to improve the training efficiency and final performance of LLM. Through\ncomparative experiments on the SQuAD and GLUE data sets, the experimental\nresults show that compared with traditional optimization algorithms (such as\nSGD, Momentum, AdaGrad, RMSProp and Adam), the adaptive optimization algorithm\nwe proposed has better accuracy and F1 score. Both have achieved significant\nimprovements, especially showed stronger training capabilities when processed\nlarge-scale texts and complex tasks. The research results verify the advantages\nof adaptive optimization algorithms in large-scale language model training and\nprovide new ideas and directions for future optimization methods."
                },
                "authors": [
                    {
                        "name": "Jiajing Chen"
                    },
                    {
                        "name": "Bingying Liu"
                    },
                    {
                        "name": "Xiaoxuan Liao"
                    },
                    {
                        "name": "Jia Gao"
                    },
                    {
                        "name": "Hongye Zheng"
                    },
                    {
                        "name": "Yue Li"
                    }
                ],
                "author_detail": {
                    "name": "Yue Li"
                },
                "author": "Yue Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10595v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10595v5",
                "updated": "2024-12-06T01:54:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    54,
                    59,
                    4,
                    341,
                    0
                ],
                "published": "2024-04-16T14:20:55Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    14,
                    20,
                    55,
                    1,
                    107,
                    0
                ],
                "title": "Automated Evaluation of Large Vision-Language Models on Self-driving\n  Corner Cases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Evaluation of Large Vision-Language Models on Self-driving\n  Corner Cases"
                },
                "summary": "Large Vision-Language Models (LVLMs) have received widespread attention for\nadvancing the interpretable self-driving. Existing evaluations of LVLMs\nprimarily focus on multi-faceted capabilities in natural circumstances, lacking\nautomated and quantifiable assessment for self-driving, let alone the severe\nroad corner cases. In this work, we propose CODA-LM, the very first benchmark\nfor the automatic evaluation of LVLMs for self-driving corner cases. We adopt a\nhierarchical data structure and prompt powerful LVLMs to analyze complex\ndriving scenes and generate high-quality pre-annotations for the human\nannotators, while for LVLM evaluation, we show that using the text-only large\nlanguage models (LLMs) as judges reveals even better alignment with human\npreferences than the LVLM judges. Moreover, with our CODA-LM, we build\nCODA-VLM, a new driving LVLM surpassing all open-sourced counterparts on\nCODA-LM. Our CODA-VLM performs comparably with GPT-4V, even surpassing GPT-4V\nby +21.42% on the regional perception task. We hope CODA-LM can become the\ncatalyst to promote interpretable self-driving empowered by LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) have received widespread attention for\nadvancing the interpretable self-driving. Existing evaluations of LVLMs\nprimarily focus on multi-faceted capabilities in natural circumstances, lacking\nautomated and quantifiable assessment for self-driving, let alone the severe\nroad corner cases. In this work, we propose CODA-LM, the very first benchmark\nfor the automatic evaluation of LVLMs for self-driving corner cases. We adopt a\nhierarchical data structure and prompt powerful LVLMs to analyze complex\ndriving scenes and generate high-quality pre-annotations for the human\nannotators, while for LVLM evaluation, we show that using the text-only large\nlanguage models (LLMs) as judges reveals even better alignment with human\npreferences than the LVLM judges. Moreover, with our CODA-LM, we build\nCODA-VLM, a new driving LVLM surpassing all open-sourced counterparts on\nCODA-LM. Our CODA-VLM performs comparably with GPT-4V, even surpassing GPT-4V\nby +21.42% on the regional perception task. We hope CODA-LM can become the\ncatalyst to promote interpretable self-driving empowered by LVLMs."
                },
                "authors": [
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Yanze Li"
                    },
                    {
                        "name": "Wenhua Zhang"
                    },
                    {
                        "name": "Yanxin Liu"
                    },
                    {
                        "name": "Pengxiang Li"
                    },
                    {
                        "name": "Ruiyuan Gao"
                    },
                    {
                        "name": "Lanqing Hong"
                    },
                    {
                        "name": "Meng Tian"
                    },
                    {
                        "name": "Xinhai Zhao"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Dit-Yan Yeung"
                    },
                    {
                        "name": "Huchuan Lu"
                    },
                    {
                        "name": "Xu Jia"
                    }
                ],
                "author_detail": {
                    "name": "Xu Jia"
                },
                "author": "Xu Jia",
                "arxiv_comment": "Accept by WACV 2025. Project Page:\n  https://coda-dataset.github.io/coda-lm/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10595v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10595v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18130v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18130v2",
                "updated": "2024-12-06T01:34:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    34,
                    38,
                    4,
                    341,
                    0
                ],
                "published": "2024-04-28T10:02:28Z",
                "published_parsed": [
                    2024,
                    4,
                    28,
                    10,
                    2,
                    28,
                    6,
                    119,
                    0
                ],
                "title": "Logic Agent: Enhancing Validity with Logic Rule Invocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logic Agent: Enhancing Validity with Logic Rule Invocation"
                },
                "summary": "Chain-of-Thought (CoT) prompting has emerged as a pivotal technique for\naugmenting the inferential capabilities of language models during reasoning\ntasks. Despite its advancements, CoT often grapples with challenges in\nvalidating reasoning validity and ensuring informativeness. Addressing these\nlimitations, this paper introduces the Logic Agent (LA), an agent-based\nframework aimed at enhancing the validity of reasoning processes in Large\nLanguage Models (LLMs) through strategic logic rule invocation. Unlike\nconventional approaches, LA transforms LLMs into logic agents that dynamically\napply propositional logic rules, initiating the reasoning process by converting\nnatural language inputs into structured logic forms. The logic agent leverages\na comprehensive set of predefined functions to systematically navigate the\nreasoning process. This methodology not only promotes the structured and\ncoherent generation of reasoning constructs but also significantly improves\ntheir interpretability and logical coherence. Through extensive\nexperimentation, we demonstrate LA's capacity to scale effectively across\nvarious model sizes, markedly improving the precision of complex reasoning\nacross diverse tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting has emerged as a pivotal technique for\naugmenting the inferential capabilities of language models during reasoning\ntasks. Despite its advancements, CoT often grapples with challenges in\nvalidating reasoning validity and ensuring informativeness. Addressing these\nlimitations, this paper introduces the Logic Agent (LA), an agent-based\nframework aimed at enhancing the validity of reasoning processes in Large\nLanguage Models (LLMs) through strategic logic rule invocation. Unlike\nconventional approaches, LA transforms LLMs into logic agents that dynamically\napply propositional logic rules, initiating the reasoning process by converting\nnatural language inputs into structured logic forms. The logic agent leverages\na comprehensive set of predefined functions to systematically navigate the\nreasoning process. This methodology not only promotes the structured and\ncoherent generation of reasoning constructs but also significantly improves\ntheir interpretability and logical coherence. Through extensive\nexperimentation, we demonstrate LA's capacity to scale effectively across\nvarious model sizes, markedly improving the precision of complex reasoning\nacross diverse tasks."
                },
                "authors": [
                    {
                        "name": "Hanmeng Liu"
                    },
                    {
                        "name": "Zhiyang Teng"
                    },
                    {
                        "name": "Chaoli Zhang"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "arxiv_comment": "The experiment is subject to certain errors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18130v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18130v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04703v1",
                "updated": "2024-12-06T01:29:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    29,
                    24,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T01:29:24Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    29,
                    24,
                    4,
                    341,
                    0
                ],
                "title": "Transformers Struggle to Learn to Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers Struggle to Learn to Search"
                },
                "summary": "Search is an ability foundational in many important tasks, and recent studies\nhave shown that large language models (LLMs) struggle to perform search\nrobustly. It is unknown whether this inability is due to a lack of data,\ninsufficient model parameters, or fundamental limitations of the transformer\narchitecture. In this work, we use the foundational graph connectivity problem\nas a testbed to generate effectively limitless high-coverage data to train\nsmall transformers and test whether they can learn to perform search. We find\nthat, when given the right training distribution, the transformer is able to\nlearn to search.\n  We analyze the algorithm that the transformer has learned through a novel\nmechanistic interpretability technique that enables us to extract the\ncomputation graph from the trained model. We find that for each vertex in the\ninput graph, transformers compute the set of vertices reachable from that\nvertex. Each layer then progressively expands these sets, allowing the model to\nsearch over a number of vertices exponential in the number of layers.\n  However, we find that as the input graph size increases, the transformer has\ngreater difficulty in learning the task. This difficulty is not resolved even\nas the number of parameters is increased, suggesting that increasing model\nscale will not lead to robust search abilities. We also find that performing\nsearch in-context (i.e., chain-of-thought) does not resolve this inability to\nlearn to search on larger graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search is an ability foundational in many important tasks, and recent studies\nhave shown that large language models (LLMs) struggle to perform search\nrobustly. It is unknown whether this inability is due to a lack of data,\ninsufficient model parameters, or fundamental limitations of the transformer\narchitecture. In this work, we use the foundational graph connectivity problem\nas a testbed to generate effectively limitless high-coverage data to train\nsmall transformers and test whether they can learn to perform search. We find\nthat, when given the right training distribution, the transformer is able to\nlearn to search.\n  We analyze the algorithm that the transformer has learned through a novel\nmechanistic interpretability technique that enables us to extract the\ncomputation graph from the trained model. We find that for each vertex in the\ninput graph, transformers compute the set of vertices reachable from that\nvertex. Each layer then progressively expands these sets, allowing the model to\nsearch over a number of vertices exponential in the number of layers.\n  However, we find that as the input graph size increases, the transformer has\ngreater difficulty in learning the task. This difficulty is not resolved even\nas the number of parameters is increased, suggesting that increasing model\nscale will not lead to robust search abilities. We also find that performing\nsearch in-context (i.e., chain-of-thought) does not resolve this inability to\nlearn to search on larger graphs."
                },
                "authors": [
                    {
                        "name": "Abulhair Saparov"
                    },
                    {
                        "name": "Srushti Pawar"
                    },
                    {
                        "name": "Shreyas Pimpalgaonkar"
                    },
                    {
                        "name": "Nitish Joshi"
                    },
                    {
                        "name": "Richard Yuanzhe Pang"
                    },
                    {
                        "name": "Vishakh Padmakumar"
                    },
                    {
                        "name": "Seyed Mehran Kazemi"
                    },
                    {
                        "name": "Najoung Kim"
                    },
                    {
                        "name": "He He"
                    }
                ],
                "author_detail": {
                    "name": "He He"
                },
                "author": "He He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04697v1",
                "updated": "2024-12-06T01:20:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    16,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T01:20:16Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    16,
                    4,
                    341,
                    0
                ],
                "title": "Privacy-Preserving Retrieval Augmented Generation with Differential\n  Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving Retrieval Augmented Generation with Differential\n  Privacy"
                },
                "summary": "With the recent remarkable advancement of large language models (LLMs), there\nhas been a growing interest in utilizing them in the domains with highly\nsensitive data that lies outside their training data. For this purpose,\nretrieval augmented generation (RAG) is particularly effective -- it assists\nLLMs by directly providing relevant information from the external knowledge\nsources. However, without extra privacy safeguards, RAG outputs risk leaking\nsensitive information from the external data source. In this work, we explore\nRAG under differential privacy (DP), a formal guarantee of data privacy. The\nmain challenge with differentially private RAG is how to generate long accurate\nanswers within a moderate privacy budget. We address this by proposing an\nalgorithm that smartly spends privacy budget only for the tokens that require\nthe sensitive information and uses the non-private LLM for other tokens. Our\nextensive empirical evaluations reveal that our algorithm outperforms the\nnon-RAG baseline under a reasonable privacy budget of $\\epsilon\\approx 10$\nacross different models and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the recent remarkable advancement of large language models (LLMs), there\nhas been a growing interest in utilizing them in the domains with highly\nsensitive data that lies outside their training data. For this purpose,\nretrieval augmented generation (RAG) is particularly effective -- it assists\nLLMs by directly providing relevant information from the external knowledge\nsources. However, without extra privacy safeguards, RAG outputs risk leaking\nsensitive information from the external data source. In this work, we explore\nRAG under differential privacy (DP), a formal guarantee of data privacy. The\nmain challenge with differentially private RAG is how to generate long accurate\nanswers within a moderate privacy budget. We address this by proposing an\nalgorithm that smartly spends privacy budget only for the tokens that require\nthe sensitive information and uses the non-private LLM for other tokens. Our\nextensive empirical evaluations reveal that our algorithm outperforms the\nnon-RAG baseline under a reasonable privacy budget of $\\epsilon\\approx 10$\nacross different models and datasets."
                },
                "authors": [
                    {
                        "name": "Tatsuki Koga"
                    },
                    {
                        "name": "Ruihan Wu"
                    },
                    {
                        "name": "Kamalika Chaudhuri"
                    }
                ],
                "author_detail": {
                    "name": "Kamalika Chaudhuri"
                },
                "author": "Kamalika Chaudhuri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04692v1",
                "updated": "2024-12-06T01:06:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    6,
                    37,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T01:06:37Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    6,
                    37,
                    4,
                    341,
                    0
                ],
                "title": "Smoothie: Label Free Language Model Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smoothie: Label Free Language Model Routing"
                },
                "summary": "Large language models (LLMs) are increasingly used in applications where LLM\ninputs may span many different tasks. Recent work has found that the choice of\nLLM is consequential, and different LLMs may be good for different input\nsamples. Prior approaches have thus explored how engineers might select an LLM\nto use for each sample (i.e. routing). While existing routing methods mostly\nrequire training auxiliary models on human-annotated data, our work explores\nwhether it is possible to perform unsupervised routing. We propose Smoothie, a\nweak supervision-inspired routing approach that requires no labeled data. Given\na set of outputs from different LLMs, Smoothie constructs a latent variable\ngraphical model over embedding representations of observable LLM outputs and\nunknown \"true\" outputs. Using this graphical model, we estimate\nsample-dependent quality scores for each LLM, and route each sample to the LLM\nwith the highest corresponding score. We find that Smoothie's LLM\nquality-scores correlate with ground-truth model quality (correctly identifying\nthe optimal model on 9/14 tasks), and that Smoothie outperforms baselines for\nrouting by up to 10 points accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used in applications where LLM\ninputs may span many different tasks. Recent work has found that the choice of\nLLM is consequential, and different LLMs may be good for different input\nsamples. Prior approaches have thus explored how engineers might select an LLM\nto use for each sample (i.e. routing). While existing routing methods mostly\nrequire training auxiliary models on human-annotated data, our work explores\nwhether it is possible to perform unsupervised routing. We propose Smoothie, a\nweak supervision-inspired routing approach that requires no labeled data. Given\na set of outputs from different LLMs, Smoothie constructs a latent variable\ngraphical model over embedding representations of observable LLM outputs and\nunknown \"true\" outputs. Using this graphical model, we estimate\nsample-dependent quality scores for each LLM, and route each sample to the LLM\nwith the highest corresponding score. We find that Smoothie's LLM\nquality-scores correlate with ground-truth model quality (correctly identifying\nthe optimal model on 9/14 tasks), and that Smoothie outperforms baselines for\nrouting by up to 10 points accuracy."
                },
                "authors": [
                    {
                        "name": "Neel Guha"
                    },
                    {
                        "name": "Mayee F. Chen"
                    },
                    {
                        "name": "Trevor Chow"
                    },
                    {
                        "name": "Ishan S. Khare"
                    },
                    {
                        "name": "Christopher Ré"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Ré"
                },
                "author": "Christopher Ré",
                "arxiv_comment": "24 pages, 8 figures, 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04690v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04690v1",
                "updated": "2024-12-06T01:05:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    5,
                    37,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T01:05:37Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    5,
                    37,
                    4,
                    341,
                    0
                ],
                "title": "LLM-Align: Utilizing Large Language Models for Entity Alignment in\n  Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Align: Utilizing Large Language Models for Entity Alignment in\n  Knowledge Graphs"
                },
                "summary": "Entity Alignment (EA) seeks to identify and match corresponding entities\nacross different Knowledge Graphs (KGs), playing a crucial role in knowledge\nfusion and integration. Embedding-based entity alignment (EA) has recently\ngained considerable attention, resulting in the emergence of many innovative\napproaches. Initially, these approaches concentrated on learning entity\nembeddings based on the structural features of knowledge graphs (KGs) as\ndefined by relation triples. Subsequent methods have integrated entities' names\nand attributes as supplementary information to improve the embeddings used for\nEA. However, existing methods lack a deep semantic understanding of entity\nattributes and relations. In this paper, we propose a Large Language Model\n(LLM) based Entity Alignment method, LLM-Align, which explores the\ninstruction-following and zero-shot capabilities of Large Language Models to\ninfer alignments of entities. LLM-Align uses heuristic methods to select\nimportant attributes and relations of entities, and then feeds the selected\ntriples of entities to an LLM to infer the alignment results. To guarantee the\nquality of alignment results, we design a multi-round voting mechanism to\nmitigate the hallucination and positional bias issues that occur with LLMs.\nExperiments on three EA datasets, demonstrating that our approach achieves\nstate-of-the-art performance compared to existing EA methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity Alignment (EA) seeks to identify and match corresponding entities\nacross different Knowledge Graphs (KGs), playing a crucial role in knowledge\nfusion and integration. Embedding-based entity alignment (EA) has recently\ngained considerable attention, resulting in the emergence of many innovative\napproaches. Initially, these approaches concentrated on learning entity\nembeddings based on the structural features of knowledge graphs (KGs) as\ndefined by relation triples. Subsequent methods have integrated entities' names\nand attributes as supplementary information to improve the embeddings used for\nEA. However, existing methods lack a deep semantic understanding of entity\nattributes and relations. In this paper, we propose a Large Language Model\n(LLM) based Entity Alignment method, LLM-Align, which explores the\ninstruction-following and zero-shot capabilities of Large Language Models to\ninfer alignments of entities. LLM-Align uses heuristic methods to select\nimportant attributes and relations of entities, and then feeds the selected\ntriples of entities to an LLM to infer the alignment results. To guarantee the\nquality of alignment results, we design a multi-round voting mechanism to\nmitigate the hallucination and positional bias issues that occur with LLMs.\nExperiments on three EA datasets, demonstrating that our approach achieves\nstate-of-the-art performance compared to existing EA methods."
                },
                "authors": [
                    {
                        "name": "Xuan Chen"
                    },
                    {
                        "name": "Tong Lu"
                    },
                    {
                        "name": "Zhichun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhichun Wang"
                },
                "author": "Zhichun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04690v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04690v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04683v1",
                "updated": "2024-12-06T00:46:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    0,
                    46,
                    20,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T00:46:20Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    0,
                    46,
                    20,
                    4,
                    341,
                    0
                ],
                "title": "From Principles to Practice: A Deep Dive into AI Ethics and Regulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Principles to Practice: A Deep Dive into AI Ethics and Regulations"
                },
                "summary": "In the rapidly evolving domain of Artificial Intelligence (AI), the complex\ninteraction between innovation and regulation has become an emerging focus of\nour society. Despite tremendous advancements in AI's capabilities to excel in\nspecific tasks and contribute to diverse sectors, establishing a high degree of\ntrust in AI-generated outputs and decisions necessitates meticulous caution and\ncontinuous oversight. A broad spectrum of stakeholders, including governmental\nbodies, private sector corporations, academic institutions, and individuals,\nhave launched significant initiatives. These efforts include developing ethical\nguidelines for AI and engaging in vibrant discussions on AI ethics, both among\nAI practitioners and within the broader society. This article thoroughly\nanalyzes the ground-breaking AI regulatory framework proposed by the European\nUnion. It delves into the fundamental ethical principles of safety,\ntransparency, non-discrimination, traceability, and environmental\nsustainability for AI developments and deployments. Considering the technical\nefforts and strategies undertaken by academics and industry to uphold these\nprinciples, we explore the synergies and conflicts among the five ethical\nprinciples. Through this lens, work presents a forward-looking perspective on\nthe future of AI regulations, advocating for a harmonized approach that\nsafeguards societal values while encouraging technological advancement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving domain of Artificial Intelligence (AI), the complex\ninteraction between innovation and regulation has become an emerging focus of\nour society. Despite tremendous advancements in AI's capabilities to excel in\nspecific tasks and contribute to diverse sectors, establishing a high degree of\ntrust in AI-generated outputs and decisions necessitates meticulous caution and\ncontinuous oversight. A broad spectrum of stakeholders, including governmental\nbodies, private sector corporations, academic institutions, and individuals,\nhave launched significant initiatives. These efforts include developing ethical\nguidelines for AI and engaging in vibrant discussions on AI ethics, both among\nAI practitioners and within the broader society. This article thoroughly\nanalyzes the ground-breaking AI regulatory framework proposed by the European\nUnion. It delves into the fundamental ethical principles of safety,\ntransparency, non-discrimination, traceability, and environmental\nsustainability for AI developments and deployments. Considering the technical\nefforts and strategies undertaken by academics and industry to uphold these\nprinciples, we explore the synergies and conflicts among the five ethical\nprinciples. Through this lens, work presents a forward-looking perspective on\nthe future of AI regulations, advocating for a harmonized approach that\nsafeguards societal values while encouraging technological advancement."
                },
                "authors": [
                    {
                        "name": "Nan Sun"
                    },
                    {
                        "name": "Yuantian Miao"
                    },
                    {
                        "name": "Hao Jiang"
                    },
                    {
                        "name": "Ming Ding"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Submitted to Artificial Intelligence Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08391v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08391v2",
                "updated": "2024-12-05T23:48:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    23,
                    48,
                    19,
                    3,
                    340,
                    0
                ],
                "published": "2024-06-12T16:41:31Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    16,
                    41,
                    31,
                    2,
                    164,
                    0
                ],
                "title": "Large Language Models Must Be Taught to Know What They Don't Know",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Must Be Taught to Know What They Don't Know"
                },
                "summary": "When using large language models (LLMs) in high-stakes applications, we need\nto know when we can trust their predictions. Some works argue that prompting\nhigh-performance LLMs is sufficient to produce calibrated uncertainties, while\nothers introduce sampling methods that can be prohibitively expensive. In this\nwork, we first argue that prompting on its own is insufficient to achieve good\ncalibration and then show that fine-tuning on a small dataset of correct and\nincorrect answers can create an uncertainty estimate with good generalization\nand small computational overhead. We show that a thousand graded examples are\nsufficient to outperform baseline methods and that training through the\nfeatures of a model is necessary for good performance and tractable for large\nopen-source models when using LoRA. We also investigate the mechanisms that\nenable reliable LLM uncertainty estimation, finding that many models can be\nused as general-purpose uncertainty estimators, applicable not just to their\nown uncertainties but also the uncertainty of other models. Lastly, we show\nthat uncertainty estimates inform human use of LLMs in human-AI collaborative\nsettings through a user study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When using large language models (LLMs) in high-stakes applications, we need\nto know when we can trust their predictions. Some works argue that prompting\nhigh-performance LLMs is sufficient to produce calibrated uncertainties, while\nothers introduce sampling methods that can be prohibitively expensive. In this\nwork, we first argue that prompting on its own is insufficient to achieve good\ncalibration and then show that fine-tuning on a small dataset of correct and\nincorrect answers can create an uncertainty estimate with good generalization\nand small computational overhead. We show that a thousand graded examples are\nsufficient to outperform baseline methods and that training through the\nfeatures of a model is necessary for good performance and tractable for large\nopen-source models when using LoRA. We also investigate the mechanisms that\nenable reliable LLM uncertainty estimation, finding that many models can be\nused as general-purpose uncertainty estimators, applicable not just to their\nown uncertainties but also the uncertainty of other models. Lastly, we show\nthat uncertainty estimates inform human use of LLMs in human-AI collaborative\nsettings through a user study."
                },
                "authors": [
                    {
                        "name": "Sanyam Kapoor"
                    },
                    {
                        "name": "Nate Gruver"
                    },
                    {
                        "name": "Manley Roberts"
                    },
                    {
                        "name": "Katherine Collins"
                    },
                    {
                        "name": "Arka Pal"
                    },
                    {
                        "name": "Umang Bhatt"
                    },
                    {
                        "name": "Adrian Weller"
                    },
                    {
                        "name": "Samuel Dooley"
                    },
                    {
                        "name": "Micah Goldblum"
                    },
                    {
                        "name": "Andrew Gordon Wilson"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Gordon Wilson"
                },
                "author": "Andrew Gordon Wilson",
                "arxiv_comment": "NeurIPS 2024 Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08391v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08391v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04661v1",
                "updated": "2024-12-05T23:10:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    23,
                    10,
                    56,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T23:10:56Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    23,
                    10,
                    56,
                    3,
                    340,
                    0
                ],
                "title": "HEAL: Hierarchical Embedding Alignment Loss for Improved Retrieval and\n  Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEAL: Hierarchical Embedding Alignment Loss for Improved Retrieval and\n  Representation Learning"
                },
                "summary": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by\nintegrating external document retrieval to provide domain-specific or\nup-to-date knowledge. The effectiveness of RAG depends on the relevance of\nretrieved documents, which is influenced by the semantic alignment of\nembeddings with the domain's specialized content. Although full fine-tuning can\nalign language models to specific domains, it is computationally intensive and\ndemands substantial data. This paper introduces Hierarchical Embedding\nAlignment Loss (HEAL), a novel method that leverages hierarchical fuzzy\nclustering with matrix factorization within contrastive learning to efficiently\nalign LLM embeddings with domain-specific content. HEAL computes\nlevel/depth-wise contrastive losses and incorporates hierarchical penalties to\nalign embeddings with the underlying relationships in label hierarchies. This\napproach enhances retrieval relevance and document classification, effectively\nreducing hallucinations in LLM outputs. In our experiments, we benchmark and\nevaluate HEAL across diverse domains, including Healthcare, Material Science,\nCyber-security, and Applied Maths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by\nintegrating external document retrieval to provide domain-specific or\nup-to-date knowledge. The effectiveness of RAG depends on the relevance of\nretrieved documents, which is influenced by the semantic alignment of\nembeddings with the domain's specialized content. Although full fine-tuning can\nalign language models to specific domains, it is computationally intensive and\ndemands substantial data. This paper introduces Hierarchical Embedding\nAlignment Loss (HEAL), a novel method that leverages hierarchical fuzzy\nclustering with matrix factorization within contrastive learning to efficiently\nalign LLM embeddings with domain-specific content. HEAL computes\nlevel/depth-wise contrastive losses and incorporates hierarchical penalties to\nalign embeddings with the underlying relationships in label hierarchies. This\napproach enhances retrieval relevance and document classification, effectively\nreducing hallucinations in LLM outputs. In our experiments, we benchmark and\nevaluate HEAL across diverse domains, including Healthcare, Material Science,\nCyber-security, and Applied Maths."
                },
                "authors": [
                    {
                        "name": "Manish Bhattarai"
                    },
                    {
                        "name": "Ryan Barron"
                    },
                    {
                        "name": "Maksim Eren"
                    },
                    {
                        "name": "Minh Vu"
                    },
                    {
                        "name": "Vesselin Grantcharov"
                    },
                    {
                        "name": "Ismael Boureima"
                    },
                    {
                        "name": "Valentin Stanev"
                    },
                    {
                        "name": "Cynthia Matuszek"
                    },
                    {
                        "name": "Vladimir Valtchinov"
                    },
                    {
                        "name": "Kim Rasmussen"
                    },
                    {
                        "name": "Boian Alexandrov"
                    }
                ],
                "author_detail": {
                    "name": "Boian Alexandrov"
                },
                "author": "Boian Alexandrov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04659v1",
                "updated": "2024-12-05T23:05:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    23,
                    5,
                    31,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T23:05:31Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    23,
                    5,
                    31,
                    3,
                    340,
                    0
                ],
                "title": "LiveNet: Robust, Minimally Invasive Multi-Robot Control for Safe and\n  Live Navigation in Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiveNet: Robust, Minimally Invasive Multi-Robot Control for Safe and\n  Live Navigation in Constrained Environments"
                },
                "summary": "Robots in densely populated real-world environments frequently encounter\nconstrained and cluttered situations such as passing through narrow doorways,\nhallways, and corridor intersections, where conflicts over limited space result\nin collisions or deadlocks among the robots. Current decentralized\nstate-of-the-art optimization- and neural network-based approaches (i) are\npredominantly designed for general open spaces, and (ii) are overly\nconservative, either guaranteeing safety, or liveness, but not both. While some\nsolutions rely on centralized conflict resolution, their highly invasive\ntrajectories make them impractical for real-world deployment. This paper\nintroduces LiveNet, a fully decentralized and robust neural network controller\nthat enables human-like yielding and passing, resulting in agile,\nnon-conservative, deadlock-free, and safe, navigation in congested,\nconflict-prone spaces. LiveNet is minimally invasive, without requiring\ninter-agent communication or cooperative behavior. The key insight behind\nLiveNet is a unified CBF formulation for simultaneous safety and liveness,\nwhich we integrate within a neural network for robustness. We evaluated LiveNet\nin simulation and found that general multi-robot optimization- and\nlearning-based navigation methods fail to even reach the goal, and while\nmethods designed specially for such environments do succeed, they are 10-20\ntimes slower, 4-5 times more invasive, and much less robust to variations in\nthe scenario configuration such as changes in the start states and goal states,\namong others. We open-source the LiveNet code at\nhttps://github.com/srikarg89/LiveNet{https://github.com/srikarg89/LiveNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robots in densely populated real-world environments frequently encounter\nconstrained and cluttered situations such as passing through narrow doorways,\nhallways, and corridor intersections, where conflicts over limited space result\nin collisions or deadlocks among the robots. Current decentralized\nstate-of-the-art optimization- and neural network-based approaches (i) are\npredominantly designed for general open spaces, and (ii) are overly\nconservative, either guaranteeing safety, or liveness, but not both. While some\nsolutions rely on centralized conflict resolution, their highly invasive\ntrajectories make them impractical for real-world deployment. This paper\nintroduces LiveNet, a fully decentralized and robust neural network controller\nthat enables human-like yielding and passing, resulting in agile,\nnon-conservative, deadlock-free, and safe, navigation in congested,\nconflict-prone spaces. LiveNet is minimally invasive, without requiring\ninter-agent communication or cooperative behavior. The key insight behind\nLiveNet is a unified CBF formulation for simultaneous safety and liveness,\nwhich we integrate within a neural network for robustness. We evaluated LiveNet\nin simulation and found that general multi-robot optimization- and\nlearning-based navigation methods fail to even reach the goal, and while\nmethods designed specially for such environments do succeed, they are 10-20\ntimes slower, 4-5 times more invasive, and much less robust to variations in\nthe scenario configuration such as changes in the start states and goal states,\namong others. We open-source the LiveNet code at\nhttps://github.com/srikarg89/LiveNet{https://github.com/srikarg89/LiveNet."
                },
                "authors": [
                    {
                        "name": "Srikar Gouru"
                    },
                    {
                        "name": "Siddharth Lakkoju"
                    },
                    {
                        "name": "Rohan Chandra"
                    }
                ],
                "author_detail": {
                    "name": "Rohan Chandra"
                },
                "author": "Rohan Chandra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04657v1",
                "updated": "2024-12-05T23:02:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    23,
                    2,
                    2,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T23:02:02Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    23,
                    2,
                    2,
                    3,
                    340,
                    0
                ],
                "title": "An Efficient Model Maintenance Approach for MLOps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Model Maintenance Approach for MLOps"
                },
                "summary": "In recent years, many industries have utilized machine learning models (ML)\nin their systems. Ideally, machine learning models should be trained on and\napplied to data from the same distributions. However, the data evolves over\ntime in many application areas, leading to data and concept drift, which in\nturn causes the performance of the ML models to degrade over time. Therefore,\nmaintaining up to date ML models plays a critical role in the MLOps pipeline.\nExisting ML model maintenance approaches are often computationally resource\nintensive, costly, time consuming, and model dependent. Thus, we propose an\nimproved MLOps pipeline, a new model maintenance approach and a Similarity\nBased Model Reuse (SimReuse) tool to address the challenges of ML model\nmaintenance. We identify seasonal and recurrent distribution patterns in time\nseries datasets throughout a preliminary study. Recurrent distribution patterns\nenable us to reuse previously trained models for similar distributions in the\nfuture, thus avoiding frequent retraining. Then, we integrated the model reuse\napproach into the MLOps pipeline and proposed our improved MLOps pipeline.\nFurthermore, we develop SimReuse, a tool to implement the new components of our\nMLOps pipeline to store models and reuse them for inference of data segments\nwith similar data distributions in the future. Our evaluation results on four\ntime series datasets demonstrate that our model reuse approach can maintain the\nperformance of models while significantly reducing maintenance time and costs.\nOur model reuse approach achieves ML performance comparable to the best\nbaseline, while being 15 times more efficient in terms of computation time and\ncosts. Therefore, industries and practitioners can benefit from our approach\nand use our tool to maintain the performance of their ML models in the\ndeployment phase to reduce their maintenance costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, many industries have utilized machine learning models (ML)\nin their systems. Ideally, machine learning models should be trained on and\napplied to data from the same distributions. However, the data evolves over\ntime in many application areas, leading to data and concept drift, which in\nturn causes the performance of the ML models to degrade over time. Therefore,\nmaintaining up to date ML models plays a critical role in the MLOps pipeline.\nExisting ML model maintenance approaches are often computationally resource\nintensive, costly, time consuming, and model dependent. Thus, we propose an\nimproved MLOps pipeline, a new model maintenance approach and a Similarity\nBased Model Reuse (SimReuse) tool to address the challenges of ML model\nmaintenance. We identify seasonal and recurrent distribution patterns in time\nseries datasets throughout a preliminary study. Recurrent distribution patterns\nenable us to reuse previously trained models for similar distributions in the\nfuture, thus avoiding frequent retraining. Then, we integrated the model reuse\napproach into the MLOps pipeline and proposed our improved MLOps pipeline.\nFurthermore, we develop SimReuse, a tool to implement the new components of our\nMLOps pipeline to store models and reuse them for inference of data segments\nwith similar data distributions in the future. Our evaluation results on four\ntime series datasets demonstrate that our model reuse approach can maintain the\nperformance of models while significantly reducing maintenance time and costs.\nOur model reuse approach achieves ML performance comparable to the best\nbaseline, while being 15 times more efficient in terms of computation time and\ncosts. Therefore, industries and practitioners can benefit from our approach\nand use our tool to maintain the performance of their ML models in the\ndeployment phase to reduce their maintenance costs."
                },
                "authors": [
                    {
                        "name": "Forough Majidi"
                    },
                    {
                        "name": "Foutse Khomh"
                    },
                    {
                        "name": "Heng Li"
                    },
                    {
                        "name": "Amin Nikanjam"
                    }
                ],
                "author_detail": {
                    "name": "Amin Nikanjam"
                },
                "author": "Amin Nikanjam",
                "arxiv_comment": "34 Pages, 25 Figures, 12 Tables, 1 Algorithm, Submitted to a journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]