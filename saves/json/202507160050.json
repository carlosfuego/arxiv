[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.10524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10524v1",
                "updated": "2025-07-14T17:49:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    49,
                    0,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T17:49:00Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    49,
                    0,
                    0,
                    195,
                    0
                ],
                "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation"
                },
                "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost."
                },
                "authors": [
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Yujin Kim"
                    },
                    {
                        "name": "Reza Bayat"
                    },
                    {
                        "name": "Sungnyun Kim"
                    },
                    {
                        "name": "Jiyoun Ha"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "Hrayr Harutyunyan"
                    },
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Aaron Courville"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "36 pages, 9 figures, 14 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v3",
                "updated": "2025-07-14T16:14:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    14,
                    49,
                    0,
                    195,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at https://github.com/THU-MIG/PrefixKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at https://github.com/THU-MIG/PrefixKV."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jiaxin Li"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "12 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10367v1",
                "updated": "2025-07-14T15:09:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T15:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline"
                },
                "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junbin Kang"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Shaohong Guo"
                    },
                    {
                        "name": "Ziyan Qiu"
                    },
                    {
                        "name": "Mingzhen You"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Anqi Yu"
                    },
                    {
                        "name": "Tianhong Ding"
                    },
                    {
                        "name": "Xinwei Hu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by NSDI'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22791v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22791v3",
                "updated": "2025-07-15T12:59:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    59,
                    47,
                    1,
                    196,
                    0
                ],
                "published": "2025-06-28T07:25:12Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    7,
                    25,
                    12,
                    5,
                    179,
                    0
                ],
                "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models"
                },
                "summary": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications."
                },
                "authors": [
                    {
                        "name": "Jianxin Yan"
                    },
                    {
                        "name": "Wangze Ni"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22791v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22791v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01465v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01465v2",
                "updated": "2025-07-14T09:45:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    45,
                    34,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-02T08:24:50Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    24,
                    50,
                    2,
                    183,
                    0
                ],
                "title": "Pruning the Tree: Rethinking RPKI Architecture From The Ground Up",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning the Tree: Rethinking RPKI Architecture From The Ground Up"
                },
                "summary": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nwhich introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70\\% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nwhich introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70\\% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations."
                },
                "authors": [
                    {
                        "name": "Haya Schulmann"
                    },
                    {
                        "name": "Niklas Vogel"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Vogel"
                },
                "author": "Niklas Vogel",
                "arxiv_comment": "Accepted for publication at NDSS2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01465v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01465v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10069v1",
                "updated": "2025-07-14T08:53:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T08:53:48Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism"
                },
                "summary": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we propose Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we propose Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs)."
                },
                "authors": [
                    {
                        "name": "Zedong Liu"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Guangming Tan"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Dingwen Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dingwen Tao"
                },
                "author": "Dingwen Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00929v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00929v3",
                "updated": "2025-07-15T07:47:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    7,
                    47,
                    52,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-01T16:36:23Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    36,
                    23,
                    1,
                    182,
                    0
                ],
                "title": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival"
                },
                "summary": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy."
                },
                "authors": [
                    {
                        "name": "Giulio Bordieri"
                    },
                    {
                        "name": "Marta Missiaggia"
                    },
                    {
                        "name": "Gianluca Lattanzi"
                    },
                    {
                        "name": "Carmen Villagrasa"
                    },
                    {
                        "name": "Yann Perrot"
                    },
                    {
                        "name": "Francesco G. Cordoni"
                    }
                ],
                "author_detail": {
                    "name": "Francesco G. Cordoni"
                },
                "author": "Francesco G. Cordoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00929v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00929v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v3",
                "updated": "2025-07-14T07:05:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    5,
                    28,
                    0,
                    195,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-Gonz√°lez"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Mart√≠n"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02814v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02814v2",
                "updated": "2025-07-14T07:03:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    3,
                    30,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-05T05:22:14Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    22,
                    14,
                    1,
                    310,
                    0
                ],
                "title": "The Hitchhiker's Guide to Programming and Optimizing Cache Coherent\n  Heterogeneous Systems: CXL, NVLink-C2C, and AMD Infinity Fabric",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hitchhiker's Guide to Programming and Optimizing Cache Coherent\n  Heterogeneous Systems: CXL, NVLink-C2C, and AMD Infinity Fabric"
                },
                "summary": "We present a thorough analysis of the use of modern heterogeneous systems\ninterconnected by various cachecoherent links, including CXL, NVLink-C2C, and\nInfinity Fabric. We studied a wide range of server systems that combined CPUs\nfrom different vendors and various types of coherent memory devices, including\nCXL memory expander, CXL pool, CXL shared memory, GH200 GPU, and AMD MI300a\nHBM. For this study, we developed a heterogeneous memory benchmark suite,\nHeimdall, to profile the performance of such heterogeneous systems and present\na detailed performance comparison across systems. By leveraging H E I M DA L L\n, we unveiled the detailed architecture design in these systems, drew\nobservations on optimizing performance for workloads, and pointed out\ndirections for future development of cache coherent heterogeneous systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a thorough analysis of the use of modern heterogeneous systems\ninterconnected by various cachecoherent links, including CXL, NVLink-C2C, and\nInfinity Fabric. We studied a wide range of server systems that combined CPUs\nfrom different vendors and various types of coherent memory devices, including\nCXL memory expander, CXL pool, CXL shared memory, GH200 GPU, and AMD MI300a\nHBM. For this study, we developed a heterogeneous memory benchmark suite,\nHeimdall, to profile the performance of such heterogeneous systems and present\na detailed performance comparison across systems. By leveraging H E I M DA L L\n, we unveiled the detailed architecture design in these systems, drew\nobservations on optimizing performance for workloads, and pointed out\ndirections for future development of cache coherent heterogeneous systems."
                },
                "authors": [
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Luyi Li"
                    },
                    {
                        "name": "Jangseon Park"
                    },
                    {
                        "name": "Jinpyo Kim"
                    },
                    {
                        "name": "Theodore Michailidis"
                    },
                    {
                        "name": "Yue Pan"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Tajana Rosing"
                    },
                    {
                        "name": "Dean Tullsen"
                    },
                    {
                        "name": "Steven Swanson"
                    },
                    {
                        "name": "Jishen Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jishen Zhao"
                },
                "author": "Jishen Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02814v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02814v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13820v2",
                "updated": "2025-07-14T02:22:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    2,
                    22,
                    43,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-21T03:52:41Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "title": "InstCache: A Predictive Cache for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstCache: A Predictive Cache for LLM Serving"
                },
                "summary": "The revolutionary capabilities of Large Language Models (LLMs) are attracting\nrapidly growing popularity and leading to soaring user requests to inference\nserving systems. Caching techniques, which leverage data reuse to reduce\ncomputation, offer opportunities to optimize the performance of LLM inference\nengines. On the one hand, the low-level key-value (KV) cache working at the\ntoken level is widely adopted, albeit it incurs significant overhead as request\nvolume grows. On the other hand, instruction-level caching, which stores full\ninstruction-response pairs, is expected to play an increasingly crucial role.\nHowever, the high variability in the content and length of instructions make it\nrare for identical instructions to recur within a short time window, presenting\nchallenges for effective caching instruction-response pairs. To address this\nchallenge, we propose InstCache, a predictive caching mechanism for LLM serving\nsystems. Leveraging the capability of LLMs, we can effectively reorder the\nrepresentation space of instruction texts and develop a sufficient level of\nspatial locality. Such spatial locality enables us to predict potential\ninstructions located in a compact region in the space, resulting in an\neffective caching system at runtime. Experimental results demonstrate that\nInstCache achieves a 2.3x higher hit rate compared to the upper bound of\ntraditional caching mechanisms on WildChat dataset and reduces the time per\noutput token of vLLM by up to 42.0% and 50.0% on LMSys and Moss datasets,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The revolutionary capabilities of Large Language Models (LLMs) are attracting\nrapidly growing popularity and leading to soaring user requests to inference\nserving systems. Caching techniques, which leverage data reuse to reduce\ncomputation, offer opportunities to optimize the performance of LLM inference\nengines. On the one hand, the low-level key-value (KV) cache working at the\ntoken level is widely adopted, albeit it incurs significant overhead as request\nvolume grows. On the other hand, instruction-level caching, which stores full\ninstruction-response pairs, is expected to play an increasingly crucial role.\nHowever, the high variability in the content and length of instructions make it\nrare for identical instructions to recur within a short time window, presenting\nchallenges for effective caching instruction-response pairs. To address this\nchallenge, we propose InstCache, a predictive caching mechanism for LLM serving\nsystems. Leveraging the capability of LLMs, we can effectively reorder the\nrepresentation space of instruction texts and develop a sufficient level of\nspatial locality. Such spatial locality enables us to predict potential\ninstructions located in a compact region in the space, resulting in an\neffective caching system at runtime. Experimental results demonstrate that\nInstCache achieves a 2.3x higher hit rate compared to the upper bound of\ntraditional caching mechanisms on WildChat dataset and reduces the time per\noutput token of vLLM by up to 42.0% and 50.0% on LMSys and Moss datasets,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Longwei Zou"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Jiamu Kang"
                    },
                    {
                        "name": "Tingfeng Liu"
                    },
                    {
                        "name": "Jiangang Kong"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09500v1",
                "updated": "2025-07-13T05:37:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    13,
                    5,
                    37,
                    33,
                    6,
                    194,
                    0
                ],
                "published": "2025-07-13T05:37:33Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    5,
                    37,
                    33,
                    6,
                    194,
                    0
                ],
                "title": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations"
                },
                "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under challenging real-world distribution shifts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under challenging real-world distribution shifts."
                },
                "authors": [
                    {
                        "name": "Yiwen Liang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Zihan Zhou"
                    },
                    {
                        "name": "Mengyao Lyu"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Shuaicheng Niu"
                    },
                    {
                        "name": "Sicheng Zhao"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "Accepted at the 33rd ACM International Conference on Multimedia(ACM\n  MM 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07776v2",
                "updated": "2025-07-13T04:42:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    13,
                    4,
                    42,
                    28,
                    6,
                    194,
                    0
                ],
                "published": "2025-02-11T18:58:04Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "title": "Auditing Prompt Caching in Language Model APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing Prompt Caching in Language Model APIs"
                },
                "summary": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known."
                },
                "authors": [
                    {
                        "name": "Chenchen Gu"
                    },
                    {
                        "name": "Xiang Lisa Li"
                    },
                    {
                        "name": "Rohith Kuditipudi"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v4",
                "updated": "2025-07-11T22:14:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    22,
                    14,
                    1,
                    4,
                    192,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "Accepted by TPAMI 2025. arXiv admin note: substantial text overlap\n  with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09025v1",
                "updated": "2025-07-11T21:19:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    21,
                    19,
                    18,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T21:19:18Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    21,
                    19,
                    18,
                    4,
                    192,
                    0
                ],
                "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lizard: An Efficient Linearization Framework for Large Language Models"
                },
                "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks."
                },
                "authors": [
                    {
                        "name": "Chien Van Nguyen"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Hanieh Deilamsalehy"
                    },
                    {
                        "name": "Puneet Mathur"
                    },
                    {
                        "name": "Viet Dac Lai"
                    },
                    {
                        "name": "Haoliang Wang"
                    },
                    {
                        "name": "Jayakumar Subramanian"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Nikos Vlassis"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Thien Huu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Thien Huu Nguyen"
                },
                "author": "Thien Huu Nguyen",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v3",
                "updated": "2025-07-11T19:57:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    19,
                    57,
                    51,
                    4,
                    192,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "HotSwap: Enabling Live Dependency Sharing in Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HotSwap: Enabling Live Dependency Sharing in Serverless Computing"
                },
                "summary": "This work presents HotSwap, a novel provider-side cold-start optimization for\nserverless computing. This optimization reduces cold-start time when booting\nand loading dependencies at runtime inside a function container. Previous\nresearch has extensively focused on reducing cold-start latency for specific\nfunctions. However, little attention has been given to skewed production\nworkloads. In such cases, cross-function optimization becomes essential.\nWithout cross-function optimization, a cloud provider is left with two equally\npoor options: (i) Either the cloud provider gives up optimization for each\nfunction in the long tail (which is slow); or (ii) the cloud provider applies\nfunction-specific optimizations (e.g., cache function images) to every function\nin the long tail (which violates the vendor's cache constraints). HotSwap\ndemonstrates cross-function optimization using a novel pre-warming strategy. In\nthis strategy, a pre-initialized live dependency image is migrated to the new\nfunction instance. At the same time, HotSwap respects the provider's cache\nconstraints, because a single pre-warmed dependency image in the cache can be\nshared among all serverless functions that require that image. HotSwap has been\ntested on seven representative functions from FunctionBench. In those tests,\nHotSwap accelerates dependency loading for those serverless functions with\nlarge dependency requirements by a factor ranging from 2.2 to 3.2. Simulation\nexperiments using Azure traces indicate that HotSwap can save 88\\% of space,\ncompared with a previous function-specific method, PreBaking, when sharing a\ndependency image among ten different functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents HotSwap, a novel provider-side cold-start optimization for\nserverless computing. This optimization reduces cold-start time when booting\nand loading dependencies at runtime inside a function container. Previous\nresearch has extensively focused on reducing cold-start latency for specific\nfunctions. However, little attention has been given to skewed production\nworkloads. In such cases, cross-function optimization becomes essential.\nWithout cross-function optimization, a cloud provider is left with two equally\npoor options: (i) Either the cloud provider gives up optimization for each\nfunction in the long tail (which is slow); or (ii) the cloud provider applies\nfunction-specific optimizations (e.g., cache function images) to every function\nin the long tail (which violates the vendor's cache constraints). HotSwap\ndemonstrates cross-function optimization using a novel pre-warming strategy. In\nthis strategy, a pre-initialized live dependency image is migrated to the new\nfunction instance. At the same time, HotSwap respects the provider's cache\nconstraints, because a single pre-warmed dependency image in the cache can be\nshared among all serverless functions that require that image. HotSwap has been\ntested on seven representative functions from FunctionBench. In those tests,\nHotSwap accelerates dependency loading for those serverless functions with\nlarge dependency requirements by a factor ranging from 2.2 to 3.2. Simulation\nexperiments using Azure traces indicate that HotSwap can save 88\\% of space,\ncompared with a previous function-specific method, PreBaking, when sharing a\ndependency image among ten different functions."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "10 pages, 7 figures. This work was accepted at the IEEE International\n  Conference on Cloud Computing 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08799v1",
                "updated": "2025-07-11T17:59:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T17:59:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "KV Cache Steering for Inducing Reasoning in Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Steering for Inducing Reasoning in Small Language Models"
                },
                "summary": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation."
                },
                "authors": [
                    {
                        "name": "Max Belitsky"
                    },
                    {
                        "name": "Dawid J. Kopiczko"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "M. Jehanzeb Mirza"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08717v1",
                "updated": "2025-07-11T16:17:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    17,
                    46,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T16:17:46Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    17,
                    46,
                    4,
                    192,
                    0
                ],
                "title": "Knowledge Graph-Based approach for Sustainable 6G End-to-End System\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph-Based approach for Sustainable 6G End-to-End System\n  Design"
                },
                "summary": "Previous generations of cellular communication, such as 5G, have been\ndesigned with the objective of improving key performance indicators (KPIs) such\nas throughput, latency, etc. However, to meet the evolving KPI demands as well\nas the ambitious sustainability targets for the ICT industry, 6G will need to\nbe designed differently. Concretely, 6G will need to consider both the\nperformance and sustainability targets for the various use cases it will serve.\nMoreover, like previous generations, 6G will have various candidate\ntechnological enablers, making the design space of the system even more\ncomplex. Furthermore, given the subjective nature of the sustainability\nindicators, in particular social sustainability, there is a significant gap in\nliterature on how technical enablers and 6G System design can be linked to\nthem. Hence, in this article a novel method for 6G end-to-end (E2E) system\ndesign based on Knowledge graphs (KG) has been introduced. It considers as its\ninput: the use case KPIs, use case sustainability requirements expressed as Key\nValues (KV) and KV Indicators (KVIs), the ability of the technological enablers\nto satisfy these KPIs and KVIs, the 6G system design principles defined in\nHexa-X-II project, the maturity of a technological enabler and the dependencies\nbetween the various enablers. As part of the KG method, a novel approach for\ndetermining the key values a technological enabler addresses, has also been\nintroduced. The effectiveness of the KG method was demonstrated by its\napplication in designing the 6G E2E system for the cooperating mobile robot use\ncase defined in the Hexa-X-II project, where 82 enablers were selected. Lastly,\nresults from proof-of-concept demonstrations for a subset of the selected\nenablers have also been provided, which reinforce the efficacy of the KG method\nfor designing a sustainable 6G system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous generations of cellular communication, such as 5G, have been\ndesigned with the objective of improving key performance indicators (KPIs) such\nas throughput, latency, etc. However, to meet the evolving KPI demands as well\nas the ambitious sustainability targets for the ICT industry, 6G will need to\nbe designed differently. Concretely, 6G will need to consider both the\nperformance and sustainability targets for the various use cases it will serve.\nMoreover, like previous generations, 6G will have various candidate\ntechnological enablers, making the design space of the system even more\ncomplex. Furthermore, given the subjective nature of the sustainability\nindicators, in particular social sustainability, there is a significant gap in\nliterature on how technical enablers and 6G System design can be linked to\nthem. Hence, in this article a novel method for 6G end-to-end (E2E) system\ndesign based on Knowledge graphs (KG) has been introduced. It considers as its\ninput: the use case KPIs, use case sustainability requirements expressed as Key\nValues (KV) and KV Indicators (KVIs), the ability of the technological enablers\nto satisfy these KPIs and KVIs, the 6G system design principles defined in\nHexa-X-II project, the maturity of a technological enabler and the dependencies\nbetween the various enablers. As part of the KG method, a novel approach for\ndetermining the key values a technological enabler addresses, has also been\nintroduced. The effectiveness of the KG method was demonstrated by its\napplication in designing the 6G E2E system for the cooperating mobile robot use\ncase defined in the Hexa-X-II project, where 82 enablers were selected. Lastly,\nresults from proof-of-concept demonstrations for a subset of the selected\nenablers have also been provided, which reinforce the efficacy of the KG method\nfor designing a sustainable 6G system."
                },
                "authors": [
                    {
                        "name": "Akshay Jain"
                    },
                    {
                        "name": "Sylvaine Kerboeuf"
                    },
                    {
                        "name": "Sokratis Barmpounakis"
                    },
                    {
                        "name": "Crist√≥bal Vinagre Z."
                    },
                    {
                        "name": "Stefan Wendt"
                    },
                    {
                        "name": "Dinh Thai Bui"
                    },
                    {
                        "name": "Pol Alemany"
                    },
                    {
                        "name": "Riccardo Nicolicchia"
                    },
                    {
                        "name": "Jos√© Mar√≠a Jorquera Valero"
                    },
                    {
                        "name": "Dani Korpi"
                    },
                    {
                        "name": "Mohammad Hossein Moghaddam"
                    },
                    {
                        "name": "Mikko A. Uusitalo"
                    },
                    {
                        "name": "Patrik Rugeland"
                    },
                    {
                        "name": "Abdelkader Outtagarts"
                    },
                    {
                        "name": "Karthik Upadhya"
                    },
                    {
                        "name": "Panagiotis Demestichas"
                    },
                    {
                        "name": "Raul Mu√±oz"
                    },
                    {
                        "name": "Manuel Gil P√©rez"
                    },
                    {
                        "name": "Daniel Adanza"
                    },
                    {
                        "name": "Ricard Vilalta"
                    }
                ],
                "author_detail": {
                    "name": "Ricard Vilalta"
                },
                "author": "Ricard Vilalta",
                "arxiv_comment": "The paper is submitted to IEEE Open Journal of the Communications\n  Society (IEEE OJCOMS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "00",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v9",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v9",
                "updated": "2025-07-11T14:27:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    27,
                    25,
                    4,
                    192,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "arxiv_comment": "Added additional variations in appendix, at the request of\n  collaborators who want to prove various properties",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v9",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v9",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08607v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08607v1",
                "updated": "2025-07-11T14:02:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    2,
                    54,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T14:02:54Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    2,
                    54,
                    4,
                    192,
                    0
                ],
                "title": "BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language\n  Models via Gaussian Discriminant Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language\n  Models via Gaussian Discriminant Analysis"
                },
                "summary": "Vision-language models (VLMs) such as CLIP achieve strong zero-shot\nrecognition but degrade significantly under \\textit{temporally evolving\ndistribution shifts} common in real-world scenarios (e.g., gradual illumination\nor seasonal changes). Existing continual test-time adaptation (CTTA) methods\nare typically built around sudden and severe distribution shifts and neglect\ntemporal continuity, leading to three core defects: limited memory cache\nrestricts long-range distribution modeling, causing catastrophic forgetting;\nentropy-based confidence becomes unreliable under temporal drift, worsening\nerror accumulation; and static visual representations misalign with evolving\ninputs. We formalize this practical problem as \\textit{Continual-Temporal\nTest-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over\ntime. To address it, we propose \\textit{BayesTTA}, a Bayesian adaptation\nframework that enforces temporally consistent predictions and dynamically\naligns visual representations. Specifically, BayesTTA incrementally estimates\nclass-conditional Gaussian mixture distributions without storing raw data,\nadaptively selects covariance structures through statistical hypothesis\ntesting, and performs calibrated inference using Gaussian discriminant analysis\n(GDA). These calibrated predictions supervise self-paced adaptation of\nnormalization layers, ensuring efficient and stable representation alignment.\nWe establish a comprehensive CT-TTA benchmark across four temporally evolving\ndatasets and further evaluate generalization on ten standard TTA datasets.\nExtensive experiments show that BayesTTA consistently outperforms\nstate-of-the-art methods, achieving significant gains while maintaining\nefficiency. Code is available at\n\\href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) such as CLIP achieve strong zero-shot\nrecognition but degrade significantly under \\textit{temporally evolving\ndistribution shifts} common in real-world scenarios (e.g., gradual illumination\nor seasonal changes). Existing continual test-time adaptation (CTTA) methods\nare typically built around sudden and severe distribution shifts and neglect\ntemporal continuity, leading to three core defects: limited memory cache\nrestricts long-range distribution modeling, causing catastrophic forgetting;\nentropy-based confidence becomes unreliable under temporal drift, worsening\nerror accumulation; and static visual representations misalign with evolving\ninputs. We formalize this practical problem as \\textit{Continual-Temporal\nTest-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over\ntime. To address it, we propose \\textit{BayesTTA}, a Bayesian adaptation\nframework that enforces temporally consistent predictions and dynamically\naligns visual representations. Specifically, BayesTTA incrementally estimates\nclass-conditional Gaussian mixture distributions without storing raw data,\nadaptively selects covariance structures through statistical hypothesis\ntesting, and performs calibrated inference using Gaussian discriminant analysis\n(GDA). These calibrated predictions supervise self-paced adaptation of\nnormalization layers, ensuring efficient and stable representation alignment.\nWe establish a comprehensive CT-TTA benchmark across four temporally evolving\ndatasets and further evaluate generalization on ten standard TTA datasets.\nExtensive experiments show that BayesTTA consistently outperforms\nstate-of-the-art methods, achieving significant gains while maintaining\nefficiency. Code is available at\n\\href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}."
                },
                "authors": [
                    {
                        "name": "Shuang Cui"
                    },
                    {
                        "name": "Jinglin Xu"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Xiongxin Tang"
                    },
                    {
                        "name": "Jiangmeng Li"
                    },
                    {
                        "name": "Jiahuan Zhou"
                    },
                    {
                        "name": "Fanjiang Xu"
                    },
                    {
                        "name": "Fuchun Sun"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08607v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v1",
                "updated": "2025-07-11T12:21:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08432v1",
                "updated": "2025-07-11T09:18:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    18,
                    41,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:18:41Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    18,
                    41,
                    4,
                    192,
                    0
                ],
                "title": "xpSHACL: Explainable SHACL Validation using Retrieval-Augmented\n  Generation and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xpSHACL: Explainable SHACL Validation using Retrieval-Augmented\n  Generation and Large Language Models"
                },
                "summary": "Shapes Constraint Language (SHACL) is a powerful language for validating RDF\ndata. Given the recent industry attention to Knowledge Graphs (KGs), more users\nneed to validate linked data properly. However, traditional SHACL validation\nengines often provide terse reports in English that are difficult for\nnon-technical users to interpret and act upon. This paper presents xpSHACL, an\nexplainable SHACL validation system that addresses this issue by combining\nrule-based justification trees with retrieval-augmented generation (RAG) and\nlarge language models (LLMs) to produce detailed, multilanguage, human-readable\nexplanations for constraint violations. A key feature of xpSHACL is its usage\nof a Violation KG to cache and reuse explanations, improving efficiency and\nconsistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shapes Constraint Language (SHACL) is a powerful language for validating RDF\ndata. Given the recent industry attention to Knowledge Graphs (KGs), more users\nneed to validate linked data properly. However, traditional SHACL validation\nengines often provide terse reports in English that are difficult for\nnon-technical users to interpret and act upon. This paper presents xpSHACL, an\nexplainable SHACL validation system that addresses this issue by combining\nrule-based justification trees with retrieval-augmented generation (RAG) and\nlarge language models (LLMs) to produce detailed, multilanguage, human-readable\nexplanations for constraint violations. A key feature of xpSHACL is its usage\nof a Violation KG to cache and reuse explanations, improving efficiency and\nconsistency."
                },
                "authors": [
                    {
                        "name": "Gustavo Correa Publio"
                    },
                    {
                        "name": "Jos√© Emilio Labra Gayo"
                    }
                ],
                "author_detail": {
                    "name": "Jos√© Emilio Labra Gayo"
                },
                "author": "Jos√© Emilio Labra Gayo",
                "arxiv_comment": "Accepted for publication in the 2nd LLM+Graph Workshop, colocated at\n  VLDB'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08422v1",
                "updated": "2025-07-11T09:07:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:07:43Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "title": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers"
                },
                "summary": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Wongi Jeong"
                    },
                    {
                        "name": "Kyungryeol Lee"
                    },
                    {
                        "name": "Hoigi Seo"
                    },
                    {
                        "name": "Se Young Chun"
                    }
                ],
                "author_detail": {
                    "name": "Se Young Chun"
                },
                "author": "Se Young Chun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08278v1",
                "updated": "2025-07-11T02:57:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    2,
                    57,
                    44,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T02:57:44Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    2,
                    57,
                    44,
                    4,
                    192,
                    0
                ],
                "title": "Observation of the electric Breit-Rabi Effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observation of the electric Breit-Rabi Effect"
                },
                "summary": "The response of an atom to external electric and magnetic fields can reveal\nfundamental atomic properties. It has long been verified that, in a static\nmagnetic field, those atomic energy levels with hyperfine interactions shift\naccording to the Breit-Rabi formula, which introduces nonlinear dependence on\nthe magnetic field. On the other hand, the corresponding Breit-Rabi dependence\non a static electric field has not been observed before due to a combination of\nexperimental challenges. Here we precisely measure the Stark shift of the\n$6s^2\\ ^1S_0\\ \\leftrightarrow\\ 6s6p\\ ^1P_1$ transition of $^{171}$Yb ($I$ =\n1/2) with cold atoms held by an optical dipole trap in a static electric field\nup to 120 kV/cm. We observe the electric Breit-Rabi effect displaying\nhigh-order ($E^4$ and $E^6$) DC Stark shifts. These effects arise from the\ninfluence of the strong electric field on hyperfine interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The response of an atom to external electric and magnetic fields can reveal\nfundamental atomic properties. It has long been verified that, in a static\nmagnetic field, those atomic energy levels with hyperfine interactions shift\naccording to the Breit-Rabi formula, which introduces nonlinear dependence on\nthe magnetic field. On the other hand, the corresponding Breit-Rabi dependence\non a static electric field has not been observed before due to a combination of\nexperimental challenges. Here we precisely measure the Stark shift of the\n$6s^2\\ ^1S_0\\ \\leftrightarrow\\ 6s6p\\ ^1P_1$ transition of $^{171}$Yb ($I$ =\n1/2) with cold atoms held by an optical dipole trap in a static electric field\nup to 120 kV/cm. We observe the electric Breit-Rabi effect displaying\nhigh-order ($E^4$ and $E^6$) DC Stark shifts. These effects arise from the\ninfluence of the strong electric field on hyperfine interactions."
                },
                "authors": [
                    {
                        "name": "S. -Z. Wang"
                    },
                    {
                        "name": "S. -B. Wang"
                    },
                    {
                        "name": "Z. -J. Tao"
                    },
                    {
                        "name": "T. Xia"
                    },
                    {
                        "name": "Z. -T. Lu"
                    }
                ],
                "author_detail": {
                    "name": "Z. -T. Lu"
                },
                "author": "Z. -T. Lu",
                "arxiv_doi": "10.1073/pnas.2423902122",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1073/pnas.2423902122",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.08278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 8 figures",
                "arxiv_journal_ref": "122 (26)e2423902122 June 27 2025",
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08232v1",
                "updated": "2025-07-11T00:36:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    0,
                    36,
                    57,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T00:36:57Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    0,
                    36,
                    57,
                    4,
                    192,
                    0
                ],
                "title": "Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and\n  Reading Comprehension?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and\n  Reading Comprehension?"
                },
                "summary": "Large Language Models (LLMs) are increasingly used as proxy students in the\ndevelopment of Intelligent Tutoring Systems (ITSs) and in piloting test\nquestions. However, to what extent these proxy students accurately emulate the\nbehavior and characteristics of real students remains an open question. To\ninvestigate this, we collected a dataset of 489 items from the National\nAssessment of Educational Progress (NAEP), covering mathematics and reading\ncomprehension in grades 4, 8, and 12. We then apply an Item Response Theory\n(IRT) model to position 11 diverse and state-of-the-art LLMs on the same\nability scale as real student populations. Our findings reveal that, without\nguidance, strong general-purpose models consistently outperform the average\nstudent at every grade, while weaker or domain-mismatched models may align\nincidentally. Using grade-enforcement prompts changes models' performance, but\nwhether they align with the average grade-level student remains highly model-\nand prompt-specific: no evaluated model-prompt pair fits the bill across\nsubjects and grades, underscoring the need for new training and evaluation\nstrategies. We conclude by providing guidelines for the selection of viable\nproxies based on our findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used as proxy students in the\ndevelopment of Intelligent Tutoring Systems (ITSs) and in piloting test\nquestions. However, to what extent these proxy students accurately emulate the\nbehavior and characteristics of real students remains an open question. To\ninvestigate this, we collected a dataset of 489 items from the National\nAssessment of Educational Progress (NAEP), covering mathematics and reading\ncomprehension in grades 4, 8, and 12. We then apply an Item Response Theory\n(IRT) model to position 11 diverse and state-of-the-art LLMs on the same\nability scale as real student populations. Our findings reveal that, without\nguidance, strong general-purpose models consistently outperform the average\nstudent at every grade, while weaker or domain-mismatched models may align\nincidentally. Using grade-enforcement prompts changes models' performance, but\nwhether they align with the average grade-level student remains highly model-\nand prompt-specific: no evaluated model-prompt pair fits the bill across\nsubjects and grades, underscoring the need for new training and evaluation\nstrategies. We conclude by providing guidelines for the selection of viable\nproxies based on our findings."
                },
                "authors": [
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "Accepted to the 20th Workshop on Innovative Use of NLP for Building\n  Educational Applications (BEA), co-located with ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08143v1",
                "updated": "2025-07-10T20:03:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    20,
                    3,
                    35,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T20:03:35Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    20,
                    3,
                    35,
                    3,
                    191,
                    0
                ],
                "title": "Compactor: Calibrated Query-Agnostic KV Cache Compression with\n  Approximate Leverage Scores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compactor: Calibrated Query-Agnostic KV Cache Compression with\n  Approximate Leverage Scores"
                },
                "summary": "Modern Large Language Models (LLMs) are increasingly trained to support very\nlarge context windows. Unfortunately the ability to use long contexts in\ngeneration is complicated by the large memory requirement of the KV cache,\nwhich scales linearly with the context length. This memory footprint is often\nthe dominant resource bottleneck in real-world deployments, limiting throughput\nand increasing serving cost. One way to address this is by compressing the KV\ncache, which can be done either with knowledge of the question being asked\n(query-aware) or without knowledge of the query (query-agnostic). We present\nCompactor, a parameter-free, query-agnostic KV compression strategy that uses\napproximate leverage scores to determine token importance. We show that\nCompactor can achieve the same performance as competing methods while retaining\n1/2 the tokens in both synthetic and real-world context tasks, with minimal\ncomputational overhead. We further introduce a procedure for context-calibrated\ncompression, which allows one to infer the maximum compression ratio a given\ncontext can support. Using context-calibrated compression, we show that\nCompactor achieves full KV performance on Longbench while reducing the KV\nmemory burden by 63%, on average. To demonstrate the efficacy and\ngeneralizability of our approach, we apply Compactor to 27 synthetic and\nreal-world tasks from RULER and Longbench, with models from both the Qwen 2.5\nand Llama 3.1 families.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Models (LLMs) are increasingly trained to support very\nlarge context windows. Unfortunately the ability to use long contexts in\ngeneration is complicated by the large memory requirement of the KV cache,\nwhich scales linearly with the context length. This memory footprint is often\nthe dominant resource bottleneck in real-world deployments, limiting throughput\nand increasing serving cost. One way to address this is by compressing the KV\ncache, which can be done either with knowledge of the question being asked\n(query-aware) or without knowledge of the query (query-agnostic). We present\nCompactor, a parameter-free, query-agnostic KV compression strategy that uses\napproximate leverage scores to determine token importance. We show that\nCompactor can achieve the same performance as competing methods while retaining\n1/2 the tokens in both synthetic and real-world context tasks, with minimal\ncomputational overhead. We further introduce a procedure for context-calibrated\ncompression, which allows one to infer the maximum compression ratio a given\ncontext can support. Using context-calibrated compression, we show that\nCompactor achieves full KV performance on Longbench while reducing the KV\nmemory burden by 63%, on average. To demonstrate the efficacy and\ngeneralizability of our approach, we apply Compactor to 27 synthetic and\nreal-world tasks from RULER and Longbench, with models from both the Qwen 2.5\nand Llama 3.1 families."
                },
                "authors": [
                    {
                        "name": "Vivek Chari"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07990v1",
                "updated": "2025-07-10T17:59:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    59,
                    2,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T17:59:02Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    59,
                    2,
                    3,
                    191,
                    0
                ],
                "title": "Multi-Granular Spatio-Temporal Token Merging for Training-Free\n  Acceleration of Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Granular Spatio-Temporal Token Merging for Training-Free\n  Acceleration of Video LLMs"
                },
                "summary": "Video large language models (LLMs) achieve strong video understanding by\nleveraging a large number of spatio-temporal tokens, but suffer from quadratic\ncomputational scaling with token count. To address this, we propose a\ntraining-free spatio-temporal token merging method, named STTM. Our key insight\nis to exploit local spatial and temporal redundancy in video data which has\nbeen overlooked in prior work. STTM first transforms each frame into\nmulti-granular spatial tokens using a coarse-to-fine search over a quadtree\nstructure, then performs directed pairwise merging across the temporal\ndimension. This decomposed merging approach outperforms existing token\nreduction methods across six video QA benchmarks. Notably, STTM achieves a\n2$\\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and\na 3$\\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is\nquery-agnostic, allowing KV cache reuse across different questions for the same\nvideo. The project page is available at https://www.jshyun.me/projects/sttm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (LLMs) achieve strong video understanding by\nleveraging a large number of spatio-temporal tokens, but suffer from quadratic\ncomputational scaling with token count. To address this, we propose a\ntraining-free spatio-temporal token merging method, named STTM. Our key insight\nis to exploit local spatial and temporal redundancy in video data which has\nbeen overlooked in prior work. STTM first transforms each frame into\nmulti-granular spatial tokens using a coarse-to-fine search over a quadtree\nstructure, then performs directed pairwise merging across the temporal\ndimension. This decomposed merging approach outperforms existing token\nreduction methods across six video QA benchmarks. Notably, STTM achieves a\n2$\\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and\na 3$\\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is\nquery-agnostic, allowing KV cache reuse across different questions for the same\nvideo. The project page is available at https://www.jshyun.me/projects/sttm."
                },
                "authors": [
                    {
                        "name": "Jeongseok Hyun"
                    },
                    {
                        "name": "Sukjun Hwang"
                    },
                    {
                        "name": "Su Ho Han"
                    },
                    {
                        "name": "Taeoh Kim"
                    },
                    {
                        "name": "Inwoong Lee"
                    },
                    {
                        "name": "Dongyoon Wee"
                    },
                    {
                        "name": "Joon-Young Lee"
                    },
                    {
                        "name": "Seon Joo Kim"
                    },
                    {
                        "name": "Minho Shim"
                    }
                ],
                "author_detail": {
                    "name": "Minho Shim"
                },
                "author": "Minho Shim",
                "arxiv_comment": "Accepted at ICCV2025; Project page:\n  https://www.jshyun.me/projects/sttm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07966v1",
                "updated": "2025-07-10T17:47:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T17:47:40Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "title": "Scaling RL to Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling RL to Long Videos"
                },
                "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 52K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves\nstrong performance on long video QA benchmarks such as VideoMME. It also\noutperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal\nreasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on\nour LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to\n2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent\nperformance gains as the number of input video frames scales. LongVILA-R1 marks\na firm step towards long video reasoning in VLMs. In addition, we release our\ntraining system for public availability that supports RL training on various\nmodalities (video, text, and audio), various models (VILA and Qwen series), and\neven image and video generation models. On a single A100 node (8 GPUs), it\nsupports RL training on hour-long videos (e.g., 3,600 frames / around 256k\ntokens).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 52K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves\nstrong performance on long video QA benchmarks such as VideoMME. It also\noutperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal\nreasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on\nour LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to\n2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent\nperformance gains as the number of input video frames scales. LongVILA-R1 marks\na firm step towards long video reasoning in VLMs. In addition, we release our\ntraining system for public availability that supports RL training on various\nmodalities (video, text, and audio), various models (VILA and Qwen series), and\neven image and video generation models. On a single A100 node (8 GPUs), it\nsupports RL training on hour-long videos (e.g., 3,600 frames / around 256k\ntokens)."
                },
                "authors": [
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Code and models are available at https://github.com/NVlabs/Long-RL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03296v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03296v3",
                "updated": "2025-07-10T17:10:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    10,
                    49,
                    3,
                    191,
                    0
                ],
                "published": "2025-06-03T18:35:56Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    18,
                    35,
                    56,
                    1,
                    154,
                    0
                ],
                "title": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs"
                },
                "summary": "Deploying large language models (LLMs) for online inference is often\nconstrained by limited GPU memory, particularly due to the growing KV cache\nduring auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a\npromising solution by offloading KV cache management and parts of attention\ncomputation to the CPU. However, a key bottleneck remains: existing schedulers\nfail to effectively overlap CPU-offloaded tasks with GPU execution during the\nlatency-critical, bandwidth-bound decode phase. This particularly penalizes\nreal-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)\nwhich are currently underserved by existing systems, especially under memory\npressure typical of edge or low-cost deployments.\n  We present APEX, a novel, profiling-informed scheduling strategy that\nmaximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems\nrelying on static rules or purely heuristic approaches, APEX dynamically\ndispatches compute across heterogeneous resources by predicting execution times\nof CPU and GPU subtasks to maximize overlap while avoiding scheduling\noverheads. We evaluate APEX on diverse workloads and GPU architectures (NVIDIA\nT4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only\nschedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%\non A10 GPUs, while preserving latency. Against the best existing hybrid\nschedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in\nlong-output settings. APEX significantly advances hybrid LLM inference\nefficiency on such memory-constrained hardware and provides a blueprint for\nscheduling in heterogeneous AI systems, filling a critical gap for efficient\nreal-time LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) for online inference is often\nconstrained by limited GPU memory, particularly due to the growing KV cache\nduring auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a\npromising solution by offloading KV cache management and parts of attention\ncomputation to the CPU. However, a key bottleneck remains: existing schedulers\nfail to effectively overlap CPU-offloaded tasks with GPU execution during the\nlatency-critical, bandwidth-bound decode phase. This particularly penalizes\nreal-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)\nwhich are currently underserved by existing systems, especially under memory\npressure typical of edge or low-cost deployments.\n  We present APEX, a novel, profiling-informed scheduling strategy that\nmaximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems\nrelying on static rules or purely heuristic approaches, APEX dynamically\ndispatches compute across heterogeneous resources by predicting execution times\nof CPU and GPU subtasks to maximize overlap while avoiding scheduling\noverheads. We evaluate APEX on diverse workloads and GPU architectures (NVIDIA\nT4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only\nschedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%\non A10 GPUs, while preserving latency. Against the best existing hybrid\nschedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in\nlong-output settings. APEX significantly advances hybrid LLM inference\nefficiency on such memory-constrained hardware and provides a blueprint for\nscheduling in heterogeneous AI systems, filling a critical gap for efficient\nreal-time LLM applications."
                },
                "authors": [
                    {
                        "name": "Jiakun Fan"
                    },
                    {
                        "name": "Yanglin Zhang"
                    },
                    {
                        "name": "Xiangchen Li"
                    },
                    {
                        "name": "Dimitrios S. Nikolopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios S. Nikolopoulos"
                },
                "author": "Dimitrios S. Nikolopoulos",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03296v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03296v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07400v1",
                "updated": "2025-07-10T03:39:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    3,
                    39,
                    23,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T03:39:23Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    3,
                    39,
                    23,
                    3,
                    191,
                    0
                ],
                "title": "KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent\n  Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent\n  Workflows"
                },
                "summary": "Large language model (LLM) based agentic workflows have become a popular\nparadigm for coordinating multiple specialized agents to solve complex tasks.\nTo improve serving efficiency, existing LLM systems employ prefix caching to\nreuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby\navoiding redundant computation across repeated invocations. However, current\nsystems typically evict KV caches using a Least Recently Used (LRU) policy,\nwhich fails to anticipate future agent usage and often discards KV caches\nshortly before their reuse. This leads to frequent cache misses and substantial\nrecomputation or swapping overhead. We present KVFlow, a workflow-aware KV\ncache management framework tailored for agentic workloads. KVFlow abstracts the\nagent execution schedule as an Agent Step Graph and assigns each agent a\nsteps-to-execution value that estimates its temporal proximity to future\nactivation. These values guide a fine-grained eviction policy at the KV node\nlevel, allowing KVFlow to preserve entries likely to be reused and efficiently\nmanage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a\nfully overlapped KV prefetching mechanism, which proactively loads required\ntensors from CPU to GPU in background threads for agents scheduled in the next\nstep, thereby avoiding cache miss stalls during generation. Compared to SGLang\nwith hierarchical radix cache, KVFlow achieves up to 1.83$\\times$ speedup for\nsingle workflows with large prompts, and up to 2.19$\\times$ speedup for\nscenarios with many concurrent workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) based agentic workflows have become a popular\nparadigm for coordinating multiple specialized agents to solve complex tasks.\nTo improve serving efficiency, existing LLM systems employ prefix caching to\nreuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby\navoiding redundant computation across repeated invocations. However, current\nsystems typically evict KV caches using a Least Recently Used (LRU) policy,\nwhich fails to anticipate future agent usage and often discards KV caches\nshortly before their reuse. This leads to frequent cache misses and substantial\nrecomputation or swapping overhead. We present KVFlow, a workflow-aware KV\ncache management framework tailored for agentic workloads. KVFlow abstracts the\nagent execution schedule as an Agent Step Graph and assigns each agent a\nsteps-to-execution value that estimates its temporal proximity to future\nactivation. These values guide a fine-grained eviction policy at the KV node\nlevel, allowing KVFlow to preserve entries likely to be reused and efficiently\nmanage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a\nfully overlapped KV prefetching mechanism, which proactively loads required\ntensors from CPU to GPU in background threads for agents scheduled in the next\nstep, thereby avoiding cache miss stalls during generation. Compared to SGLang\nwith hierarchical radix cache, KVFlow achieves up to 1.83$\\times$ speedup for\nsingle workflows with large prompts, and up to 2.19$\\times$ speedup for\nscenarios with many concurrent workflows."
                },
                "authors": [
                    {
                        "name": "Zaifeng Pan"
                    },
                    {
                        "name": "Ajjkumar Patel"
                    },
                    {
                        "name": "Zhengding Hu"
                    },
                    {
                        "name": "Yipeng Shen"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Wan-Lu Li"
                    },
                    {
                        "name": "Lianhui Qin"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Yufei Ding"
                    }
                ],
                "author_detail": {
                    "name": "Yufei Ding"
                },
                "author": "Yufei Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08045v1",
                "updated": "2025-07-10T01:51:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    51,
                    17,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T01:51:17Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    51,
                    17,
                    3,
                    191,
                    0
                ],
                "title": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing"
                },
                "summary": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Junyi Wen"
                    },
                    {
                        "name": "Junyuan Liang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07290v1",
                "updated": "2025-07-09T21:18:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    21,
                    18,
                    35,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T21:18:35Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    21,
                    18,
                    35,
                    2,
                    190,
                    0
                ],
                "title": "Stabilization of the first-order phase transition character and\n  Enhancement of the Electrocaloric Effect by NBT substitution in BaTiO$_3$\n  ceramics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stabilization of the first-order phase transition character and\n  Enhancement of the Electrocaloric Effect by NBT substitution in BaTiO$_3$\n  ceramics"
                },
                "summary": "The electrocaloric properties of BaTiO$_3$-based lead-free ferroelectric\nmaterials have been widely investigated. One approach to achieving a large\nelectrocaloric response is to exploit the substantial polarization change\nassociated with the first-order phase transition at the Curie temperature.\nFollowing this strategy, we investigated the electrocaloric response of\n(1$-x$)BaTiO$_3$-$x$Na$_{0.5}$Bi$_{0.5}$TiO$_3$ (BT-NBT) ceramics for x = 0.05,\n0.10, 0.20, and 0.30. In this BT-rich region of the solid solution, it is\nestablished that increasing the NBT content enhances the tetragonality of\nBaTiO$_3$. We show that this increase in tetragonality helps maintain the\nfirst-order nature of the phase transition and enables a correspondingly large\nelectrocaloric response, despite the simultaneous enhancement of relaxor\nferroelectric character with NBT substitution. A significantly large effective\nelectrocaloric temperature change ($\\Delta T_{\\mathrm{eff}}$) of ~1.65 K was\nobtained for the x = 0.20 composition under an applied field of 40 kV/cm using\ndirect electrocaloric measurements, in reasonable agreement with the indirect\nresults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The electrocaloric properties of BaTiO$_3$-based lead-free ferroelectric\nmaterials have been widely investigated. One approach to achieving a large\nelectrocaloric response is to exploit the substantial polarization change\nassociated with the first-order phase transition at the Curie temperature.\nFollowing this strategy, we investigated the electrocaloric response of\n(1$-x$)BaTiO$_3$-$x$Na$_{0.5}$Bi$_{0.5}$TiO$_3$ (BT-NBT) ceramics for x = 0.05,\n0.10, 0.20, and 0.30. In this BT-rich region of the solid solution, it is\nestablished that increasing the NBT content enhances the tetragonality of\nBaTiO$_3$. We show that this increase in tetragonality helps maintain the\nfirst-order nature of the phase transition and enables a correspondingly large\nelectrocaloric response, despite the simultaneous enhancement of relaxor\nferroelectric character with NBT substitution. A significantly large effective\nelectrocaloric temperature change ($\\Delta T_{\\mathrm{eff}}$) of ~1.65 K was\nobtained for the x = 0.20 composition under an applied field of 40 kV/cm using\ndirect electrocaloric measurements, in reasonable agreement with the indirect\nresults."
                },
                "authors": [
                    {
                        "name": "M. Karakaya"
                    },
                    {
                        "name": "I. Gurbuz"
                    },
                    {
                        "name": "L. Fulanovic"
                    },
                    {
                        "name": "U. Adem"
                    }
                ],
                "author_detail": {
                    "name": "U. Adem"
                },
                "author": "U. Adem",
                "arxiv_doi": "10.1039/D4TC01735H",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1039/D4TC01735H",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.07290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "accepted version of the article published in J. Mater. Chem. C. 10\n  Pages, 7 Figures. Plus SI file as a single pdf",
                "arxiv_journal_ref": "J. Mater. Chem. C, 2024,12, 19612-19619",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06739v1",
                "updated": "2025-07-09T10:53:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    53,
                    5,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T10:53:05Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    53,
                    5,
                    2,
                    190,
                    0
                ],
                "title": "PromptTea: Let Prompts Tell TeaCache the Optimal Threshold",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptTea: Let Prompts Tell TeaCache the Optimal Threshold"
                },
                "summary": "Despite recent progress in video generation, inference speed remains a major\nbottleneck. A common acceleration strategy involves reusing model outputs via\ncaching mechanisms at fixed intervals. However, we find that such\nfixed-frequency reuse significantly degrades quality in complex scenes, while\nmanually tuning reuse thresholds is inefficient and lacks robustness. To\naddress this, we propose Prompt-Complexity-Aware (PCA) caching, a method that\nautomatically adjusts reuse thresholds based on scene complexity estimated\ndirectly from the input prompt. By incorporating prompt-derived semantic cues,\nPCA enables more adaptive and informed reuse decisions than conventional\ncaching methods. We also revisit the assumptions behind TeaCache and identify a\nkey limitation: it suffers from poor input-output relationship modeling due to\nan oversimplified prior. To overcome this, we decouple the noisy input, enhance\nthe contribution of meaningful textual information, and improve the model's\npredictive accuracy through multivariate polynomial feature expansion. To\nfurther reduce computational cost, we replace the static CFGCache with\nDynCFGCache, a dynamic mechanism that selectively reuses classifier-free\nguidance (CFG) outputs based on estimated output variations. This allows for\nmore flexible reuse without compromising output quality. Extensive experiments\ndemonstrate that our approach achieves significant acceleration-for example,\n2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across\na range of scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent progress in video generation, inference speed remains a major\nbottleneck. A common acceleration strategy involves reusing model outputs via\ncaching mechanisms at fixed intervals. However, we find that such\nfixed-frequency reuse significantly degrades quality in complex scenes, while\nmanually tuning reuse thresholds is inefficient and lacks robustness. To\naddress this, we propose Prompt-Complexity-Aware (PCA) caching, a method that\nautomatically adjusts reuse thresholds based on scene complexity estimated\ndirectly from the input prompt. By incorporating prompt-derived semantic cues,\nPCA enables more adaptive and informed reuse decisions than conventional\ncaching methods. We also revisit the assumptions behind TeaCache and identify a\nkey limitation: it suffers from poor input-output relationship modeling due to\nan oversimplified prior. To overcome this, we decouple the noisy input, enhance\nthe contribution of meaningful textual information, and improve the model's\npredictive accuracy through multivariate polynomial feature expansion. To\nfurther reduce computational cost, we replace the static CFGCache with\nDynCFGCache, a dynamic mechanism that selectively reuses classifier-free\nguidance (CFG) outputs based on estimated output variations. This allows for\nmore flexible reuse without compromising output quality. Extensive experiments\ndemonstrate that our approach achieves significant acceleration-for example,\n2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across\na range of scenes."
                },
                "authors": [
                    {
                        "name": "Zishen Huang"
                    },
                    {
                        "name": "Chunyu Yang"
                    },
                    {
                        "name": "Mengyuan Ren"
                    }
                ],
                "author_detail": {
                    "name": "Mengyuan Ren"
                },
                "author": "Mengyuan Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06444v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06444v2",
                "updated": "2025-07-09T07:47:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    7,
                    47,
                    59,
                    2,
                    190,
                    0
                ],
                "published": "2025-06-06T18:05:45Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    18,
                    5,
                    45,
                    4,
                    157,
                    0
                ],
                "title": "Saffron-1: Safety Inference Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saffron-1: Safety Inference Scaling"
                },
                "summary": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron ."
                },
                "authors": [
                    {
                        "name": "Ruizhong Qiu"
                    },
                    {
                        "name": "Gaotang Li"
                    },
                    {
                        "name": "Tianxin Wei"
                    },
                    {
                        "name": "Jingrui He"
                    },
                    {
                        "name": "Hanghang Tong"
                    }
                ],
                "author_detail": {
                    "name": "Hanghang Tong"
                },
                "author": "Hanghang Tong",
                "arxiv_comment": "Previous title: \"Saffron-1: Towards an Inference Scaling Paradigm for\n  LLM Safety Assurance\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06444v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06444v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06567v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06567v1",
                "updated": "2025-07-09T05:43:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    43,
                    43,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T05:43:43Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    43,
                    43,
                    2,
                    190,
                    0
                ],
                "title": "SlimCaching: Edge Caching of Mixture-of-Experts for Distributed\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlimCaching: Edge Caching of Mixture-of-Experts for Distributed\n  Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models improve the scalability of large language\nmodels (LLMs) by activating only a small subset of relevant experts per input.\nHowever, the sheer number of expert networks in an MoE model introduces a\nsignificant storage burden for an edge device. To address this challenge, we\nconsider a scenario where experts are dispersed within an edge network for\ndistributed inference. Based on the popular Top-$K$ expert selection strategy,\nwe formulate a latency minimization problem by optimizing expert caching on\nedge servers under storage constraints. When $K=1$, the problem reduces to a\nmonotone submodular maximization problem with knapsack constraints, for which\nwe design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.\nFor the general case where $K\\geq1$, expert co-activation within the same MoE\nlayer introduces non-submodularity, causing greedy methods to be ineffective.\nTo tackle this issue, we propose a successive greedy decomposition method to\ndecompose the original problem into a series of subproblems, with each being\nsolved by a dynamic programming approach. Furthermore, we design an accelerated\nalgorithm based on the max-convolution technique to obtain the approximate\nsolution with a provable guarantee in polynomial time. Simulation results on\nvarious MoE models demonstrate that our method significantly reduces inference\nlatency compared to existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models improve the scalability of large language\nmodels (LLMs) by activating only a small subset of relevant experts per input.\nHowever, the sheer number of expert networks in an MoE model introduces a\nsignificant storage burden for an edge device. To address this challenge, we\nconsider a scenario where experts are dispersed within an edge network for\ndistributed inference. Based on the popular Top-$K$ expert selection strategy,\nwe formulate a latency minimization problem by optimizing expert caching on\nedge servers under storage constraints. When $K=1$, the problem reduces to a\nmonotone submodular maximization problem with knapsack constraints, for which\nwe design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.\nFor the general case where $K\\geq1$, expert co-activation within the same MoE\nlayer introduces non-submodularity, causing greedy methods to be ineffective.\nTo tackle this issue, we propose a successive greedy decomposition method to\ndecompose the original problem into a series of subproblems, with each being\nsolved by a dynamic programming approach. Furthermore, we design an accelerated\nalgorithm based on the max-convolution technique to obtain the approximate\nsolution with a provable guarantee in polynomial time. Simulation results on\nvarious MoE models demonstrate that our method significantly reduces inference\nlatency compared to existing baselines."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "arxiv_comment": "14 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06567v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06567v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v3",
                "updated": "2025-07-09T04:43:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    4,
                    43,
                    59,
                    2,
                    190,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06517v1",
                "updated": "2025-07-09T03:33:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    3,
                    33,
                    44,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T03:33:44Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    3,
                    33,
                    44,
                    2,
                    190,
                    0
                ],
                "title": "SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and\n  Deep Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and\n  Deep Layers"
                },
                "summary": "Large Language Models (LLMs) have achieved impressive accomplishments in\nrecent years. However, the increasing memory consumption of KV cache has\npossessed a significant challenge to the inference system. Eviction methods\nhave revealed the inherent redundancy within the KV cache, demonstrating its\npotential for reduction, particularly in deeper layers. However, KV cache\nreduction for shallower layers has been found to be insufficient. Based on our\nobservation that, the KV cache exhibits a high degree of similarity. Based on\nthis observation, we proposed a novel KV cache reduction method, SpindleKV,\nwhich balances both shallow and deep layers. For deep layers, we employ an\nattention weight based eviction method, while for shallow layers, we apply a\ncodebook based replacement approach which is learnt by similarity and merging\npolicy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma\nfaced by other attention based eviction methods. Experiments on two common\nbenchmarks with three different LLMs shown that SpindleKV obtained better KV\ncache reduction effect compared to baseline methods, while preserving similar\nor even better model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved impressive accomplishments in\nrecent years. However, the increasing memory consumption of KV cache has\npossessed a significant challenge to the inference system. Eviction methods\nhave revealed the inherent redundancy within the KV cache, demonstrating its\npotential for reduction, particularly in deeper layers. However, KV cache\nreduction for shallower layers has been found to be insufficient. Based on our\nobservation that, the KV cache exhibits a high degree of similarity. Based on\nthis observation, we proposed a novel KV cache reduction method, SpindleKV,\nwhich balances both shallow and deep layers. For deep layers, we employ an\nattention weight based eviction method, while for shallow layers, we apply a\ncodebook based replacement approach which is learnt by similarity and merging\npolicy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma\nfaced by other attention based eviction methods. Experiments on two common\nbenchmarks with three different LLMs shown that SpindleKV obtained better KV\ncache reduction effect compared to baseline methods, while preserving similar\nor even better model performance."
                },
                "authors": [
                    {
                        "name": "Zicong Tang"
                    },
                    {
                        "name": "Shi Luohe"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Baoyuan Qi"
                    },
                    {
                        "name": "Guoming Liu"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "arxiv_comment": "Accepted by ACL 2025 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18890v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18890v2",
                "updated": "2025-07-09T02:35:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    2,
                    35,
                    21,
                    2,
                    190,
                    0
                ],
                "published": "2025-02-26T07:10:08Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    7,
                    10,
                    8,
                    2,
                    57,
                    0
                ],
                "title": "TokenSwift: Lossless Acceleration of Ultra Long Sequence Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSwift: Lossless Acceleration of Ultra Long Sequence Generation"
                },
                "summary": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift."
                },
                "authors": [
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Junzhe Shen"
                    },
                    {
                        "name": "Zixia Jia"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Zilong Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zilong Zheng"
                },
                "author": "Zilong Zheng",
                "arxiv_comment": "Accepted By ICML25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18890v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18890v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00768v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00768v2",
                "updated": "2025-07-08T21:23:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    21,
                    23,
                    30,
                    1,
                    189,
                    0
                ],
                "published": "2025-05-01T18:00:40Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    0,
                    40,
                    3,
                    121,
                    0
                ],
                "title": "Optomechanical resource for fault-tolerant quantum computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optomechanical resource for fault-tolerant quantum computing"
                },
                "summary": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive."
                },
                "authors": [
                    {
                        "name": "Margaret Pavlovich"
                    },
                    {
                        "name": "Peter Rakich"
                    },
                    {
                        "name": "Shruti Puri"
                    }
                ],
                "author_detail": {
                    "name": "Shruti Puri"
                },
                "author": "Shruti Puri",
                "arxiv_comment": "21 pages, 10 figures. Supplement 31 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00768v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00768v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06349v1",
                "updated": "2025-07-08T19:20:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    19,
                    20,
                    30,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T19:20:30Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    19,
                    20,
                    30,
                    1,
                    189,
                    0
                ],
                "title": "Multi-Queue SSD I/O Modeling & Its Implications for Data Structure\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Queue SSD I/O Modeling & Its Implications for Data Structure\n  Design"
                },
                "summary": "Understanding the performance profiles of storage devices and how best to\nutilize them has always been non-trivial due to factors such as seek times,\ncaching, scheduling, concurrent access, flash wear-out, and garbage collection.\nHowever, analytical frameworks that provide simplified abstractions of storage\nperformance can still be accurate enough to evaluate external memory algorithms\nand data structures at the design stage. For example, the Disk Access Machine\n(DAM) model assumes that a storage device transfers data in fixed-size blocks\nof size B and that all transfers have unit latency. This abstraction is already\nsufficient to explain some of the benefits of data structures such as B-trees\nand Log-Structured Merge trees (LSM trees); however, storage technology\nadvances have significantly reduced current models' accuracy and utility.\n  This paper introduces the Multi-Queue Solid State Drive (MQSSD) model, a new\nstorage abstraction. This model builds upon previous models and aims to more\naccurately represent the performance characteristics of modern storage\nhardware. We identify key performance-critical aspects of modern multi-queue\nsolid-state drives on which we base our model and demonstrate these\ncharacteristics on actual hardware. We then show how our model can be applied\nto LSM-tree-based storage engines to optimize them for modern storage hardware.\nWe highlight that leveraging concurrent access is crucial for fully utilizing\nthe high throughput of multi-queue SSDs, enabling designs that may appear\ncounterintuitive under traditional paradigms We then validate these insights\nthrough experiments using Facebook's LSM-tree-based key-value store, RocksDB.\nWe conclude that the MQSSD model offers a more accurate abstraction of modern\nhardware than previous models, allowing for greater insight and optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the performance profiles of storage devices and how best to\nutilize them has always been non-trivial due to factors such as seek times,\ncaching, scheduling, concurrent access, flash wear-out, and garbage collection.\nHowever, analytical frameworks that provide simplified abstractions of storage\nperformance can still be accurate enough to evaluate external memory algorithms\nand data structures at the design stage. For example, the Disk Access Machine\n(DAM) model assumes that a storage device transfers data in fixed-size blocks\nof size B and that all transfers have unit latency. This abstraction is already\nsufficient to explain some of the benefits of data structures such as B-trees\nand Log-Structured Merge trees (LSM trees); however, storage technology\nadvances have significantly reduced current models' accuracy and utility.\n  This paper introduces the Multi-Queue Solid State Drive (MQSSD) model, a new\nstorage abstraction. This model builds upon previous models and aims to more\naccurately represent the performance characteristics of modern storage\nhardware. We identify key performance-critical aspects of modern multi-queue\nsolid-state drives on which we base our model and demonstrate these\ncharacteristics on actual hardware. We then show how our model can be applied\nto LSM-tree-based storage engines to optimize them for modern storage hardware.\nWe highlight that leveraging concurrent access is crucial for fully utilizing\nthe high throughput of multi-queue SSDs, enabling designs that may appear\ncounterintuitive under traditional paradigms We then validate these insights\nthrough experiments using Facebook's LSM-tree-based key-value store, RocksDB.\nWe conclude that the MQSSD model offers a more accurate abstraction of modern\nhardware than previous models, allowing for greater insight and optimization."
                },
                "authors": [
                    {
                        "name": "Erin Ransom"
                    },
                    {
                        "name": "Andrew Lim"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Michael Mitzenmacher"
                },
                "author": "Michael Mitzenmacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23367v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23367v3",
                "updated": "2025-07-08T12:34:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    12,
                    34,
                    10,
                    1,
                    189,
                    0
                ],
                "published": "2025-03-30T08:51:19Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning"
                },
                "summary": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR."
                },
                "authors": [
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Jiangshan Wang"
                    },
                    {
                        "name": "Tao Dai"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23367v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23367v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07061v1",
                "updated": "2025-07-08T09:20:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    20,
                    12,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T09:20:12Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    20,
                    12,
                    1,
                    189,
                    0
                ],
                "title": "An Ensemble Embedding Approach for Improving Semantic Caching\n  Performance in LLM-based Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Ensemble Embedding Approach for Improving Semantic Caching\n  Performance in LLM-based Systems"
                },
                "summary": "Semantic caching enhances the efficiency of large language model (LLM)\nsystems by identifying semantically similar queries, storing responses once,\nand serving them for subsequent equivalent requests. However, existing semantic\ncaching frameworks rely on single embedding models for query representation,\nwhich limits their ability to capture the diverse semantic relationships\npresent in real-world query distributions. This paper presents an ensemble\nembedding approach that combines multiple embedding models through a trained\nmeta-encoder to improve semantic similarity detection in LLM caching systems.\nWe evaluate our method using the Quora Question Pairs (QQP) dataset, measuring\ncache hit ratios, cache miss ratios, token savings, and response times. Our\nensemble approach achieves a 92\\% cache hit ratio for semantically equivalent\nqueries while maintaining an 85\\% accuracy in correctly rejecting\nnon-equivalent queries as cache misses. These results demonstrate that ensemble\nembedding methods significantly outperform single-model approaches in\ndistinguishing between semantically similar and dissimilar queries, leading to\nmore effective caching performance and reduced computational overhead in\nLLM-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caching enhances the efficiency of large language model (LLM)\nsystems by identifying semantically similar queries, storing responses once,\nand serving them for subsequent equivalent requests. However, existing semantic\ncaching frameworks rely on single embedding models for query representation,\nwhich limits their ability to capture the diverse semantic relationships\npresent in real-world query distributions. This paper presents an ensemble\nembedding approach that combines multiple embedding models through a trained\nmeta-encoder to improve semantic similarity detection in LLM caching systems.\nWe evaluate our method using the Quora Question Pairs (QQP) dataset, measuring\ncache hit ratios, cache miss ratios, token savings, and response times. Our\nensemble approach achieves a 92\\% cache hit ratio for semantically equivalent\nqueries while maintaining an 85\\% accuracy in correctly rejecting\nnon-equivalent queries as cache misses. These results demonstrate that ensemble\nembedding methods significantly outperform single-model approaches in\ndistinguishing between semantically similar and dissimilar queries, leading to\nmore effective caching performance and reduced computational overhead in\nLLM-based systems."
                },
                "authors": [
                    {
                        "name": "Shervin Ghaffari"
                    },
                    {
                        "name": "Zohre Bahranifard"
                    },
                    {
                        "name": "Mohammad Akbari"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Akbari"
                },
                "author": "Mohammad Akbari",
                "arxiv_comment": "10 pages, 8 figures, 2 table. Submitted to the Journal of Information\n  Science",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v4",
                "updated": "2025-07-08T07:10:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    7,
                    10,
                    6,
                    1,
                    189,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, an image and video generative DiT variant\nenhanced with Long-Skip-Connections (LSCs) - the key efficiency component in\nU-Nets. Theoretical spectral norm and visualization analysis demonstrate how\nLSCs stabilize feature dynamics. Skip-DiT architecture and its stabilized\ndynamic feature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across the image and video generation tasks demonstrate that\nSkip-DiT achieves: (1) 4.4 times training acceleration and faster convergence,\n(2) 1.5-2 times inference acceleration with negligible quality loss and high\nfidelity to the original output, outperforming existing DiT caching methods\nacross various quantitative metrics. Our findings establish\nLong-Skip-Connections as critical architectural components for stable and\nefficient diffusion transformers. Codes are provided in the\nhttps://github.com/OpenSparseLLMs/Skip-DiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, an image and video generative DiT variant\nenhanced with Long-Skip-Connections (LSCs) - the key efficiency component in\nU-Nets. Theoretical spectral norm and visualization analysis demonstrate how\nLSCs stabilize feature dynamics. Skip-DiT architecture and its stabilized\ndynamic feature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across the image and video generation tasks demonstrate that\nSkip-DiT achieves: (1) 4.4 times training acceleration and faster convergence,\n(2) 1.5-2 times inference acceleration with negligible quality loss and high\nfidelity to the original output, outperforming existing DiT caching methods\nacross various quantitative metrics. Our findings establish\nLong-Skip-Connections as critical architectural components for stable and\nefficient diffusion transformers. Codes are provided in the\nhttps://github.com/OpenSparseLLMs/Skip-DiT."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.03622v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.03622v3",
                "updated": "2025-07-08T02:15:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    2,
                    15,
                    7,
                    1,
                    189,
                    0
                ],
                "published": "2023-06-06T12:19:05Z",
                "published_parsed": [
                    2023,
                    6,
                    6,
                    12,
                    19,
                    5,
                    1,
                    157,
                    0
                ],
                "title": "Torpor: GPU-Enabled Serverless Computing for Low-Latency,\n  Resource-Efficient Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Torpor: GPU-Enabled Serverless Computing for Low-Latency,\n  Resource-Efficient Inference"
                },
                "summary": "Serverless computing offers a compelling cloud model for online inference\nservices. However, existing serverless platforms lack efficient support for\nGPUs, hindering their ability to deliver high-performance inference. In this\npaper, we present Torpor, a serverless platform for GPU-efficient, low-latency\ninference. To enable efficient sharing of a node's GPUs among numerous\ninference functions, Torpor maintains models in main memory and dynamically\nswaps them onto GPUs upon request arrivals (i.e., late binding with model\nswapping). Torpor uses various techniques, including asynchronous API\nredirection, GPU runtime sharing, pipelined model execution, and efficient GPU\nmemory management, to minimize latency overhead caused by model swapping.\nAdditionally, we design an interference-aware request scheduling algorithm that\nutilizes high-speed GPU interconnects to meet latency service-level objectives\n(SLOs) for individual inference functions. We have implemented Torpor and\nevaluated its performance in a production environment. Utilizing late binding\nand model swapping, Torpor can concurrently serve hundreds of inference\nfunctions on a worker node with 4 GPUs, while achieving latency performance\ncomparable to native execution, where each model is cached exclusively on a\nGPU. Pilot deployment in a leading commercial serverless cloud shows that\nTorpor reduces the GPU provisioning cost by 70% and 65% for users and the\nplatform, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing offers a compelling cloud model for online inference\nservices. However, existing serverless platforms lack efficient support for\nGPUs, hindering their ability to deliver high-performance inference. In this\npaper, we present Torpor, a serverless platform for GPU-efficient, low-latency\ninference. To enable efficient sharing of a node's GPUs among numerous\ninference functions, Torpor maintains models in main memory and dynamically\nswaps them onto GPUs upon request arrivals (i.e., late binding with model\nswapping). Torpor uses various techniques, including asynchronous API\nredirection, GPU runtime sharing, pipelined model execution, and efficient GPU\nmemory management, to minimize latency overhead caused by model swapping.\nAdditionally, we design an interference-aware request scheduling algorithm that\nutilizes high-speed GPU interconnects to meet latency service-level objectives\n(SLOs) for individual inference functions. We have implemented Torpor and\nevaluated its performance in a production environment. Utilizing late binding\nand model swapping, Torpor can concurrently serve hundreds of inference\nfunctions on a worker node with 4 GPUs, while achieving latency performance\ncomparable to native execution, where each model is cached exclusively on a\nGPU. Pilot deployment in a leading commercial serverless cloud shows that\nTorpor reduces the GPU provisioning cost by 70% and 65% for users and the\nplatform, respectively."
                },
                "authors": [
                    {
                        "name": "Minchen Yu"
                    },
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Dong Chen"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Xiaonan Luo"
                    },
                    {
                        "name": "Zhuohao Li"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Ruichuan Chen"
                    },
                    {
                        "name": "Dapeng Nie"
                    },
                    {
                        "name": "Haoran Yang"
                    },
                    {
                        "name": "Yu Ding"
                    }
                ],
                "author_detail": {
                    "name": "Yu Ding"
                },
                "author": "Yu Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.03622v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.03622v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01827v2",
                "updated": "2025-07-08T00:51:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    0,
                    51,
                    16,
                    1,
                    189,
                    0
                ],
                "published": "2024-12-02T18:59:53Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders"
                },
                "summary": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/."
                },
                "authors": [
                    {
                        "name": "Ziqi Pang"
                    },
                    {
                        "name": "Tianyuan Zhang"
                    },
                    {
                        "name": "Fujun Luan"
                    },
                    {
                        "name": "Yunze Man"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Xiong Wang"
                },
                "author": "Yu-Xiong Wang",
                "arxiv_comment": "Project page: https://rand-ar.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07120v1",
                "updated": "2025-07-07T19:47:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    19,
                    47,
                    24,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T19:47:24Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    19,
                    47,
                    24,
                    0,
                    188,
                    0
                ],
                "title": "Helix Parallelism: Rethinking Sharding Strategies for Interactive\n  Multi-Million-Token LLM Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Helix Parallelism: Rethinking Sharding Strategies for Interactive\n  Multi-Million-Token LLM Decoding"
                },
                "summary": "As LLMs scale to multi-million-token KV histories, real-time autoregressive\ndecoding under tight Token-to-Token Latency (TTL) constraints faces growing\npressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN)\nweights and reading long KV caches. While Tensor Parallelism (TP) helps\nmitigate the cost of FFN weight reads, it does not scale well for attention.\nWhen TP width exceeds the number of KV heads, it leads to inefficient KV\nduplication, limits parallelism, and constrains batch size. Simultaneously,\nDRAM reads for long KV histories scale linearly with batch size, further\ncapping efficiency.\n  We introduce Helix Parallelism, a hybrid execution strategy that applies KV\nparallelism during attention to shard KV caches across GPUs, then reuses the\nsame GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN\ncomputation. To preserve exact attention behavior, Helix includes a lightweight\ncommunication step. To minimize the exposed communication cost, we introduce\nHelix HOP-B. Helix HOP-B effectively minimizes communication overhead through\nbatchwise overlap, preserving low TTL while improving GPU efficiency. Compared\nto conventional parallelism approaches, Helix reduces TTL by up to 1.5x at\nfixed batch sizes and supports up to 32x larger batches under the same latency\nbudget for DeepSeek-R1, pushing forward the throughput-latency Pareto on\nBlackwell and making real-time inference with ultra-long-sequence practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs scale to multi-million-token KV histories, real-time autoregressive\ndecoding under tight Token-to-Token Latency (TTL) constraints faces growing\npressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN)\nweights and reading long KV caches. While Tensor Parallelism (TP) helps\nmitigate the cost of FFN weight reads, it does not scale well for attention.\nWhen TP width exceeds the number of KV heads, it leads to inefficient KV\nduplication, limits parallelism, and constrains batch size. Simultaneously,\nDRAM reads for long KV histories scale linearly with batch size, further\ncapping efficiency.\n  We introduce Helix Parallelism, a hybrid execution strategy that applies KV\nparallelism during attention to shard KV caches across GPUs, then reuses the\nsame GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN\ncomputation. To preserve exact attention behavior, Helix includes a lightweight\ncommunication step. To minimize the exposed communication cost, we introduce\nHelix HOP-B. Helix HOP-B effectively minimizes communication overhead through\nbatchwise overlap, preserving low TTL while improving GPU efficiency. Compared\nto conventional parallelism approaches, Helix reduces TTL by up to 1.5x at\nfixed batch sizes and supports up to 32x larger batches under the same latency\nbudget for DeepSeek-R1, pushing forward the throughput-latency Pareto on\nBlackwell and making real-time inference with ultra-long-sequence practical."
                },
                "authors": [
                    {
                        "name": "Nidhi Bhatia"
                    },
                    {
                        "name": "Ankit More"
                    },
                    {
                        "name": "Ritika Borkar"
                    },
                    {
                        "name": "Tiyasa Mitra"
                    },
                    {
                        "name": "Ramon Matas"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Maximilian Golub"
                    },
                    {
                        "name": "Dheevatsa Mudigere"
                    },
                    {
                        "name": "Brian Pharris"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    }
                ],
                "author_detail": {
                    "name": "Bita Darvish Rouhani"
                },
                "author": "Bita Darvish Rouhani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05240v1",
                "updated": "2025-07-07T17:49:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    49,
                    41,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:49:41Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    49,
                    41,
                    0,
                    188,
                    0
                ],
                "title": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling"
                },
                "summary": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\n\\href{https://streamvln.github.io/}{https://streamvln.github.io/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\n\\href{https://streamvln.github.io/}{https://streamvln.github.io/}."
                },
                "authors": [
                    {
                        "name": "Meng Wei"
                    },
                    {
                        "name": "Chenyang Wan"
                    },
                    {
                        "name": "Xiqian Yu"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Yuqiang Yang"
                    },
                    {
                        "name": "Xiaohan Mao"
                    },
                    {
                        "name": "Chenming Zhu"
                    },
                    {
                        "name": "Wenzhe Cai"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Xihui Liu"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04967v1",
                "updated": "2025-07-07T13:10:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    10,
                    1,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T13:10:01Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    10,
                    1,
                    0,
                    188,
                    0
                ],
                "title": "The Case for Instance-Optimized LLMs in OLAP Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Case for Instance-Optimized LLMs in OLAP Databases"
                },
                "summary": "Large Language Models (LLMs) can enhance analytics systems with powerful data\nsummarization, cleaning, and semantic transformation capabilities. However,\ndeploying LLMs at scale -- processing millions to billions of rows -- remains\nprohibitively expensive in computation and memory. We present IOLM-DB, a novel\nsystem that makes LLM-enhanced database queries practical through\nquery-specific model optimization. Instead of using general-purpose LLMs,\nIOLM-DB generates lightweight, specialized models tailored to each query's\nspecific needs using representative data samples. IOLM-DB reduces model\nfootprints by up to 76% and increases throughput by up to 3.31$\\times$ while\nmaintaining accuracy through aggressive compression techniques, including\nquantization, sparsification, and structural pruning. We further show how our\napproach enables higher parallelism on existing hardware and seamlessly\nsupports caching and batching strategies to reduce overheads. Our prototype\ndemonstrates that leveraging LLM queries inside analytics systems is feasible\nat scale, opening new possibilities for future OLAP applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can enhance analytics systems with powerful data\nsummarization, cleaning, and semantic transformation capabilities. However,\ndeploying LLMs at scale -- processing millions to billions of rows -- remains\nprohibitively expensive in computation and memory. We present IOLM-DB, a novel\nsystem that makes LLM-enhanced database queries practical through\nquery-specific model optimization. Instead of using general-purpose LLMs,\nIOLM-DB generates lightweight, specialized models tailored to each query's\nspecific needs using representative data samples. IOLM-DB reduces model\nfootprints by up to 76% and increases throughput by up to 3.31$\\times$ while\nmaintaining accuracy through aggressive compression techniques, including\nquantization, sparsification, and structural pruning. We further show how our\napproach enables higher parallelism on existing hardware and seamlessly\nsupports caching and batching strategies to reduce overheads. Our prototype\ndemonstrates that leveraging LLM queries inside analytics systems is feasible\nat scale, opening new possibilities for future OLAP applications."
                },
                "authors": [
                    {
                        "name": "Bardia Mohammadi"
                    },
                    {
                        "name": "Laurent Bindschaedler"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Bindschaedler"
                },
                "author": "Laurent Bindschaedler",
                "arxiv_journal_ref": "27th International Workshop on Design, Optimization, Languages and\n  Analytical Processing of Big Data 2025. CEUR-WS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14374v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14374v2",
                "updated": "2025-07-07T09:25:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    9,
                    25,
                    21,
                    0,
                    188,
                    0
                ],
                "published": "2025-04-19T18:25:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation"
                },
                "summary": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model."
                },
                "authors": [
                    {
                        "name": "Max L√ºbke"
                    },
                    {
                        "name": "Marco De Lucia"
                    },
                    {
                        "name": "Stefan Petri"
                    },
                    {
                        "name": "Bettina Schnor"
                    }
                ],
                "author_detail": {
                    "name": "Bettina Schnor"
                },
                "author": "Bettina Schnor",
                "arxiv_doi": "10.1007/978-3-031-97635-3_28",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-97635-3_28",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.14374v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14374v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Long version, 15 pages, 6 figures; Short version (8 pages) included\n  in the proceedings of \"25th International Conference on Computational\n  Science\" (ICCS25)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04697v1",
                "updated": "2025-07-07T06:33:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    33,
                    59,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T06:33:59Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    33,
                    59,
                    0,
                    188,
                    0
                ],
                "title": "Performance Evaluation of General Purpose Large Language Models for\n  Basic Linear Algebra Subprograms Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Evaluation of General Purpose Large Language Models for\n  Basic Linear Algebra Subprograms Code Generation"
                },
                "summary": "Generative AI technology based on Large Language Models (LLM) has been\ndeveloped and applied to assist or automatically generate program codes. In\nthis paper, we evaluate the capability of existing general LLMs for Basic\nLinear Algebra Subprograms (BLAS) code generation for CPUs. We use two LLMs\nprovided by OpenAI: GPT-4.1, a Generative Pre-trained Transformer (GPT) model,\nand o4-mini, one of the o-series of Reasoning models. Both have been released\nin April 2025. For the routines from level-1 to 3 BLAS, we tried to generate\n(1) C code without optimization from routine name only, (2) C code with basic\nperformance optimizations (thread parallelization, SIMD vectorization, and\ncache blocking) from routine name only, and (3) C code with basic performance\noptimizations based on Fortran reference code. As a result, we found that\ncorrect code can be generated in many cases even when only routine name are\ngiven. We also confirmed that thread parallelization with OpenMP, SIMD\nvectorization, and cache blocking can be implemented to some extent, and that\nthe code is faster than the reference code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI technology based on Large Language Models (LLM) has been\ndeveloped and applied to assist or automatically generate program codes. In\nthis paper, we evaluate the capability of existing general LLMs for Basic\nLinear Algebra Subprograms (BLAS) code generation for CPUs. We use two LLMs\nprovided by OpenAI: GPT-4.1, a Generative Pre-trained Transformer (GPT) model,\nand o4-mini, one of the o-series of Reasoning models. Both have been released\nin April 2025. For the routines from level-1 to 3 BLAS, we tried to generate\n(1) C code without optimization from routine name only, (2) C code with basic\nperformance optimizations (thread parallelization, SIMD vectorization, and\ncache blocking) from routine name only, and (3) C code with basic performance\noptimizations based on Fortran reference code. As a result, we found that\ncorrect code can be generated in many cases even when only routine name are\ngiven. We also confirmed that thread parallelization with OpenMP, SIMD\nvectorization, and cache blocking can be implemented to some extent, and that\nthe code is faster than the reference code."
                },
                "authors": [
                    {
                        "name": "Daichi Mukunoki"
                    },
                    {
                        "name": "Shun-ichiro Hayashi"
                    },
                    {
                        "name": "Tetsuya Hoshino"
                    },
                    {
                        "name": "Takahiro Katagiri"
                    }
                ],
                "author_detail": {
                    "name": "Takahiro Katagiri"
                },
                "author": "Takahiro Katagiri",
                "arxiv_comment": "8 pages, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04416v1",
                "updated": "2025-07-06T15:08:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "published": "2025-07-06T15:08:49Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "title": "RAT: Bridging RNN Efficiency and Attention Accuracy in Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAT: Bridging RNN Efficiency and Attention Accuracy in Language Modeling"
                },
                "summary": "Transformers have become the cornerstone of modern large-scale language\nmodels; however, their dependence on softmax attention poses a major\ncomputational bottleneck, particularly in long-context settings. In this work,\nrather than following prevalent approaches such as linear attention (or SSMs)\nand local attention, we introduce an intermediate design called \\rat between\nrecurrence and attention mechanisms. It partitions the input into chunks,\napplies a simple linear recurrence within each chunk to capture local\ndependencies, and then performs softmax attention across chunks to model\nlong-range interactions. By adjusting the size of the chunk, \\rat enables\nflexible trade-offs, combining the strengths of RNN and attention. Empirically,\nwith a chunk size of 16, the \\rat layer achieves a \\(7\\times\\) improvement in\ntraining speed with 100K token sequences and \\(9\\times\\) in generation at 4K\nsequence length, while maintaining similar or sometimes even better accuracy\ncompared to standard attention. We demonstrate this by training 1.3B parameter\nmodels from scratch and performing large-scale evaluations, including short-\nand long-context benchmarks, as well as supervised fine-tuning~(SFT). We\nfurther propose a hybrid architecture that interleaves \\rat with local\nattention. By combining efficient long-range modeling with strong local\ninteractions, this hybrid design not only improves inference speed and reduces\ncache memory usage compared to attention, but also consistently enhances\nperformance, for example, achieving an average 1 point gain in commonsense\nreasoning tasks, up to 4 points on code tasks, and a 1 point Rouge-L increase\nin a summarization SFT task. Code is available at\nhttps://github.com/CLAIRE-Labo/RAT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the cornerstone of modern large-scale language\nmodels; however, their dependence on softmax attention poses a major\ncomputational bottleneck, particularly in long-context settings. In this work,\nrather than following prevalent approaches such as linear attention (or SSMs)\nand local attention, we introduce an intermediate design called \\rat between\nrecurrence and attention mechanisms. It partitions the input into chunks,\napplies a simple linear recurrence within each chunk to capture local\ndependencies, and then performs softmax attention across chunks to model\nlong-range interactions. By adjusting the size of the chunk, \\rat enables\nflexible trade-offs, combining the strengths of RNN and attention. Empirically,\nwith a chunk size of 16, the \\rat layer achieves a \\(7\\times\\) improvement in\ntraining speed with 100K token sequences and \\(9\\times\\) in generation at 4K\nsequence length, while maintaining similar or sometimes even better accuracy\ncompared to standard attention. We demonstrate this by training 1.3B parameter\nmodels from scratch and performing large-scale evaluations, including short-\nand long-context benchmarks, as well as supervised fine-tuning~(SFT). We\nfurther propose a hybrid architecture that interleaves \\rat with local\nattention. By combining efficient long-range modeling with strong local\ninteractions, this hybrid design not only improves inference speed and reduces\ncache memory usage compared to attention, but also consistently enhances\nperformance, for example, achieving an average 1 point gain in commonsense\nreasoning tasks, up to 4 points on code tasks, and a 1 point Rouge-L increase\nin a summarization SFT task. Code is available at\nhttps://github.com/CLAIRE-Labo/RAT"
                },
                "authors": [
                    {
                        "name": "Xiuying Wei"
                    },
                    {
                        "name": "Anunay Yadav"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01110v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01110v2",
                "updated": "2025-07-05T15:51:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    15,
                    51,
                    57,
                    5,
                    186,
                    0
                ],
                "published": "2025-07-01T18:12:43Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    18,
                    12,
                    43,
                    1,
                    182,
                    0
                ],
                "title": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale\n  Reconstruction with External Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale\n  Reconstruction with External Memory"
                },
                "summary": "Gaussian Splatting has emerged as a high-performance technique for novel view\nsynthesis, enabling real-time rendering and high-quality reconstruction of\nsmall scenes. However, scaling to larger environments has so far relied on\npartitioning the scene into chunks -- a strategy that introduces artifacts at\nchunk boundaries, complicates training across varying scales, and is poorly\nsuited to unstructured scenarios such as city-scale flyovers combined with\nstreet-level views. Moreover, rendering remains fundamentally limited by GPU\nmemory, as all visible chunks must reside in VRAM simultaneously. We introduce\nA LoD of Gaussians, a framework for training and rendering ultra-large-scale\nGaussian scenes on a single consumer-grade GPU -- without partitioning. Our\nmethod stores the full scene out-of-core (e.g., in CPU memory) and trains a\nLevel-of-Detail (LoD) representation directly, dynamically streaming only the\nrelevant Gaussians. A hybrid data structure combining Gaussian hierarchies with\nSequential Point Trees enables efficient, view-dependent LoD selection, while a\nlightweight caching and view scheduling system exploits temporal coherence to\nsupport real-time streaming and rendering. Together, these innovations enable\nseamless multi-scale reconstruction and interactive visualization of complex\nscenes -- from broad aerial views to fine-grained ground-level details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Splatting has emerged as a high-performance technique for novel view\nsynthesis, enabling real-time rendering and high-quality reconstruction of\nsmall scenes. However, scaling to larger environments has so far relied on\npartitioning the scene into chunks -- a strategy that introduces artifacts at\nchunk boundaries, complicates training across varying scales, and is poorly\nsuited to unstructured scenarios such as city-scale flyovers combined with\nstreet-level views. Moreover, rendering remains fundamentally limited by GPU\nmemory, as all visible chunks must reside in VRAM simultaneously. We introduce\nA LoD of Gaussians, a framework for training and rendering ultra-large-scale\nGaussian scenes on a single consumer-grade GPU -- without partitioning. Our\nmethod stores the full scene out-of-core (e.g., in CPU memory) and trains a\nLevel-of-Detail (LoD) representation directly, dynamically streaming only the\nrelevant Gaussians. A hybrid data structure combining Gaussian hierarchies with\nSequential Point Trees enables efficient, view-dependent LoD selection, while a\nlightweight caching and view scheduling system exploits temporal coherence to\nsupport real-time streaming and rendering. Together, these innovations enable\nseamless multi-scale reconstruction and interactive visualization of complex\nscenes -- from broad aerial views to fine-grained ground-level details."
                },
                "authors": [
                    {
                        "name": "Felix Windisch"
                    },
                    {
                        "name": "Lukas Radl"
                    },
                    {
                        "name": "Thomas K√∂hler"
                    },
                    {
                        "name": "Michael Steiner"
                    },
                    {
                        "name": "Dieter Schmalstieg"
                    },
                    {
                        "name": "Markus Steinberger"
                    }
                ],
                "author_detail": {
                    "name": "Markus Steinberger"
                },
                "author": "Markus Steinberger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01110v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01110v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05344v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05344v2",
                "updated": "2025-07-05T15:40:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    15,
                    40,
                    51,
                    5,
                    186,
                    0
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "title": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM."
                },
                "authors": [
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05344v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05344v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00901v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00901v2",
                "updated": "2025-07-05T13:37:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    13,
                    37,
                    48,
                    5,
                    186,
                    0
                ],
                "published": "2025-05-01T22:32:29Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    32,
                    29,
                    3,
                    121,
                    0
                ],
                "title": "Heterogeneous Memory Benchmarking Toolkit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Memory Benchmarking Toolkit"
                },
                "summary": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems. MemScope enables\nprecise characterization of the temporal behavior of available memory modules\nunder configurable contention stress scenarios. MemScope leverages kernel-level\ncontrol over physical memory allocation, cache maintenance, CPU state,\ninterrupts, and I/O device activity to accurately benchmark heterogeneous\nmemory subsystems. This gives us the privilege to directly map pieces of\ncontiguous physical memory and instantiate allocators, allowing us to finely\ncontrol cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU-FPGA platform demonstrates its capability to\nprecisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems. MemScope enables\nprecise characterization of the temporal behavior of available memory modules\nunder configurable contention stress scenarios. MemScope leverages kernel-level\ncontrol over physical memory allocation, cache maintenance, CPU state,\ninterrupts, and I/O device activity to accurately benchmark heterogeneous\nmemory subsystems. This gives us the privilege to directly map pieces of\ncontiguous physical memory and instantiate allocators, allowing us to finely\ncontrol cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU-FPGA platform demonstrates its capability to\nprecisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system."
                },
                "authors": [
                    {
                        "name": "Golsana Ghaemi"
                    },
                    {
                        "name": "Gabriel Franco"
                    },
                    {
                        "name": "Kazem Taram"
                    },
                    {
                        "name": "Renato Mancuso"
                    }
                ],
                "author_detail": {
                    "name": "Renato Mancuso"
                },
                "author": "Renato Mancuso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00901v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00901v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03980v1",
                "updated": "2025-07-05T10:11:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    10,
                    11,
                    37,
                    5,
                    186,
                    0
                ],
                "published": "2025-07-05T10:11:37Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    10,
                    11,
                    37,
                    5,
                    186,
                    0
                ],
                "title": "Combination generators with optimal cache utilization and communication\n  free parallel execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combination generators with optimal cache utilization and communication\n  free parallel execution"
                },
                "summary": "We introduce an efficient and elegant combination generator for producing all\ncombinations of size less than or equal to K, designed for exhaustive\ngeneration and combinatorial optimization tasks. This generator can be\nimplemented to achieve what we define as optimal efficiency: constant amortized\ntime, optimal cache utilization, embarrassingly parallel execution, and a\nrecursive structure compatible with pruning-based search. These properties are\ndifficult to satisfy simultaneously in existing generators. For example,\nclassical Gray code or lexicographic generators are typically list-based and\nsequentially defined, making them difficult to vectorized, inefficient in cache\nusage, and inherently hard to parallelize. Generators based on unranking\nmethods, while easy to parallelize, are non-recursive. These limitations reduce\ntheir applicability in our target applications, where both computational\nefficiency and recursion are crucial. We adapt Bird's algebra of\nprogramming-style calculation to derive our algorithms, a formalism for\ndeveloping correct-by-construction programs from specifications. As a result,\nall generators in this paper are first formulated in their clearest\nspecification, and efficient definitions are derived constructively through\nequational reasoning, resulting in concise and elegant divide-and-conquer\ndefinitions. Beyond presenting a combination generator, we extend our approach\nto construct generators for K-permutations, nested combinations of\ncombinations, and nested permutation-combination structures. To the best of our\nknowledge, the literature has not previously reported generators for these\nnested structures. We also develop sequential variants that produce\nconfigurations in Gray code-compatible orders -- such as the revolving door\nordering -- which are particularly useful for constructing nested generators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an efficient and elegant combination generator for producing all\ncombinations of size less than or equal to K, designed for exhaustive\ngeneration and combinatorial optimization tasks. This generator can be\nimplemented to achieve what we define as optimal efficiency: constant amortized\ntime, optimal cache utilization, embarrassingly parallel execution, and a\nrecursive structure compatible with pruning-based search. These properties are\ndifficult to satisfy simultaneously in existing generators. For example,\nclassical Gray code or lexicographic generators are typically list-based and\nsequentially defined, making them difficult to vectorized, inefficient in cache\nusage, and inherently hard to parallelize. Generators based on unranking\nmethods, while easy to parallelize, are non-recursive. These limitations reduce\ntheir applicability in our target applications, where both computational\nefficiency and recursion are crucial. We adapt Bird's algebra of\nprogramming-style calculation to derive our algorithms, a formalism for\ndeveloping correct-by-construction programs from specifications. As a result,\nall generators in this paper are first formulated in their clearest\nspecification, and efficient definitions are derived constructively through\nequational reasoning, resulting in concise and elegant divide-and-conquer\ndefinitions. Beyond presenting a combination generator, we extend our approach\nto construct generators for K-permutations, nested combinations of\ncombinations, and nested permutation-combination structures. To the best of our\nknowledge, the literature has not previously reported generators for these\nnested structures. We also develop sequential variants that produce\nconfigurations in Gray code-compatible orders -- such as the revolving door\nordering -- which are particularly useful for constructing nested generators."
                },
                "authors": [
                    {
                        "name": "Xi He"
                    },
                    {
                        "name": "Max. A. Little"
                    }
                ],
                "author_detail": {
                    "name": "Max. A. Little"
                },
                "author": "Max. A. Little",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03919v1",
                "updated": "2025-07-05T06:55:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    6,
                    55,
                    45,
                    5,
                    186,
                    0
                ],
                "published": "2025-07-05T06:55:45Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    6,
                    55,
                    45,
                    5,
                    186,
                    0
                ],
                "title": "PFCS: Prime Factorization Cache System for Deterministic Data\n  Relationship Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PFCS: Prime Factorization Cache System for Deterministic Data\n  Relationship Discovery"
                },
                "summary": "Cache systems fundamentally limit modern computing performance due to their\ninability to precisely capture data relationships. While achieving 85-92% hit\nrates, traditional systems rely on statistical heuristics that cannot guarantee\nrelationship discovery, leading to suboptimal prefetching and resource waste.\nWe present PFCS (Prime Factorization Cache System), which leverages the\nmathematical uniqueness of prime factorization to achieve deterministic\nrelationship discovery with zero false positives. PFCS assigns unique primes to\ndata elements and represents relationships as composite numbers, enabling the\nrecovery of perfect relationships through factorization. A comprehensive\nevaluation across database, ML, and HPC workloads demonstrates an average\nperformance improvement of x 6.2, 98.9% hit rates, and a 38% power reduction\ncompared to state-of-the-art systems. The mathematical foundation provides\nformal guarantees impossible with approximation-based approaches, establishing\na new paradigm for cache system design",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache systems fundamentally limit modern computing performance due to their\ninability to precisely capture data relationships. While achieving 85-92% hit\nrates, traditional systems rely on statistical heuristics that cannot guarantee\nrelationship discovery, leading to suboptimal prefetching and resource waste.\nWe present PFCS (Prime Factorization Cache System), which leverages the\nmathematical uniqueness of prime factorization to achieve deterministic\nrelationship discovery with zero false positives. PFCS assigns unique primes to\ndata elements and represents relationships as composite numbers, enabling the\nrecovery of perfect relationships through factorization. A comprehensive\nevaluation across database, ML, and HPC workloads demonstrates an average\nperformance improvement of x 6.2, 98.9% hit rates, and a 38% power reduction\ncompared to state-of-the-art systems. The mathematical foundation provides\nformal guarantees impossible with approximation-based approaches, establishing\na new paradigm for cache system design"
                },
                "authors": [
                    {
                        "name": "Duy Le"
                    }
                ],
                "author_detail": {
                    "name": "Duy Le"
                },
                "author": "Duy Le",
                "arxiv_comment": "6 pages, 3 figures, 3 algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06483v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06483v3",
                "updated": "2025-07-05T01:08:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    1,
                    8,
                    40,
                    5,
                    186,
                    0
                ],
                "published": "2024-06-10T17:22:17Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    17,
                    22,
                    17,
                    0,
                    162,
                    0
                ],
                "title": "A Taxonomy and Comparative Analysis of IPv4 Identifier Selection\n  Correctness, Security, and Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Taxonomy and Comparative Analysis of IPv4 Identifier Selection\n  Correctness, Security, and Performance"
                },
                "summary": "The battle for a more secure Internet is waged on many fronts, including the\nmost basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an\nIPv4 header field as old as the Internet with an equally long history as an\nexploited side channel for scanning network properties, inferring off-path\nconnections, and poisoning DNS caches. This article taxonomizes the 25-year\nhistory of IPID-based exploits and the corresponding changes to IPID selection\nmethods. By mathematically analyzing these methods' correctness and security\nand empirically evaluating their performance, we reveal recommendations for\nbest practice as well as shortcomings of current operating system\nimplementations, emphasizing the value of systematic evaluations in network\nsecurity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The battle for a more secure Internet is waged on many fronts, including the\nmost basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an\nIPv4 header field as old as the Internet with an equally long history as an\nexploited side channel for scanning network properties, inferring off-path\nconnections, and poisoning DNS caches. This article taxonomizes the 25-year\nhistory of IPID-based exploits and the corresponding changes to IPID selection\nmethods. By mathematically analyzing these methods' correctness and security\nand empirically evaluating their performance, we reveal recommendations for\nbest practice as well as shortcomings of current operating system\nimplementations, emphasizing the value of systematic evaluations in network\nsecurity."
                },
                "authors": [
                    {
                        "name": "Joshua J. Daymude"
                    },
                    {
                        "name": "Antonio M. Espinoza"
                    },
                    {
                        "name": "Sean Bergen"
                    },
                    {
                        "name": "Benjamin Mixon-Baca"
                    },
                    {
                        "name": "Jeffrey Knockel"
                    },
                    {
                        "name": "Jedidiah R. Crandall"
                    }
                ],
                "author_detail": {
                    "name": "Jedidiah R. Crandall"
                },
                "author": "Jedidiah R. Crandall",
                "arxiv_comment": "36 pages, 11 figures, 2 tables, 1 algorithm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06483v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06483v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03812v1",
                "updated": "2025-07-04T21:09:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    21,
                    9,
                    51,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T21:09:51Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    21,
                    9,
                    51,
                    4,
                    185,
                    0
                ],
                "title": "Memory- and compute-optimized geometric multigrid GMGPolar for\n  curvilinear coordinate representations -- Applications to fusion plasma",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory- and compute-optimized geometric multigrid GMGPolar for\n  curvilinear coordinate representations -- Applications to fusion plasma"
                },
                "summary": "Tokamak fusion reactors are actively studied as a means of realizing energy\nproduction from plasma fusion. However, due to the substantial cost and time\nrequired to construct fusion reactors and run physical experiments, numerical\nexperiments are indispensable for understanding plasma physics inside tokamaks,\nsupporting the design and engineering phase, and optimizing future reactor\ndesigns. Geometric multigrid methods are optimal solvers for many problems that\narise from the discretization of partial differential equations. It has been\nshown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson\nequation in linear complexity and with only small memory requirements compared\nto other state-of-the-art solvers. In this paper, we present a completely\nrefactored and object-oriented version of GMGPolar which offers two different\nmatrix-free implementations. Among other things, we leverage the\nSherman-Morrison formula to solve cyclic tridiagonal systems from circular line\nsolvers without additional fill-in and we apply reordering to optimize cache\naccess of circular and radial smoothing operations. With the Give approach,\nmemory requirements are further reduced and speedups of four to seven are\nobtained for usual test cases. For the Take approach, speedups of 16 to 18 can\nbe attained.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokamak fusion reactors are actively studied as a means of realizing energy\nproduction from plasma fusion. However, due to the substantial cost and time\nrequired to construct fusion reactors and run physical experiments, numerical\nexperiments are indispensable for understanding plasma physics inside tokamaks,\nsupporting the design and engineering phase, and optimizing future reactor\ndesigns. Geometric multigrid methods are optimal solvers for many problems that\narise from the discretization of partial differential equations. It has been\nshown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson\nequation in linear complexity and with only small memory requirements compared\nto other state-of-the-art solvers. In this paper, we present a completely\nrefactored and object-oriented version of GMGPolar which offers two different\nmatrix-free implementations. Among other things, we leverage the\nSherman-Morrison formula to solve cyclic tridiagonal systems from circular line\nsolvers without additional fill-in and we apply reordering to optimize cache\naccess of circular and radial smoothing operations. With the Give approach,\nmemory requirements are further reduced and speedups of four to seven are\nobtained for usual test cases. For the Take approach, speedups of 16 to 18 can\nbe attained."
                },
                "authors": [
                    {
                        "name": "Julian Litz"
                    },
                    {
                        "name": "Philippe Leleux"
                    },
                    {
                        "name": "Carola Kruse"
                    },
                    {
                        "name": "Joscha Gedicke"
                    },
                    {
                        "name": "Martin J. K√ºhn"
                    }
                ],
                "author_detail": {
                    "name": "Martin J. K√ºhn"
                },
                "author": "Martin J. K√ºhn",
                "arxiv_comment": "29 pages, 10 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Q25, 65Y20, 65Y05, 65N55, 65N06, 65B99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03445v1",
                "updated": "2025-07-04T10:01:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    10,
                    1,
                    10,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T10:01:10Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    10,
                    1,
                    10,
                    4,
                    185,
                    0
                ],
                "title": "Quantum Algorithm for the Fixed-Radius Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Algorithm for the Fixed-Radius Neighbor Search"
                },
                "summary": "The neighbor search is a computationally demanding problem, usually both\ntime- and memory-consuming. The main problem of this kind of algorithms is the\nlong execution time due to cache misses. In this work, we propose a quantum\nalgorithm for the Fixed RAdius Neighbor Search problem (FRANS) based on the\nfixed-point version of Grover's algorithm. We derive an efficient circuit for\nsolving the FRANS with linear query complexity with the number of particles\n$N$. The quantum circuit returns the list of all the neighbors' pairs within\nthe fixed radius, together with their distance, avoiding the slow down given by\ncache miss. We explicitly write the Grover's operator and analyze its gate\ncomplexity. The whole algorithm has complexity of\n$\\mathcal{O}(M^{\\frac{1}{2}}N^{2})$ in the worst-case scenario, where $M$ is\nthe number of neighboring pairs, and uses $\\mathcal{O}(\\log N)$ number of\nqubits. By employing extra ancilla qubits the depth of the circuit can be\nbrought down to $\\mathcal{O}(N\\log N)$ at the cost of $\\mathcal{O}(N)$ qubits\nfor unstructured dataset, or $\\mathcal{O}(\\text{poly}(\\log N))$ qubits for\nstructured datasets. Finally we assess the resilience of the model to the\nreadout error, suggesting an error correction-free strategy to check the\naccuracy of the results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The neighbor search is a computationally demanding problem, usually both\ntime- and memory-consuming. The main problem of this kind of algorithms is the\nlong execution time due to cache misses. In this work, we propose a quantum\nalgorithm for the Fixed RAdius Neighbor Search problem (FRANS) based on the\nfixed-point version of Grover's algorithm. We derive an efficient circuit for\nsolving the FRANS with linear query complexity with the number of particles\n$N$. The quantum circuit returns the list of all the neighbors' pairs within\nthe fixed radius, together with their distance, avoiding the slow down given by\ncache miss. We explicitly write the Grover's operator and analyze its gate\ncomplexity. The whole algorithm has complexity of\n$\\mathcal{O}(M^{\\frac{1}{2}}N^{2})$ in the worst-case scenario, where $M$ is\nthe number of neighboring pairs, and uses $\\mathcal{O}(\\log N)$ number of\nqubits. By employing extra ancilla qubits the depth of the circuit can be\nbrought down to $\\mathcal{O}(N\\log N)$ at the cost of $\\mathcal{O}(N)$ qubits\nfor unstructured dataset, or $\\mathcal{O}(\\text{poly}(\\log N))$ qubits for\nstructured datasets. Finally we assess the resilience of the model to the\nreadout error, suggesting an error correction-free strategy to check the\naccuracy of the results."
                },
                "authors": [
                    {
                        "name": "Luca Cappelli"
                    },
                    {
                        "name": "Claudio Sanavio"
                    },
                    {
                        "name": "Alessandro Andrea Zecchi"
                    },
                    {
                        "name": "Giuseppe Murante"
                    },
                    {
                        "name": "Sauro Succi"
                    }
                ],
                "author_detail": {
                    "name": "Sauro Succi"
                },
                "author": "Sauro Succi",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03396v1",
                "updated": "2025-07-04T09:03:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    9,
                    3,
                    18,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T09:03:18Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    9,
                    3,
                    18,
                    4,
                    185,
                    0
                ],
                "title": "Numerical investigation of the effect of high voltage frequency on the\n  density of RONS species in the air atmospheric pressure gas discharge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical investigation of the effect of high voltage frequency on the\n  density of RONS species in the air atmospheric pressure gas discharge"
                },
                "summary": "In the last few decades, studies in various fields of plasma technology have\nexpanded and its application in different processes has increased. Therefore,\nthe achievement of a desirable and practical plasma with specific\ncharacteristics is of particular importance. The frequency of the applied\nvoltage is one of the important factors that play a role in the physical and\nchemical characteristics. In this research, changes in the density of active\nspecies produced in an electrical discharge using a dielectric barrier and air\nworking gas have been investigated from a frequency of 500 Hz to 500 kHz, and\nby applying a constant voltage of 2 kV, have been investigated. For this\npurpose, 87 different reactions with specific collision cross-sections were\ndefined in COMSOL Multiphysics. Other parameters, including current-voltage\nwaveform, electric field, and species densitywere evaluated. The results show\nthat under completely identical conditions, the electron temperature\ndistribution changes with increasing applied frequency, and the density of\nreactive oxygen and nitrogen species RONS decreases, but O shows an increasing\ntrend. It should be noted that the simulation results are in good agreement\nwith previous experimental and simulation reports. These results offer valuable\ninsights into optimizing plasma parameters for different applications,\npotentially resulting in better treatment outcomes across a range of\ntherapeutic domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the last few decades, studies in various fields of plasma technology have\nexpanded and its application in different processes has increased. Therefore,\nthe achievement of a desirable and practical plasma with specific\ncharacteristics is of particular importance. The frequency of the applied\nvoltage is one of the important factors that play a role in the physical and\nchemical characteristics. In this research, changes in the density of active\nspecies produced in an electrical discharge using a dielectric barrier and air\nworking gas have been investigated from a frequency of 500 Hz to 500 kHz, and\nby applying a constant voltage of 2 kV, have been investigated. For this\npurpose, 87 different reactions with specific collision cross-sections were\ndefined in COMSOL Multiphysics. Other parameters, including current-voltage\nwaveform, electric field, and species densitywere evaluated. The results show\nthat under completely identical conditions, the electron temperature\ndistribution changes with increasing applied frequency, and the density of\nreactive oxygen and nitrogen species RONS decreases, but O shows an increasing\ntrend. It should be noted that the simulation results are in good agreement\nwith previous experimental and simulation reports. These results offer valuable\ninsights into optimizing plasma parameters for different applications,\npotentially resulting in better treatment outcomes across a range of\ntherapeutic domains."
                },
                "authors": [
                    {
                        "name": "Fariborz Momtazzadeh"
                    },
                    {
                        "name": "Farshad Sohbatzadeh"
                    },
                    {
                        "name": "Hamed Soltani Ahmadi"
                    },
                    {
                        "name": "Ramin Mehrabifard"
                    }
                ],
                "author_detail": {
                    "name": "Ramin Mehrabifard"
                },
                "author": "Ramin Mehrabifard",
                "arxiv_doi": "10.1007/S40042-025-01392-9.",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/S40042-025-01392-9.",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.03396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v3",
                "updated": "2025-07-04T06:49:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    6,
                    49,
                    31,
                    4,
                    185,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Cache Eviction for LLMs via Attention Output Error-Based Token\n  Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Cache Eviction for LLMs via Attention Output Error-Based Token\n  Selection"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "14 pages, 3 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15431v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15431v3",
                "updated": "2025-07-04T06:36:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    6,
                    36,
                    38,
                    4,
                    185,
                    0
                ],
                "published": "2025-05-21T12:11:53Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    11,
                    53,
                    2,
                    141,
                    0
                ],
                "title": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought"
                },
                "summary": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models."
                },
                "authors": [
                    {
                        "name": "Tencent Hunyuan Team"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Botong Zhou"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Chayse Zhou"
                    },
                    {
                        "name": "ChenChen Zhang"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Chenhao Wang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Guanwei Zhang"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Haipeng Luo"
                    },
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Kejiao Li"
                    },
                    {
                        "name": "Keyao Wang"
                    },
                    {
                        "name": "Lan Jiang"
                    },
                    {
                        "name": "Lixin Liu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Peiqi Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Qianbiao Xiang"
                    },
                    {
                        "name": "Qibin Liu"
                    },
                    {
                        "name": "Qingfeng Sun"
                    },
                    {
                        "name": "Richard Guo"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Tian Zhang"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Weidong Han"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Weijin Zhou"
                    },
                    {
                        "name": "Weikang Wang"
                    },
                    {
                        "name": "Wesleye Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yang Du"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Yulong Wang"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Zenan Xu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "ZhenXiang Yan"
                    },
                    {
                        "name": "Zheng Fang"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Zhuoyu Li"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Alex Yan"
                    },
                    {
                        "name": "Ande Liang"
                    },
                    {
                        "name": "Baitong Liu"
                    },
                    {
                        "name": "Beiping Pan"
                    },
                    {
                        "name": "Bin Xing"
                    },
                    {
                        "name": "Binghong Wu"
                    },
                    {
                        "name": "Bingxin Qu"
                    },
                    {
                        "name": "Bolin Ni"
                    },
                    {
                        "name": "Boyu Wu"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Cheng Jiang"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Chengjun Liu"
                    },
                    {
                        "name": "Chengxu Yang"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Chiyu Wang"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Daisy Yi"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Fanyang Lu"
                    },
                    {
                        "name": "Fei Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Feng Zheng"
                    },
                    {
                        "name": "Guanghua Yu"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Guohua Wang"
                    },
                    {
                        "name": "Haisheng Lin"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Hao Lu"
                    },
                    {
                        "name": "Haoqing Jiang"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Haotian Zhu"
                    },
                    {
                        "name": "Huangjin Dai"
                    },
                    {
                        "name": "Huankui Chen"
                    },
                    {
                        "name": "Huawen Feng"
                    },
                    {
                        "name": "Huihui Cai"
                    },
                    {
                        "name": "Huxin Peng"
                    },
                    {
                        "name": "Jackson Lv"
                    },
                    {
                        "name": "Jiacheng Shi"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Jiangtao Guan"
                    },
                    {
                        "name": "Jianing Xu"
                    },
                    {
                        "name": "Jianwei Cai"
                    },
                    {
                        "name": "Jiarong Zhang"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Jieneng Yang"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jin lv"
                    },
                    {
                        "name": "Jing Zhao"
                    },
                    {
                        "name": "Jinjian Li"
                    },
                    {
                        "name": "Jinxing Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Juntao Guo"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Lei Fu"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Li Liu"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Liya Zhan"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Long Xu"
                    },
                    {
                        "name": "Mao Zheng"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Nanli Chen"
                    },
                    {
                        "name": "Peirui Chen"
                    },
                    {
                        "name": "Peng He"
                    },
                    {
                        "name": "Pengju Pan"
                    },
                    {
                        "name": "Pengzhi Wei"
                    },
                    {
                        "name": "Qi Yang"
                    },
                    {
                        "name": "Qi Yi"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Ruixu Zhou"
                    },
                    {
                        "name": "Shaofeng Zhang"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Shihao Xu"
                    },
                    {
                        "name": "Shuaishuai Chang"
                    },
                    {
                        "name": "Shulin Liu"
                    },
                    {
                        "name": "SiQi Wang"
                    },
                    {
                        "name": "Songjia Feng"
                    },
                    {
                        "name": "Songling Yuan"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Tianjiao Lang"
                    },
                    {
                        "name": "Tongkai Li"
                    },
                    {
                        "name": "Wei Deng"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Weigang Zhang"
                    },
                    {
                        "name": "Weixuan Sun"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Wenzhi Sun"
                    },
                    {
                        "name": "Wenzhuo Jia"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Xiangyu He"
                    },
                    {
                        "name": "Xianshun Ren"
                    },
                    {
                        "name": "XiaoYing Zhu"
                    },
                    {
                        "name": "Xiaolong Guo"
                    },
                    {
                        "name": "Xiaoxue Li"
                    },
                    {
                        "name": "Xiaoyu Ma"
                    },
                    {
                        "name": "Xican Lu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Xinyu Guan"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xudong Gao"
                    },
                    {
                        "name": "Xun Luo"
                    },
                    {
                        "name": "Xuxiang Qi"
                    },
                    {
                        "name": "Yangkun Chen"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yanling Xiao"
                    },
                    {
                        "name": "Yantao Mai"
                    },
                    {
                        "name": "Yanze Chen"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Yeting Yang"
                    },
                    {
                        "name": "YiFan Song"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Yijiao Zhu"
                    },
                    {
                        "name": "Yinhe Wu"
                    },
                    {
                        "name": "Yixian Liu"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuanjun Cai"
                    },
                    {
                        "name": "Yuanlin Tu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Yuhao Jiang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Yuhui Hu"
                    },
                    {
                        "name": "Yujin Lin"
                    },
                    {
                        "name": "Yun Yang"
                    },
                    {
                        "name": "Yunhao Wang"
                    },
                    {
                        "name": "Yusong Zhang"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Zelong Zhang"
                    },
                    {
                        "name": "Zhan Yu"
                    },
                    {
                        "name": "Zhaoliang Yang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Zhenyu Huang"
                    },
                    {
                        "name": "Zhiguang Liu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    },
                    {
                        "name": "Zhiqing Kui"
                    },
                    {
                        "name": "Zhiyin Zeng"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Zhuo Han"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Zigang Geng"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Ziyan Tang"
                    },
                    {
                        "name": "Ziyuan Zhu"
                    },
                    {
                        "name": "Zonglei Zhu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijiang Xu"
                },
                "author": "Zhijiang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15431v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15431v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03231v1",
                "updated": "2025-07-04T00:16:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    0,
                    16,
                    15,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T00:16:15Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    0,
                    16,
                    15,
                    4,
                    185,
                    0
                ],
                "title": "Robust and Efficient Embedded Convex Optimization through First-Order\n  Adaptive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust and Efficient Embedded Convex Optimization through First-Order\n  Adaptive Caching"
                },
                "summary": "Recent advances in Model Predictive Control (MPC) leveraging a combination of\nfirst-order methods, such as the Alternating Direction Method of Multipliers\n(ADMM), and offline precomputation and caching of select operations, have\nexcitingly enabled real-time MPC on microcontrollers. Unfortunately, these\napproaches require the use of fixed hyperparameters, limiting their\nadaptability and overall performance. In this work, we introduce First-Order\nAdaptive Caching, which precomputes not only select matrix operations but also\ntheir sensitivities to hyperparameter variations, enabling online\nhyperparameter updates without full recomputation of the cache. We demonstrate\nthe effectiveness of our approach on a number of dynamic quadrotor tasks,\nachieving up to a 63.4% reduction in ADMM iterations over the use of optimized\nfixed hyperparameters and approaching 70% of the performance of a full cache\nrecomputation, while reducing the computational cost from O(n^3) to O(n^2)\ncomplexity. This performance enables us to perform figure-eight trajectories on\na 27g tiny quadrotor under wind disturbances. We release our implementation\nopen-source for the benefit of the wider robotics community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Model Predictive Control (MPC) leveraging a combination of\nfirst-order methods, such as the Alternating Direction Method of Multipliers\n(ADMM), and offline precomputation and caching of select operations, have\nexcitingly enabled real-time MPC on microcontrollers. Unfortunately, these\napproaches require the use of fixed hyperparameters, limiting their\nadaptability and overall performance. In this work, we introduce First-Order\nAdaptive Caching, which precomputes not only select matrix operations but also\ntheir sensitivities to hyperparameter variations, enabling online\nhyperparameter updates without full recomputation of the cache. We demonstrate\nthe effectiveness of our approach on a number of dynamic quadrotor tasks,\nachieving up to a 63.4% reduction in ADMM iterations over the use of optimized\nfixed hyperparameters and approaching 70% of the performance of a full cache\nrecomputation, while reducing the computational cost from O(n^3) to O(n^2)\ncomplexity. This performance enables us to perform figure-eight trajectories on\na 27g tiny quadrotor under wind disturbances. We release our implementation\nopen-source for the benefit of the wider robotics community."
                },
                "authors": [
                    {
                        "name": "Ishaan Mahajan"
                    },
                    {
                        "name": "Brian Plancher"
                    }
                ],
                "author_detail": {
                    "name": "Brian Plancher"
                },
                "author": "Brian Plancher",
                "arxiv_comment": "Accepted to IROS 2025, 7 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03153v1",
                "updated": "2025-07-03T20:20:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    20,
                    20,
                    33,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T20:20:33Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    20,
                    20,
                    33,
                    3,
                    184,
                    0
                ],
                "title": "HGCA: Hybrid GPU-CPU Attention for Long Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HGCA: Hybrid GPU-CPU Attention for Long Context LLM Inference"
                },
                "summary": "Scaling inference for large language models (LLMs) is increasingly\nconstrained by limited GPU memory, especially due to growing key-value (KV)\ncaches required for long-context generation. While existing approaches offload\nKV caches to CPU memory or apply sparse attention to reduce GPU load, they\noften underutilize CPU compute resources and compromise accuracy. We present\nHGCA, a hybrid CPU-GPU attention mechanism that enables scalable,\nhigh-throughput LLM inference with near-full attention quality. HGCA performs\ndense attention on recently generated KV entries retained in GPU memory and\nparallel sparse attention on selected, salient KV entries in CPU memory. The\nattention outputs are efficiently merged using log-sum-exp fusion, minimizing\nPCIe transfer overhead. HGCA also introduces a finegrained, per-head\nsparsification strategy optimized for CPU execution, preserving contextual\nrelevance while reducing computation. Our implementation seamlessly integrates\ninto existing LLM frameworks without requiring model retraining. Experiments\nacross diverse models and workloads show that HGCA achieves superior\nscalability, supports longer sequences and larger batch sizes, and outperforms\nexisting sparse attention baselines in both performance and accuracy -- all on\ncommodity GPU hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling inference for large language models (LLMs) is increasingly\nconstrained by limited GPU memory, especially due to growing key-value (KV)\ncaches required for long-context generation. While existing approaches offload\nKV caches to CPU memory or apply sparse attention to reduce GPU load, they\noften underutilize CPU compute resources and compromise accuracy. We present\nHGCA, a hybrid CPU-GPU attention mechanism that enables scalable,\nhigh-throughput LLM inference with near-full attention quality. HGCA performs\ndense attention on recently generated KV entries retained in GPU memory and\nparallel sparse attention on selected, salient KV entries in CPU memory. The\nattention outputs are efficiently merged using log-sum-exp fusion, minimizing\nPCIe transfer overhead. HGCA also introduces a finegrained, per-head\nsparsification strategy optimized for CPU execution, preserving contextual\nrelevance while reducing computation. Our implementation seamlessly integrates\ninto existing LLM frameworks without requiring model retraining. Experiments\nacross diverse models and workloads show that HGCA achieves superior\nscalability, supports longer sequences and larger batch sizes, and outperforms\nexisting sparse attention baselines in both performance and accuracy -- all on\ncommodity GPU hardware."
                },
                "authors": [
                    {
                        "name": "Weishu Deng"
                    },
                    {
                        "name": "Yujie Yang"
                    },
                    {
                        "name": "Peiran Du"
                    },
                    {
                        "name": "Lingfeng Xiang"
                    },
                    {
                        "name": "Zhen Lin"
                    },
                    {
                        "name": "Chen Zhong"
                    },
                    {
                        "name": "Song Jiang"
                    },
                    {
                        "name": "Hui Lu"
                    },
                    {
                        "name": "Jia Rao"
                    }
                ],
                "author_detail": {
                    "name": "Jia Rao"
                },
                "author": "Jia Rao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02860v1",
                "updated": "2025-07-03T17:59:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    54,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:59:54Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    54,
                    3,
                    184,
                    0
                ],
                "title": "Less is Enough: Training-Free Video Diffusion Acceleration via\n  Runtime-Adaptive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is Enough: Training-Free Video Diffusion Acceleration via\n  Runtime-Adaptive Caching"
                },
                "summary": "Video generation models have demonstrated remarkable performance, yet their\nbroader adoption remains constrained by slow inference speeds and substantial\ncomputational costs, primarily due to the iterative nature of the denoising\nprocess. Addressing this bottleneck is essential for democratizing advanced\nvideo synthesis technologies and enabling their integration into real-world\napplications. This work proposes EasyCache, a training-free acceleration\nframework for video diffusion models. EasyCache introduces a lightweight,\nruntime-adaptive caching mechanism that dynamically reuses previously computed\ntransformation vectors, avoiding redundant computations during inference.\nUnlike prior approaches, EasyCache requires no offline profiling,\npre-computation, or extensive parameter tuning. We conduct comprehensive\nstudies on various large-scale video generation models, including OpenSora,\nWan2.1, and HunyuanVideo. Our method achieves leading acceleration performance,\nreducing inference time by up to 2.1-3.3$\\times$ compared to the original\nbaselines while maintaining high visual fidelity with a significant up to 36%\nPSNR improvement compared to the previous SOTA method. This improvement makes\nour EasyCache a efficient and highly accessible solution for high-quality video\ngeneration in both research and practical applications. The code is available\nat https://github.com/H-EmbodVis/EasyCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video generation models have demonstrated remarkable performance, yet their\nbroader adoption remains constrained by slow inference speeds and substantial\ncomputational costs, primarily due to the iterative nature of the denoising\nprocess. Addressing this bottleneck is essential for democratizing advanced\nvideo synthesis technologies and enabling their integration into real-world\napplications. This work proposes EasyCache, a training-free acceleration\nframework for video diffusion models. EasyCache introduces a lightweight,\nruntime-adaptive caching mechanism that dynamically reuses previously computed\ntransformation vectors, avoiding redundant computations during inference.\nUnlike prior approaches, EasyCache requires no offline profiling,\npre-computation, or extensive parameter tuning. We conduct comprehensive\nstudies on various large-scale video generation models, including OpenSora,\nWan2.1, and HunyuanVideo. Our method achieves leading acceleration performance,\nreducing inference time by up to 2.1-3.3$\\times$ compared to the original\nbaselines while maintaining high visual fidelity with a significant up to 36%\nPSNR improvement compared to the previous SOTA method. This improvement makes\nour EasyCache a efficient and highly accessible solution for high-quality video\ngeneration in both research and practical applications. The code is available\nat https://github.com/H-EmbodVis/EasyCache."
                },
                "authors": [
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Dingkang Liang"
                    },
                    {
                        "name": "Kaijin Chen"
                    },
                    {
                        "name": "Tianrui Feng"
                    },
                    {
                        "name": "Xiwu Chen"
                    },
                    {
                        "name": "Hongkai Lin"
                    },
                    {
                        "name": "Yikang Ding"
                    },
                    {
                        "name": "Feiyang Tan"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "arxiv_comment": "The code is made available at\n  https://github.com/H-EmbodVis/EasyCache. Project page:\n  https://h-embodvis.github.io/EasyCache/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04789v2",
                "updated": "2025-07-03T17:11:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    11,
                    28,
                    3,
                    184,
                    0
                ],
                "published": "2023-12-08T02:03:55Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    2,
                    3,
                    55,
                    4,
                    342,
                    0
                ],
                "title": "HybridTier: an Adaptive and Lightweight CXL-Memory Tiering System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybridTier: an Adaptive and Lightweight CXL-Memory Tiering System"
                },
                "summary": "Modern workloads are demanding increasingly larger memory capacity. Compute\nExpress Link (CXL)-based memory tiering has emerged as a promising solution for\naddressing this problem by utilizing traditional DRAM alongside slow-tier CXL\nmemory devices. We analyze prior tiering systems and observe two challenges for\nhigh-performance memory tiering: adapting to skewed but dynamically varying\ndata hotness distributions while minimizing memory and cache overhead due to\ntiering.\n  To address these challenges, we propose HybridTier, an adaptive and\nlightweight tiering system for CXL memory. HybridTier tracks both long-term\ndata access frequency and short-term access momentum \\emph{simultaneously} to\naccurately capture and adapt to shifting hotness distributions. HybridTier\nreduces the metadata memory overhead by tracking data accesses\n\\emph{probabilistically}, obtaining higher memory efficiency by trading off a\nsmall amount of tracking inaccuracy that has a negligible impact on application\nperformance. To reduce cache overhead, HybridTier uses lightweight data\nstructures that optimize for data locality to track data hotness. Our\nevaluations show that HybridTier outperforms prior systems by up to $91\\%$\n($19\\%$ geomean), incurring $2.0-7.8\\times$ less memory overhead and\n$1.7-3.5\\times$ less cache misses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern workloads are demanding increasingly larger memory capacity. Compute\nExpress Link (CXL)-based memory tiering has emerged as a promising solution for\naddressing this problem by utilizing traditional DRAM alongside slow-tier CXL\nmemory devices. We analyze prior tiering systems and observe two challenges for\nhigh-performance memory tiering: adapting to skewed but dynamically varying\ndata hotness distributions while minimizing memory and cache overhead due to\ntiering.\n  To address these challenges, we propose HybridTier, an adaptive and\nlightweight tiering system for CXL memory. HybridTier tracks both long-term\ndata access frequency and short-term access momentum \\emph{simultaneously} to\naccurately capture and adapt to shifting hotness distributions. HybridTier\nreduces the metadata memory overhead by tracking data accesses\n\\emph{probabilistically}, obtaining higher memory efficiency by trading off a\nsmall amount of tracking inaccuracy that has a negligible impact on application\nperformance. To reduce cache overhead, HybridTier uses lightweight data\nstructures that optimize for data locality to track data hotness. Our\nevaluations show that HybridTier outperforms prior systems by up to $91\\%$\n($19\\%$ geomean), incurring $2.0-7.8\\times$ less memory overhead and\n$1.7-3.5\\times$ less cache misses."
                },
                "authors": [
                    {
                        "name": "Kevin Song"
                    },
                    {
                        "name": "Jiacheng Yang"
                    },
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Jishen Zhao"
                    },
                    {
                        "name": "Sihang Liu"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    }
                ],
                "author_detail": {
                    "name": "Gennady Pekhimenko"
                },
                "author": "Gennady Pekhimenko",
                "arxiv_doi": "10.1145/3676642.3736119",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676642.3736119",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.04789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Appears in the Proceedings of the 30th ACM International Conference\n  on Architectural Support for Programming Languages and Operating Systems,\n  Volume 3 (ASPLOS 25)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v3",
                "updated": "2025-07-03T16:06:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    6,
                    35,
                    3,
                    184,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02659v1",
                "updated": "2025-07-03T14:20:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T14:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding"
                },
                "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup."
                },
                "authors": [
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Yicheng Lin"
                    },
                    {
                        "name": "Chenzheng Su"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21817v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21817v3",
                "updated": "2025-07-03T08:22:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    8,
                    22,
                    27,
                    3,
                    184,
                    0
                ],
                "published": "2025-03-26T04:16:48Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    4,
                    16,
                    48,
                    2,
                    85,
                    0
                ],
                "title": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping"
                },
                "summary": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency."
                },
                "authors": [
                    {
                        "name": "Weili Zeng"
                    },
                    {
                        "name": "Ziyuan Huang"
                    },
                    {
                        "name": "Kaixiang Ji"
                    },
                    {
                        "name": "Yichao Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yichao Yan"
                },
                "author": "Yichao Yan",
                "arxiv_comment": "Accepted by ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21817v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21817v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02397v1",
                "updated": "2025-07-03T07:49:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    49,
                    18,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T07:49:18Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    49,
                    18,
                    3,
                    184,
                    0
                ],
                "title": "Direct Reconstruction of Terahertz-driven Subcycle Electron Emission\n  Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Reconstruction of Terahertz-driven Subcycle Electron Emission\n  Dynamics"
                },
                "summary": "While field-driven electron emission is theoretically understood down to the\nsubcycle regime, its direct experimental temporal characterization using\nlong-wavelength terahertz (THz) fields remains elusive. Here, by driving a\ngraphite tip with phase-stable quasi-single-cycle THz pulses, we reveal\ndistinct subcycle electron emission dynamics including: (1) At a\ncarrier-envelope phase (CEP) near zero, spectral peaks scale linearly with THz\nfield strength, characteristic of subcycle emission; (2) At the opposite CEP,\ndominant deceleration fields generate stationary low-energy peaks. Crucially,\nwe develop a pump-probe-free, direct reconstruction method extracting electron\npulse profiles solely from measured energy spectra, obtaining durations from\n97.3 to 114.3 fs as the field increases (191-290 kV/cm). Phase-resolved\nsimulations further reveal a 71.2% modulation in the cutoff energy and a\nnear-total (99.7%) suppression of the emission current. This work not only\nvalidates the Fowler-Nordheim model under THz excitation but also establishes a\ngeneral framework for the direct temporal characterization of subcycle electron\nemission, opening pathways for precise electron control in ultrafast electron\nsources and lightwave nanoelectronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While field-driven electron emission is theoretically understood down to the\nsubcycle regime, its direct experimental temporal characterization using\nlong-wavelength terahertz (THz) fields remains elusive. Here, by driving a\ngraphite tip with phase-stable quasi-single-cycle THz pulses, we reveal\ndistinct subcycle electron emission dynamics including: (1) At a\ncarrier-envelope phase (CEP) near zero, spectral peaks scale linearly with THz\nfield strength, characteristic of subcycle emission; (2) At the opposite CEP,\ndominant deceleration fields generate stationary low-energy peaks. Crucially,\nwe develop a pump-probe-free, direct reconstruction method extracting electron\npulse profiles solely from measured energy spectra, obtaining durations from\n97.3 to 114.3 fs as the field increases (191-290 kV/cm). Phase-resolved\nsimulations further reveal a 71.2% modulation in the cutoff energy and a\nnear-total (99.7%) suppression of the emission current. This work not only\nvalidates the Fowler-Nordheim model under THz excitation but also establishes a\ngeneral framework for the direct temporal characterization of subcycle electron\nemission, opening pathways for precise electron control in ultrafast electron\nsources and lightwave nanoelectronics."
                },
                "authors": [
                    {
                        "name": "Jiakang Mao"
                    },
                    {
                        "name": "Yushan Zeng"
                    },
                    {
                        "name": "Hongyang Li"
                    },
                    {
                        "name": "Liwei Song"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Ruxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruxin Li"
                },
                "author": "Ruxin Li",
                "arxiv_comment": "16 pages, 5 figures, references added",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22618v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22618v3",
                "updated": "2025-07-03T04:51:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    4,
                    51,
                    5,
                    3,
                    184,
                    0
                ],
                "published": "2025-05-28T17:39:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    39,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding"
                },
                "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs."
                },
                "authors": [
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22618v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22618v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02227v1",
                "updated": "2025-07-03T01:22:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    1,
                    22,
                    57,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T01:22:57Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    1,
                    22,
                    57,
                    3,
                    184,
                    0
                ],
                "title": "PhysicsCorrect: A Training-Free Approach for Stable Neural PDE\n  Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhysicsCorrect: A Training-Free Approach for Stable Neural PDE\n  Simulations"
                },
                "summary": "Neural networks have emerged as powerful surrogates for solving partial\ndifferential equations (PDEs), offering significant computational speedups over\ntraditional methods. However, these models suffer from a critical limitation:\nerror accumulation during long-term rollouts, where small inaccuracies compound\nexponentially, eventually causing complete divergence from physically valid\nsolutions. We present PhysicsCorrect, a training-free correction framework that\nenforces PDE consistency at each prediction step by formulating correction as a\nlinearized inverse problem based on PDE residuals. Our key innovation is an\nefficient caching strategy that precomputes the Jacobian and its pseudoinverse\nduring an offline warm-up phase, reducing computational overhead by two orders\nof magnitude compared to standard correction approaches. Across three\nrepresentative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and\nthe chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction\nerrors by up to 100x while adding negligible inference time (under 5\\%). The\nframework integrates seamlessly with diverse architectures including Fourier\nNeural Operators, UNets, and Vision Transformers, effectively transforming\nunstable neural surrogates into reliable simulation tools that bridge the gap\nbetween deep learning's computational efficiency and the physical fidelity\ndemanded by practical scientific applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks have emerged as powerful surrogates for solving partial\ndifferential equations (PDEs), offering significant computational speedups over\ntraditional methods. However, these models suffer from a critical limitation:\nerror accumulation during long-term rollouts, where small inaccuracies compound\nexponentially, eventually causing complete divergence from physically valid\nsolutions. We present PhysicsCorrect, a training-free correction framework that\nenforces PDE consistency at each prediction step by formulating correction as a\nlinearized inverse problem based on PDE residuals. Our key innovation is an\nefficient caching strategy that precomputes the Jacobian and its pseudoinverse\nduring an offline warm-up phase, reducing computational overhead by two orders\nof magnitude compared to standard correction approaches. Across three\nrepresentative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and\nthe chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction\nerrors by up to 100x while adding negligible inference time (under 5\\%). The\nframework integrates seamlessly with diverse architectures including Fourier\nNeural Operators, UNets, and Vision Transformers, effectively transforming\nunstable neural surrogates into reliable simulation tools that bridge the gap\nbetween deep learning's computational efficiency and the physical fidelity\ndemanded by practical scientific applications."
                },
                "authors": [
                    {
                        "name": "Xinquan Huang"
                    },
                    {
                        "name": "Paris Perdikaris"
                    }
                ],
                "author_detail": {
                    "name": "Paris Perdikaris"
                },
                "author": "Paris Perdikaris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01652v1",
                "updated": "2025-07-02T12:27:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    27,
                    6,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T12:27:06Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    27,
                    6,
                    2,
                    183,
                    0
                ],
                "title": "Autoregressive Image Generation with Linear Complexity: A Spatial-Aware\n  Decay Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Linear Complexity: A Spatial-Aware\n  Decay Perspective"
                },
                "summary": "Autoregressive (AR) models have garnered significant attention in image\ngeneration for their ability to effectively capture both local and global\nstructures within visual data. However, prevalent AR models predominantly rely\non the transformer architectures, which are beset by quadratic computational\ncomplexity concerning input sequence length and substantial memory overhead due\nto the necessity of maintaining key-value caches. Although linear attention\nmechanisms have successfully reduced this burden in language models, our\ninitial experiments reveal that they significantly degrade image generation\nquality because of their inability to capture critical long-range dependencies\nin visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a\nnovel attention mechanism that explicitly preserves genuine 2D spatial\nrelationships within the flattened image sequences by computing\nposition-dependent decay factors based on true 2D spatial location rather than\n1D sequence positions. Based on this mechanism, we present LASADGen, an\nautoregressive image generator that enables selective attention to relevant\nspatial contexts with linear complexity. Experiments on ImageNet show LASADGen\nachieves state-of-the-art image generation performance and computational\nefficiency, bridging the gap between linear attention's efficiency and spatial\nunderstanding needed for high-quality generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) models have garnered significant attention in image\ngeneration for their ability to effectively capture both local and global\nstructures within visual data. However, prevalent AR models predominantly rely\non the transformer architectures, which are beset by quadratic computational\ncomplexity concerning input sequence length and substantial memory overhead due\nto the necessity of maintaining key-value caches. Although linear attention\nmechanisms have successfully reduced this burden in language models, our\ninitial experiments reveal that they significantly degrade image generation\nquality because of their inability to capture critical long-range dependencies\nin visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a\nnovel attention mechanism that explicitly preserves genuine 2D spatial\nrelationships within the flattened image sequences by computing\nposition-dependent decay factors based on true 2D spatial location rather than\n1D sequence positions. Based on this mechanism, we present LASADGen, an\nautoregressive image generator that enables selective attention to relevant\nspatial contexts with linear complexity. Experiments on ImageNet show LASADGen\nachieves state-of-the-art image generation performance and computational\nefficiency, bridging the gap between linear attention's efficiency and spatial\nunderstanding needed for high-quality generation."
                },
                "authors": [
                    {
                        "name": "Yuxin Mao"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Jinxing Zhou"
                    },
                    {
                        "name": "Hui Deng"
                    },
                    {
                        "name": "Xuyang Shen"
                    },
                    {
                        "name": "Bin Fan"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Yiran Zhong"
                    },
                    {
                        "name": "Yuchao Dai"
                    }
                ],
                "author_detail": {
                    "name": "Yuchao Dai"
                },
                "author": "Yuchao Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.10318v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.10318v4",
                "updated": "2025-07-02T10:16:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    16,
                    58,
                    2,
                    183,
                    0
                ],
                "published": "2022-12-20T15:09:30Z",
                "published_parsed": [
                    2022,
                    12,
                    20,
                    15,
                    9,
                    30,
                    1,
                    354,
                    0
                ],
                "title": "Learned-Database Systems Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned-Database Systems Security"
                },
                "summary": "A learned database system uses machine learning (ML) internally to improve\nperformance. We can expect such systems to be vulnerable to some adversarial-ML\nattacks. Often, the learned component is shared between mutually-distrusting\nusers or processes, much like microarchitectural resources such as caches,\npotentially giving rise to highly-realistic attacker models. However, compared\nto attacks on other ML-based systems, attackers face a level of indirection as\nthey cannot interact directly with the learned model. Additionally, the\ndifference between the attack surface of learned and non-learned versions of\nthe same system is often subtle. These factors obfuscate the de-facto risks\nthat the incorporation of ML carries. We analyze the root causes of\npotentially-increased attack surface in learned database systems and develop a\nframework for identifying vulnerabilities that stem from the use of ML. We\napply our framework to a broad set of learned components currently being\nexplored in the database community. To empirically validate the vulnerabilities\nsurfaced by our framework, we choose 3 of them and implement and evaluate\nexploits against these. We show that the use of ML cause leakage of past\nqueries in a database, enable a poisoning attack that causes exponential memory\nblowup in an index structure and crashes it in seconds, and enable index users\nto snoop on each others' key distributions by timing queries over their own\nkeys. We find that adversarial ML is an universal threat against learned\ncomponents in database systems, point to open research gaps in our\nunderstanding of learned-systems security, and conclude by discussing\nmitigations, while noting that data leakage is inherent in systems whose\nlearned component is shared between multiple parties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A learned database system uses machine learning (ML) internally to improve\nperformance. We can expect such systems to be vulnerable to some adversarial-ML\nattacks. Often, the learned component is shared between mutually-distrusting\nusers or processes, much like microarchitectural resources such as caches,\npotentially giving rise to highly-realistic attacker models. However, compared\nto attacks on other ML-based systems, attackers face a level of indirection as\nthey cannot interact directly with the learned model. Additionally, the\ndifference between the attack surface of learned and non-learned versions of\nthe same system is often subtle. These factors obfuscate the de-facto risks\nthat the incorporation of ML carries. We analyze the root causes of\npotentially-increased attack surface in learned database systems and develop a\nframework for identifying vulnerabilities that stem from the use of ML. We\napply our framework to a broad set of learned components currently being\nexplored in the database community. To empirically validate the vulnerabilities\nsurfaced by our framework, we choose 3 of them and implement and evaluate\nexploits against these. We show that the use of ML cause leakage of past\nqueries in a database, enable a poisoning attack that causes exponential memory\nblowup in an index structure and crashes it in seconds, and enable index users\nto snoop on each others' key distributions by timing queries over their own\nkeys. We find that adversarial ML is an universal threat against learned\ncomponents in database systems, point to open research gaps in our\nunderstanding of learned-systems security, and conclude by discussing\nmitigations, while noting that data leakage is inherent in systems whose\nlearned component is shared between multiple parties."
                },
                "authors": [
                    {
                        "name": "Roei Schuster"
                    },
                    {
                        "name": "Jin Peng Zhou"
                    },
                    {
                        "name": "Thorsten Eisenhofer"
                    },
                    {
                        "name": "Paul Grubbs"
                    },
                    {
                        "name": "Nicolas Papernot"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Papernot"
                },
                "author": "Nicolas Papernot",
                "arxiv_comment": "Accepted at TMLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.10318v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.10318v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01438v1",
                "updated": "2025-07-02T07:47:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    47,
                    28,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T07:47:28Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    47,
                    28,
                    2,
                    183,
                    0
                ],
                "title": "EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices"
                },
                "summary": "Large Language Models (LLMs) have gained significant attention due to their\nversatility across a wide array of applications. Fine-tuning LLMs with\nparameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these\nmodels to efficiently adapt to downstream tasks without extensive retraining.\nDeploying fine-tuned LLMs on multi-tenant edge devices offers substantial\nbenefits, such as reduced latency, enhanced privacy, and personalized\nresponses. However, serving LLMs efficiently on resource-constrained edge\ndevices presents critical challenges, including the complexity of adapter\nselection for different tasks and memory overhead from frequent adapter\nswapping. Moreover, given the multiple requests in multi-tenant settings,\nprocessing requests sequentially results in underutilization of computational\nresources and increased latency. This paper introduces EdgeLoRA, an efficient\nsystem for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA\nincorporates three key innovations: (1) an adaptive adapter selection mechanism\nto streamline the adapter configuration process; (2) heterogeneous memory\nmanagement, leveraging intelligent adapter caching and pooling to mitigate\nmemory operation overhead; and (3) batch LoRA inference, enabling efficient\nbatch processing to significantly reduce computational latency. Comprehensive\nevaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly\noutperforms the status quo (i.e., llama.cpp) in terms of both latency and\nthroughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times\nboost in throughput. Even more impressively, it can serve several orders of\nmagnitude more adapters simultaneously. These results highlight EdgeLoRA's\npotential to transform edge deployment of LLMs in multi-tenant scenarios,\noffering a scalable and efficient solution for resource-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained significant attention due to their\nversatility across a wide array of applications. Fine-tuning LLMs with\nparameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these\nmodels to efficiently adapt to downstream tasks without extensive retraining.\nDeploying fine-tuned LLMs on multi-tenant edge devices offers substantial\nbenefits, such as reduced latency, enhanced privacy, and personalized\nresponses. However, serving LLMs efficiently on resource-constrained edge\ndevices presents critical challenges, including the complexity of adapter\nselection for different tasks and memory overhead from frequent adapter\nswapping. Moreover, given the multiple requests in multi-tenant settings,\nprocessing requests sequentially results in underutilization of computational\nresources and increased latency. This paper introduces EdgeLoRA, an efficient\nsystem for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA\nincorporates three key innovations: (1) an adaptive adapter selection mechanism\nto streamline the adapter configuration process; (2) heterogeneous memory\nmanagement, leveraging intelligent adapter caching and pooling to mitigate\nmemory operation overhead; and (3) batch LoRA inference, enabling efficient\nbatch processing to significantly reduce computational latency. Comprehensive\nevaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly\noutperforms the status quo (i.e., llama.cpp) in terms of both latency and\nthroughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times\nboost in throughput. Even more impressively, it can serve several orders of\nmagnitude more adapters simultaneously. These results highlight EdgeLoRA's\npotential to transform edge deployment of LLMs in multi-tenant scenarios,\noffering a scalable and efficient solution for resource-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Zheyu Shen"
                    },
                    {
                        "name": "Yexiao He"
                    },
                    {
                        "name": "Ziyao Wang"
                    },
                    {
                        "name": "Yuning Zhang"
                    },
                    {
                        "name": "Guoheng Sun"
                    },
                    {
                        "name": "Wanghao Ye"
                    },
                    {
                        "name": "Ang Li"
                    }
                ],
                "author_detail": {
                    "name": "Ang Li"
                },
                "author": "Ang Li",
                "arxiv_doi": "10.1145/3711875.3729141",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711875.3729141",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.01438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20187v2",
                "updated": "2025-07-02T05:12:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    5,
                    12,
                    29,
                    2,
                    183,
                    0
                ],
                "published": "2025-06-25T07:26:42Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    26,
                    42,
                    2,
                    176,
                    0
                ],
                "title": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU"
                },
                "summary": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup."
                },
                "authors": [
                    {
                        "name": "He Sun"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Mingjun Xiao"
                    },
                    {
                        "name": "Chengzhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chengzhong Xu"
                },
                "author": "Chengzhong Xu",
                "arxiv_comment": "15 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02006v1",
                "updated": "2025-07-02T00:35:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    0,
                    35,
                    43,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T00:35:43Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    0,
                    35,
                    43,
                    2,
                    183,
                    0
                ],
                "title": "AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design"
                },
                "summary": "Graph convolutional networks (GCNs) are fundamental in various scientific\napplications, ranging from biomedical protein-protein interactions (PPI) to\nlarge-scale recommendation systems. An essential component for modeling graph\nstructures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As\nthe size of graph data continues to scale up, SpGEMMs are often conducted in an\nout-of-core fashion due to limited GPU memory space in resource-constrained\nsystems. Albeit recent efforts that aim to alleviate the memory constraints of\nout-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory\nlayout, or performing the computation in sparse format, current systems suffer\nfrom both high I/O latency and GPU under-utilization issues.\n  In this paper, we first identify the problems of existing systems, where\nsparse format data alignment and memory allocation are the main performance\nbottlenecks, and propose AIRES, a novel algorithm-system co-design solution to\naccelerate out-of-core SpGEMM computation for GCNs. Specifically, from the\nalgorithm angle, AIRES proposes to alleviate the data alignment issues on the\nblock level for matrices in sparse formats and develops a tiling algorithm to\nfacilitate row block-wise alignment. On the system level, AIRES employs a\nthree-phase dynamic scheduling that features a dual-way data transfer strategy\nutilizing a tiered memory system: integrating GPU memory, GPU Direct Storage\n(GDS), and host memory to reduce I/O latency and improve throughput.\nEvaluations show that AIRES significantly outperforms the state-of-the-art\nmethods, achieving up to 1.8x lower latency in real-world graph processing\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph convolutional networks (GCNs) are fundamental in various scientific\napplications, ranging from biomedical protein-protein interactions (PPI) to\nlarge-scale recommendation systems. An essential component for modeling graph\nstructures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As\nthe size of graph data continues to scale up, SpGEMMs are often conducted in an\nout-of-core fashion due to limited GPU memory space in resource-constrained\nsystems. Albeit recent efforts that aim to alleviate the memory constraints of\nout-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory\nlayout, or performing the computation in sparse format, current systems suffer\nfrom both high I/O latency and GPU under-utilization issues.\n  In this paper, we first identify the problems of existing systems, where\nsparse format data alignment and memory allocation are the main performance\nbottlenecks, and propose AIRES, a novel algorithm-system co-design solution to\naccelerate out-of-core SpGEMM computation for GCNs. Specifically, from the\nalgorithm angle, AIRES proposes to alleviate the data alignment issues on the\nblock level for matrices in sparse formats and develops a tiling algorithm to\nfacilitate row block-wise alignment. On the system level, AIRES employs a\nthree-phase dynamic scheduling that features a dual-way data transfer strategy\nutilizing a tiered memory system: integrating GPU memory, GPU Direct Storage\n(GDS), and host memory to reduce I/O latency and improve throughput.\nEvaluations show that AIRES significantly outperforms the state-of-the-art\nmethods, achieving up to 1.8x lower latency in real-world graph processing\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Shakya Jayakody"
                    },
                    {
                        "name": "Youpeng Zhao"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "36th IEEE International Conference on Application-Specific Systems,\n  Architectures and Processors. (Accepted)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01216v1",
                "updated": "2025-07-01T22:27:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    22,
                    27,
                    21,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T22:27:21Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    22,
                    27,
                    21,
                    1,
                    182,
                    0
                ],
                "title": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning"
                },
                "summary": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data, labels or fine-tuned models to the\nserver. To address those issues, we develop PAE MobiLLM, a privacy-aware and\nefficient LLM FT method which can be deployed on the mobile device via\nserver-assisted additive side-tuning. To further accelerate FT convergence and\nimprove computing efficiency, PAE MobiLLM integrates activation caching on the\nserver side, which allows the server to reuse historical activations and saves\nthe mobile device from repeatedly computing forward passes for the recurring\ndata samples. Besides, to reduce communication cost, PAE MobiLLM develops a\none-token (i.e., ``pivot'' token) activation shortcut that transmits only a\nsingle activation dimension instead of full activation matrices to guide the\nside network tuning. Last but not least, PAE MobiLLM introduces the additive\nadapter side-network design which makes the server train the adapter modules\nbased on device-defined prediction differences rather than raw ground-truth\nlabels. In this way, the server can only assist device-defined side-network\ncomputing, and learn nothing about data, labels or fine-tuned models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data, labels or fine-tuned models to the\nserver. To address those issues, we develop PAE MobiLLM, a privacy-aware and\nefficient LLM FT method which can be deployed on the mobile device via\nserver-assisted additive side-tuning. To further accelerate FT convergence and\nimprove computing efficiency, PAE MobiLLM integrates activation caching on the\nserver side, which allows the server to reuse historical activations and saves\nthe mobile device from repeatedly computing forward passes for the recurring\ndata samples. Besides, to reduce communication cost, PAE MobiLLM develops a\none-token (i.e., ``pivot'' token) activation shortcut that transmits only a\nsingle activation dimension instead of full activation matrices to guide the\nside network tuning. Last but not least, PAE MobiLLM introduces the additive\nadapter side-network design which makes the server train the adapter modules\nbased on device-defined prediction differences rather than raw ground-truth\nlabels. In this way, the server can only assist device-defined side-network\ncomputing, and learn nothing about data, labels or fine-tuned models."
                },
                "authors": [
                    {
                        "name": "Xingke Yang"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Zhiyi Wan"
                    },
                    {
                        "name": "Sicong Li"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Xiaoqi Qi"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Tomoaki Ohtsuki"
                    },
                    {
                        "name": "Xin Fu"
                    },
                    {
                        "name": "Miao Pan"
                    }
                ],
                "author_detail": {
                    "name": "Miao Pan"
                },
                "author": "Miao Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15682v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15682v2",
                "updated": "2025-07-01T21:27:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    21,
                    27,
                    40,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-18T17:59:50Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    59,
                    50,
                    2,
                    169,
                    0
                ],
                "title": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model"
                },
                "summary": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1$.$dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1$.$dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad."
                },
                "authors": [
                    {
                        "name": "Anirud Aggarwal"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    },
                    {
                        "name": "Matthew Gwilliam"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Gwilliam"
                },
                "author": "Matthew Gwilliam",
                "arxiv_comment": "29 pages, 22 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15682v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15682v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01154v1",
                "updated": "2025-07-01T19:28:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    19,
                    28,
                    37,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T19:28:37Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    19,
                    28,
                    37,
                    1,
                    182,
                    0
                ],
                "title": "FlashDP: Private Training Large Language Models with Efficient DP-SGD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashDP: Private Training Large Language Models with Efficient DP-SGD"
                },
                "summary": "As large language models (LLMs) increasingly underpin technological\nadvancements, the privacy of their training data emerges as a critical concern.\nDifferential Privacy (DP) serves as a rigorous mechanism to protect this data,\nyet its integration via Differentially Private Stochastic Gradient Descent\n(DP-SGD) introduces substantial challenges, primarily due to the complexities\nof per-sample gradient clipping. Current explicit methods, such as Opacus,\nnecessitate extensive storage for per-sample gradients, significantly inflating\nmemory requirements. Conversely, implicit methods like GhostClip reduce storage\nneeds by recalculating gradients multiple times, which leads to inefficiencies\ndue to redundant computations. This paper introduces FlashDP, an innovative\ncache-friendly per-layer DP-SGD that consolidates necessary operations into a\nsingle task, calculating gradients only once in a fused manner. This approach\nnot only diminishes memory movement by up to \\textbf{50\\%} but also cuts down\nredundant computations by \\textbf{20\\%}, compared to previous methods.\nConsequently, FlashDP does not increase memory demands and achieves a\n\\textbf{90\\%} throughput compared to the Non-DP method on a four-A100 system\nduring the pre-training of the Llama-13B model, while maintaining parity with\nstandard per-layer clipped DP-SGD in terms of accuracy. These advancements\nestablish FlashDP as a pivotal development for efficient and privacy-preserving\ntraining of LLMs. FlashDP's code has been open-sourced in\nhttps://github.com/kaustpradalab/flashdp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) increasingly underpin technological\nadvancements, the privacy of their training data emerges as a critical concern.\nDifferential Privacy (DP) serves as a rigorous mechanism to protect this data,\nyet its integration via Differentially Private Stochastic Gradient Descent\n(DP-SGD) introduces substantial challenges, primarily due to the complexities\nof per-sample gradient clipping. Current explicit methods, such as Opacus,\nnecessitate extensive storage for per-sample gradients, significantly inflating\nmemory requirements. Conversely, implicit methods like GhostClip reduce storage\nneeds by recalculating gradients multiple times, which leads to inefficiencies\ndue to redundant computations. This paper introduces FlashDP, an innovative\ncache-friendly per-layer DP-SGD that consolidates necessary operations into a\nsingle task, calculating gradients only once in a fused manner. This approach\nnot only diminishes memory movement by up to \\textbf{50\\%} but also cuts down\nredundant computations by \\textbf{20\\%}, compared to previous methods.\nConsequently, FlashDP does not increase memory demands and achieves a\n\\textbf{90\\%} throughput compared to the Non-DP method on a four-A100 system\nduring the pre-training of the Llama-13B model, while maintaining parity with\nstandard per-layer clipped DP-SGD in terms of accuracy. These advancements\nestablish FlashDP as a pivotal development for efficient and privacy-preserving\ntraining of LLMs. FlashDP's code has been open-sourced in\nhttps://github.com/kaustpradalab/flashdp."
                },
                "authors": [
                    {
                        "name": "Liangyu Wang"
                    },
                    {
                        "name": "Junxiao Wang"
                    },
                    {
                        "name": "Jie Ren"
                    },
                    {
                        "name": "Zihang Xiang"
                    },
                    {
                        "name": "David E. Keyes"
                    },
                    {
                        "name": "Di Wang"
                    }
                ],
                "author_detail": {
                    "name": "Di Wang"
                },
                "author": "Di Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00797v1",
                "updated": "2025-07-01T14:30:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    30,
                    31,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T14:30:31Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    30,
                    31,
                    1,
                    182,
                    0
                ],
                "title": "VEDA: Efficient LLM Generation Through Voting-based KV Cache Eviction\n  and Dataflow-flexible Accelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VEDA: Efficient LLM Generation Through Voting-based KV Cache Eviction\n  and Dataflow-flexible Accelerator"
                },
                "summary": "Large Language Models (LLMs) excel in natural language processing tasks but\npose significant computational and memory challenges for edge deployment due to\ntheir intensive resource demands. This work addresses the efficiency of LLM\ninference by algorithm-hardware-dataflow tri-optimizations. We propose a novel\nvoting-based KV cache eviction algorithm, balancing hardware efficiency and\nalgorithm accuracy by adaptively identifying unimportant kv vectors. From a\ndataflow perspective, we introduce a flexible-product dataflow and a runtime\nreconfigurable PE array for matrix-vector multiplication. The proposed approach\neffectively handles the diverse dimensional requirements and solves the\nchallenges of incrementally varying sequence lengths. Additionally, an\nelement-serial scheduling scheme is proposed for nonlinear operations, such as\nsoftmax and layer normalization (layernorm). Results demonstrate a substantial\nreduction in latency, accompanied by a significant decrease in hardware\ncomplexity, from O(N) to O(1). The proposed solution is realized in a\ncustom-designed accelerator, VEDA, which outperforms existing hardware\nplatforms. This research represents a significant advancement in LLM inference\non resource-constrained edge devices, facilitating real-time processing,\nenhancing data privacy, and enabling model customization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in natural language processing tasks but\npose significant computational and memory challenges for edge deployment due to\ntheir intensive resource demands. This work addresses the efficiency of LLM\ninference by algorithm-hardware-dataflow tri-optimizations. We propose a novel\nvoting-based KV cache eviction algorithm, balancing hardware efficiency and\nalgorithm accuracy by adaptively identifying unimportant kv vectors. From a\ndataflow perspective, we introduce a flexible-product dataflow and a runtime\nreconfigurable PE array for matrix-vector multiplication. The proposed approach\neffectively handles the diverse dimensional requirements and solves the\nchallenges of incrementally varying sequence lengths. Additionally, an\nelement-serial scheduling scheme is proposed for nonlinear operations, such as\nsoftmax and layer normalization (layernorm). Results demonstrate a substantial\nreduction in latency, accompanied by a significant decrease in hardware\ncomplexity, from O(N) to O(1). The proposed solution is realized in a\ncustom-designed accelerator, VEDA, which outperforms existing hardware\nplatforms. This research represents a significant advancement in LLM inference\non resource-constrained edge devices, facilitating real-time processing,\nenhancing data privacy, and enabling model customization."
                },
                "authors": [
                    {
                        "name": "Zhican Wang"
                    },
                    {
                        "name": "Hongxiang Fan"
                    },
                    {
                        "name": "Haroon Waris"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Zhenyu Li"
                    },
                    {
                        "name": "Jianfei Jiang"
                    },
                    {
                        "name": "Yanan Sun"
                    },
                    {
                        "name": "Guanghui He"
                    }
                ],
                "author_detail": {
                    "name": "Guanghui He"
                },
                "author": "Guanghui He",
                "arxiv_comment": "DAC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00727v1",
                "updated": "2025-07-01T13:17:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    13,
                    17,
                    46,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T13:17:46Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    13,
                    17,
                    46,
                    1,
                    182,
                    0
                ],
                "title": "On Hierarchical Coded Caching with Offline Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Hierarchical Coded Caching with Offline Users"
                },
                "summary": "This paper studies a two-layer hierarchical network in which some users are\noffline during the content delivery phase. A two-layer hierarchical network\nconsists of a single server connected to multiple cache-aided mirror sites, and\neach mirror site is connected to a distinct set of cache-aided users. A scheme\nfor such a hierarchical system with offline users has been proposed recently\nbut considered a special case where all mirror caches have zero memory, which\nis a significant limitation. We propose an array known as a hierarchical\nhotplug placement delivery array (HHPDA), which describes the placement and\ndelivery phases of a coded caching scheme for a general two-layer hierarchical\nnetwork with offline users. Further, we construct a class of HHPDAs using\ncombinatorial t-designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies a two-layer hierarchical network in which some users are\noffline during the content delivery phase. A two-layer hierarchical network\nconsists of a single server connected to multiple cache-aided mirror sites, and\neach mirror site is connected to a distinct set of cache-aided users. A scheme\nfor such a hierarchical system with offline users has been proposed recently\nbut considered a special case where all mirror caches have zero memory, which\nis a significant limitation. We propose an array known as a hierarchical\nhotplug placement delivery array (HHPDA), which describes the placement and\ndelivery phases of a coded caching scheme for a general two-layer hierarchical\nnetwork with offline users. Further, we construct a class of HHPDAs using\ncombinatorial t-designs."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "A short version of this is accepted for presentation in 2025 IEEE\n  Information Theory Workshop; 8 pages, one figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00716v1",
                "updated": "2025-07-01T12:51:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    51,
                    9,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T12:51:09Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    51,
                    9,
                    1,
                    182,
                    0
                ],
                "title": "Accelerating Loading WebGraphs in ParaGrapher",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Loading WebGraphs in ParaGrapher"
                },
                "summary": "ParaGrapher is a graph loading API and library that enables graph processing\nframeworks to load large-scale compressed graphs with minimal overhead. This\ncapability accelerates the design and implementation of new high-performance\ngraph algorithms and their evaluation on a wide range of graphs and across\ndifferent frameworks. However, our previous study identified two major\nlimitations in ParaGrapher: inefficient utilization of high-bandwidth storage\nand reduced decompression bandwidth due to increased compression ratios. To\naddress these limitations, we present two optimizations for ParaGrapher in this\npaper. To improve storage utilization, particularly for high-bandwidth storage,\nwe introduce ParaGrapher-FUSE (PG-Fuse) a filesystem based on the FUSE\n(Filesystem in User Space). PG-Fuse optimizes storage access by increasing the\nsize of requested blocks, reducing the number of calls to the underlying\nfilesystem, and caching the received blocks in memory for future calls. To\nimprove the decompression bandwidth, we introduce CompBin, a compact binary\nrepresentation of the CSR format. CompBin facilitates direct accesses to\nneighbors while preventing storage usage for unused bytes. Our evaluation on 12\nreal-world and synthetic graphs with up to 128 billion edges shows that PG-Fuse\nand CompBin achieve up to 7.6 and 21.8 times speedup, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParaGrapher is a graph loading API and library that enables graph processing\nframeworks to load large-scale compressed graphs with minimal overhead. This\ncapability accelerates the design and implementation of new high-performance\ngraph algorithms and their evaluation on a wide range of graphs and across\ndifferent frameworks. However, our previous study identified two major\nlimitations in ParaGrapher: inefficient utilization of high-bandwidth storage\nand reduced decompression bandwidth due to increased compression ratios. To\naddress these limitations, we present two optimizations for ParaGrapher in this\npaper. To improve storage utilization, particularly for high-bandwidth storage,\nwe introduce ParaGrapher-FUSE (PG-Fuse) a filesystem based on the FUSE\n(Filesystem in User Space). PG-Fuse optimizes storage access by increasing the\nsize of requested blocks, reducing the number of calls to the underlying\nfilesystem, and caching the received blocks in memory for future calls. To\nimprove the decompression bandwidth, we introduce CompBin, a compact binary\nrepresentation of the CSR format. CompBin facilitates direct accesses to\nneighbors while preventing storage usage for unused bytes. Our evaluation on 12\nreal-world and synthetic graphs with up to 128 billion edges shows that PG-Fuse\nand CompBin achieve up to 7.6 and 21.8 times speedup, respectively."
                },
                "authors": [
                    {
                        "name": "Mohsen Koohi Esfahani"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Koohi Esfahani"
                },
                "author": "Mohsen Koohi Esfahani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00715v1",
                "updated": "2025-07-01T12:42:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    42,
                    6,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T12:42:06Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    42,
                    6,
                    1,
                    182,
                    0
                ],
                "title": "EARN: Efficient Inference Acceleration for LLM-based Generative\n  Recommendation by Register Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EARN: Efficient Inference Acceleration for LLM-based Generative\n  Recommendation by Register Tokens"
                },
                "summary": "Large Language Model-based generative recommendation (LLMRec) has achieved\nnotable success, but it suffers from high inference latency due to massive\ncomputational overhead and memory pressure of KV Cache. Existing KV Cache\nreduction methods face critical limitations: cache compression offers marginal\nacceleration given recommendation tasks' short decoding steps, while prompt\ncompression risks discarding vital interaction history. Through systematic\nanalysis of attention patterns in LLMRec, we uncover two pivotal insights: 1)\nlayer-wise attention sparsity inversion where early layers retain dense\ninformative patterns while later layers exhibit high redundancy, and 2) dual\nattention sinks phenomenon where attention scores concentrate on both head and\ntail tokens of input sequences. Motivated by these insights, we propose EARN,\nan efficient inference framework that leverages the early layers to compress\ninformation into register tokens placed at the input sequence boundaries, then\nfocuses solely on these tokens in the subsequent layers. Extensive experiments\non three datasets, two LLMRec methods and two LLM architectures demonstrate\nEARN's superiority, achieving up to 3.79x speedup and 80.8% KV Cache reduction\nwith better accuracy than the general finetuning approach. Our work bridges the\nefficiency-effectiveness gap in LLMRec, offering practical deployment\nadvantages for industrial scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-based generative recommendation (LLMRec) has achieved\nnotable success, but it suffers from high inference latency due to massive\ncomputational overhead and memory pressure of KV Cache. Existing KV Cache\nreduction methods face critical limitations: cache compression offers marginal\nacceleration given recommendation tasks' short decoding steps, while prompt\ncompression risks discarding vital interaction history. Through systematic\nanalysis of attention patterns in LLMRec, we uncover two pivotal insights: 1)\nlayer-wise attention sparsity inversion where early layers retain dense\ninformative patterns while later layers exhibit high redundancy, and 2) dual\nattention sinks phenomenon where attention scores concentrate on both head and\ntail tokens of input sequences. Motivated by these insights, we propose EARN,\nan efficient inference framework that leverages the early layers to compress\ninformation into register tokens placed at the input sequence boundaries, then\nfocuses solely on these tokens in the subsequent layers. Extensive experiments\non three datasets, two LLMRec methods and two LLM architectures demonstrate\nEARN's superiority, achieving up to 3.79x speedup and 80.8% KV Cache reduction\nwith better accuracy than the general finetuning approach. Our work bridges the\nefficiency-effectiveness gap in LLMRec, offering practical deployment\nadvantages for industrial scenarios."
                },
                "authors": [
                    {
                        "name": "Chaoqun Yang"
                    },
                    {
                        "name": "Xinyu Lin"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Teng Sun"
                    },
                    {
                        "name": "Xianjing Han"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "arxiv_comment": "Accepted by KDD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00614v1",
                "updated": "2025-07-01T09:47:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    9,
                    47,
                    38,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T09:47:38Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    9,
                    47,
                    38,
                    1,
                    182,
                    0
                ],
                "title": "Structural, dielectric, and ferroelectric characteristics of the\n  low-temperature sintered 65PMN-35PT sample for electroceramic applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structural, dielectric, and ferroelectric characteristics of the\n  low-temperature sintered 65PMN-35PT sample for electroceramic applications"
                },
                "summary": "A single-phase 65PMN-35PT ceramic was synthesized at a relatively low\ntemperature (875 oC) using a modified columbite method. X-ray diffraction\nanalysis confirmed the single-phase formation of perovskite 65PMN-35PT with a\ntetragonal structure. Morphological studies indicated that the sample consisted\nof small grains with a size of about 2 micro-m. The dielectric properties of\nthe material demonstrate its relaxor behavior near the ferroelectric transition\ntemperature, TC = 457 K. The saturation and remnant polarization values of\napproximately 25.9 and 20.1 micro-C cm-2 were achieved for an electrically\npoled sample. Additionally, the poling induced a negative internal electric\nfield of about -0.2 kV cm-1 was detected due to the presence of ferroelectric\nnano-grains in this bulk 65PMN-35PT sample. These observed characteristics of\nthe pyrochlore-free 65PMN-35PT ceramic are similar to those of its\nsingle-crystal counterpart.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A single-phase 65PMN-35PT ceramic was synthesized at a relatively low\ntemperature (875 oC) using a modified columbite method. X-ray diffraction\nanalysis confirmed the single-phase formation of perovskite 65PMN-35PT with a\ntetragonal structure. Morphological studies indicated that the sample consisted\nof small grains with a size of about 2 micro-m. The dielectric properties of\nthe material demonstrate its relaxor behavior near the ferroelectric transition\ntemperature, TC = 457 K. The saturation and remnant polarization values of\napproximately 25.9 and 20.1 micro-C cm-2 were achieved for an electrically\npoled sample. Additionally, the poling induced a negative internal electric\nfield of about -0.2 kV cm-1 was detected due to the presence of ferroelectric\nnano-grains in this bulk 65PMN-35PT sample. These observed characteristics of\nthe pyrochlore-free 65PMN-35PT ceramic are similar to those of its\nsingle-crystal counterpart."
                },
                "authors": [
                    {
                        "name": "B. Ramachandran"
                    },
                    {
                        "name": "N. Sudarshan"
                    },
                    {
                        "name": "G. Mangamma"
                    },
                    {
                        "name": "M. S. Ramachandra Rao"
                    }
                ],
                "author_detail": {
                    "name": "M. S. Ramachandra Rao"
                },
                "author": "M. S. Ramachandra Rao",
                "arxiv_doi": "10.1007/s10832-025-00423-y",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10832-025-00423-y",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.00614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 7 figures, 1 Table and Accepted for publication in Journal\n  of Electroceramics",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00462v1",
                "updated": "2025-07-01T06:22:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    6,
                    22,
                    0,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T06:22:00Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    6,
                    22,
                    0,
                    1,
                    182,
                    0
                ],
                "title": "Unleashing the Potential of All Test Samples: Mean-Shift Guided\n  Test-Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of All Test Samples: Mean-Shift Guided\n  Test-Time Adaptation"
                },
                "summary": "Visual-language models (VLMs) like CLIP exhibit strong generalization but\nstruggle with distribution shifts at test time. Existing training-free\ntest-time adaptation (TTA) methods operate strictly within CLIP's original\nfeature space, relying on high-confidence samples while overlooking the\npotential of low-confidence ones. We propose MS-TTA, a training-free approach\nthat enhances feature representations beyond CLIP's space using a single-step\nk-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA\nimproves feature compactness and class separability, leading to more stable\nadaptation. Additionally, a cache of refined embeddings further enhances\ninference by providing Mean Shift enhanced logits. Extensive evaluations on OOD\nand cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms\nstate-of-the-art training-free TTA methods, achieving robust adaptation without\nrequiring additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual-language models (VLMs) like CLIP exhibit strong generalization but\nstruggle with distribution shifts at test time. Existing training-free\ntest-time adaptation (TTA) methods operate strictly within CLIP's original\nfeature space, relying on high-confidence samples while overlooking the\npotential of low-confidence ones. We propose MS-TTA, a training-free approach\nthat enhances feature representations beyond CLIP's space using a single-step\nk-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA\nimproves feature compactness and class separability, leading to more stable\nadaptation. Additionally, a cache of refined embeddings further enhances\ninference by providing Mean Shift enhanced logits. Extensive evaluations on OOD\nand cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms\nstate-of-the-art training-free TTA methods, achieving robust adaptation without\nrequiring additional training."
                },
                "authors": [
                    {
                        "name": "Jizhou Han"
                    },
                    {
                        "name": "Chenhao Ding"
                    },
                    {
                        "name": "SongLin Dong"
                    },
                    {
                        "name": "Yuhang He"
                    },
                    {
                        "name": "Xinyuan Gao"
                    },
                    {
                        "name": "Yihong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Yihong Gong"
                },
                "author": "Yihong Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12036v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12036v3",
                "updated": "2025-07-01T05:46:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    5,
                    46,
                    31,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-23T00:01:52Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    0,
                    1,
                    52,
                    4,
                    143,
                    0
                ],
                "title": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models"
                },
                "summary": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels."
                },
                "authors": [
                    {
                        "name": "Yanting Miao"
                    },
                    {
                        "name": "William Loh"
                    },
                    {
                        "name": "Pacal Poupart"
                    },
                    {
                        "name": "Suraj Kothawade"
                    }
                ],
                "author_detail": {
                    "name": "Suraj Kothawade"
                },
                "author": "Suraj Kothawade",
                "arxiv_comment": "17 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12036v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12036v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14051v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14051v2",
                "updated": "2025-06-30T19:01:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    19,
                    1,
                    18,
                    0,
                    181,
                    0
                ],
                "published": "2025-02-19T19:12:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression"
                },
                "summary": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme."
                },
                "authors": [
                    {
                        "name": "Payman Behnam"
                    },
                    {
                        "name": "Yaosheng Fu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Po-An Tsai"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14051v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14051v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24060v1",
                "updated": "2025-06-30T17:07:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    7,
                    59,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T17:07:59Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    7,
                    59,
                    0,
                    181,
                    0
                ],
                "title": "Combinatorial Multi-Access Coded Caching with Private Caches under\n  Intersecting Index Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combinatorial Multi-Access Coded Caching with Private Caches under\n  Intersecting Index Constraints"
                },
                "summary": "We consider the coded caching system where each user, equipped with a private\ncache, accesses a distinct r-subset of access caches. A central server housing\na library of files populates both private and access caches using uncoded\nplacement. In this work, we focus on a constrained indexing regime, referred to\nas the intersection class, in which the sets used to index the demands of each\nuser must have a nonempty intersection. This regime models resource-limited IoT\nscenarios such as edge-assisted IoT systems, where devices with small private\ncaches connect to a small number of shared caches. We provide a necessary and\nsufficient condition under which the system parameters fall within this\nintersection class. Under this condition, we propose a centralized coded\ncaching scheme and characterize its rate-memory trade-off. Next, we define a\nuniform-intersection subclass and establish a condition under which the system\nbelongs to this subclass. Within this subclass, the proposed scheme has a\nregular structure, with each transmission benefiting the same number of users,\nand we characterize its rate-memory trade-off. Additionally, we derive an index\ncoding-based lower bound on the minimum achievable worst-case rate under\nuncoded placement. Finally, we provide numerical comparisons between the rate\nof the proposed scheme, the new lower bound, and bounds from the original work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the coded caching system where each user, equipped with a private\ncache, accesses a distinct r-subset of access caches. A central server housing\na library of files populates both private and access caches using uncoded\nplacement. In this work, we focus on a constrained indexing regime, referred to\nas the intersection class, in which the sets used to index the demands of each\nuser must have a nonempty intersection. This regime models resource-limited IoT\nscenarios such as edge-assisted IoT systems, where devices with small private\ncaches connect to a small number of shared caches. We provide a necessary and\nsufficient condition under which the system parameters fall within this\nintersection class. Under this condition, we propose a centralized coded\ncaching scheme and characterize its rate-memory trade-off. Next, we define a\nuniform-intersection subclass and establish a condition under which the system\nbelongs to this subclass. Within this subclass, the proposed scheme has a\nregular structure, with each transmission benefiting the same number of users,\nand we characterize its rate-memory trade-off. Additionally, we derive an index\ncoding-based lower bound on the minimum achievable worst-case rate under\nuncoded placement. Finally, we provide numerical comparisons between the rate\nof the proposed scheme, the new lower bound, and bounds from the original work."
                },
                "authors": [
                    {
                        "name": "Dhruv Pratap Singh"
                    },
                    {
                        "name": "Anjana A. Mahesh"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "9 pages and 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05118v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05118v4",
                "updated": "2025-06-30T16:23:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    23,
                    35,
                    0,
                    181,
                    0
                ],
                "published": "2024-05-08T15:16:02Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    15,
                    16,
                    2,
                    2,
                    129,
                    0
                ],
                "title": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms"
                },
                "summary": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning."
                },
                "authors": [
                    {
                        "name": "Ari Rasch"
                    }
                ],
                "author_detail": {
                    "name": "Ari Rasch"
                },
                "author": "Ari Rasch",
                "arxiv_doi": "10.1145/3665643",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3665643",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.05118v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05118v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A short version of this paper is published at ACM TOPLAS and\n  presented at PLDI'24",
                "arxiv_journal_ref": "ACM Trans. Program. Lang. Syst. (May 2024)",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23809v1",
                "updated": "2025-06-30T12:55:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    55,
                    59,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T12:55:59Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    55,
                    59,
                    0,
                    181,
                    0
                ],
                "title": "Large-scale Neural Network Quantum States for ab initio Quantum\n  Chemistry Simulations on Fugaku",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale Neural Network Quantum States for ab initio Quantum\n  Chemistry Simulations on Fugaku"
                },
                "summary": "Solving quantum many-body problems is one of the fundamental challenges in\nquantum chemistry. While neural network quantum states (NQS) have emerged as a\npromising computational tool, its training process incurs exponentially growing\ncomputational demands, becoming prohibitively expensive for large-scale\nmolecular systems and creating fundamental scalability barriers for real-world\napplications. To address above challenges, we present \\ours, a high-performance\nNQS training framework for \\textit{ab initio} electronic structure\ncalculations. First, we propose a scalable sampling parallelism strategy with\nmulti-layers workload division and hybrid sampling scheme, which break the\nscalability barriers for large-scale NQS training. Then, we introduce\nmulti-level parallelism local energy parallelism, enabling more efficient local\nenergy computation. Last, we employ cache-centric optimization for\ntransformer-based \\textit{ansatz} and incorporate it with sampling parallelism\nstrategy, which further speedup up the NQS training and achieve stable memory\nfootprint at scale. Experiments demonstrate that \\ours accelerate NQS training\nwith up to 8.41x speedup and attains a parallel efficiency up to 95.8\\% when\nscaling to 1,536 nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving quantum many-body problems is one of the fundamental challenges in\nquantum chemistry. While neural network quantum states (NQS) have emerged as a\npromising computational tool, its training process incurs exponentially growing\ncomputational demands, becoming prohibitively expensive for large-scale\nmolecular systems and creating fundamental scalability barriers for real-world\napplications. To address above challenges, we present \\ours, a high-performance\nNQS training framework for \\textit{ab initio} electronic structure\ncalculations. First, we propose a scalable sampling parallelism strategy with\nmulti-layers workload division and hybrid sampling scheme, which break the\nscalability barriers for large-scale NQS training. Then, we introduce\nmulti-level parallelism local energy parallelism, enabling more efficient local\nenergy computation. Last, we employ cache-centric optimization for\ntransformer-based \\textit{ansatz} and incorporate it with sampling parallelism\nstrategy, which further speedup up the NQS training and achieve stable memory\nfootprint at scale. Experiments demonstrate that \\ours accelerate NQS training\nwith up to 8.41x speedup and attains a parallel efficiency up to 95.8\\% when\nscaling to 1,536 nodes."
                },
                "authors": [
                    {
                        "name": "Hongtao Xu"
                    },
                    {
                        "name": "Zibo Wu"
                    },
                    {
                        "name": "Mingzhen Li"
                    },
                    {
                        "name": "Weile Jia"
                    }
                ],
                "author_detail": {
                    "name": "Weile Jia"
                },
                "author": "Weile Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02236v2",
                "updated": "2025-06-30T05:54:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    54,
                    40,
                    0,
                    181,
                    0
                ],
                "published": "2025-03-04T03:18:56Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "title": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference"
                },
                "summary": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy."
                },
                "authors": [
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Xinhao Luo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Wentao Ni"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Yuhao Zhu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Chen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Chen Jin"
                },
                "author": "Chen Jin",
                "arxiv_doi": "10.1109/HPCA61900.2025.00112",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/HPCA61900.2025.00112",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.02236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12494v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12494v2",
                "updated": "2025-06-30T05:45:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    45,
                    43,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-14T13:16:31Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    13,
                    16,
                    31,
                    5,
                    165,
                    0
                ],
                "title": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}."
                },
                "authors": [
                    {
                        "name": "Zhuocheng Zhang"
                    },
                    {
                        "name": "Yang Feng"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Accepted by ACL 2025 Demo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12494v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12494v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02922v2",
                "updated": "2025-06-30T05:21:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    21,
                    58,
                    0,
                    181,
                    0
                ],
                "published": "2025-05-05T18:01:17Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    18,
                    1,
                    17,
                    0,
                    125,
                    0
                ],
                "title": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference"
                },
                "summary": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy."
                },
                "authors": [
                    {
                        "name": "Yaoqi Chen"
                    },
                    {
                        "name": "Jinkai Zhang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Jingjia Luo"
                    },
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Jiawei Jiang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23488v1",
                "updated": "2025-06-30T03:22:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    3,
                    22,
                    32,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T03:22:32Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    3,
                    22,
                    32,
                    0,
                    181,
                    0
                ],
                "title": "Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent\n  Metasurfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent\n  Metasurfaces"
                },
                "summary": "Wireless communication systems face significant challenges in meeting the\nincreasing demands for higher data rates and more reliable connectivity in\ncomplex environments. Stacked intelligent metasurfaces (SIMs) have emerged as a\npromising technology for realizing wave-domain signal processing, with mobile\nSIMs offering superior communication performance compared to their fixed\ncounterparts. In this paper, we investigate a novel unmanned aerial vehicle\n(UAV)-mounted SIMs (UAV-SIMs) assisted communication system within the\nlow-altitude economy (LAE) networks paradigm, where UAVs function as both base\nstations that cache SIM-processed data and mobile platforms that flexibly\ndeploy SIMs to enhance uplink communications from ground users. To maximize\nnetwork capacity, we formulate a UAV-SIM-based joint optimization problem\n(USBJOP) that comprehensively addresses three critical aspects: the association\nbetween UAV-SIMs and users, the three-dimensional positioning of UAV-SIMs, and\nthe phase shifts across multiple SIM layers. Due to the inherent non-convexity\nand NP-hardness of USBJOP, we decompose it into three sub-optimization\nproblems, \\textit{i.e.}, association between UAV-SIMs and users optimization\nproblem (AUUOP), UAV location optimization problem (ULOP), and UAV-SIM phase\nshifts optimization problem (USPSOP), and solve them using an alternating\noptimization strategy. Specifically, we transform AUUOP and ULOP into convex\nforms solvable by the CVX tool, while addressing USPSOP through a generative\nartificial intelligence (GAI)-based hybrid optimization algorithm. Simulations\ndemonstrate that our proposed approach significantly outperforms benchmark\nschemes, achieving approximately 1.5 times higher network capacity compared to\nsuboptimal alternatives. Additionally, our proposed GAI method reduces the\nalgorithm runtime by 10\\% while maintaining solution quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless communication systems face significant challenges in meeting the\nincreasing demands for higher data rates and more reliable connectivity in\ncomplex environments. Stacked intelligent metasurfaces (SIMs) have emerged as a\npromising technology for realizing wave-domain signal processing, with mobile\nSIMs offering superior communication performance compared to their fixed\ncounterparts. In this paper, we investigate a novel unmanned aerial vehicle\n(UAV)-mounted SIMs (UAV-SIMs) assisted communication system within the\nlow-altitude economy (LAE) networks paradigm, where UAVs function as both base\nstations that cache SIM-processed data and mobile platforms that flexibly\ndeploy SIMs to enhance uplink communications from ground users. To maximize\nnetwork capacity, we formulate a UAV-SIM-based joint optimization problem\n(USBJOP) that comprehensively addresses three critical aspects: the association\nbetween UAV-SIMs and users, the three-dimensional positioning of UAV-SIMs, and\nthe phase shifts across multiple SIM layers. Due to the inherent non-convexity\nand NP-hardness of USBJOP, we decompose it into three sub-optimization\nproblems, \\textit{i.e.}, association between UAV-SIMs and users optimization\nproblem (AUUOP), UAV location optimization problem (ULOP), and UAV-SIM phase\nshifts optimization problem (USPSOP), and solve them using an alternating\noptimization strategy. Specifically, we transform AUUOP and ULOP into convex\nforms solvable by the CVX tool, while addressing USPSOP through a generative\nartificial intelligence (GAI)-based hybrid optimization algorithm. Simulations\ndemonstrate that our proposed approach significantly outperforms benchmark\nschemes, achieving approximately 1.5 times higher network capacity compared to\nsuboptimal alternatives. Additionally, our proposed GAI method reduces the\nalgorithm runtime by 10\\% while maintaining solution quality."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Mingzhe Fan"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Hongyang Pan"
                    },
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Chuang Zhang"
                    },
                    {
                        "name": "Linyao Li"
                    },
                    {
                        "name": "Changyuan Zhao"
                    },
                    {
                        "name": "Chau Yuen"
                    }
                ],
                "author_detail": {
                    "name": "Chau Yuen"
                },
                "author": "Chau Yuen",
                "arxiv_comment": "This paper has been already submitted to TCCN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23405v1",
                "updated": "2025-06-29T21:55:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    29,
                    21,
                    55,
                    58,
                    6,
                    180,
                    0
                ],
                "published": "2025-06-29T21:55:58Z",
                "published_parsed": [
                    2025,
                    6,
                    29,
                    21,
                    55,
                    58,
                    6,
                    180,
                    0
                ],
                "title": "CMOS+X: Stacking Persistent Embedded Memories based on Oxide Transistors\n  upon GPGPU Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMOS+X: Stacking Persistent Embedded Memories based on Oxide Transistors\n  upon GPGPU Platforms"
                },
                "summary": "In contemporary general-purpose graphics processing units (GPGPUs), the\ncontinued increase in raw arithmetic throughput is constrained by the\ncapabilities of the register file (single-cycle) and last-level cache (high\nbandwidth), which require the delivery of operands at a cadence demanded by\nwide single-instruction multiple-data (SIMD) lanes. Enhancing the capacity,\ndensity, or bandwidth of these memories can unlock substantial performance\ngains; however, the recent stagnation of SRAM bit-cell scaling leads to\ninequivalent losses in compute density.\n  To address the challenges posed by SRAM's scaling and leakage power\nconsumption, this paper explores the potential CMOS+X integration of amorphous\noxide semiconductor (AOS) transistors in capacitive, persistent memory\ntopologies (e.g., 1T1C eDRAM, 2T0C/3T0C Gain Cell) as alternative cells in\nmulti-ported and high-bandwidth banked GPGPU memories. A detailed study of the\ndensity and energy tradeoffs of back-end-of-line (BEOL) integrated memories\nutilizing monolithic 3D (M3D)-integrated multiplexed arrays is conducted, while\naccounting for the macro-level limitations of integrating AOS candidate\nstructures proposed by the device community (an aspect often overlooked in\nprior work). By exploiting the short lifetime of register operands, we propose\na multi-ported AOS gain-cell capable of delivering 3x the read ports in ~76% of\nthe footprint of SRAM with over 70% lower standby power, enabling enhancements\nto compute capacity, such as larger warp sizes or processor counts. Benchmarks\nrun on a validated NVIDIA Ampere-class GPU model, using a modified version of\nAccel-Sim, demonstrate improvements of up to 5.2x the performance per watt and\nan average 8% higher geometric mean instruction per cycle (IPC) on various\ncompute- and memory-bound tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In contemporary general-purpose graphics processing units (GPGPUs), the\ncontinued increase in raw arithmetic throughput is constrained by the\ncapabilities of the register file (single-cycle) and last-level cache (high\nbandwidth), which require the delivery of operands at a cadence demanded by\nwide single-instruction multiple-data (SIMD) lanes. Enhancing the capacity,\ndensity, or bandwidth of these memories can unlock substantial performance\ngains; however, the recent stagnation of SRAM bit-cell scaling leads to\ninequivalent losses in compute density.\n  To address the challenges posed by SRAM's scaling and leakage power\nconsumption, this paper explores the potential CMOS+X integration of amorphous\noxide semiconductor (AOS) transistors in capacitive, persistent memory\ntopologies (e.g., 1T1C eDRAM, 2T0C/3T0C Gain Cell) as alternative cells in\nmulti-ported and high-bandwidth banked GPGPU memories. A detailed study of the\ndensity and energy tradeoffs of back-end-of-line (BEOL) integrated memories\nutilizing monolithic 3D (M3D)-integrated multiplexed arrays is conducted, while\naccounting for the macro-level limitations of integrating AOS candidate\nstructures proposed by the device community (an aspect often overlooked in\nprior work). By exploiting the short lifetime of register operands, we propose\na multi-ported AOS gain-cell capable of delivering 3x the read ports in ~76% of\nthe footprint of SRAM with over 70% lower standby power, enabling enhancements\nto compute capacity, such as larger warp sizes or processor counts. Benchmarks\nrun on a validated NVIDIA Ampere-class GPU model, using a modified version of\nAccel-Sim, demonstrate improvements of up to 5.2x the performance per watt and\nan average 8% higher geometric mean instruction per cycle (IPC) on various\ncompute- and memory-bound tasks."
                },
                "authors": [
                    {
                        "name": "Faaiq Waqar"
                    },
                    {
                        "name": "Ming-Yen Lee"
                    },
                    {
                        "name": "Seongwon Yoon"
                    },
                    {
                        "name": "Seongkwang Lim"
                    },
                    {
                        "name": "Shimeng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Shimeng Yu"
                },
                "author": "Shimeng Yu",
                "arxiv_comment": "14 pages, 18 figures, 4 tables, 4 equations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2; B.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01988v1",
                "updated": "2025-06-28T13:02:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    13,
                    2,
                    17,
                    5,
                    179,
                    0
                ],
                "published": "2025-06-28T13:02:17Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    13,
                    2,
                    17,
                    5,
                    179,
                    0
                ],
                "title": "Scaling Out Chip Interconnect Networks with Implicit Sequence Numbers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Out Chip Interconnect Networks with Implicit Sequence Numbers"
                },
                "summary": "As AI models outpace the capabilities of single processors, interconnects\nacross chips have become a critical enabler for scalable computing. These\nprocessors exchange massive amounts of data at cache-line granularity,\nprompting the adoption of new interconnect protocols like CXL, NVLink, and\nUALink, designed for high bandwidth and small payloads. However, the increasing\ntransfer rates of these protocols heighten susceptibility to errors. While\nmechanisms like Cyclic Redundancy Check (CRC) and Forward Error Correction\n(FEC) are standard for reliable data transmission, scaling chip interconnects\nto multi-node configurations introduces new challenges, particularly in\nmanaging silently dropped flits in switching devices. This paper introduces\nImplicit Sequence Number (ISN), a novel mechanism that ensures precise flit\ndrop detection and in-order delivery without adding header overhead.\nAdditionally, we propose Reliability Extended Link (RXL), an extension of CXL\nthat incorporates ISN to support scalable, reliable multi-node interconnects\nwhile maintaining compatibility with the existing flit structure. By elevating\nCRC to a transport-layer mechanism for end-to-end data and sequence integrity,\nand relying on FEC for link-layer error correction and detection, RXL delivers\nrobust reliability and scalability without compromising bandwidth efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI models outpace the capabilities of single processors, interconnects\nacross chips have become a critical enabler for scalable computing. These\nprocessors exchange massive amounts of data at cache-line granularity,\nprompting the adoption of new interconnect protocols like CXL, NVLink, and\nUALink, designed for high bandwidth and small payloads. However, the increasing\ntransfer rates of these protocols heighten susceptibility to errors. While\nmechanisms like Cyclic Redundancy Check (CRC) and Forward Error Correction\n(FEC) are standard for reliable data transmission, scaling chip interconnects\nto multi-node configurations introduces new challenges, particularly in\nmanaging silently dropped flits in switching devices. This paper introduces\nImplicit Sequence Number (ISN), a novel mechanism that ensures precise flit\ndrop detection and in-order delivery without adding header overhead.\nAdditionally, we propose Reliability Extended Link (RXL), an extension of CXL\nthat incorporates ISN to support scalable, reliable multi-node interconnects\nwhile maintaining compatibility with the existing flit structure. By elevating\nCRC to a transport-layer mechanism for end-to-end data and sequence integrity,\nand relying on FEC for link-layer error correction and detection, RXL delivers\nrobust reliability and scalability without compromising bandwidth efficiency."
                },
                "authors": [
                    {
                        "name": "Giyong Jung"
                    },
                    {
                        "name": "Saeid Gorgin"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungrae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jungrae Kim"
                },
                "author": "Jungrae Kim",
                "arxiv_comment": "12 pages, 8 figures. This paper is accepted for [2025 The\n  International Conference for High Performance Computing, Networking, Storage\n  and Analysis (SC)]",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12593v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12593v4",
                "updated": "2025-06-28T06:24:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    6,
                    24,
                    44,
                    5,
                    179,
                    0
                ],
                "published": "2024-06-18T13:25:18Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    13,
                    25,
                    18,
                    1,
                    170,
                    0
                ],
                "title": "PromptDSI: Prompt-based Rehearsal-free Continual Learning for Document\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptDSI: Prompt-based Rehearsal-free Continual Learning for Document\n  Retrieval"
                },
                "summary": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora."
                },
                "authors": [
                    {
                        "name": "Tuan-Luc Huynh"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Yinwei Wei"
                    },
                    {
                        "name": "Trung Le"
                    },
                    {
                        "name": "Dragan Gasevic"
                    },
                    {
                        "name": "Yuan-Fang Li"
                    },
                    {
                        "name": "Thanh-Toan Do"
                    }
                ],
                "author_detail": {
                    "name": "Thanh-Toan Do"
                },
                "author": "Thanh-Toan Do",
                "arxiv_comment": "ECML PKDD 2025 Research track. Camera-ready version. Code is\n  available at https://github.com/LouisDo2108/PromptDSI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12593v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12593v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05460v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05460v4",
                "updated": "2025-06-28T03:53:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    3,
                    53,
                    17,
                    5,
                    179,
                    0
                ],
                "published": "2024-12-25T10:11:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation"
                },
                "summary": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively affects key Service Level Objectives (SLOs),\nsuch as time to first token (TTFT) and time per output token (TPOT). We\nintroduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that\nseparates the encoding, prefill, and decode stages onto dedicated resources.\nUnlike current systems, which bundle encoding and prefill together, our\napproach decouples these steps, unlocking new opportunities and optimizations.\nThese include a mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize the encoding load within a request, a module for\noptimal resource allocation for disaggregated serving, and a novel\nrole-switching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more\nimages per request, and 2.2x larger KV caches. Furthermore, it leads to\nsignificant improvements in SLO attainment (up to 90-100% improvement) and TTFT\n(up to 71% reduction), compared to systems that do not disaggregate. The code\nis available at https://github.com/vbdi/epdserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively affects key Service Level Objectives (SLOs),\nsuch as time to first token (TTFT) and time per output token (TPOT). We\nintroduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that\nseparates the encoding, prefill, and decode stages onto dedicated resources.\nUnlike current systems, which bundle encoding and prefill together, our\napproach decouples these steps, unlocking new opportunities and optimizations.\nThese include a mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize the encoding load within a request, a module for\noptimal resource allocation for disaggregated serving, and a novel\nrole-switching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more\nimages per request, and 2.2x larger KV caches. Furthermore, it leads to\nsignificant improvements in SLO attainment (up to 90-100% improvement) and TTFT\n(up to 71% reduction), compared to systems that do not disaggregate. The code\nis available at https://github.com/vbdi/epdserve."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Yifan Hu"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Linzi Xing"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "arxiv_comment": "17 pages, 12 figures, 9 tables",
                "arxiv_journal_ref": "International Conference on Machine Proceedings of the 42nd\n  International Conference on Machine Learning, Vancouver, Canada. PMLR 267,\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05460v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05460v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22396v1",
                "updated": "2025-06-27T17:10:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    10,
                    32,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T17:10:32Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    10,
                    32,
                    4,
                    178,
                    0
                ],
                "title": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting,\n  KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting,\n  KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization"
                },
                "summary": "Inference accounts for the majority of latency and energy consumption in\nlarge language model (LLM) deployments, often exceeding 90% of total cost.\nWhile training-time efficiency has seen extensive progress, runtime\noptimization remains a key bottleneck, particularly under autoregressive\ndecoding. Existing approaches -- such as pruning, quantization, early exits,\nand speculative decoding -- often require retraining, architectural changes, or\ndisrupt decoding compatibility. We introduce QuickSilver, a modular,\ntoken-level framework that enables semantic adaptivity at inference time\nwithout altering model weights or structure. QuickSilver integrates four\nsynergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged\nrepresentations; (ii) KV Cache Skipping, which selectively suppresses memory\nwrites to reduce attention overhead; and (iii) Contextual Token Fusion, which\ncollapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on\nfrozen, dense models and requires no auxiliary networks. Applied to GPT-2 and\nLlama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP\nreduction with negligible perplexity degradation (<=0.2).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference accounts for the majority of latency and energy consumption in\nlarge language model (LLM) deployments, often exceeding 90% of total cost.\nWhile training-time efficiency has seen extensive progress, runtime\noptimization remains a key bottleneck, particularly under autoregressive\ndecoding. Existing approaches -- such as pruning, quantization, early exits,\nand speculative decoding -- often require retraining, architectural changes, or\ndisrupt decoding compatibility. We introduce QuickSilver, a modular,\ntoken-level framework that enables semantic adaptivity at inference time\nwithout altering model weights or structure. QuickSilver integrates four\nsynergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged\nrepresentations; (ii) KV Cache Skipping, which selectively suppresses memory\nwrites to reduce attention overhead; and (iii) Contextual Token Fusion, which\ncollapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on\nfrozen, dense models and requires no auxiliary networks. Applied to GPT-2 and\nLlama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP\nreduction with negligible perplexity degradation (<=0.2)."
                },
                "authors": [
                    {
                        "name": "Danush Khanna"
                    },
                    {
                        "name": "Aditya Kumar Guru"
                    },
                    {
                        "name": "Srivarshinee Sridhar"
                    },
                    {
                        "name": "Zidan Ahmed"
                    },
                    {
                        "name": "Rubhav Bahirwani"
                    },
                    {
                        "name": "Meetu Malhotra"
                    },
                    {
                        "name": "Vinija Jain"
                    },
                    {
                        "name": "Aman Chadha"
                    },
                    {
                        "name": "Amitava Das"
                    },
                    {
                        "name": "Kripabandhu Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Kripabandhu Ghosh"
                },
                "author": "Kripabandhu Ghosh",
                "arxiv_comment": "Preprint. Under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22033v1",
                "updated": "2025-06-27T09:27:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    27,
                    4,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T09:27:04Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    27,
                    4,
                    4,
                    178,
                    0
                ],
                "title": "SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient\n  Pipeline-Parallel LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient\n  Pipeline-Parallel LLM Inference"
                },
                "summary": "As inference workloads for large language models (LLMs) scale to meet growing\nuser demand, pipeline parallelism (PP) has become a widely adopted strategy for\nmulti-GPU deployment, particularly in cross-node setups, to improve key-value\n(KV) cache capacity and inference throughput. However, PP suffers from inherent\ninefficiencies caused by three types of execution bubbles-load-imbalance,\nintra-stage, and inter-stage-which limit pipeline saturation. We present\nSiPipe, a heterogeneous pipeline design that improves throughput by leveraging\nunderutilized CPU resources to offload auxiliary computation and communication.\nSiPipe incorporates three key techniques-CPU sampling, a token-safe execution\nmodel, and structure-aware transmission-to mitigate pipeline bubbles and\nimprove execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1\ntimes higher throughput, 43% lower per-token latency, and up to 23% higher\naverage GPU utilization compared to the state-of-the-art vLLM under the same PP\nconfiguration, demonstrating its generality across LLMs and deployment\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As inference workloads for large language models (LLMs) scale to meet growing\nuser demand, pipeline parallelism (PP) has become a widely adopted strategy for\nmulti-GPU deployment, particularly in cross-node setups, to improve key-value\n(KV) cache capacity and inference throughput. However, PP suffers from inherent\ninefficiencies caused by three types of execution bubbles-load-imbalance,\nintra-stage, and inter-stage-which limit pipeline saturation. We present\nSiPipe, a heterogeneous pipeline design that improves throughput by leveraging\nunderutilized CPU resources to offload auxiliary computation and communication.\nSiPipe incorporates three key techniques-CPU sampling, a token-safe execution\nmodel, and structure-aware transmission-to mitigate pipeline bubbles and\nimprove execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1\ntimes higher throughput, 43% lower per-token latency, and up to 23% higher\naverage GPU utilization compared to the state-of-the-art vLLM under the same PP\nconfiguration, demonstrating its generality across LLMs and deployment\nscenarios."
                },
                "authors": [
                    {
                        "name": "Yongchao He"
                    },
                    {
                        "name": "Bohan Zhao"
                    },
                    {
                        "name": "Zheng Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Cao"
                },
                "author": "Zheng Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.10543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10543v1",
                "updated": "2025-07-14T17:59:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    59,
                    8,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T17:59:08Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    59,
                    8,
                    0,
                    195,
                    0
                ],
                "title": "MP1: Mean Flow Tames Policy Learning in 1-step for Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MP1: Mean Flow Tames Policy Learning in 1-step for Robotic Manipulation"
                },
                "summary": "In robot manipulation, robot learning has become a prevailing approach.\nHowever, generative models within this field face a fundamental trade-off\nbetween the slow, iterative sampling of diffusion models and the architectural\nconstraints of faster Flow-based methods, which often rely on explicit\nconsistency losses. To address these limitations, we introduce MP1, which pairs\n3D point-cloud inputs with the MeanFlow paradigm to generate action\ntrajectories in one network function evaluation (1-NFE). By directly learning\nthe interval-averaged velocity via the MeanFlow Identity, our policy avoids any\nadditional consistency constraints. This formulation eliminates numerical\nODE-solver errors during inference, yielding more precise trajectories. MP1\nfurther incorporates CFG for improved trajectory controllability while\nretaining 1-NFE inference without reintroducing structural constraints. Because\nsubtle scene-context variations are critical for robot learning, especially in\nfew-shot learning, we introduce a lightweight Dispersive Loss that repels state\nembeddings during training, boosting generalization without slowing inference.\nWe validate our method on the Adroit and Meta-World benchmarks, as well as in\nreal-world scenarios. Experimental results show MP1 achieves superior average\ntask success rates, outperforming DP3 by 10.2% and FlowPolicy by 7.3%. Its\naverage inference time is only 6.8 ms-19x faster than DP3 and nearly 2x faster\nthan FlowPolicy. Our code is available at https://mp1-2254.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In robot manipulation, robot learning has become a prevailing approach.\nHowever, generative models within this field face a fundamental trade-off\nbetween the slow, iterative sampling of diffusion models and the architectural\nconstraints of faster Flow-based methods, which often rely on explicit\nconsistency losses. To address these limitations, we introduce MP1, which pairs\n3D point-cloud inputs with the MeanFlow paradigm to generate action\ntrajectories in one network function evaluation (1-NFE). By directly learning\nthe interval-averaged velocity via the MeanFlow Identity, our policy avoids any\nadditional consistency constraints. This formulation eliminates numerical\nODE-solver errors during inference, yielding more precise trajectories. MP1\nfurther incorporates CFG for improved trajectory controllability while\nretaining 1-NFE inference without reintroducing structural constraints. Because\nsubtle scene-context variations are critical for robot learning, especially in\nfew-shot learning, we introduce a lightweight Dispersive Loss that repels state\nembeddings during training, boosting generalization without slowing inference.\nWe validate our method on the Adroit and Meta-World benchmarks, as well as in\nreal-world scenarios. Experimental results show MP1 achieves superior average\ntask success rates, outperforming DP3 by 10.2% and FlowPolicy by 7.3%. Its\naverage inference time is only 6.8 ms-19x faster than DP3 and nearly 2x faster\nthan FlowPolicy. Our code is available at https://mp1-2254.github.io/."
                },
                "authors": [
                    {
                        "name": "Juyi Sheng"
                    },
                    {
                        "name": "Ziyi Wang"
                    },
                    {
                        "name": "Peiming Li"
                    },
                    {
                        "name": "Mengyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Mengyuan Liu"
                },
                "author": "Mengyuan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10540v1",
                "updated": "2025-07-14T17:58:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    58,
                    2,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T17:58:02Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    58,
                    2,
                    0,
                    195,
                    0
                ],
                "title": "Fusing LLM Capabilities with Routing Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusing LLM Capabilities with Routing Data"
                },
                "summary": "The rapid advancement of large language models (LLMs) has created a vibrant\necosystem of diverse architectures, each with unique strengths due to\ndifferences in design, training data, and objectives. However, most\napplications still rely on a single backend model, limiting coverage of\ncapabilities and leading to inefficiencies in performance and token cost when\ntackling complex tasks. We highlight an underexploited opportunity: LLM routing\ndata, produced when hosting platforms route diverse queries to different\nmodels, which can reveal comparative strengths across tasks. To address this,\nwe propose FusionBench, a comprehensive routing benchmark covering 14 tasks\nacross five domains with 20 open-source LLMs (8B to 671B parameters), capturing\n103M tokens and summarizing reusable thought templates from top models.\nBuilding on this, we introduce FusionFactory, a systematic fusion framework\nwith three levels: (1) query-level fusion, tailoring routers for each query\nusing both direct responses and reasoning-augmented outputs; (2) thought-level\nfusion, leveraging abstract templates derived from top-performing LLMs' answers\nto similar queries; and (3) model-level fusion, transferring capabilities\nbetween models via distillation, using top responses or highest judge scores as\ntraining data. Experiments show FusionFactory consistently outperforms the best\nindividual LLM across all 14 benchmarks, with optimal fusion configurations\nvarying by benchmark, demonstrating the value of systematic LLM fusion in\nharnessing complementary strengths and improving overall performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has created a vibrant\necosystem of diverse architectures, each with unique strengths due to\ndifferences in design, training data, and objectives. However, most\napplications still rely on a single backend model, limiting coverage of\ncapabilities and leading to inefficiencies in performance and token cost when\ntackling complex tasks. We highlight an underexploited opportunity: LLM routing\ndata, produced when hosting platforms route diverse queries to different\nmodels, which can reveal comparative strengths across tasks. To address this,\nwe propose FusionBench, a comprehensive routing benchmark covering 14 tasks\nacross five domains with 20 open-source LLMs (8B to 671B parameters), capturing\n103M tokens and summarizing reusable thought templates from top models.\nBuilding on this, we introduce FusionFactory, a systematic fusion framework\nwith three levels: (1) query-level fusion, tailoring routers for each query\nusing both direct responses and reasoning-augmented outputs; (2) thought-level\nfusion, leveraging abstract templates derived from top-performing LLMs' answers\nto similar queries; and (3) model-level fusion, transferring capabilities\nbetween models via distillation, using top responses or highest judge scores as\ntraining data. Experiments show FusionFactory consistently outperforms the best\nindividual LLM across all 14 benchmarks, with optimal fusion configurations\nvarying by benchmark, demonstrating the value of systematic LLM fusion in\nharnessing complementary strengths and improving overall performance."
                },
                "authors": [
                    {
                        "name": "Tao Feng"
                    },
                    {
                        "name": "Haozhen Zhang"
                    },
                    {
                        "name": "Zijie Lei"
                    },
                    {
                        "name": "Pengrui Han"
                    },
                    {
                        "name": "Mostofa Patwary"
                    },
                    {
                        "name": "Mohammad Shoeybi"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Jiaxuan You"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxuan You"
                },
                "author": "Jiaxuan You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10535v1",
                "updated": "2025-07-14T17:56:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    56,
                    29,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T17:56:29Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    56,
                    29,
                    0,
                    195,
                    0
                ],
                "title": "CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks"
                },
                "summary": "Large Language Models (LLMs) have significantly advanced the state-of-the-art\nin various coding tasks. Beyond directly answering user queries, LLMs can also\nserve as judges, assessing and comparing the quality of responses generated by\nother models. Such an evaluation capability is crucial both for benchmarking\ndifferent LLMs and for improving response quality through response ranking.\nHowever, despite the growing adoption of the LLM-as-a-Judge paradigm, its\neffectiveness in coding scenarios remains underexplored due to the absence of\ndedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a\nbenchmark explicitly designed to evaluate the performance of LLM-as-a-Judge\nmodels across three critical coding tasks: code generation, code repair, and\nunit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge\nmodels, we find that recent thinking models significantly outperform\nnon-thinking models on our carefully designed code judging tasks. Notably, even\nrelatively small thinking models, such as Qwen3-8B, can outperform specially\ntrained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still\nexhibit significant randomness in their judgment of coding tasks. For pairwise\njudging tasks, simply changing the order in which responses are presented can\nsubstantially impact accuracy. In addition, when judging code and unit tests\nwritten by different LLMs, LLM-as-a-Judge models also show variance in\nperformance. This sensitivity raises concerns about the reliability and\nconsistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal\nprompting strategies for LLM-as-a-Judge. We find that using pair-wise\ncomparison outperforms scalar point-wise judging. Furthermore, retaining\ncomments and reasoning in the full, unprocessed LLM response leads to improved\njudge performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly advanced the state-of-the-art\nin various coding tasks. Beyond directly answering user queries, LLMs can also\nserve as judges, assessing and comparing the quality of responses generated by\nother models. Such an evaluation capability is crucial both for benchmarking\ndifferent LLMs and for improving response quality through response ranking.\nHowever, despite the growing adoption of the LLM-as-a-Judge paradigm, its\neffectiveness in coding scenarios remains underexplored due to the absence of\ndedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a\nbenchmark explicitly designed to evaluate the performance of LLM-as-a-Judge\nmodels across three critical coding tasks: code generation, code repair, and\nunit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge\nmodels, we find that recent thinking models significantly outperform\nnon-thinking models on our carefully designed code judging tasks. Notably, even\nrelatively small thinking models, such as Qwen3-8B, can outperform specially\ntrained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still\nexhibit significant randomness in their judgment of coding tasks. For pairwise\njudging tasks, simply changing the order in which responses are presented can\nsubstantially impact accuracy. In addition, when judging code and unit tests\nwritten by different LLMs, LLM-as-a-Judge models also show variance in\nperformance. This sensitivity raises concerns about the reliability and\nconsistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal\nprompting strategies for LLM-as-a-Judge. We find that using pair-wise\ncomparison outperforms scalar point-wise judging. Furthermore, retaining\ncomments and reasoning in the full, unprocessed LLM response leads to improved\njudge performance."
                },
                "authors": [
                    {
                        "name": "Hongchao Jiang"
                    },
                    {
                        "name": "Yiming Chen"
                    },
                    {
                        "name": "Yushi Cao"
                    },
                    {
                        "name": "Hung-yi Lee"
                    },
                    {
                        "name": "Robby T. Tan"
                    }
                ],
                "author_detail": {
                    "name": "Robby T. Tan"
                },
                "author": "Robby T. Tan",
                "arxiv_comment": "Dataset is available at\n  https://huggingface.co/datasets/mattymchen/codejudgebench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10532v1",
                "updated": "2025-07-14T17:55:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    55,
                    15,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T17:55:15Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    55,
                    15,
                    0,
                    195,
                    0
                ],
                "title": "Reasoning or Memorization? Unreliable Results of Reinforcement Learning\n  Due to Data Contamination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning or Memorization? Unreliable Results of Reinforcement Learning\n  Due to Data Contamination"
                },
                "summary": "The reasoning capabilities of large language models (LLMs) have been a\nlongstanding focus of research. Recent works have further enhanced these\ncapabilities using reinforcement learning (RL), with many new methods claiming\nsignificant improvements with minimal or no external supervision. Surprisingly,\nsome studies even suggest that random or incorrect reward signals can enhance\nreasoning performance. However, these breakthroughs are mostly reported on the\nQwen2.5 model family and evaluated on well-known benchmarks such as MATH-500,\nAMC, and AIME, while failing to achieve similar gains on other models like\nLlama, which warrants further investigation. Our analysis shows that although\nQwen2.5 achieves strong mathematical reasoning performance, its pretraining on\nlarge-scale web corpora makes it vulnerable to data contamination in popular\nbenchmarks. As a result, results derived from these benchmarks may be\nunreliable. To address this, we introduce a generator that produces fully\nsynthetic arithmetic problems of arbitrary length and difficulty, yielding a\nclean dataset we call RandomCalculation. Using these leakage-free datasets, we\nshow that only accurate reward signals consistently improve performance, while\nnoisy or incorrect signals do not. We advocate for evaluating RL methods on\nuncontaminated benchmarks and across diverse model families to ensure\ntrustworthy conclusions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning capabilities of large language models (LLMs) have been a\nlongstanding focus of research. Recent works have further enhanced these\ncapabilities using reinforcement learning (RL), with many new methods claiming\nsignificant improvements with minimal or no external supervision. Surprisingly,\nsome studies even suggest that random or incorrect reward signals can enhance\nreasoning performance. However, these breakthroughs are mostly reported on the\nQwen2.5 model family and evaluated on well-known benchmarks such as MATH-500,\nAMC, and AIME, while failing to achieve similar gains on other models like\nLlama, which warrants further investigation. Our analysis shows that although\nQwen2.5 achieves strong mathematical reasoning performance, its pretraining on\nlarge-scale web corpora makes it vulnerable to data contamination in popular\nbenchmarks. As a result, results derived from these benchmarks may be\nunreliable. To address this, we introduce a generator that produces fully\nsynthetic arithmetic problems of arbitrary length and difficulty, yielding a\nclean dataset we call RandomCalculation. Using these leakage-free datasets, we\nshow that only accurate reward signals consistently improve performance, while\nnoisy or incorrect signals do not. We advocate for evaluating RL methods on\nuncontaminated benchmarks and across diverse model families to ensure\ntrustworthy conclusions."
                },
                "authors": [
                    {
                        "name": "Mingqi Wu"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Qiaole Dong"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Senjie Jin"
                    },
                    {
                        "name": "Xiaoran Fan"
                    },
                    {
                        "name": "Yuhao Zhou"
                    },
                    {
                        "name": "Yanwei Fu"
                    },
                    {
                        "name": "Qin Liu"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10530v1",
                "updated": "2025-07-14T17:54:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    54,
                    47,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T17:54:47Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    54,
                    47,
                    0,
                    195,
                    0
                ],
                "title": "Accurate generation of chemical reaction transition states by\n  conditional flow matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate generation of chemical reaction transition states by\n  conditional flow matching"
                },
                "summary": "Transition state (TS) structures define the critical geometries and energy\nbarriers underlying chemical reactivity, yet their fleeting nature renders them\nexperimentally elusive and drives the reliance on costly, high-throughput\ndensity functional theory (DFT) calculations. Here, we introduce TS-GEN, a\nconditional flow-matching generative model that maps samples from a simple\nGaussian prior directly to transition-state saddle-point geometries in a\nsingle, deterministic pass. By embedding both reactant and product\nconformations as conditioning information, TS-GEN learns to transport latent\nnoise to true TS structures via an optimal-transport path, effectively\nreplacing the iterative optimization common in nudged-elastic band or\nstring-method algorithms. TS-GEN delivers unprecedented accuracy, achieving a\nroot-mean-square deviation of $0.004\\ \\rm{\\mathring{A}}$ (vs. $0.103\\\n\\rm{\\mathring{A}}$ for prior state-of-the-art) and a mean barrier-height error\nof $1.019\\ {\\rm kcal/mol}$ (vs. $2.864\\ {\\rm kcal/mol}$), while requiring only\n$0.06\\ {\\rm s}$ GPU time per inference. Over 87% of generated TSs meet\nchemical-accuracy criteria ($<1.58\\ {\\rm kcal/mol}$ error), substantially\noutpacing existing methods. TS-GEN also exhibits strong transferability to\nout-of-distribution reactions from a larger database. By uniting sub-angstrom\nprecision, sub-second speed, and broad applicability, TS-GEN will be highly\nuseful for high-throughput exploration of complex reaction networks, paving the\nway to the exploration of novel chemical reaction mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transition state (TS) structures define the critical geometries and energy\nbarriers underlying chemical reactivity, yet their fleeting nature renders them\nexperimentally elusive and drives the reliance on costly, high-throughput\ndensity functional theory (DFT) calculations. Here, we introduce TS-GEN, a\nconditional flow-matching generative model that maps samples from a simple\nGaussian prior directly to transition-state saddle-point geometries in a\nsingle, deterministic pass. By embedding both reactant and product\nconformations as conditioning information, TS-GEN learns to transport latent\nnoise to true TS structures via an optimal-transport path, effectively\nreplacing the iterative optimization common in nudged-elastic band or\nstring-method algorithms. TS-GEN delivers unprecedented accuracy, achieving a\nroot-mean-square deviation of $0.004\\ \\rm{\\mathring{A}}$ (vs. $0.103\\\n\\rm{\\mathring{A}}$ for prior state-of-the-art) and a mean barrier-height error\nof $1.019\\ {\\rm kcal/mol}$ (vs. $2.864\\ {\\rm kcal/mol}$), while requiring only\n$0.06\\ {\\rm s}$ GPU time per inference. Over 87% of generated TSs meet\nchemical-accuracy criteria ($<1.58\\ {\\rm kcal/mol}$ error), substantially\noutpacing existing methods. TS-GEN also exhibits strong transferability to\nout-of-distribution reactions from a larger database. By uniting sub-angstrom\nprecision, sub-second speed, and broad applicability, TS-GEN will be highly\nuseful for high-throughput exploration of complex reaction networks, paving the\nway to the exploration of novel chemical reaction mechanisms."
                },
                "authors": [
                    {
                        "name": "Ping Tuo"
                    },
                    {
                        "name": "Jiale Chen"
                    },
                    {
                        "name": "Ju Li"
                    }
                ],
                "author_detail": {
                    "name": "Ju Li"
                },
                "author": "Ju Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10527v1",
                "updated": "2025-07-14T17:50:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    50,
                    3,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T17:50:03Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    50,
                    3,
                    0,
                    195,
                    0
                ],
                "title": "Multi-epoch spectro-photometric characterization of the minimoon 2024\n  PT$_5$ in the visible and near-infrared",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-epoch spectro-photometric characterization of the minimoon 2024\n  PT$_5$ in the visible and near-infrared"
                },
                "summary": "2024 PT$_5$ is a tiny ($D\\leq10$ m) near-Earth asteroid (NEA) discovered in\nAugust 2024. 2024 PT$_5$ was gravitationally bound to the Earth-Moon system\nfrom September to November 2024 and classified as a minimoon. Several quick\nresponse observations suggest the lunar ejecta origin of 2024 PT$_5$, while\nrotation state and albedo, essential properties to investigate its origin, are\nnot well constrained. We performed visible to near-infrared multicolor\nphotometry of 2024 PT$_5$ from data taken using the TriColor CMOS Camera and\nSpectrograph (TriCCS) on the Seimei 3.8 m telescope during 2025 January 4-10.\nThe Seimei/TriCCS observations of 2024 PT$_5$ cover phase angles from 14 deg to\n27 deg, and were obtained in the $g$, $r$, $i$, and $z$ bands in the Pan-STARRS\nsystem. In addition, we analyzed $Y$, $J$, $H$, and $K$ photometry taken with\nthe Multi-Object Spectrograph for Infrared Exploration (MOSFIRE) on the Keck I\n10-m telescope taken on 2025 January 16-17. Our lightcurves show brightness\nvariations over time periods of several tens of minutes. We infer that 2024\nPT$_5$ is in a tumbling state and has a lightcurve amplitude of about 0.3 mag.\nVisible and near-infrared color indices of 2024 PT$_5$, $g-r=0.567\\pm0.044$,\n$r-i=0.155\\pm0.009$, $r-z=0.147\\pm0.066$, $Y-J=0.557\\pm0.046$,\n$J-H=0.672\\pm0.078$, and $H-Ks=0.148\\pm0.098$, indicate that 2024 PT$_5$ is an\nS-complex asteroid, largely consistent with previous observations. Using the\n$H$-$G$ model, we derived an absolute magnitude $H_{V,HG}$ of $27.72\\pm0.09$\nand a slope parameter $G_V$ of $0.223\\pm0.073$ in V-band. A geometric albedo of\n2024 PT$_5$ is derived to be $0.26\\pm0.07$ from the slope of its photometric\nphase curve. This albedo value is typical of the S- and Q-type NEAs. The color\nproperties of 2024 PT$_5$ derived from our observations match rock samples\ntaken from the lunar surface, which agrees with previous studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2024 PT$_5$ is a tiny ($D\\leq10$ m) near-Earth asteroid (NEA) discovered in\nAugust 2024. 2024 PT$_5$ was gravitationally bound to the Earth-Moon system\nfrom September to November 2024 and classified as a minimoon. Several quick\nresponse observations suggest the lunar ejecta origin of 2024 PT$_5$, while\nrotation state and albedo, essential properties to investigate its origin, are\nnot well constrained. We performed visible to near-infrared multicolor\nphotometry of 2024 PT$_5$ from data taken using the TriColor CMOS Camera and\nSpectrograph (TriCCS) on the Seimei 3.8 m telescope during 2025 January 4-10.\nThe Seimei/TriCCS observations of 2024 PT$_5$ cover phase angles from 14 deg to\n27 deg, and were obtained in the $g$, $r$, $i$, and $z$ bands in the Pan-STARRS\nsystem. In addition, we analyzed $Y$, $J$, $H$, and $K$ photometry taken with\nthe Multi-Object Spectrograph for Infrared Exploration (MOSFIRE) on the Keck I\n10-m telescope taken on 2025 January 16-17. Our lightcurves show brightness\nvariations over time periods of several tens of minutes. We infer that 2024\nPT$_5$ is in a tumbling state and has a lightcurve amplitude of about 0.3 mag.\nVisible and near-infrared color indices of 2024 PT$_5$, $g-r=0.567\\pm0.044$,\n$r-i=0.155\\pm0.009$, $r-z=0.147\\pm0.066$, $Y-J=0.557\\pm0.046$,\n$J-H=0.672\\pm0.078$, and $H-Ks=0.148\\pm0.098$, indicate that 2024 PT$_5$ is an\nS-complex asteroid, largely consistent with previous observations. Using the\n$H$-$G$ model, we derived an absolute magnitude $H_{V,HG}$ of $27.72\\pm0.09$\nand a slope parameter $G_V$ of $0.223\\pm0.073$ in V-band. A geometric albedo of\n2024 PT$_5$ is derived to be $0.26\\pm0.07$ from the slope of its photometric\nphase curve. This albedo value is typical of the S- and Q-type NEAs. The color\nproperties of 2024 PT$_5$ derived from our observations match rock samples\ntaken from the lunar surface, which agrees with previous studies."
                },
                "authors": [
                    {
                        "name": "Jin Beniyama"
                    },
                    {
                        "name": "Bryce T. Bolin"
                    },
                    {
                        "name": "Alexey V. Sergeyev"
                    },
                    {
                        "name": "Marco Delbo"
                    },
                    {
                        "name": "Laura-May Abron"
                    },
                    {
                        "name": "Matthew Belyakov"
                    },
                    {
                        "name": "Tomohiko Sekiguchi"
                    },
                    {
                        "name": "Seiko Takagi"
                    }
                ],
                "author_detail": {
                    "name": "Seiko Takagi"
                },
                "author": "Seiko Takagi",
                "arxiv_comment": "Accepted for publication in Astronomy & Astrophysics (A&A). Abstract\n  shortened for arXiv. Any comments are welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10522v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10522v1",
                "updated": "2025-07-14T17:47:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    47,
                    28,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T17:47:28Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    47,
                    28,
                    0,
                    195,
                    0
                ],
                "title": "DeepResearch$^{\\text{Eco}}$: A Recursive Agentic Workflow for Complex\n  Scientific Question Answering in Ecology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepResearch$^{\\text{Eco}}$: A Recursive Agentic Workflow for Complex\n  Scientific Question Answering in Ecology"
                },
                "summary": "We introduce DeepResearch$^{\\text{Eco}}$, a novel agentic LLM-based system\nfor automated scientific synthesis that supports recursive, depth- and\nbreadth-controlled exploration of original research questions -- enhancing\nsearch diversity and nuance in the retrieval of relevant scientific literature.\nUnlike conventional retrieval-augmented generation pipelines, DeepResearch\nenables user-controllable synthesis with transparent reasoning and\nparameter-driven configurability, facilitating high-throughput integration of\ndomain-specific evidence while maintaining analytical rigor. Applied to 49\necological research questions, DeepResearch achieves up to a 21-fold increase\nin source integration and a 14.9-fold rise in sources integrated per 1,000\nwords. High-parameter settings yield expert-level analytical depth and\ncontextual diversity.\n  Source code available at: https://github.com/sciknoworg/deep-research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce DeepResearch$^{\\text{Eco}}$, a novel agentic LLM-based system\nfor automated scientific synthesis that supports recursive, depth- and\nbreadth-controlled exploration of original research questions -- enhancing\nsearch diversity and nuance in the retrieval of relevant scientific literature.\nUnlike conventional retrieval-augmented generation pipelines, DeepResearch\nenables user-controllable synthesis with transparent reasoning and\nparameter-driven configurability, facilitating high-throughput integration of\ndomain-specific evidence while maintaining analytical rigor. Applied to 49\necological research questions, DeepResearch achieves up to a 21-fold increase\nin source integration and a 14.9-fold rise in sources integrated per 1,000\nwords. High-parameter settings yield expert-level analytical depth and\ncontextual diversity.\n  Source code available at: https://github.com/sciknoworg/deep-research."
                },
                "authors": [
                    {
                        "name": "Jennifer D'Souza"
                    },
                    {
                        "name": "Endres Keno Sander"
                    },
                    {
                        "name": "Andrei Aioanei"
                    }
                ],
                "author_detail": {
                    "name": "Andrei Aioanei"
                },
                "author": "Andrei Aioanei",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10522v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10522v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10517v1",
                "updated": "2025-07-14T17:42:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    42,
                    57,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T17:42:57Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    42,
                    57,
                    0,
                    195,
                    0
                ],
                "title": "Closure of superstatistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Closure of superstatistics"
                },
                "summary": "Plasmas and other systems with long-range interactions are commonly found in\nnon-equilibrium steady states that are outside traditional Boltzmann-Gibbs\nstatistics, but can be described using generalized statistical mechanics\nframeworks such as superstatistics, where steady states are treated as\nsuperpositions of canonical ensembles under a temperature distribution. In this\nwork we solve the problem of inferring the possible steady states of a\ncomposite system $AB$ where subsystem $A$ is described by superstatistics and\n$E_{AB} = E_A + E_B$. Our result establishes a closure property of\nsuperstatistics, namely that $A$ is described by superstatistics if and only if\n$AB$ and $B$ are also superstatistical with the same temperature distribution.\nSome consequences of this result are discussed, such as the impossibility of\nlocal thermal equilibrium (LTE) for additive subsystems in non-canonical steady\nstates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plasmas and other systems with long-range interactions are commonly found in\nnon-equilibrium steady states that are outside traditional Boltzmann-Gibbs\nstatistics, but can be described using generalized statistical mechanics\nframeworks such as superstatistics, where steady states are treated as\nsuperpositions of canonical ensembles under a temperature distribution. In this\nwork we solve the problem of inferring the possible steady states of a\ncomposite system $AB$ where subsystem $A$ is described by superstatistics and\n$E_{AB} = E_A + E_B$. Our result establishes a closure property of\nsuperstatistics, namely that $A$ is described by superstatistics if and only if\n$AB$ and $B$ are also superstatistical with the same temperature distribution.\nSome consequences of this result are discussed, such as the impossibility of\nlocal thermal equilibrium (LTE) for additive subsystems in non-canonical steady\nstates."
                },
                "authors": [
                    {
                        "name": "Sergio Davis"
                    }
                ],
                "author_detail": {
                    "name": "Sergio Davis"
                },
                "author": "Sergio Davis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10511v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10511v2",
                "updated": "2025-07-15T08:59:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    8,
                    59,
                    45,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-14T17:37:56Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    37,
                    56,
                    0,
                    195,
                    0
                ],
                "title": "Constructing Confidence Intervals for Infinite-Dimensional Functional\n  Prameters by Highly Adaptive Lasso",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing Confidence Intervals for Infinite-Dimensional Functional\n  Prameters by Highly Adaptive Lasso"
                },
                "summary": "Estimating the conditional mean function is a central task in statistical\nlearning. In this paper, we consider estimation and inference for a\nnonparametric class of real-valued c\\`adl\\`ag functions with bounded sectional\nvariation (Gill et al., 1995), using the Highly Adaptive Lasso (HAL) (van der\nLaan, 2015; Benkeser and van der Laan, 2016; van der Laan, 2023), a flexible\nempirical risk minimizer over linear combinations of tensor products of zero-\nor higher-order spline basis functions under an L1 norm constraint. Building on\nrecent theoretical advances in asymptotic normality and uniform convergence\nrates for higher-order spline HAL estimators (van der Laan, 2023), this work\nfocuses on constructing robust confidence intervals for HAL-based conditional\nmean estimators. To address regularization bias, we propose a targeted HAL with\na debiasing step to remove bias for the conditional mean, and also consider a\nrelaxed HAL estimator to reduce bias. We also introduce both global and local\nundersmoothing strategies to adaptively select the working model, reducing bias\nrelative to variance. Combined with delta-method-based variance estimation, we\nconstruct confidence intervals for conditional means based on HAL. Through\nsimulations, we evaluate combinations of estimation and model selection\nstrategies, showing that our methods substantially reduce bias and yield\nconfidence intervals with coverage rates close to nominal levels across\nscenarios. We also provide recommendations for different estimation objectives\nand illustrate the generality of our framework by applying it to estimate\nconditional average treatment effect (CATE) functions, highlighting how\nHAL-based inference extends to other infinite-dimensional, non-pathwise\ndifferentiable parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating the conditional mean function is a central task in statistical\nlearning. In this paper, we consider estimation and inference for a\nnonparametric class of real-valued c\\`adl\\`ag functions with bounded sectional\nvariation (Gill et al., 1995), using the Highly Adaptive Lasso (HAL) (van der\nLaan, 2015; Benkeser and van der Laan, 2016; van der Laan, 2023), a flexible\nempirical risk minimizer over linear combinations of tensor products of zero-\nor higher-order spline basis functions under an L1 norm constraint. Building on\nrecent theoretical advances in asymptotic normality and uniform convergence\nrates for higher-order spline HAL estimators (van der Laan, 2023), this work\nfocuses on constructing robust confidence intervals for HAL-based conditional\nmean estimators. To address regularization bias, we propose a targeted HAL with\na debiasing step to remove bias for the conditional mean, and also consider a\nrelaxed HAL estimator to reduce bias. We also introduce both global and local\nundersmoothing strategies to adaptively select the working model, reducing bias\nrelative to variance. Combined with delta-method-based variance estimation, we\nconstruct confidence intervals for conditional means based on HAL. Through\nsimulations, we evaluate combinations of estimation and model selection\nstrategies, showing that our methods substantially reduce bias and yield\nconfidence intervals with coverage rates close to nominal levels across\nscenarios. We also provide recommendations for different estimation objectives\nand illustrate the generality of our framework by applying it to estimate\nconditional average treatment effect (CATE) functions, highlighting how\nHAL-based inference extends to other infinite-dimensional, non-pathwise\ndifferentiable parameters."
                },
                "authors": [
                    {
                        "name": "Wenxin Zhang"
                    },
                    {
                        "name": "Junming Shi"
                    },
                    {
                        "name": "Alan Hubbard"
                    },
                    {
                        "name": "Mark van der Laan"
                    }
                ],
                "author_detail": {
                    "name": "Mark van der Laan"
                },
                "author": "Mark van der Laan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10511v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10511v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10510v1",
                "updated": "2025-07-14T17:34:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    34,
                    49,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T17:34:49Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    34,
                    49,
                    0,
                    195,
                    0
                ],
                "title": "Chat with AI: The Surprising Turn of Real-time Video Communication from\n  Human to AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chat with AI: The Surprising Turn of Real-time Video Communication from\n  Human to AI"
                },
                "summary": "AI Video Chat emerges as a new paradigm for Real-time Communication (RTC),\nwhere one peer is not a human, but a Multimodal Large Language Model (MLLM).\nThis makes interaction between humans and AI more intuitive, as if chatting\nface-to-face with a real person. However, this poses significant challenges to\nlatency, because the MLLM inference takes up most of the response time, leaving\nvery little time for video streaming. Due to network uncertainty and\ninstability, transmission latency becomes a critical bottleneck preventing AI\nfrom being like a real person. To address this, we propose Artic, an\nAI-oriented Real-time Communication framework, exploring the network\nrequirement shift from \"humans watching video\" to \"AI understanding video\". To\nreduce bitrate dramatically while maintaining MLLM accuracy, we propose\nContext-Aware Video Streaming that recognizes the importance of each video\nregion for chat and allocates bitrate almost exclusively to chat-important\nregions. To avoid packet retransmission, we propose Loss-Resilient Adaptive\nFrame Rate that leverages previous frames to substitute for lost/delayed frames\nwhile avoiding bitrate waste. To evaluate the impact of video streaming quality\non MLLM accuracy, we build the first benchmark, named Degraded Video\nUnderstanding Benchmark (DeViBench). Finally, we discuss some open questions\nand ongoing solutions for AI Video Chat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Video Chat emerges as a new paradigm for Real-time Communication (RTC),\nwhere one peer is not a human, but a Multimodal Large Language Model (MLLM).\nThis makes interaction between humans and AI more intuitive, as if chatting\nface-to-face with a real person. However, this poses significant challenges to\nlatency, because the MLLM inference takes up most of the response time, leaving\nvery little time for video streaming. Due to network uncertainty and\ninstability, transmission latency becomes a critical bottleneck preventing AI\nfrom being like a real person. To address this, we propose Artic, an\nAI-oriented Real-time Communication framework, exploring the network\nrequirement shift from \"humans watching video\" to \"AI understanding video\". To\nreduce bitrate dramatically while maintaining MLLM accuracy, we propose\nContext-Aware Video Streaming that recognizes the importance of each video\nregion for chat and allocates bitrate almost exclusively to chat-important\nregions. To avoid packet retransmission, we propose Loss-Resilient Adaptive\nFrame Rate that leverages previous frames to substitute for lost/delayed frames\nwhile avoiding bitrate waste. To evaluate the impact of video streaming quality\non MLLM accuracy, we build the first benchmark, named Degraded Video\nUnderstanding Benchmark (DeViBench). Finally, we discuss some open questions\nand ongoing solutions for AI Video Chat."
                },
                "authors": [
                    {
                        "name": "Jiangkai Wu"
                    },
                    {
                        "name": "Zhiyuan Ren"
                    },
                    {
                        "name": "Liming Liu"
                    },
                    {
                        "name": "Xinggong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggong Zhang"
                },
                "author": "Xinggong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10494v1",
                "updated": "2025-07-14T17:18:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    18,
                    7,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T17:18:07Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    18,
                    7,
                    0,
                    195,
                    0
                ],
                "title": "Split Happens: Combating Advanced Threats with Split Learning and\n  Function Secret Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split Happens: Combating Advanced Threats with Split Learning and\n  Function Secret Sharing"
                },
                "summary": "Split Learning (SL) -- splits a model into two distinct parts to help protect\nclient data while enhancing Machine Learning (ML) processes. Though promising,\nSL has proven vulnerable to different attacks, thus raising concerns about how\neffective it may be in terms of data privacy. Recent works have shown promising\nresults for securing SL through the use of a novel paradigm, named Function\nSecret Sharing (FSS), in which servers obtain shares of a function they compute\nand operate on a public input hidden with a random mask. However, these works\nfall short in addressing the rising number of attacks which exist on SL. In\nSplitHappens, we expand the combination of FSS and SL to U-shaped SL. Similarly\nto other works, we are able to make use of the benefits of SL by reducing the\ncommunication and computational costs of FSS. However, a U-shaped SL provides a\nhigher security guarantee than previous works, allowing a client to keep the\nlabels of the training data secret, without having to share them with the\nserver. Through this, we are able to generalize the security analysis of\nprevious works and expand it to different attack vectors, such as modern model\ninversion attacks as well as label inference attacks. We tested our approach\nfor two different convolutional neural networks on different datasets. These\nexperiments show the effectiveness of our approach in reducing the training\ntime as well as the communication costs when compared to simply using FSS while\nmatching prior accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split Learning (SL) -- splits a model into two distinct parts to help protect\nclient data while enhancing Machine Learning (ML) processes. Though promising,\nSL has proven vulnerable to different attacks, thus raising concerns about how\neffective it may be in terms of data privacy. Recent works have shown promising\nresults for securing SL through the use of a novel paradigm, named Function\nSecret Sharing (FSS), in which servers obtain shares of a function they compute\nand operate on a public input hidden with a random mask. However, these works\nfall short in addressing the rising number of attacks which exist on SL. In\nSplitHappens, we expand the combination of FSS and SL to U-shaped SL. Similarly\nto other works, we are able to make use of the benefits of SL by reducing the\ncommunication and computational costs of FSS. However, a U-shaped SL provides a\nhigher security guarantee than previous works, allowing a client to keep the\nlabels of the training data secret, without having to share them with the\nserver. Through this, we are able to generalize the security analysis of\nprevious works and expand it to different attack vectors, such as modern model\ninversion attacks as well as label inference attacks. We tested our approach\nfor two different convolutional neural networks on different datasets. These\nexperiments show the effectiveness of our approach in reducing the training\ntime as well as the communication costs when compared to simply using FSS while\nmatching prior accuracy."
                },
                "authors": [
                    {
                        "name": "Tanveer Khan"
                    },
                    {
                        "name": "Mindaugas Budzys"
                    },
                    {
                        "name": "Antonis Michalas"
                    }
                ],
                "author_detail": {
                    "name": "Antonis Michalas"
                },
                "author": "Antonis Michalas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10485v1",
                "updated": "2025-07-14T17:04:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    4,
                    5,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T17:04:05Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    4,
                    5,
                    0,
                    195,
                    0
                ],
                "title": "Overcoming catastrophic forgetting in neural networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overcoming catastrophic forgetting in neural networks"
                },
                "summary": "Catastrophic forgetting is the primary challenge that hinders continual\nlearning, which refers to a neural network ability to sequentially learn\nmultiple tasks while retaining previously acquired knowledge. Elastic Weight\nConsolidation, a regularization-based approach inspired by synaptic\nconsolidation in biological neural systems, has been used to overcome this\nproblem. In this study prior research is replicated and extended by evaluating\nEWC in supervised learning settings using the PermutedMNIST and RotatedMNIST\nbenchmarks. Through systematic comparisons with L2 regularization and\nstochastic gradient descent (SGD) without regularization, we analyze how\ndifferent approaches balance knowledge retention and adaptability. Our results\nconfirm what was shown in previous research, showing that EWC significantly\nreduces forgetting compared to naive training while slightly compromising\nlearning efficiency on new tasks. Moreover, we investigate the impact of\ndropout regularization and varying hyperparameters, offering insights into the\ngeneralization of EWC across diverse learning scenarios. These results\nunderscore EWC's potential as a viable solution for lifelong learning in neural\nnetworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Catastrophic forgetting is the primary challenge that hinders continual\nlearning, which refers to a neural network ability to sequentially learn\nmultiple tasks while retaining previously acquired knowledge. Elastic Weight\nConsolidation, a regularization-based approach inspired by synaptic\nconsolidation in biological neural systems, has been used to overcome this\nproblem. In this study prior research is replicated and extended by evaluating\nEWC in supervised learning settings using the PermutedMNIST and RotatedMNIST\nbenchmarks. Through systematic comparisons with L2 regularization and\nstochastic gradient descent (SGD) without regularization, we analyze how\ndifferent approaches balance knowledge retention and adaptability. Our results\nconfirm what was shown in previous research, showing that EWC significantly\nreduces forgetting compared to naive training while slightly compromising\nlearning efficiency on new tasks. Moreover, we investigate the impact of\ndropout regularization and varying hyperparameters, offering insights into the\ngeneralization of EWC across diverse learning scenarios. These results\nunderscore EWC's potential as a viable solution for lifelong learning in neural\nnetworks."
                },
                "authors": [
                    {
                        "name": "Brandon Shuen Yi Loke"
                    },
                    {
                        "name": "Filippo Quadri"
                    },
                    {
                        "name": "Gabriel Vivanco"
                    },
                    {
                        "name": "Maximilian Casagrande"
                    },
                    {
                        "name": "Sa√∫l Fenollosa"
                    }
                ],
                "author_detail": {
                    "name": "Sa√∫l Fenollosa"
                },
                "author": "Sa√∫l Fenollosa",
                "arxiv_comment": "7 pages, 5 figures, EE-411 Fundamentals of inference and learning\n  course project",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10475v1",
                "updated": "2025-07-14T16:55:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    55,
                    57,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T16:55:57Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    55,
                    57,
                    0,
                    195,
                    0
                ],
                "title": "Can You Detect the Difference?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can You Detect the Difference?"
                },
                "summary": "The rapid advancement of large language models (LLMs) has raised concerns\nabout reliably detecting AI-generated text. Stylometric metrics work well on\nautoregressive (AR) outputs, but their effectiveness on diffusion-based models\nis unknown. We present the first systematic comparison of diffusion-generated\ntext (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity,\nburstiness, lexical diversity, readability, and BLEU/ROUGE scores show that\nLLaDA closely mimics human text in perplexity and burstiness, yielding high\nfalse-negative rates for AR-oriented detectors. LLaMA shows much lower\nperplexity but reduced lexical fidelity. Relying on any single metric fails to\nseparate diffusion outputs from human writing. We highlight the need for\ndiffusion-aware detectors and outline directions such as hybrid models,\ndiffusion-specific stylometric signatures, and robust watermarking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has raised concerns\nabout reliably detecting AI-generated text. Stylometric metrics work well on\nautoregressive (AR) outputs, but their effectiveness on diffusion-based models\nis unknown. We present the first systematic comparison of diffusion-generated\ntext (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity,\nburstiness, lexical diversity, readability, and BLEU/ROUGE scores show that\nLLaDA closely mimics human text in perplexity and burstiness, yielding high\nfalse-negative rates for AR-oriented detectors. LLaMA shows much lower\nperplexity but reduced lexical fidelity. Relying on any single metric fails to\nseparate diffusion outputs from human writing. We highlight the need for\ndiffusion-aware detectors and outline directions such as hybrid models,\ndiffusion-specific stylometric signatures, and robust watermarking."
                },
                "authors": [
                    {
                        "name": "ƒ∞smail Tarƒ±m"
                    },
                    {
                        "name": "Aytuƒü Onan"
                    }
                ],
                "author_detail": {
                    "name": "Aytuƒü Onan"
                },
                "author": "Aytuƒü Onan",
                "arxiv_comment": "11 pages, 3 figures, 2 tables. Code and data:\n  https://github.com/ismailtrm/ceng_404. Cross-list requested to cs.AI for\n  AI-safety relevance",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10473v1",
                "updated": "2025-07-14T16:54:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    54,
                    57,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T16:54:57Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    54,
                    57,
                    0,
                    195,
                    0
                ],
                "title": "GT-Loc: Unifying When and Where in Images Through a Joint Embedding\n  Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GT-Loc: Unifying When and Where in Images Through a Joint Embedding\n  Space"
                },
                "summary": "Timestamp prediction aims to determine when an image was captured using only\nvisual information, supporting applications such as metadata correction,\nretrieval, and digital forensics. In outdoor scenarios, hourly estimates rely\non cues like brightness, hue, and shadow positioning, while seasonal changes\nand weather inform date estimation. However, these visual cues significantly\ndepend on geographic context, closely linking timestamp prediction to\ngeo-localization. To address this interdependence, we introduce GT-Loc, a novel\nretrieval-based method that jointly predicts the capture time (hour and month)\nand geo-location (GPS coordinates) of an image. Our approach employs separate\nencoders for images, time, and location, aligning their embeddings within a\nshared high-dimensional feature space. Recognizing the cyclical nature of time,\ninstead of conventional contrastive learning with hard positives and negatives,\nwe propose a temporal metric-learning objective providing soft targets by\nmodeling pairwise time differences over a cyclical toroidal surface. We present\nnew benchmarks demonstrating that our joint optimization surpasses previous\ntime prediction methods, even those using the ground-truth geo-location as an\ninput during inference. Additionally, our approach achieves competitive results\non standard geo-localization tasks, and the unified embedding space facilitates\ncompositional and text-based image retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timestamp prediction aims to determine when an image was captured using only\nvisual information, supporting applications such as metadata correction,\nretrieval, and digital forensics. In outdoor scenarios, hourly estimates rely\non cues like brightness, hue, and shadow positioning, while seasonal changes\nand weather inform date estimation. However, these visual cues significantly\ndepend on geographic context, closely linking timestamp prediction to\ngeo-localization. To address this interdependence, we introduce GT-Loc, a novel\nretrieval-based method that jointly predicts the capture time (hour and month)\nand geo-location (GPS coordinates) of an image. Our approach employs separate\nencoders for images, time, and location, aligning their embeddings within a\nshared high-dimensional feature space. Recognizing the cyclical nature of time,\ninstead of conventional contrastive learning with hard positives and negatives,\nwe propose a temporal metric-learning objective providing soft targets by\nmodeling pairwise time differences over a cyclical toroidal surface. We present\nnew benchmarks demonstrating that our joint optimization surpasses previous\ntime prediction methods, even those using the ground-truth geo-location as an\ninput during inference. Additionally, our approach achieves competitive results\non standard geo-localization tasks, and the unified embedding space facilitates\ncompositional and text-based image retrieval."
                },
                "authors": [
                    {
                        "name": "David G. Shatwell"
                    },
                    {
                        "name": "Ishan Rajendrakumar Dave"
                    },
                    {
                        "name": "Sirnam Swetha"
                    },
                    {
                        "name": "Mubarak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Mubarak Shah"
                },
                "author": "Mubarak Shah",
                "arxiv_comment": "Accepted in ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10472v1",
                "updated": "2025-07-14T16:53:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    53,
                    19,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T16:53:19Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    53,
                    19,
                    0,
                    195,
                    0
                ],
                "title": "MLAR: Multi-layer Large Language Model-based Robotic Process Automation\n  Applicant Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLAR: Multi-layer Large Language Model-based Robotic Process Automation\n  Applicant Tracking"
                },
                "summary": "This paper introduces an innovative Applicant Tracking System (ATS) enhanced\nby a novel Robotic process automation (RPA) framework or as further referred to\nas MLAR. Traditional recruitment processes often encounter bottlenecks in\nresume screening and candidate shortlisting due to time and resource\nconstraints. MLAR addresses these challenges employing Large Language Models\n(LLMs) in three distinct layers: extracting key characteristics from job\npostings in the first layer, parsing applicant resume to identify education,\nexperience, skills in the second layer, and similarity matching in the third\nlayer. These features are then matched through advanced semantic algorithms to\nidentify the best candidates efficiently. Our approach integrates seamlessly\ninto existing RPA pipelines, automating resume parsing, job matching, and\ncandidate notifications. Extensive performance benchmarking shows that MLAR\noutperforms the leading RPA platforms, including UiPath and Automation\nAnywhere, in high-volume resume-processing tasks. When processing 2,400\nresumes, MLAR achieved an average processing time of 5.4 seconds per resume,\nreducing processing time by approximately 16.9% compared to Automation Anywhere\nand 17.1% compared to UiPath. These results highlight the potential of MLAR to\ntransform recruitment workflows by providing an efficient, accurate, and\nscalable solution tailored to modern hiring needs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an innovative Applicant Tracking System (ATS) enhanced\nby a novel Robotic process automation (RPA) framework or as further referred to\nas MLAR. Traditional recruitment processes often encounter bottlenecks in\nresume screening and candidate shortlisting due to time and resource\nconstraints. MLAR addresses these challenges employing Large Language Models\n(LLMs) in three distinct layers: extracting key characteristics from job\npostings in the first layer, parsing applicant resume to identify education,\nexperience, skills in the second layer, and similarity matching in the third\nlayer. These features are then matched through advanced semantic algorithms to\nidentify the best candidates efficiently. Our approach integrates seamlessly\ninto existing RPA pipelines, automating resume parsing, job matching, and\ncandidate notifications. Extensive performance benchmarking shows that MLAR\noutperforms the leading RPA platforms, including UiPath and Automation\nAnywhere, in high-volume resume-processing tasks. When processing 2,400\nresumes, MLAR achieved an average processing time of 5.4 seconds per resume,\nreducing processing time by approximately 16.9% compared to Automation Anywhere\nand 17.1% compared to UiPath. These results highlight the potential of MLAR to\ntransform recruitment workflows by providing an efficient, accurate, and\nscalable solution tailored to modern hiring needs."
                },
                "authors": [
                    {
                        "name": "Mohamed T. Younes"
                    },
                    {
                        "name": "Omar Walid"
                    },
                    {
                        "name": "Mai Hassan"
                    },
                    {
                        "name": "Ali Hamdi"
                    }
                ],
                "author_detail": {
                    "name": "Ali Hamdi"
                },
                "author": "Ali Hamdi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10468v1",
                "updated": "2025-07-14T16:46:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    46,
                    30,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T16:46:30Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    46,
                    30,
                    0,
                    195,
                    0
                ],
                "title": "From BERT to Qwen: Hate Detection across architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From BERT to Qwen: Hate Detection across architectures"
                },
                "summary": "Online platforms struggle to curb hate speech without over-censoring\nlegitimate discourse. Early bidirectional transformer encoders made big\nstrides, but the arrival of ultra-large autoregressive LLMs promises deeper\ncontext-awareness. Whether this extra scale actually improves practical\nhate-speech detection on real-world text remains unverified. Our study puts\nthis question to the test by benchmarking both model families, classic encoders\nand next-generation LLMs, on curated corpora of online interactions for\nhate-speech detection (Hate or No Hate).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online platforms struggle to curb hate speech without over-censoring\nlegitimate discourse. Early bidirectional transformer encoders made big\nstrides, but the arrival of ultra-large autoregressive LLMs promises deeper\ncontext-awareness. Whether this extra scale actually improves practical\nhate-speech detection on real-world text remains unverified. Our study puts\nthis question to the test by benchmarking both model families, classic encoders\nand next-generation LLMs, on curated corpora of online interactions for\nhate-speech detection (Hate or No Hate)."
                },
                "authors": [
                    {
                        "name": "Ariadna Mon"
                    },
                    {
                        "name": "Sa√∫l Fenollosa"
                    },
                    {
                        "name": "Jon Lecumberri"
                    }
                ],
                "author_detail": {
                    "name": "Jon Lecumberri"
                },
                "author": "Jon Lecumberri",
                "arxiv_comment": "4 pages, 5 figures. EE-559 Deep Learning course project (Group 11)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10465v1",
                "updated": "2025-07-14T16:45:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    45,
                    0,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T16:45:00Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    45,
                    0,
                    0,
                    195,
                    0
                ],
                "title": "Flexible Modeling of Multivariate Skewed and Heavy-Tailed Data via a\n  Non-Central Skew t Distribution: Application to Tumor Shape Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexible Modeling of Multivariate Skewed and Heavy-Tailed Data via a\n  Non-Central Skew t Distribution: Application to Tumor Shape Data"
                },
                "summary": "We propose a flexible formulation of the multivariate non-central skew t\n(NCST) distribution, defined by scaling skew-normal random vectors with\nindependent chi-squared variables. This construction extends the classical\nmultivariate t family by allowing both asymmetry and non-centrality, which\nprovides an alternative to existing skew t models that often rely on\nrestrictive assumptions for tractability. We derive key theoretical properties\nof the NCST distribution, which includes its moment structure, affine\ntransformation behavior, and the distribution of quadratic forms. Due to the\nlack of a closed-form density, we implement a Monte Carlo likelihood\napproximation to enable maximum likelihood estimation and evaluate its\nperformance through simulation studies. To demonstrate practical utility, we\napply the NCST model to breast cancer diagnostic data, modeling multiple\nfeatures of tumor shape. The NCST model achieves a superior fit based on\ninformation criteria and visual diagnostics, particularly in the presence of\nskewness and heavy tails compared to standard alternatives, including the\nmultivariate normal, skew normal, and Azzalini's skew $t$ distribution. Our\nfindings suggest that the NCST distribution offers a useful and interpretable\nchoice for modeling complex multivariate data, which highlights promising\ndirections for future development in likelihood inference, Bayesian\ncomputation, and applications involving asymmetry and non-Gaussian dependence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a flexible formulation of the multivariate non-central skew t\n(NCST) distribution, defined by scaling skew-normal random vectors with\nindependent chi-squared variables. This construction extends the classical\nmultivariate t family by allowing both asymmetry and non-centrality, which\nprovides an alternative to existing skew t models that often rely on\nrestrictive assumptions for tractability. We derive key theoretical properties\nof the NCST distribution, which includes its moment structure, affine\ntransformation behavior, and the distribution of quadratic forms. Due to the\nlack of a closed-form density, we implement a Monte Carlo likelihood\napproximation to enable maximum likelihood estimation and evaluate its\nperformance through simulation studies. To demonstrate practical utility, we\napply the NCST model to breast cancer diagnostic data, modeling multiple\nfeatures of tumor shape. The NCST model achieves a superior fit based on\ninformation criteria and visual diagnostics, particularly in the presence of\nskewness and heavy tails compared to standard alternatives, including the\nmultivariate normal, skew normal, and Azzalini's skew $t$ distribution. Our\nfindings suggest that the NCST distribution offers a useful and interpretable\nchoice for modeling complex multivariate data, which highlights promising\ndirections for future development in likelihood inference, Bayesian\ncomputation, and applications involving asymmetry and non-Gaussian dependence."
                },
                "authors": [
                    {
                        "name": "Abeer M. Hasan"
                    },
                    {
                        "name": "Ying-Ju Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ying-Ju Chen"
                },
                "author": "Ying-Ju Chen",
                "arxiv_comment": "22 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10454v2",
                "updated": "2025-07-14T16:40:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    40,
                    57,
                    0,
                    195,
                    0
                ],
                "published": "2024-12-12T07:25:37Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    25,
                    37,
                    3,
                    347,
                    0
                ],
                "title": "An Interoperable Machine Learning Pipeline for Pediatric Obesity Risk\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Interoperable Machine Learning Pipeline for Pediatric Obesity Risk\n  Estimation"
                },
                "summary": "Reliable prediction of pediatric obesity can offer a valuable resource to\nproviders, helping them engage in timely preventive interventions before the\ndisease is established. Many efforts have been made to develop ML-based\npredictive models of obesity, and some studies have reported high predictive\nperformances. However, no commonly used clinical decision support tool based on\nexisting ML models currently exists. This study presents a novel end-to-end\npipeline specifically designed for pediatric obesity prediction, which supports\nthe entire process of data extraction, inference, and communication via an API\nor a user interface. While focusing only on routinely recorded data in\npediatric electronic health records (EHRs), our pipeline uses a diverse\nexpert-curated list of medical concepts to predict the 1-3 years risk of\ndeveloping obesity. Furthermore, by using the Fast Healthcare Interoperability\nResources (FHIR) standard in our design procedure, we specifically target\nfacilitating low-effort integration of our pipeline with different EHR systems.\nIn our experiments, we report the effectiveness of the predictive model as well\nas its alignment with the feedback from various stakeholders, including ML\nscientists, providers, health IT personnel, health administration\nrepresentatives, and patient group representatives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable prediction of pediatric obesity can offer a valuable resource to\nproviders, helping them engage in timely preventive interventions before the\ndisease is established. Many efforts have been made to develop ML-based\npredictive models of obesity, and some studies have reported high predictive\nperformances. However, no commonly used clinical decision support tool based on\nexisting ML models currently exists. This study presents a novel end-to-end\npipeline specifically designed for pediatric obesity prediction, which supports\nthe entire process of data extraction, inference, and communication via an API\nor a user interface. While focusing only on routinely recorded data in\npediatric electronic health records (EHRs), our pipeline uses a diverse\nexpert-curated list of medical concepts to predict the 1-3 years risk of\ndeveloping obesity. Furthermore, by using the Fast Healthcare Interoperability\nResources (FHIR) standard in our design procedure, we specifically target\nfacilitating low-effort integration of our pipeline with different EHR systems.\nIn our experiments, we report the effectiveness of the predictive model as well\nas its alignment with the feedback from various stakeholders, including ML\nscientists, providers, health IT personnel, health administration\nrepresentatives, and patient group representatives."
                },
                "authors": [
                    {
                        "name": "Hamed Fayyaz"
                    },
                    {
                        "name": "Mehak Gupta"
                    },
                    {
                        "name": "Alejandra Perez Ramirez"
                    },
                    {
                        "name": "Claudine Jurkovitz"
                    },
                    {
                        "name": "H. Timothy Bunnell"
                    },
                    {
                        "name": "Thao-Ly T. Phan"
                    },
                    {
                        "name": "Rahmatollah Beheshti"
                    }
                ],
                "author_detail": {
                    "name": "Rahmatollah Beheshti"
                },
                "author": "Rahmatollah Beheshti",
                "arxiv_comment": "This paper has been accepted in Machine Learning for Health (ML4H)\n  Symposium. Link: https://proceedings.mlr.press/v259/fayyaz25a.html",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10463v1",
                "updated": "2025-07-14T16:40:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    40,
                    37,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T16:40:37Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    40,
                    37,
                    0,
                    195,
                    0
                ],
                "title": "Solving the compute crisis with physics-based ASICs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving the compute crisis with physics-based ASICs"
                },
                "summary": "Escalating artificial intelligence (AI) demands expose a critical \"compute\ncrisis\" characterized by unsustainable energy consumption, prohibitive training\ncosts, and the approaching limits of conventional CMOS scaling. Physics-based\nApplication-Specific Integrated Circuits (ASICs) present a transformative\nparadigm by directly harnessing intrinsic physical dynamics for computation\nrather than expending resources to enforce idealized digital abstractions. By\nrelaxing the constraints needed for traditional ASICs, like enforced\nstatelessness, unidirectionality, determinism, and synchronization, these\ndevices aim to operate as exact realizations of physical processes, offering\nsubstantial gains in energy efficiency and computational throughput. This\napproach enables novel co-design strategies, aligning algorithmic requirements\nwith the inherent computational primitives of physical systems. Physics-based\nASICs could accelerate critical AI applications like diffusion models,\nsampling, optimization, and neural network inference as well as traditional\ncomputational workloads like scientific simulation of materials and molecules.\nUltimately, this vision points towards a future of heterogeneous,\nhighly-specialized computing platforms capable of overcoming current scaling\nbottlenecks and unlocking new frontiers in computational power and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Escalating artificial intelligence (AI) demands expose a critical \"compute\ncrisis\" characterized by unsustainable energy consumption, prohibitive training\ncosts, and the approaching limits of conventional CMOS scaling. Physics-based\nApplication-Specific Integrated Circuits (ASICs) present a transformative\nparadigm by directly harnessing intrinsic physical dynamics for computation\nrather than expending resources to enforce idealized digital abstractions. By\nrelaxing the constraints needed for traditional ASICs, like enforced\nstatelessness, unidirectionality, determinism, and synchronization, these\ndevices aim to operate as exact realizations of physical processes, offering\nsubstantial gains in energy efficiency and computational throughput. This\napproach enables novel co-design strategies, aligning algorithmic requirements\nwith the inherent computational primitives of physical systems. Physics-based\nASICs could accelerate critical AI applications like diffusion models,\nsampling, optimization, and neural network inference as well as traditional\ncomputational workloads like scientific simulation of materials and molecules.\nUltimately, this vision points towards a future of heterogeneous,\nhighly-specialized computing platforms capable of overcoming current scaling\nbottlenecks and unlocking new frontiers in computational power and efficiency."
                },
                "authors": [
                    {
                        "name": "Maxwell Aifer"
                    },
                    {
                        "name": "Zach Belateche"
                    },
                    {
                        "name": "Suraj Bramhavar"
                    },
                    {
                        "name": "Kerem Y. Camsari"
                    },
                    {
                        "name": "Patrick J. Coles"
                    },
                    {
                        "name": "Gavin Crooks"
                    },
                    {
                        "name": "Douglas J. Durian"
                    },
                    {
                        "name": "Andrea J. Liu"
                    },
                    {
                        "name": "Anastasia Marchenkova"
                    },
                    {
                        "name": "Antonio J. Martinez"
                    },
                    {
                        "name": "Peter L. McMahon"
                    },
                    {
                        "name": "Faris Sbahi"
                    },
                    {
                        "name": "Benjamin Weiner"
                    },
                    {
                        "name": "Logan G. Wright"
                    }
                ],
                "author_detail": {
                    "name": "Logan G. Wright"
                },
                "author": "Logan G. Wright",
                "arxiv_comment": "16 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10457v1",
                "updated": "2025-07-14T16:37:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    37,
                    5,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T16:37:05Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    37,
                    5,
                    0,
                    195,
                    0
                ],
                "title": "Logic layer Prompt Control Injection (LPCI): A Novel Security\n  Vulnerability Class in Agentic Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logic layer Prompt Control Injection (LPCI): A Novel Security\n  Vulnerability Class in Agentic Systems"
                },
                "summary": "The integration of large language models (LLMs) into enterprise systems has\ncreated a new class of covert security vulnerabilities, particularly within\nlogic-execution layers and persistent-memory contexts. In this paper, we\nintroduce Logic-Layer Prompt Control Injection (LPCI), a novel attack category\nin which encoded, delayed, and conditionally triggered payloads are embedded in\nmemory, vector stores, or tool outputs. These payloads can bypass conventional\ninput filters and trigger unauthorised behaviour across sessions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language models (LLMs) into enterprise systems has\ncreated a new class of covert security vulnerabilities, particularly within\nlogic-execution layers and persistent-memory contexts. In this paper, we\nintroduce Logic-Layer Prompt Control Injection (LPCI), a novel attack category\nin which encoded, delayed, and conditionally triggered payloads are embedded in\nmemory, vector stores, or tool outputs. These payloads can bypass conventional\ninput filters and trigger unauthorised behaviour across sessions."
                },
                "authors": [
                    {
                        "name": "Hammad Atta"
                    },
                    {
                        "name": "Ken Huang"
                    },
                    {
                        "name": "Manish Bhatt"
                    },
                    {
                        "name": "Kamal Ahmed"
                    },
                    {
                        "name": "Muhammad Aziz Ul Haq"
                    },
                    {
                        "name": "Yasir Mehmood"
                    }
                ],
                "author_detail": {
                    "name": "Yasir Mehmood"
                },
                "author": "Yasir Mehmood",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12355v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12355v3",
                "updated": "2025-07-15T05:21:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    5,
                    21,
                    49,
                    1,
                    196,
                    0
                ],
                "published": "2025-04-16T02:33:19Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    2,
                    33,
                    19,
                    2,
                    106,
                    0
                ],
                "title": "Leveraging Large Language Models for Multi-Class and Multi-Label\n  Detection of Drug Use and Overdose Symptoms on Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Multi-Class and Multi-Label\n  Detection of Drug Use and Overdose Symptoms on Social Media"
                },
                "summary": "Drug overdose remains a critical global health issue, often driven by misuse\nof opioids, painkillers, and psychiatric medications. Traditional research\nmethods face limitations, whereas social media offers real-time insights into\nself-reported substance use and overdose symptoms. This study proposes an\nAI-driven NLP framework trained on annotated social media data to detect\ncommonly used drugs and associated overdose symptoms. Using a hybrid annotation\nstrategy with LLMs and human annotators, we applied traditional ML models,\nneural networks, and advanced transformer-based models. Our framework achieved\n98% accuracy in multi-class and 97% in multi-label classification,\noutperforming baseline models by up to 8%. These findings highlight the\npotential of AI for supporting public health surveillance and personalized\nintervention strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drug overdose remains a critical global health issue, often driven by misuse\nof opioids, painkillers, and psychiatric medications. Traditional research\nmethods face limitations, whereas social media offers real-time insights into\nself-reported substance use and overdose symptoms. This study proposes an\nAI-driven NLP framework trained on annotated social media data to detect\ncommonly used drugs and associated overdose symptoms. Using a hybrid annotation\nstrategy with LLMs and human annotators, we applied traditional ML models,\nneural networks, and advanced transformer-based models. Our framework achieved\n98% accuracy in multi-class and 97% in multi-label classification,\noutperforming baseline models by up to 8%. These findings highlight the\npotential of AI for supporting public health surveillance and personalized\nintervention strategies."
                },
                "authors": [
                    {
                        "name": "Muhammad Ahmad"
                    },
                    {
                        "name": "Fida Ullah"
                    },
                    {
                        "name": "Muhammad Usman"
                    },
                    {
                        "name": "Umyh Habiba"
                    },
                    {
                        "name": "ldar Batyrshin"
                    },
                    {
                        "name": "Grigori Sidorov"
                    }
                ],
                "author_detail": {
                    "name": "Grigori Sidorov"
                },
                "author": "Grigori Sidorov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12355v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12355v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10445v1",
                "updated": "2025-07-14T16:28:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    28,
                    0,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T16:28:00Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    28,
                    0,
                    0,
                    195,
                    0
                ],
                "title": "Referential ambiguity and clarification requests: comparing human and\n  LLM behaviour",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Referential ambiguity and clarification requests: comparing human and\n  LLM behaviour"
                },
                "summary": "In this work we examine LLMs' ability to ask clarification questions in\ntask-oriented dialogues that follow the asynchronous\ninstruction-giver/instruction-follower format. We present a new corpus that\ncombines two existing annotations of the Minecraft Dialogue Corpus -- one for\nreference and ambiguity in reference, and one for SDRT including clarifications\n-- into a single common format providing the necessary information to\nexperiment with clarifications and their relation to ambiguity. With this\ncorpus we compare LLM actions with original human-generated clarification\nquestions, examining how both humans and LLMs act in the case of ambiguity. We\nfind that there is only a weak link between ambiguity and humans producing\nclarification questions in these dialogues, and low correlation between humans\nand LLMs. Humans hardly ever produce clarification questions for referential\nambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce\nmore clarification questions for referential ambiguity, but less so for task\nuncertainty. We question if LLMs' ability to ask clarification questions is\npredicated on their recent ability to simulate reasoning, and test this with\ndifferent reasoning approaches, finding that reasoning does appear to increase\nquestion frequency and relevancy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we examine LLMs' ability to ask clarification questions in\ntask-oriented dialogues that follow the asynchronous\ninstruction-giver/instruction-follower format. We present a new corpus that\ncombines two existing annotations of the Minecraft Dialogue Corpus -- one for\nreference and ambiguity in reference, and one for SDRT including clarifications\n-- into a single common format providing the necessary information to\nexperiment with clarifications and their relation to ambiguity. With this\ncorpus we compare LLM actions with original human-generated clarification\nquestions, examining how both humans and LLMs act in the case of ambiguity. We\nfind that there is only a weak link between ambiguity and humans producing\nclarification questions in these dialogues, and low correlation between humans\nand LLMs. Humans hardly ever produce clarification questions for referential\nambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce\nmore clarification questions for referential ambiguity, but less so for task\nuncertainty. We question if LLMs' ability to ask clarification questions is\npredicated on their recent ability to simulate reasoning, and test this with\ndifferent reasoning approaches, finding that reasoning does appear to increase\nquestion frequency and relevancy."
                },
                "authors": [
                    {
                        "name": "Chris Madge"
                    },
                    {
                        "name": "Matthew Purver"
                    },
                    {
                        "name": "Massimo Poesio"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Poesio"
                },
                "author": "Massimo Poesio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10429v1",
                "updated": "2025-07-14T16:18:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    18,
                    12,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T16:18:12Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    18,
                    12,
                    0,
                    195,
                    0
                ],
                "title": "Inference of time delay in stochastic systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference of time delay in stochastic systems"
                },
                "summary": "Time delay is ubiquitous in many experimental and real-world situations. It\nis often unclear whether time delay plays a significant role in observed\nphenomena, and if it does, how long the time lag really is. This would be\ninvaluable knowledge when analyzing and modeling such systems. Hitherto, no\nuniversal method is available by which the time delay can be inferred. To\naddress this problem, we propose and demonstrate two different methods to infer\ntime delay in overdamped Langevin systems with delayed feedback. In the first\npart, we focus on the power spectral density based on the positional data and\nuse a characteristic signature of the time delay to infer the delay time. In\nlimiting cases, we establish a direct relation of the observations made for\nnonlinear time-delayed feedback forces to analytical results obtained for the\nlinear system. In other situations despite the absence of this direct relation,\nthe characteristic signature remains and can be exploited by a semiautomatic\nmethod to infer the delay time. Furthermore, it may not always desirable or\npossible to observe a system for a long time to infer dependencies and\nparameters. Thus, in the second part, we propose for the first time a probing\nmethod combined with a neural network to infer the delay time, which requires\nonly short observation time series. These proposed methods for inferring time\ndelays in stochastic systems may prove to be valuable tools for gaining deeper\ninsight into the role of delay across a wide range of applications -- from the\nbehavior of individual colloidal particles under feedback control to emergent\ncollective phenomena such as flocking and swarming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time delay is ubiquitous in many experimental and real-world situations. It\nis often unclear whether time delay plays a significant role in observed\nphenomena, and if it does, how long the time lag really is. This would be\ninvaluable knowledge when analyzing and modeling such systems. Hitherto, no\nuniversal method is available by which the time delay can be inferred. To\naddress this problem, we propose and demonstrate two different methods to infer\ntime delay in overdamped Langevin systems with delayed feedback. In the first\npart, we focus on the power spectral density based on the positional data and\nuse a characteristic signature of the time delay to infer the delay time. In\nlimiting cases, we establish a direct relation of the observations made for\nnonlinear time-delayed feedback forces to analytical results obtained for the\nlinear system. In other situations despite the absence of this direct relation,\nthe characteristic signature remains and can be exploited by a semiautomatic\nmethod to infer the delay time. Furthermore, it may not always desirable or\npossible to observe a system for a long time to infer dependencies and\nparameters. Thus, in the second part, we propose for the first time a probing\nmethod combined with a neural network to infer the delay time, which requires\nonly short observation time series. These proposed methods for inferring time\ndelays in stochastic systems may prove to be valuable tools for gaining deeper\ninsight into the role of delay across a wide range of applications -- from the\nbehavior of individual colloidal particles under feedback control to emergent\ncollective phenomena such as flocking and swarming."
                },
                "authors": [
                    {
                        "name": "Robin A. Kopp"
                    },
                    {
                        "name": "Sabine H. L. Klapp"
                    },
                    {
                        "name": "Deepak Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Deepak Gupta"
                },
                "author": "Deepak Gupta",
                "arxiv_comment": "22 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.data-an",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10427v1",
                "updated": "2025-07-14T16:16:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    16,
                    12,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T16:16:12Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    16,
                    12,
                    0,
                    195,
                    0
                ],
                "title": "Towards Emotion Co-regulation with LLM-powered Socially Assistive\n  Robots: Integrating LLM Prompts and Robotic Behaviors to Support\n  Parent-Neurodivergent Child Dyads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Emotion Co-regulation with LLM-powered Socially Assistive\n  Robots: Integrating LLM Prompts and Robotic Behaviors to Support\n  Parent-Neurodivergent Child Dyads"
                },
                "summary": "Socially Assistive Robotics (SAR) has shown promise in supporting emotion\nregulation for neurodivergent children. Recently, there has been increasing\ninterest in leveraging advanced technologies to assist parents in co-regulating\nemotions with their children. However, limited research has explored the\nintegration of large language models (LLMs) with SAR to facilitate emotion\nco-regulation between parents and children with neurodevelopmental disorders.\nTo address this gap, we developed an LLM-powered social robot by deploying a\nspeech communication module on the MiRo-E robotic platform. This supervised\nautonomous system integrates LLM prompts and robotic behaviors to deliver\ntailored interventions for both parents and neurodivergent children. Pilot\ntests were conducted with two parent-child dyads, followed by a qualitative\nanalysis. The findings reveal MiRo-E's positive impacts on interaction dynamics\nand its potential to facilitate emotion regulation, along with identified\ndesign and technical challenges. Based on these insights, we provide design\nimplications to advance the future development of LLM-powered SAR for mental\nhealth applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Socially Assistive Robotics (SAR) has shown promise in supporting emotion\nregulation for neurodivergent children. Recently, there has been increasing\ninterest in leveraging advanced technologies to assist parents in co-regulating\nemotions with their children. However, limited research has explored the\nintegration of large language models (LLMs) with SAR to facilitate emotion\nco-regulation between parents and children with neurodevelopmental disorders.\nTo address this gap, we developed an LLM-powered social robot by deploying a\nspeech communication module on the MiRo-E robotic platform. This supervised\nautonomous system integrates LLM prompts and robotic behaviors to deliver\ntailored interventions for both parents and neurodivergent children. Pilot\ntests were conducted with two parent-child dyads, followed by a qualitative\nanalysis. The findings reveal MiRo-E's positive impacts on interaction dynamics\nand its potential to facilitate emotion regulation, along with identified\ndesign and technical challenges. Based on these insights, we provide design\nimplications to advance the future development of LLM-powered SAR for mental\nhealth applications."
                },
                "authors": [
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Felix Schijve"
                    },
                    {
                        "name": "Sheng Li"
                    },
                    {
                        "name": "Yuye Yang"
                    },
                    {
                        "name": "Jun Hu"
                    },
                    {
                        "name": "Emilia Barakova"
                    }
                ],
                "author_detail": {
                    "name": "Emilia Barakova"
                },
                "author": "Emilia Barakova",
                "arxiv_comment": "Submission for the IROS 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v3",
                "updated": "2025-07-14T16:14:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    14,
                    49,
                    0,
                    195,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at https://github.com/THU-MIG/PrefixKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at https://github.com/THU-MIG/PrefixKV."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jiaxin Li"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "12 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10419v1",
                "updated": "2025-07-14T16:00:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    0,
                    51,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T16:00:51Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    0,
                    51,
                    0,
                    195,
                    0
                ],
                "title": "Multiple Choice Learning of Low Rank Adapters for Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple Choice Learning of Low Rank Adapters for Language Modeling"
                },
                "summary": "We propose LoRA-MCL, a training scheme that extends next-token prediction in\nlanguage models with a method designed to decode diverse, plausible sentence\ncontinuations at inference time. Traditional language modeling is an\nintrinsically ill-posed problem: given a context, multiple futures may be\nequally plausible. Our approach leverages Multiple Choice Learning (MCL) and\nthe Winner-Takes-All (WTA) loss to efficiently handle ambiguity through\nLow-Rank Adaptation (LoRA). We provide a theoretical interpretation of applying\nMultiple Choice Learning to Language Modeling, assuming the data is generated\nfrom a mixture of distributions. To illustrate the proposed approach, we use\ndata sampled from mixtures of Markov chains. We then demonstrate with extensive\nexperiments on real-world visual and audio captioning tasks that our method\nachieves high diversity and relevance in generated outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose LoRA-MCL, a training scheme that extends next-token prediction in\nlanguage models with a method designed to decode diverse, plausible sentence\ncontinuations at inference time. Traditional language modeling is an\nintrinsically ill-posed problem: given a context, multiple futures may be\nequally plausible. Our approach leverages Multiple Choice Learning (MCL) and\nthe Winner-Takes-All (WTA) loss to efficiently handle ambiguity through\nLow-Rank Adaptation (LoRA). We provide a theoretical interpretation of applying\nMultiple Choice Learning to Language Modeling, assuming the data is generated\nfrom a mixture of distributions. To illustrate the proposed approach, we use\ndata sampled from mixtures of Markov chains. We then demonstrate with extensive\nexperiments on real-world visual and audio captioning tasks that our method\nachieves high diversity and relevance in generated outputs."
                },
                "authors": [
                    {
                        "name": "Victor Letzelter"
                    },
                    {
                        "name": "Hugo Malard"
                    },
                    {
                        "name": "Mathieu Fontaine"
                    },
                    {
                        "name": "Ga√´l Richard"
                    },
                    {
                        "name": "Slim Essid"
                    },
                    {
                        "name": "Andrei Bursuc"
                    },
                    {
                        "name": "Patrick P√©rez"
                    }
                ],
                "author_detail": {
                    "name": "Patrick P√©rez"
                },
                "author": "Patrick P√©rez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10409v2",
                "updated": "2025-07-15T13:17:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    13,
                    17,
                    11,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-14T15:54:06Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    54,
                    6,
                    0,
                    195,
                    0
                ],
                "title": "Energy Efficiency in AI for 5G and Beyond: A DeepRx Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Efficiency in AI for 5G and Beyond: A DeepRx Case Study"
                },
                "summary": "This study addresses the challenge of balancing energy efficiency with\nperformance in AI/ML models, focusing on DeepRX, a deep learning receiver based\non a fully convolutional ResNet architecture. We evaluate the energy\nconsumption of DeepRX, considering factors including FLOPs/Watt and\nFLOPs/clock, and find consistency between estimated and actual energy usage,\ninfluenced by memory access patterns. The research extends to comparing energy\ndynamics during training and inference phases. A key contribution is the\napplication of knowledge distillation (KD) to train a compact DeepRX student\nmodel that emulates the performance of the teacher model but with reduced\nenergy consumption. We experiment with different student model sizes, optimal\nteacher sizes, and KD hyperparameters. Performance is measured by comparing the\nBit Error Rate (BER) performance versus Signal-to-Interference & Noise Ratio\n(SINR) values of the distilled model and a model trained from scratch. The\ndistilled models demonstrate a lower error floor across SINR levels,\nhighlighting the effectiveness of KD in achieving energy-efficient AI\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study addresses the challenge of balancing energy efficiency with\nperformance in AI/ML models, focusing on DeepRX, a deep learning receiver based\non a fully convolutional ResNet architecture. We evaluate the energy\nconsumption of DeepRX, considering factors including FLOPs/Watt and\nFLOPs/clock, and find consistency between estimated and actual energy usage,\ninfluenced by memory access patterns. The research extends to comparing energy\ndynamics during training and inference phases. A key contribution is the\napplication of knowledge distillation (KD) to train a compact DeepRX student\nmodel that emulates the performance of the teacher model but with reduced\nenergy consumption. We experiment with different student model sizes, optimal\nteacher sizes, and KD hyperparameters. Performance is measured by comparing the\nBit Error Rate (BER) performance versus Signal-to-Interference & Noise Ratio\n(SINR) values of the distilled model and a model trained from scratch. The\ndistilled models demonstrate a lower error floor across SINR levels,\nhighlighting the effectiveness of KD in achieving energy-efficient AI\nsolutions."
                },
                "authors": [
                    {
                        "name": "Amine Lbath"
                    },
                    {
                        "name": "Ibtissam Labriji"
                    }
                ],
                "author_detail": {
                    "name": "Ibtissam Labriji"
                },
                "author": "Ibtissam Labriji",
                "arxiv_doi": "10.1109/EuCNC/6GSummit60053.2024.10597065",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/EuCNC/6GSummit60053.2024.10597065",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.10409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10404v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10404v1",
                "updated": "2025-07-14T15:50:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    50,
                    58,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T15:50:58Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    50,
                    58,
                    0,
                    195,
                    0
                ],
                "title": "Two-step semiparametric empirical likelihood inference from\n  capture-recapture data with missing covariates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-step semiparametric empirical likelihood inference from\n  capture-recapture data with missing covariates"
                },
                "summary": "Missing covariates are not uncommon in capture-recapture studies. When\ncovariate information is missing at random in capture-recapture data, an\nempirical full likelihood method has been demonstrated to outperform\nconditional-likelihood-based methods in abundance estimation. However, the\nfully observed covariates must be discrete, and the method is not directly\napplicable to continuous-time capture-recapture data. Based on the Binomial and\nPoisson regression models, we propose a two-step semiparametric empirical\nlikelihood approach for abundance estimation in the presence of missing\ncovariates, regardless of whether the fully observed covariates are discrete or\ncontinuous. We show that the maximum semiparametric empirical likelihood\nestimators for the underlying parameters and the abundance are asymptotically\nnormal, and more efficient than the counterpart for a completely known\nnon-missingness probability. After scaling, the empirical likelihood ratio test\nstatistic for abundance follows a limiting chi-square distribution with one\ndegree of freedom. The proposed approach is further extended to one-inflated\ncount regression models, and a score-like test is constructed to assess whether\none-inflation exists among the number of captures. Our simulation shows that,\ncompared with the previous method, the proposed method not only performs better\nin correcting bias, but also has a more accurate coverage in the presence of\nfully observed continuous covariates, although there may be a slight efficiency\nloss when the fully observed covariates are only discrete. The performance of\nthe new method is illustrated by an analysis of the Hong Kong prinia data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missing covariates are not uncommon in capture-recapture studies. When\ncovariate information is missing at random in capture-recapture data, an\nempirical full likelihood method has been demonstrated to outperform\nconditional-likelihood-based methods in abundance estimation. However, the\nfully observed covariates must be discrete, and the method is not directly\napplicable to continuous-time capture-recapture data. Based on the Binomial and\nPoisson regression models, we propose a two-step semiparametric empirical\nlikelihood approach for abundance estimation in the presence of missing\ncovariates, regardless of whether the fully observed covariates are discrete or\ncontinuous. We show that the maximum semiparametric empirical likelihood\nestimators for the underlying parameters and the abundance are asymptotically\nnormal, and more efficient than the counterpart for a completely known\nnon-missingness probability. After scaling, the empirical likelihood ratio test\nstatistic for abundance follows a limiting chi-square distribution with one\ndegree of freedom. The proposed approach is further extended to one-inflated\ncount regression models, and a score-like test is constructed to assess whether\none-inflation exists among the number of captures. Our simulation shows that,\ncompared with the previous method, the proposed method not only performs better\nin correcting bias, but also has a more accurate coverage in the presence of\nfully observed continuous covariates, although there may be a slight efficiency\nloss when the fully observed covariates are only discrete. The performance of\nthe new method is illustrated by an analysis of the Hong Kong prinia data."
                },
                "authors": [
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Yukun Liu"
                    },
                    {
                        "name": "Pengfei Li"
                    },
                    {
                        "name": "Riquan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Riquan Zhang"
                },
                "author": "Riquan Zhang",
                "arxiv_doi": "10.1007/s11749-024-00921-1",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s11749-024-00921-1",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.10404v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10404v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Test (2024), 33, 786-808",
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05429v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05429v2",
                "updated": "2025-07-14T15:36:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    36,
                    29,
                    0,
                    195,
                    0
                ],
                "published": "2025-05-08T17:19:11Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    19,
                    11,
                    3,
                    128,
                    0
                ],
                "title": "Theoretical modeling of approximate universality of tidally deformed\n  neutron stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theoretical modeling of approximate universality of tidally deformed\n  neutron stars"
                },
                "summary": "Quasi-universal relations are known to exist among various neutron star\nobservables that do not depend sensitively on the underlying nuclear matter\nequations of state. For example, some of these relations imply that the tidally\ninduced multipole moments are approximately characterized by the electric-type\nquadrupolar tidal deformability. Such relations can be used to reduce the\nnumber of independent tidal parameters in gravitational-waveform modeling,\nthereby allowing us to infer extreme nuclear matter properties more accurately\nand test General Relativity in an insensitive manner to uncertainties in\nnuclear physics. We present a comprehensive theoretical investigation into\napproximate universality of neutron stars. Our approach employs a semi-analytic\nrelativistic stellar interior model, which extends the Tolman VII solution,\nthereby enabling a refined exploration of the tidal properties of nonrotating\nstars within a semi-analytic framework. The derived power-law relations among\nvarious tidal deformabilities -- referred to as the universal Love relations --\nagree well with expressions in previous work found empirically. We elucidate\nhow the equation-of-state dependence is suppressed in a particular combination\nof macroscopic physical parameters induced by perturbations and demonstrate\nthat the relation between the moment of inertia and electric-type quadrupolar\ntidal deformability (I-Love relation) rests on the same underlying mechanism.\nOur findings indicate that the approximate universality of neutron stars can be\nattributed to low compressibility, consistent with some of the previous studies\non the possible origin of the universality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quasi-universal relations are known to exist among various neutron star\nobservables that do not depend sensitively on the underlying nuclear matter\nequations of state. For example, some of these relations imply that the tidally\ninduced multipole moments are approximately characterized by the electric-type\nquadrupolar tidal deformability. Such relations can be used to reduce the\nnumber of independent tidal parameters in gravitational-waveform modeling,\nthereby allowing us to infer extreme nuclear matter properties more accurately\nand test General Relativity in an insensitive manner to uncertainties in\nnuclear physics. We present a comprehensive theoretical investigation into\napproximate universality of neutron stars. Our approach employs a semi-analytic\nrelativistic stellar interior model, which extends the Tolman VII solution,\nthereby enabling a refined exploration of the tidal properties of nonrotating\nstars within a semi-analytic framework. The derived power-law relations among\nvarious tidal deformabilities -- referred to as the universal Love relations --\nagree well with expressions in previous work found empirically. We elucidate\nhow the equation-of-state dependence is suppressed in a particular combination\nof macroscopic physical parameters induced by perturbations and demonstrate\nthat the relation between the moment of inertia and electric-type quadrupolar\ntidal deformability (I-Love relation) rests on the same underlying mechanism.\nOur findings indicate that the approximate universality of neutron stars can be\nattributed to low compressibility, consistent with some of the previous studies\non the possible origin of the universality."
                },
                "authors": [
                    {
                        "name": "Takuya Katagiri"
                    },
                    {
                        "name": "Gowtham Rishi Mukkamala"
                    },
                    {
                        "name": "Kent Yagi"
                    }
                ],
                "author_detail": {
                    "name": "Kent Yagi"
                },
                "author": "Kent Yagi",
                "arxiv_comment": "22pages, 16figures, v2: minor corrections, accepted for publication\n  in PRD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05429v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05429v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10392v1",
                "updated": "2025-07-14T15:31:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    31,
                    31,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T15:31:31Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    31,
                    31,
                    0,
                    195,
                    0
                ],
                "title": "Zorse: Optimizing LLM Training Efficiency on Heterogeneous GPU Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zorse: Optimizing LLM Training Efficiency on Heterogeneous GPU Clusters"
                },
                "summary": "Large language models (LLMs) require vast amounts of GPU compute to train,\nbut limited availability and high costs of GPUs make homogeneous clusters\nimpractical for many organizations. Instead, assembling heterogeneous clusters\nby pooling together GPUs of different generations allows them to achieve higher\naggregate compute and make use of all available GPUs. However, training on\nheterogeneous clusters presents several challenges, including load balancing\nacross GPUs, optimizing memory usage to accommodate varying memory capacities,\nand ensuring communication-efficient training over diverse network\ninterconnects potentially spanning multiple datacenters. In this paper, we make\nthe case that efficient training on heterogeneous clusters requires (1) the\nintegration of pipeline parallelism and data parallelism in a manner that is\nboth communication- and memory-efficient, and (2) a more adaptable\nconfiguration of pipeline and data parallelism, which includes the capability\nto flexibly partition GPUs into asymmetric pipeline parallel stages and to\nincorporate heterogeneous GPUs within the same data parallelism group. We\npropose Zorse, the first system to unify all these capabilities while\nincorporating a planner that automatically configures training strategies for a\ngiven workload. Our evaluation shows that Zorse significantly outperforms\nstate-of-the-art systems in heterogeneous training scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) require vast amounts of GPU compute to train,\nbut limited availability and high costs of GPUs make homogeneous clusters\nimpractical for many organizations. Instead, assembling heterogeneous clusters\nby pooling together GPUs of different generations allows them to achieve higher\naggregate compute and make use of all available GPUs. However, training on\nheterogeneous clusters presents several challenges, including load balancing\nacross GPUs, optimizing memory usage to accommodate varying memory capacities,\nand ensuring communication-efficient training over diverse network\ninterconnects potentially spanning multiple datacenters. In this paper, we make\nthe case that efficient training on heterogeneous clusters requires (1) the\nintegration of pipeline parallelism and data parallelism in a manner that is\nboth communication- and memory-efficient, and (2) a more adaptable\nconfiguration of pipeline and data parallelism, which includes the capability\nto flexibly partition GPUs into asymmetric pipeline parallel stages and to\nincorporate heterogeneous GPUs within the same data parallelism group. We\npropose Zorse, the first system to unify all these capabilities while\nincorporating a planner that automatically configures training strategies for a\ngiven workload. Our evaluation shows that Zorse significantly outperforms\nstate-of-the-art systems in heterogeneous training scenarios."
                },
                "authors": [
                    {
                        "name": "Runsheng Benson Guo"
                    },
                    {
                        "name": "Utkarsh Anand"
                    },
                    {
                        "name": "Khuzaima Daudjee"
                    },
                    {
                        "name": "Rathijit Sen"
                    }
                ],
                "author_detail": {
                    "name": "Rathijit Sen"
                },
                "author": "Rathijit Sen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10388v1",
                "updated": "2025-07-14T15:29:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    29,
                    11,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T15:29:11Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    29,
                    11,
                    0,
                    195,
                    0
                ],
                "title": "Semiparametric empirical likelihood inference for abundance from\n  one-inflated capture-recapture data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semiparametric empirical likelihood inference for abundance from\n  one-inflated capture-recapture data"
                },
                "summary": "Abundance estimation from capture-recapture data is of great importance in\nmany disciplines. Analysis of capture-recapture data is often complicated by\nthe existence of one-inflation and heterogeneity problems. Simultaneously\ntaking these issues into account, existing abundance estimation methods are\nusually constructed on the basis of conditional likelihood (CL) under\none-inflated zero-truncated count models. However, the resulting\nHorvitz-Thompson-type estimators may be unstable, and the resulting Wald-type\nconfidence intervals may exhibit severe undercoverage. In this paper, we\npropose a semiparametric empirical likelihood (EL) approach to abundance\nestimation under one-inflated binomial and Poisson regression models. We show\nthat the maximum EL estimator for the abundance follows an asymptotically\nnormal distribution and that the EL ratio statistic of abundance follows a\nlimiting chi-square distribution with one degree of freedom. To facilitate\ncomputation of the EL method, we develop an expectation-maximization (EM)\nalgorithm, and establish its appealing convergence property. We also propose a\nnew score test for the existence of one-inflation and prove its asymptotic\nnormality. Our simulation studies indicate that compared with CL-based methods,\nthe maximum EL estimator has a smaller mean square error, the EL ratio\nconfidence interval has a remarkable gain in coverage probability, and the\nproposed score test is more powerful. The advantages of the proposed approaches\nare further demonstrated by analyses of prinia data from Hong Kong and drug\nuser data from Bangkok.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abundance estimation from capture-recapture data is of great importance in\nmany disciplines. Analysis of capture-recapture data is often complicated by\nthe existence of one-inflation and heterogeneity problems. Simultaneously\ntaking these issues into account, existing abundance estimation methods are\nusually constructed on the basis of conditional likelihood (CL) under\none-inflated zero-truncated count models. However, the resulting\nHorvitz-Thompson-type estimators may be unstable, and the resulting Wald-type\nconfidence intervals may exhibit severe undercoverage. In this paper, we\npropose a semiparametric empirical likelihood (EL) approach to abundance\nestimation under one-inflated binomial and Poisson regression models. We show\nthat the maximum EL estimator for the abundance follows an asymptotically\nnormal distribution and that the EL ratio statistic of abundance follows a\nlimiting chi-square distribution with one degree of freedom. To facilitate\ncomputation of the EL method, we develop an expectation-maximization (EM)\nalgorithm, and establish its appealing convergence property. We also propose a\nnew score test for the existence of one-inflation and prove its asymptotic\nnormality. Our simulation studies indicate that compared with CL-based methods,\nthe maximum EL estimator has a smaller mean square error, the EL ratio\nconfidence interval has a remarkable gain in coverage probability, and the\nproposed score test is more powerful. The advantages of the proposed approaches\nare further demonstrated by analyses of prinia data from Hong Kong and drug\nuser data from Bangkok."
                },
                "authors": [
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Pengfei Li"
                    },
                    {
                        "name": "Yukun Liu"
                    },
                    {
                        "name": "Riquan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Riquan Zhang"
                },
                "author": "Riquan Zhang",
                "arxiv_doi": "10.1002/bimj.202100231",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/bimj.202100231",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.10388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Biometrical Journal (2022), 64, 1040-1055",
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11168v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11168v3",
                "updated": "2025-07-14T15:27:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    27,
                    11,
                    0,
                    195,
                    0
                ],
                "published": "2025-04-15T13:16:02Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    16,
                    2,
                    1,
                    105,
                    0
                ],
                "title": "Bypassing LLM Guardrails: An Empirical Analysis of Evasion Attacks\n  against Prompt Injection and Jailbreak Detection Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bypassing LLM Guardrails: An Empirical Analysis of Evasion Attacks\n  against Prompt Injection and Jailbreak Detection Systems"
                },
                "summary": "Large Language Models (LLMs) guardrail systems are designed to protect\nagainst prompt injection and jailbreak attacks. However, they remain vulnerable\nto evasion techniques. We demonstrate two approaches for bypassing LLM prompt\ninjection and jailbreak detection systems via traditional character injection\nmethods and algorithmic Adversarial Machine Learning (AML) evasion techniques.\nThrough testing against six prominent protection systems, including Microsoft's\nAzure Prompt Shield and Meta's Prompt Guard, we show that both methods can be\nused to evade detection while maintaining adversarial utility achieving in some\ninstances up to 100% evasion success. Furthermore, we demonstrate that\nadversaries can enhance Attack Success Rates (ASR) against black-box targets by\nleveraging word importance ranking computed by offline white-box models. Our\nfindings reveal vulnerabilities within current LLM protection mechanisms and\nhighlight the need for more robust guardrail systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) guardrail systems are designed to protect\nagainst prompt injection and jailbreak attacks. However, they remain vulnerable\nto evasion techniques. We demonstrate two approaches for bypassing LLM prompt\ninjection and jailbreak detection systems via traditional character injection\nmethods and algorithmic Adversarial Machine Learning (AML) evasion techniques.\nThrough testing against six prominent protection systems, including Microsoft's\nAzure Prompt Shield and Meta's Prompt Guard, we show that both methods can be\nused to evade detection while maintaining adversarial utility achieving in some\ninstances up to 100% evasion success. Furthermore, we demonstrate that\nadversaries can enhance Attack Success Rates (ASR) against black-box targets by\nleveraging word importance ranking computed by offline white-box models. Our\nfindings reveal vulnerabilities within current LLM protection mechanisms and\nhighlight the need for more robust guardrail systems."
                },
                "authors": [
                    {
                        "name": "William Hackett"
                    },
                    {
                        "name": "Lewis Birch"
                    },
                    {
                        "name": "Stefan Trawicki"
                    },
                    {
                        "name": "Neeraj Suri"
                    },
                    {
                        "name": "Peter Garraghan"
                    }
                ],
                "author_detail": {
                    "name": "Peter Garraghan"
                },
                "author": "Peter Garraghan",
                "arxiv_comment": "14 pages, 5 figures, 11 tables. To be published in LLMSec 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11168v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11168v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10382v1",
                "updated": "2025-07-14T15:23:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    23,
                    11,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T15:23:11Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    23,
                    11,
                    0,
                    195,
                    0
                ],
                "title": "Leveraging RAG-LLMs for Urban Mobility Simulation and Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging RAG-LLMs for Urban Mobility Simulation and Analysis"
                },
                "summary": "With the rise of smart mobility and shared e-mobility services, numerous\nadvanced technologies have been applied to this field. Cloud-based traffic\nsimulation solutions have flourished, offering increasingly realistic\nrepresentations of the evolving mobility landscape. LLMs have emerged as\npioneering tools, providing robust support for various applications, including\nintelligent decision-making, user interaction, and real-time traffic analysis.\nAs user demand for e-mobility continues to grow, delivering comprehensive\nend-to-end solutions has become crucial. In this paper, we present a\ncloud-based, LLM-powered shared e-mobility platform, integrated with a mobile\napplication for personalized route recommendations. The optimization module is\nevaluated based on travel time and cost across different traffic scenarios.\nAdditionally, the LLM-powered RAG framework is evaluated at the schema level\nfor different users, using various evaluation methods. Schema-level RAG with\nXiYanSQL achieves an average execution accuracy of 0.81 on system operator\nqueries and 0.98 on user queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of smart mobility and shared e-mobility services, numerous\nadvanced technologies have been applied to this field. Cloud-based traffic\nsimulation solutions have flourished, offering increasingly realistic\nrepresentations of the evolving mobility landscape. LLMs have emerged as\npioneering tools, providing robust support for various applications, including\nintelligent decision-making, user interaction, and real-time traffic analysis.\nAs user demand for e-mobility continues to grow, delivering comprehensive\nend-to-end solutions has become crucial. In this paper, we present a\ncloud-based, LLM-powered shared e-mobility platform, integrated with a mobile\napplication for personalized route recommendations. The optimization module is\nevaluated based on travel time and cost across different traffic scenarios.\nAdditionally, the LLM-powered RAG framework is evaluated at the schema level\nfor different users, using various evaluation methods. Schema-level RAG with\nXiYanSQL achieves an average execution accuracy of 0.81 on system operator\nqueries and 0.98 on user queries."
                },
                "authors": [
                    {
                        "name": "Yue Ding"
                    },
                    {
                        "name": "Conor McCarthy"
                    },
                    {
                        "name": "Kevin O'Shea"
                    },
                    {
                        "name": "Mingming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Mingming Liu"
                },
                "author": "Mingming Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02083v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02083v2",
                "updated": "2025-07-14T15:17:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    17,
                    16,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-02T18:41:44Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    18,
                    41,
                    44,
                    2,
                    183,
                    0
                ],
                "title": "Measuring Scientific Capabilities of Language Models with a Systems\n  Biology Dry Lab",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Scientific Capabilities of Language Models with a Systems\n  Biology Dry Lab"
                },
                "summary": "Designing experiments and result interpretations are core scientific\ncompetencies, particularly in biology, where researchers perturb complex\nsystems to uncover the underlying systems. Recent efforts to evaluate the\nscientific capabilities of large language models (LLMs) fail to test these\ncompetencies because wet-lab experimentation is prohibitively expensive: in\nexpertise, time and equipment. We introduce SciGym, a first-in-class benchmark\nthat assesses LLMs' iterative experiment design and analysis abilities in\nopen-ended scientific discovery tasks. SciGym overcomes the challenge of\nwet-lab costs by running a dry lab of biological systems. These models, encoded\nin Systems Biology Markup Language, are efficient for generating simulated\ndata, making them ideal testbeds for experimentation on realistically complex\nsystems. We evaluated six frontier LLMs on 137 small systems, and released a\ntotal of 350 systems. Our evaluation shows that while more capable models\ndemonstrated superior performance, all models' performance declined\nsignificantly as system complexity increased, suggesting substantial room for\nimprovement in the scientific capabilities of LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing experiments and result interpretations are core scientific\ncompetencies, particularly in biology, where researchers perturb complex\nsystems to uncover the underlying systems. Recent efforts to evaluate the\nscientific capabilities of large language models (LLMs) fail to test these\ncompetencies because wet-lab experimentation is prohibitively expensive: in\nexpertise, time and equipment. We introduce SciGym, a first-in-class benchmark\nthat assesses LLMs' iterative experiment design and analysis abilities in\nopen-ended scientific discovery tasks. SciGym overcomes the challenge of\nwet-lab costs by running a dry lab of biological systems. These models, encoded\nin Systems Biology Markup Language, are efficient for generating simulated\ndata, making them ideal testbeds for experimentation on realistically complex\nsystems. We evaluated six frontier LLMs on 137 small systems, and released a\ntotal of 350 systems. Our evaluation shows that while more capable models\ndemonstrated superior performance, all models' performance declined\nsignificantly as system complexity increased, suggesting substantial room for\nimprovement in the scientific capabilities of LLM agents."
                },
                "authors": [
                    {
                        "name": "Haonan Duan"
                    },
                    {
                        "name": "Stephen Zhewen Lu"
                    },
                    {
                        "name": "Caitlin Fiona Harrigan"
                    },
                    {
                        "name": "Nishkrit Desai"
                    },
                    {
                        "name": "Jiarui Lu"
                    },
                    {
                        "name": "Micha≈Ç Koziarski"
                    },
                    {
                        "name": "Leonardo Cotta"
                    },
                    {
                        "name": "Chris J. Maddison"
                    }
                ],
                "author_detail": {
                    "name": "Chris J. Maddison"
                },
                "author": "Chris J. Maddison",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02083v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02083v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06238v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06238v2",
                "updated": "2025-07-14T15:16:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    16,
                    18,
                    0,
                    195,
                    0
                ],
                "published": "2024-10-08T17:54:03Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    17,
                    54,
                    3,
                    1,
                    282,
                    0
                ],
                "title": "EVOLvE: Evaluating and Optimizing LLMs For In-Context Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOLvE: Evaluating and Optimizing LLMs For In-Context Exploration"
                },
                "summary": "Despite their success in many domains, large language models (LLMs) remain\nunder-studied in scenarios requiring optimal decision-making under uncertainty.\nThis is crucial as many real-world applications, ranging from personalized\nrecommendations to healthcare interventions, demand that LLMs not only predict\nbut also actively learn to make optimal decisions through exploration. In this\nwork, we measure LLMs' (in)ability to make optimal decisions in bandits, a\nstate-less reinforcement learning setting relevant to many applications. We\ndevelop a comprehensive suite of environments, including both context-free and\ncontextual bandits with varying task difficulties, to benchmark LLMs'\nperformance. Motivated by the existence of optimal exploration algorithms, we\npropose efficient ways to integrate this algorithmic knowledge into LLMs: by\nproviding explicit algorithm-guided support during inference; and through\nalgorithm distillation via in-context demonstrations and fine-tuning, using\nsynthetic data generated from these algorithms. Impressively, these techniques\nallow us to achieve superior exploration performance with smaller models,\nsurpassing larger models on various tasks. We conducted an extensive ablation\nstudy to shed light on various factors, such as task difficulty and data\nrepresentation, that influence the efficiency of LLM exploration. Additionally,\nwe conduct a rigorous analysis of the LLM's exploration efficiency using the\nconcept of regret, linking its ability to explore to the model size and\nunderlying algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their success in many domains, large language models (LLMs) remain\nunder-studied in scenarios requiring optimal decision-making under uncertainty.\nThis is crucial as many real-world applications, ranging from personalized\nrecommendations to healthcare interventions, demand that LLMs not only predict\nbut also actively learn to make optimal decisions through exploration. In this\nwork, we measure LLMs' (in)ability to make optimal decisions in bandits, a\nstate-less reinforcement learning setting relevant to many applications. We\ndevelop a comprehensive suite of environments, including both context-free and\ncontextual bandits with varying task difficulties, to benchmark LLMs'\nperformance. Motivated by the existence of optimal exploration algorithms, we\npropose efficient ways to integrate this algorithmic knowledge into LLMs: by\nproviding explicit algorithm-guided support during inference; and through\nalgorithm distillation via in-context demonstrations and fine-tuning, using\nsynthetic data generated from these algorithms. Impressively, these techniques\nallow us to achieve superior exploration performance with smaller models,\nsurpassing larger models on various tasks. We conducted an extensive ablation\nstudy to shed light on various factors, such as task difficulty and data\nrepresentation, that influence the efficiency of LLM exploration. Additionally,\nwe conduct a rigorous analysis of the LLM's exploration efficiency using the\nconcept of regret, linking its ability to explore to the model size and\nunderlying algorithm."
                },
                "authors": [
                    {
                        "name": "Allen Nie"
                    },
                    {
                        "name": "Yi Su"
                    },
                    {
                        "name": "Bo Chang"
                    },
                    {
                        "name": "Jonathan N. Lee"
                    },
                    {
                        "name": "Ed H. Chi"
                    },
                    {
                        "name": "Quoc V. Le"
                    },
                    {
                        "name": "Minmin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Minmin Chen"
                },
                "author": "Minmin Chen",
                "arxiv_comment": "28 pages. Published at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06238v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10373v1",
                "updated": "2025-07-14T15:14:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    14,
                    27,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T15:14:27Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    14,
                    27,
                    0,
                    195,
                    0
                ],
                "title": "Post-reduction inference for confidence sets of models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-reduction inference for confidence sets of models"
                },
                "summary": "Sparsity in a regression context makes the model itself an object of\ninterest, pointing to a confidence set of models as the appropriate\npresentation of evidence. A difficulty in areas such as genomics, where the\nnumber of candidate variables is vast, arises from the need for preliminary\nreduction prior to the assessment of models. The present paper considers a\nresolution using inferential separations fundamental to the Fisherian approach\nto conditional inference, namely, the sufficiency/co-sufficiency separation,\nand the ancillary/co-ancillary separation. The advantage of these separations\nis that no direction for departure from any hypothesised model is needed,\navoiding issues that would otherwise arise from using the same data for\nreduction and for model assessment. In idealised cases with no nuisance\nparameters, the separations extract all the information in the data, solely for\nthe purpose for which it is useful, without loss or redundancy. The extent to\nwhich estimation of nuisance parameters affects the idealised information\nextraction is illustrated in detail for the normal-theory linear regression\nmodel, extending immediately to a log-normal accelerated-life model for\ntime-to-event outcomes. This idealised analysis provides insight into when\nsample-splitting is likely to perform as well as, or better than, the\nco-sufficient or ancillary tests, and when it may be unreliable. The\nconsiderations involved in extending the detailed implementation to canonical\nexponential-family and more general regression models are briefly discussed. As\npart of the analysis for the Gaussian model, we introduce a modified version of\nthe refitted cross-validation estimator of Fan et al. (2012), whose\ndistribution theory is exact in an appropriate conditional sense.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparsity in a regression context makes the model itself an object of\ninterest, pointing to a confidence set of models as the appropriate\npresentation of evidence. A difficulty in areas such as genomics, where the\nnumber of candidate variables is vast, arises from the need for preliminary\nreduction prior to the assessment of models. The present paper considers a\nresolution using inferential separations fundamental to the Fisherian approach\nto conditional inference, namely, the sufficiency/co-sufficiency separation,\nand the ancillary/co-ancillary separation. The advantage of these separations\nis that no direction for departure from any hypothesised model is needed,\navoiding issues that would otherwise arise from using the same data for\nreduction and for model assessment. In idealised cases with no nuisance\nparameters, the separations extract all the information in the data, solely for\nthe purpose for which it is useful, without loss or redundancy. The extent to\nwhich estimation of nuisance parameters affects the idealised information\nextraction is illustrated in detail for the normal-theory linear regression\nmodel, extending immediately to a log-normal accelerated-life model for\ntime-to-event outcomes. This idealised analysis provides insight into when\nsample-splitting is likely to perform as well as, or better than, the\nco-sufficient or ancillary tests, and when it may be unreliable. The\nconsiderations involved in extending the detailed implementation to canonical\nexponential-family and more general regression models are briefly discussed. As\npart of the analysis for the Gaussian model, we introduce a modified version of\nthe refitted cross-validation estimator of Fan et al. (2012), whose\ndistribution theory is exact in an appropriate conditional sense."
                },
                "authors": [
                    {
                        "name": "Heather Battey"
                    },
                    {
                        "name": "Daniel Garcia Rasines"
                    },
                    {
                        "name": "Yanbo Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yanbo Tang"
                },
                "author": "Yanbo Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15622v2",
                "updated": "2025-07-14T15:13:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    13,
                    31,
                    0,
                    195,
                    0
                ],
                "published": "2025-06-18T16:51:34Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    16,
                    51,
                    34,
                    2,
                    169,
                    0
                ],
                "title": "Models for cyclic infinity operads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Models for cyclic infinity operads"
                },
                "summary": "We construct model structures on cyclic dendroidal sets and cyclic dendroidal\nspaces for cyclic quasi-operads and complete cyclic dendroidal Segal spaces,\nrespectively. We show these models are Quillen equivalent to the model\nstructure for simplicial cyclic operads. This answers in the affirmative a\nquestion of the second author and Drummond-Cole concerning model structures for\ncyclic $\\infty$-operads. We infer similar statements for planar cyclic\n$\\infty$-operads, providing the model-categorical foundation needed to complete\nWalde's program on the relationship between cyclic 2-Segal spaces and planar\ncyclic $\\infty$-operads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We construct model structures on cyclic dendroidal sets and cyclic dendroidal\nspaces for cyclic quasi-operads and complete cyclic dendroidal Segal spaces,\nrespectively. We show these models are Quillen equivalent to the model\nstructure for simplicial cyclic operads. This answers in the affirmative a\nquestion of the second author and Drummond-Cole concerning model structures for\ncyclic $\\infty$-operads. We infer similar statements for planar cyclic\n$\\infty$-operads, providing the model-categorical foundation needed to complete\nWalde's program on the relationship between cyclic 2-Segal spaces and planar\ncyclic $\\infty$-operads."
                },
                "authors": [
                    {
                        "name": "Brandon Doherty"
                    },
                    {
                        "name": "Philip Hackney"
                    }
                ],
                "author_detail": {
                    "name": "Philip Hackney"
                },
                "author": "Philip Hackney",
                "arxiv_comment": "41 pages, 7 figures. v2: updates to references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "18M85, 18N40, 55U35, 18N70",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15285v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15285v2",
                "updated": "2025-07-14T14:56:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    14,
                    56,
                    52,
                    0,
                    195,
                    0
                ],
                "published": "2025-06-18T09:08:42Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    9,
                    8,
                    42,
                    2,
                    169,
                    0
                ],
                "title": "AI-driven visual monitoring of industrial assembly tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-driven visual monitoring of industrial assembly tasks"
                },
                "summary": "Visual monitoring of industrial assembly tasks is critical for preventing\nequipment damage due to procedural errors and ensuring worker safety. Although\ncommercial solutions exist, they typically require rigid workspace setups or\nthe application of visual markers to simplify the problem. We introduce ViMAT,\na novel AI-driven system for real-time visual monitoring of assembly tasks that\noperates without these constraints. ViMAT combines a perception module that\nextracts visual observations from multi-view video streams with a reasoning\nmodule that infers the most likely action being performed based on the observed\nassembly state and prior task knowledge. We validate ViMAT on two assembly\ntasks, involving the replacement of LEGO components and the reconfiguration of\nhydraulic press molds, demonstrating its effectiveness through quantitative and\nqualitative analysis in challenging real-world scenarios characterized by\npartial and uncertain visual observations. Project page:\nhttps://tev-fbk.github.io/ViMAT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual monitoring of industrial assembly tasks is critical for preventing\nequipment damage due to procedural errors and ensuring worker safety. Although\ncommercial solutions exist, they typically require rigid workspace setups or\nthe application of visual markers to simplify the problem. We introduce ViMAT,\na novel AI-driven system for real-time visual monitoring of assembly tasks that\noperates without these constraints. ViMAT combines a perception module that\nextracts visual observations from multi-view video streams with a reasoning\nmodule that infers the most likely action being performed based on the observed\nassembly state and prior task knowledge. We validate ViMAT on two assembly\ntasks, involving the replacement of LEGO components and the reconfiguration of\nhydraulic press molds, demonstrating its effectiveness through quantitative and\nqualitative analysis in challenging real-world scenarios characterized by\npartial and uncertain visual observations. Project page:\nhttps://tev-fbk.github.io/ViMAT"
                },
                "authors": [
                    {
                        "name": "Mattia Nardon"
                    },
                    {
                        "name": "Stefano Messelodi"
                    },
                    {
                        "name": "Antonio Granata"
                    },
                    {
                        "name": "Fabio Poiesi"
                    },
                    {
                        "name": "Alberto Danese"
                    },
                    {
                        "name": "Davide Boscaini"
                    }
                ],
                "author_detail": {
                    "name": "Davide Boscaini"
                },
                "author": "Davide Boscaini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15285v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15285v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02620v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02620v2",
                "updated": "2025-07-14T14:55:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    14,
                    55,
                    53,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-03T13:47:42Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    47,
                    42,
                    3,
                    184,
                    0
                ],
                "title": "FlowSpec: Continuous Pipelined Speculative Decoding for Efficient\n  Distributed LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowSpec: Continuous Pipelined Speculative Decoding for Efficient\n  Distributed LLM Inference"
                },
                "summary": "Distributed inference serves as a promising approach to enabling the\ninference of large language models (LLMs) at the network edge. It distributes\nthe inference process to multiple devices to ensure that the LLMs can fit into\nthe device memory. Recent pipeline-based approaches have the potential to\nparallelize communication and computation, which helps reduce inference\nlatency. However, the benefit diminishes when the inference request at the\nnetwork edge is sparse, where pipeline is typically at low utilization. To\nenable efficient distributed LLM inference at the edge, we propose\n\\textbf{FlowSpec}, a pipeline-parallel tree-based speculative decoding\nframework. FlowSpec incorporates three key mechanisms to improve decoding\nefficiency: 1) score-based step-wise verification prioritizes more important\ndraft tokens to bring earlier accpeted tokens; 2) efficient draft management to\nprune invalid tokens while maintaining correct causal relationship during\nverification; 3) dynamic draft expansion strategies to supply high-quality\nspeculative inputs. These techniques work in concert to enhance both pipeline\nutilization and speculative efficiency. We evaluate FlowSpec on a real-world\ntestbed with other baselines. Experimental results demonstrate that our\nproposed framework significantly improves inference speed across diverse models\nand configurations, achieving speedup ratios 1.28$\\times$-1.79$\\times$ compared\nto baselines. Our code is publicly available at\n\\href{https://github.com/Leosang-lx/FlowSpec#}{https://github.com/Leosang-lx/FlowSpec\\#}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed inference serves as a promising approach to enabling the\ninference of large language models (LLMs) at the network edge. It distributes\nthe inference process to multiple devices to ensure that the LLMs can fit into\nthe device memory. Recent pipeline-based approaches have the potential to\nparallelize communication and computation, which helps reduce inference\nlatency. However, the benefit diminishes when the inference request at the\nnetwork edge is sparse, where pipeline is typically at low utilization. To\nenable efficient distributed LLM inference at the edge, we propose\n\\textbf{FlowSpec}, a pipeline-parallel tree-based speculative decoding\nframework. FlowSpec incorporates three key mechanisms to improve decoding\nefficiency: 1) score-based step-wise verification prioritizes more important\ndraft tokens to bring earlier accpeted tokens; 2) efficient draft management to\nprune invalid tokens while maintaining correct causal relationship during\nverification; 3) dynamic draft expansion strategies to supply high-quality\nspeculative inputs. These techniques work in concert to enhance both pipeline\nutilization and speculative efficiency. We evaluate FlowSpec on a real-world\ntestbed with other baselines. Experimental results demonstrate that our\nproposed framework significantly improves inference speed across diverse models\nand configurations, achieving speedup ratios 1.28$\\times$-1.79$\\times$ compared\nto baselines. Our code is publicly available at\n\\href{https://github.com/Leosang-lx/FlowSpec#}{https://github.com/Leosang-lx/FlowSpec\\#}"
                },
                "authors": [
                    {
                        "name": "Xing Liu"
                    },
                    {
                        "name": "Lizhuo Luo"
                    },
                    {
                        "name": "Ming Tang"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "16 pages, and the last 3 are appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02620v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02620v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03280v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03280v2",
                "updated": "2025-07-14T14:47:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    14,
                    47,
                    47,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-04T03:56:04Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    3,
                    56,
                    4,
                    4,
                    185,
                    0
                ],
                "title": "Modeling Item-Level Dynamic Variability with Residual Diffusion for\n  Bundle Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Item-Level Dynamic Variability with Residual Diffusion for\n  Bundle Recommendation"
                },
                "summary": "Existing solutions for bundle recommendation(BR) have achieved remarkable\neffectiveness for predicting the user's preference for prebuilt bundles.\nHowever, bundle-item(B-I) affiliation will vary dynamically in real scenarios.\nFor example, a bundle themed as 'casual outfit', may add 'hat' or remove\n'watch' due to factors such as seasonal variations, changes in user pes or\ninventory adjustments. Our empirical study demonstrates that the performance of\nmainstream BR models will fluctuate or even decline regarding item-level\nvariability. This paper makes the first attempt to referencaddress the above\nproblem and proposes a novel Residual Diffusion for Bundle\nRecommendation(RDiffBR) as a model-agnostic generative framework which can\nassist a BR model in adapting this scenario. During the initial training of the\nBR model, RDiffBR employs a residual diffusion model to process the item-level\nbundle embeddings which are generated by BR model to represent bundle theme via\na forward-reverse process. In the inference stage, RDiffBR reverses item-level\nbundle embeddings obtained by the well-trained bundle model under B-I\nvariability scenarios to generate the effective item-level bundle embeddings.\nIn particular, the residual connection in our residual approximator\nsignificantly enhances item-level bundle embeddings generation ability of BR\nmodels. Experiments on six BR models and four public datasets from different\ndomains show that RDiffBR improves the performance of Recall and NDCG of\nbackbone BR models by up to 23%, while only increased training time about\n4%.Codes and datasets are available at\nhttps://anonymous.4open.science/r/RDiffBR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing solutions for bundle recommendation(BR) have achieved remarkable\neffectiveness for predicting the user's preference for prebuilt bundles.\nHowever, bundle-item(B-I) affiliation will vary dynamically in real scenarios.\nFor example, a bundle themed as 'casual outfit', may add 'hat' or remove\n'watch' due to factors such as seasonal variations, changes in user pes or\ninventory adjustments. Our empirical study demonstrates that the performance of\nmainstream BR models will fluctuate or even decline regarding item-level\nvariability. This paper makes the first attempt to referencaddress the above\nproblem and proposes a novel Residual Diffusion for Bundle\nRecommendation(RDiffBR) as a model-agnostic generative framework which can\nassist a BR model in adapting this scenario. During the initial training of the\nBR model, RDiffBR employs a residual diffusion model to process the item-level\nbundle embeddings which are generated by BR model to represent bundle theme via\na forward-reverse process. In the inference stage, RDiffBR reverses item-level\nbundle embeddings obtained by the well-trained bundle model under B-I\nvariability scenarios to generate the effective item-level bundle embeddings.\nIn particular, the residual connection in our residual approximator\nsignificantly enhances item-level bundle embeddings generation ability of BR\nmodels. Experiments on six BR models and four public datasets from different\ndomains show that RDiffBR improves the performance of Recall and NDCG of\nbackbone BR models by up to 23%, while only increased training time about\n4%.Codes and datasets are available at\nhttps://anonymous.4open.science/r/RDiffBR."
                },
                "authors": [
                    {
                        "name": "Dong Zhang"
                    },
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Xiaohui Tao"
                    },
                    {
                        "name": "Meng Sun"
                    },
                    {
                        "name": "Jimmy Xiangji Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Xiangji Huang"
                },
                "author": "Jimmy Xiangji Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03280v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03280v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10342v1",
                "updated": "2025-07-14T14:47:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    14,
                    47,
                    1,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T14:47:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    14,
                    47,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "Using AI to replicate human experimental results: a motion study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using AI to replicate human experimental results: a motion study"
                },
                "summary": "This paper explores the potential of large language models (LLMs) as reliable\nanalytical tools in linguistic research, focusing on the emergence of affective\nmeanings in temporal expressions involving manner-of-motion verbs. While LLMs\nlike GPT-4 have shown promise across a range of tasks, their ability to\nreplicate nuanced human judgements remains under scrutiny. We conducted four\npsycholinguistic studies (on emergent meanings, valence shifts, verb choice in\nemotional contexts, and sentence-emoji associations) first with human\nparticipants and then replicated the same tasks using an LLM. Results across\nall studies show a striking convergence between human and AI responses, with\nstatistical analyses (e.g., Spearman's rho = .73-.96) indicating strong\ncorrelations in both rating patterns and categorical choices. While minor\ndivergences were observed in some cases, these did not alter the overall\ninterpretative outcomes. These findings offer compelling evidence that LLMs can\naugment traditional human-based experimentation, enabling broader-scale studies\nwithout compromising interpretative validity. This convergence not only\nstrengthens the empirical foundation of prior human-based findings but also\nopens possibilities for hypothesis generation and data expansion through AI.\nUltimately, our study supports the use of LLMs as credible and informative\ncollaborators in linguistic inquiry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of large language models (LLMs) as reliable\nanalytical tools in linguistic research, focusing on the emergence of affective\nmeanings in temporal expressions involving manner-of-motion verbs. While LLMs\nlike GPT-4 have shown promise across a range of tasks, their ability to\nreplicate nuanced human judgements remains under scrutiny. We conducted four\npsycholinguistic studies (on emergent meanings, valence shifts, verb choice in\nemotional contexts, and sentence-emoji associations) first with human\nparticipants and then replicated the same tasks using an LLM. Results across\nall studies show a striking convergence between human and AI responses, with\nstatistical analyses (e.g., Spearman's rho = .73-.96) indicating strong\ncorrelations in both rating patterns and categorical choices. While minor\ndivergences were observed in some cases, these did not alter the overall\ninterpretative outcomes. These findings offer compelling evidence that LLMs can\naugment traditional human-based experimentation, enabling broader-scale studies\nwithout compromising interpretative validity. This convergence not only\nstrengthens the empirical foundation of prior human-based findings but also\nopens possibilities for hypothesis generation and data expansion through AI.\nUltimately, our study supports the use of LLMs as credible and informative\ncollaborators in linguistic inquiry."
                },
                "authors": [
                    {
                        "name": "Rosa Illan Castillo"
                    },
                    {
                        "name": "Javier Valenzuela"
                    }
                ],
                "author_detail": {
                    "name": "Javier Valenzuela"
                },
                "author": "Javier Valenzuela",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10338v1",
                "updated": "2025-07-14T14:43:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    14,
                    43,
                    14,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T14:43:14Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    14,
                    43,
                    14,
                    0,
                    195,
                    0
                ],
                "title": "AssertCoder: LLM-Based Assertion Generation via Multimodal Specification\n  Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AssertCoder: LLM-Based Assertion Generation via Multimodal Specification\n  Extraction"
                },
                "summary": "Assertion-Based Verification (ABV) is critical for ensuring functional\ncorrectness in modern hardware systems. However, manually writing high-quality\nSVAs remains labor-intensive and error-prone. To bridge this gap, we propose\nAssertCoder, a novel unified framework that automatically generates\nhigh-quality SVAs directly from multimodal hardware design specifications.\nAssertCoder employs a modality-sensitive preprocessing to parse heterogeneous\nspecification formats (text, tables, diagrams, and formulas), followed by a set\nof dedicated semantic analyzers that extract structured representations aligned\nwith signal-level semantics. These representations are utilized to drive\nassertion synthesis via multi-step chain-of-thought (CoT) prompting. The\nframework incorporates a mutation-based evaluation approach to assess assertion\nquality via model checking and further refine the generated assertions.\nExperimental evaluation across three real-world Register-Transfer Level (RTL)\ndesigns demonstrates AssertCoder's superior performance, achieving an average\nincrease of 8.4% in functional correctness and 5.8% in mutation detection\ncompared to existing state-of-the-art approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assertion-Based Verification (ABV) is critical for ensuring functional\ncorrectness in modern hardware systems. However, manually writing high-quality\nSVAs remains labor-intensive and error-prone. To bridge this gap, we propose\nAssertCoder, a novel unified framework that automatically generates\nhigh-quality SVAs directly from multimodal hardware design specifications.\nAssertCoder employs a modality-sensitive preprocessing to parse heterogeneous\nspecification formats (text, tables, diagrams, and formulas), followed by a set\nof dedicated semantic analyzers that extract structured representations aligned\nwith signal-level semantics. These representations are utilized to drive\nassertion synthesis via multi-step chain-of-thought (CoT) prompting. The\nframework incorporates a mutation-based evaluation approach to assess assertion\nquality via model checking and further refine the generated assertions.\nExperimental evaluation across three real-world Register-Transfer Level (RTL)\ndesigns demonstrates AssertCoder's superior performance, achieving an average\nincrease of 8.4% in functional correctness and 5.8% in mutation detection\ncompared to existing state-of-the-art approaches."
                },
                "authors": [
                    {
                        "name": "Enyuan Tian"
                    },
                    {
                        "name": "Yiwei Ci"
                    },
                    {
                        "name": "Qiusong Yang"
                    },
                    {
                        "name": "Yufeng Li"
                    },
                    {
                        "name": "Zhichao Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Lyu"
                },
                "author": "Zhichao Lyu",
                "arxiv_comment": "7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22791v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22791v3",
                "updated": "2025-07-15T12:59:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    59,
                    47,
                    1,
                    196,
                    0
                ],
                "published": "2025-06-28T07:25:12Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    7,
                    25,
                    12,
                    5,
                    179,
                    0
                ],
                "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models"
                },
                "summary": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications."
                },
                "authors": [
                    {
                        "name": "Jianxin Yan"
                    },
                    {
                        "name": "Wangze Ni"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22791v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22791v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10326v1",
                "updated": "2025-07-14T14:34:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    14,
                    34,
                    15,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T14:34:15Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    14,
                    34,
                    15,
                    0,
                    195,
                    0
                ],
                "title": "Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation"
                },
                "summary": "Prompt engineering has proven to be a crucial step in leveraging pretrained\nlarge language models (LLMs) in solving various real-world tasks. Numerous\nsolutions have been proposed that seek to automate prompt engineering by using\nthe model itself to edit prompts. However, the majority of state-of-the-art\napproaches are evaluated on tasks that require minimal prompt templates and on\nvery large and highly capable LLMs. In contrast, solving complex tasks that\nrequire detailed information to be included in the prompt increases the amount\nof text that needs to be optimised. Furthermore, smaller models have been shown\nto be more sensitive to prompt design. To address these challenges, we propose\nan evolutionary search approach to automated discrete prompt optimisation\nconsisting of two phases. In the first phase, grammar-guided genetic\nprogramming is invoked to synthesise prompt-creating programmes by searching\nthe space of programmes populated by function compositions of syntactic,\ndictionary-based and LLM-based prompt-editing functions. In the second phase,\nlocal search is applied to explore the neighbourhoods of best-performing\nprogrammes in an attempt to further fine-tune their performance. Our approach\noutperforms three state-of-the-art prompt optimisation approaches,\nPromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose\nLLMs in four domain-specific challenging tasks. We also illustrate several\nexamples where these benchmark methods suffer relatively severe performance\ndegradation, while our approach improves performance in almost all task-model\ncombinations, only incurring minimal degradation when it does not.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt engineering has proven to be a crucial step in leveraging pretrained\nlarge language models (LLMs) in solving various real-world tasks. Numerous\nsolutions have been proposed that seek to automate prompt engineering by using\nthe model itself to edit prompts. However, the majority of state-of-the-art\napproaches are evaluated on tasks that require minimal prompt templates and on\nvery large and highly capable LLMs. In contrast, solving complex tasks that\nrequire detailed information to be included in the prompt increases the amount\nof text that needs to be optimised. Furthermore, smaller models have been shown\nto be more sensitive to prompt design. To address these challenges, we propose\nan evolutionary search approach to automated discrete prompt optimisation\nconsisting of two phases. In the first phase, grammar-guided genetic\nprogramming is invoked to synthesise prompt-creating programmes by searching\nthe space of programmes populated by function compositions of syntactic,\ndictionary-based and LLM-based prompt-editing functions. In the second phase,\nlocal search is applied to explore the neighbourhoods of best-performing\nprogrammes in an attempt to further fine-tune their performance. Our approach\noutperforms three state-of-the-art prompt optimisation approaches,\nPromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose\nLLMs in four domain-specific challenging tasks. We also illustrate several\nexamples where these benchmark methods suffer relatively severe performance\ndegradation, while our approach improves performance in almost all task-model\ncombinations, only incurring minimal degradation when it does not."
                },
                "authors": [
                    {
                        "name": "Muzhaffar Hazman"
                    },
                    {
                        "name": "Minh-Khoi Pham"
                    },
                    {
                        "name": "Shweta Soundararajan"
                    },
                    {
                        "name": "Goncalo Mordido"
                    },
                    {
                        "name": "Leonardo Custode"
                    },
                    {
                        "name": "David Lynch"
                    },
                    {
                        "name": "Giorgio Cruciata"
                    },
                    {
                        "name": "Yucheng Shi"
                    },
                    {
                        "name": "Hongmeng Song"
                    },
                    {
                        "name": "Wang Chao"
                    },
                    {
                        "name": "Pan Yue"
                    },
                    {
                        "name": "Aleksandar Milenovic"
                    },
                    {
                        "name": "Alexandros Agapitos"
                    }
                ],
                "author_detail": {
                    "name": "Alexandros Agapitos"
                },
                "author": "Alexandros Agapitos",
                "arxiv_comment": "Accepted for Publication at ECAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04225v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04225v2",
                "updated": "2025-07-14T14:33:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    14,
                    33,
                    47,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-06T03:30:45Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    3,
                    30,
                    45,
                    6,
                    187,
                    0
                ],
                "title": "Zero-Shot Cyclic Peptide Design via Composable Geometric Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Cyclic Peptide Design via Composable Geometric Constraints"
                },
                "summary": "Cyclic peptides, characterized by geometric constraints absent in linear\npeptides, offer enhanced biochemical properties, presenting new opportunities\nto address unmet medical needs. However, designing target-specific cyclic\npeptides remains underexplored due to limited training data. To bridge the gap,\nwe propose CP-Composer, a novel generative framework that enables zero-shot\ncyclic peptide generation via composable geometric constraints. Our approach\ndecomposes complex cyclization patterns into unit constraints, which are\nincorporated into a diffusion model through geometric conditioning on nodes and\nedges. During training, the model learns from unit constraints and their random\ncombinations in linear peptides, while at inference, novel constraint\ncombinations required for cyclization are imposed as input. Experiments show\nthat our model, despite trained with linear peptides, is capable of generating\ndiverse target-binding cyclic peptides, reaching success rates from 38% to 84%\non different cyclization strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyclic peptides, characterized by geometric constraints absent in linear\npeptides, offer enhanced biochemical properties, presenting new opportunities\nto address unmet medical needs. However, designing target-specific cyclic\npeptides remains underexplored due to limited training data. To bridge the gap,\nwe propose CP-Composer, a novel generative framework that enables zero-shot\ncyclic peptide generation via composable geometric constraints. Our approach\ndecomposes complex cyclization patterns into unit constraints, which are\nincorporated into a diffusion model through geometric conditioning on nodes and\nedges. During training, the model learns from unit constraints and their random\ncombinations in linear peptides, while at inference, novel constraint\ncombinations required for cyclization are imposed as input. Experiments show\nthat our model, despite trained with linear peptides, is capable of generating\ndiverse target-binding cyclic peptides, reaching success rates from 38% to 84%\non different cyclization strategies."
                },
                "authors": [
                    {
                        "name": "Dapeng Jiang"
                    },
                    {
                        "name": "Xiangzhe Kong"
                    },
                    {
                        "name": "Jiaqi Han"
                    },
                    {
                        "name": "Mingyu Li"
                    },
                    {
                        "name": "Rui Jiao"
                    },
                    {
                        "name": "Wenbing Huang"
                    },
                    {
                        "name": "Stefano Ermon"
                    },
                    {
                        "name": "Jianzhu Ma"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04225v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04225v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12864v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12864v3",
                "updated": "2025-07-14T14:30:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    14,
                    30,
                    57,
                    0,
                    195,
                    0
                ],
                "published": "2025-05-19T08:48:12Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    8,
                    48,
                    12,
                    0,
                    139,
                    0
                ],
                "title": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams"
                },
                "summary": "Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a\nnovel benchmark derived from 340 law exams spanning 116 law school courses\nacross a range of subjects and degree levels. The dataset comprises 4,886 law\nexam questions in English and German, including 2,841 long-form, open-ended\nquestions and 2,045 multiple-choice questions. Besides reference answers, the\nopen questions are also accompanied by explicit guidance outlining the expected\nlegal reasoning approach such as issue spotting, rule recall, or rule\napplication. Our evaluation on both open-ended and multiple-choice questions\npresent significant challenges for current LLMs; in particular, they notably\nstruggle with open questions that require structured, multi-step legal\nreasoning. Moreover, our results underscore the effectiveness of the dataset in\ndifferentiating between models with varying capabilities. Adopting an\nLLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate\nhow model-generated reasoning steps can be evaluated consistently and\naccurately. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. Project page:\nhttps://lexam-benchmark.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a\nnovel benchmark derived from 340 law exams spanning 116 law school courses\nacross a range of subjects and degree levels. The dataset comprises 4,886 law\nexam questions in English and German, including 2,841 long-form, open-ended\nquestions and 2,045 multiple-choice questions. Besides reference answers, the\nopen questions are also accompanied by explicit guidance outlining the expected\nlegal reasoning approach such as issue spotting, rule recall, or rule\napplication. Our evaluation on both open-ended and multiple-choice questions\npresent significant challenges for current LLMs; in particular, they notably\nstruggle with open questions that require structured, multi-step legal\nreasoning. Moreover, our results underscore the effectiveness of the dataset in\ndifferentiating between models with varying capabilities. Adopting an\nLLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate\nhow model-generated reasoning steps can be evaluated consistently and\naccurately. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. Project page:\nhttps://lexam-benchmark.github.io/"
                },
                "authors": [
                    {
                        "name": "Yu Fan"
                    },
                    {
                        "name": "Jingwei Ni"
                    },
                    {
                        "name": "Jakob Merane"
                    },
                    {
                        "name": "Etienne Salimbeni"
                    },
                    {
                        "name": "Yang Tian"
                    },
                    {
                        "name": "Yoan Hermstr√ºwer"
                    },
                    {
                        "name": "Yinya Huang"
                    },
                    {
                        "name": "Mubashara Akhtar"
                    },
                    {
                        "name": "Florian Geering"
                    },
                    {
                        "name": "Oliver Dreyer"
                    },
                    {
                        "name": "Daniel Brunner"
                    },
                    {
                        "name": "Markus Leippold"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Alexander Stremitzer"
                    },
                    {
                        "name": "Christoph Engel"
                    },
                    {
                        "name": "Elliott Ash"
                    },
                    {
                        "name": "Joel Niklaus"
                    }
                ],
                "author_detail": {
                    "name": "Joel Niklaus"
                },
                "author": "Joel Niklaus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12864v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12864v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10317v1",
                "updated": "2025-07-14T14:25:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    14,
                    25,
                    17,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T14:25:17Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    14,
                    25,
                    17,
                    0,
                    195,
                    0
                ],
                "title": "Gaussian Process Methods for Very Large Astrometric Data Sets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Process Methods for Very Large Astrometric Data Sets"
                },
                "summary": "We present a novel non-parametric method for inferring smooth models of the\nmean velocity field and velocity dispersion tensor of the Milky Way from\nastrometric data. Our approach is based on Stochastic Variational Gaussian\nProcess Regression (SVGPR) and provides an attractive alternative to binning\nprocedures. SVGPR is an approximation to standard GPR, the latter of which\nsuffers severe computational scaling with N and assumes independently\ndistributed Gaussian Noise. In the Galaxy however, velocity measurements\nexhibit scatter from both observational uncertainty and the intrinsic velocity\ndispersion of the distribution function. We exploit the factorization property\nof the objective function in SVGPR to simultaneously model both the mean\nvelocity field and velocity dispersion tensor as separate Gaussian Processes.\nThis achieves a computational complexity of O(M^3) versus GPR's O(N^3), where M\n<< N is a subset of points chosen in a principled way to summarize the data.\nApplied to a sample of ~8 x 10^5 stars from the Gaia DR3 Radial Velocity\nSurvey, we construct differentiable profiles of the mean velocity and velocity\ndispersion as functions of height above the Galactic midplane. We find\nasymmetric features in all three diagonal components of the velocity dispersion\ntensor, providing evidence that the vertical dynamics of the Milky Way are in a\nstate of disequilibrium. Furthermore, our dispersion profiles exhibit\ncorrelated structures at several locations in |z|, which we interpret as\nsignatures of the Gaia phase spiral. These results demonstrate that our method\nprovides a promising direction for data-driven analyses of Galactic dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel non-parametric method for inferring smooth models of the\nmean velocity field and velocity dispersion tensor of the Milky Way from\nastrometric data. Our approach is based on Stochastic Variational Gaussian\nProcess Regression (SVGPR) and provides an attractive alternative to binning\nprocedures. SVGPR is an approximation to standard GPR, the latter of which\nsuffers severe computational scaling with N and assumes independently\ndistributed Gaussian Noise. In the Galaxy however, velocity measurements\nexhibit scatter from both observational uncertainty and the intrinsic velocity\ndispersion of the distribution function. We exploit the factorization property\nof the objective function in SVGPR to simultaneously model both the mean\nvelocity field and velocity dispersion tensor as separate Gaussian Processes.\nThis achieves a computational complexity of O(M^3) versus GPR's O(N^3), where M\n<< N is a subset of points chosen in a principled way to summarize the data.\nApplied to a sample of ~8 x 10^5 stars from the Gaia DR3 Radial Velocity\nSurvey, we construct differentiable profiles of the mean velocity and velocity\ndispersion as functions of height above the Galactic midplane. We find\nasymmetric features in all three diagonal components of the velocity dispersion\ntensor, providing evidence that the vertical dynamics of the Milky Way are in a\nstate of disequilibrium. Furthermore, our dispersion profiles exhibit\ncorrelated structures at several locations in |z|, which we interpret as\nsignatures of the Gaia phase spiral. These results demonstrate that our method\nprovides a promising direction for data-driven analyses of Galactic dynamics."
                },
                "authors": [
                    {
                        "name": "Timothy Hapitas"
                    },
                    {
                        "name": "Lawrence M. Widrow"
                    },
                    {
                        "name": "Thavisha E. Dharmawardena"
                    },
                    {
                        "name": "Daniel Foreman-Mackey"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Foreman-Mackey"
                },
                "author": "Daniel Foreman-Mackey",
                "arxiv_comment": "25 pages, 12 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10311v1",
                "updated": "2025-07-14T14:15:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    14,
                    15,
                    47,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T14:15:47Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    14,
                    15,
                    47,
                    0,
                    195,
                    0
                ],
                "title": "Recognizing Dementia from Neuropsychological Tests with State Space\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recognizing Dementia from Neuropsychological Tests with State Space\n  Models"
                },
                "summary": "Early detection of dementia is critical for timely medical intervention and\nimproved patient outcomes. Neuropsychological tests are widely used for\ncognitive assessment but have traditionally relied on manual scoring. Automatic\ndementia classification (ADC) systems aim to infer cognitive decline directly\nfrom speech recordings of such tests. We propose Demenba, a novel ADC framework\nbased on state space models, which scale linearly in memory and computation\nwith sequence length. Trained on over 1,000 hours of cognitive assessments\nadministered to Framingham Heart Study participants, some of whom were\ndiagnosed with dementia through adjudicated review, our method outperforms\nprior approaches in fine-grained dementia classification by 21\\%, while using\nfewer parameters. We further analyze its scaling behavior and demonstrate that\nour model gains additional improvement when fused with large language models,\npaving the way for more transparent and scalable dementia assessment tools.\nCode: https://anonymous.4open.science/r/Demenba-0861",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early detection of dementia is critical for timely medical intervention and\nimproved patient outcomes. Neuropsychological tests are widely used for\ncognitive assessment but have traditionally relied on manual scoring. Automatic\ndementia classification (ADC) systems aim to infer cognitive decline directly\nfrom speech recordings of such tests. We propose Demenba, a novel ADC framework\nbased on state space models, which scale linearly in memory and computation\nwith sequence length. Trained on over 1,000 hours of cognitive assessments\nadministered to Framingham Heart Study participants, some of whom were\ndiagnosed with dementia through adjudicated review, our method outperforms\nprior approaches in fine-grained dementia classification by 21\\%, while using\nfewer parameters. We further analyze its scaling behavior and demonstrate that\nour model gains additional improvement when fused with large language models,\npaving the way for more transparent and scalable dementia assessment tools.\nCode: https://anonymous.4open.science/r/Demenba-0861"
                },
                "authors": [
                    {
                        "name": "Liming Wang"
                    },
                    {
                        "name": "Saurabhchand Bhati"
                    },
                    {
                        "name": "Cody Karjadi"
                    },
                    {
                        "name": "Rhoda Au"
                    },
                    {
                        "name": "James Glass"
                    }
                ],
                "author_detail": {
                    "name": "James Glass"
                },
                "author": "James Glass",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05649v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05649v2",
                "updated": "2025-07-14T14:12:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    14,
                    12,
                    59,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-08T04:01:53Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    4,
                    1,
                    53,
                    1,
                    189,
                    0
                ],
                "title": "DESIGN: Encrypted GNN Inference via Server-Side Input Graph Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DESIGN: Encrypted GNN Inference via Server-Side Input Graph Pruning"
                },
                "summary": "Graph Neural Networks (GNNs) have achieved state-of-the-art performance in\nvarious graph-based learning tasks. However, enabling privacy-preserving GNNs\nin encrypted domains, such as under Fully Homomorphic Encryption (FHE),\ntypically incurs substantial computational overhead, rendering real-time and\nprivacy-preserving inference impractical. In this work, we propose DESIGN\n(EncrypteD GNN Inference via sErver-Side Input Graph pruNing), a novel\nframework for efficient encrypted GNN inference. DESIGN tackles the critical\nefficiency limitations of existing FHE GNN approaches, which often overlook\ninput data redundancy and apply uniform computational strategies. Our framework\nachieves significant performance gains through a hierarchical optimization\nstrategy executed entirely on the server: first, FHE-compatible node importance\nscores (based on encrypted degree statistics) are computed from the encrypted\ngraph. These scores then guide a homomorphic partitioning process, generating\nmulti-level importance masks directly under FHE. This dynamically generated\nmask facilitates both input graph pruning (by logically removing unimportant\nelements) and a novel adaptive polynomial activation scheme, where activation\ncomplexity is tailored to node importance levels. Empirical evaluations\ndemonstrate that DESIGN substantially accelerates FHE GNN inference compared to\nstate-of-the-art methods while maintaining competitive model accuracy,\npresenting a robust solution for secure graph analytics. Our implementation is\npublicly available at https://github.com/LabRAI/DESIGN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have achieved state-of-the-art performance in\nvarious graph-based learning tasks. However, enabling privacy-preserving GNNs\nin encrypted domains, such as under Fully Homomorphic Encryption (FHE),\ntypically incurs substantial computational overhead, rendering real-time and\nprivacy-preserving inference impractical. In this work, we propose DESIGN\n(EncrypteD GNN Inference via sErver-Side Input Graph pruNing), a novel\nframework for efficient encrypted GNN inference. DESIGN tackles the critical\nefficiency limitations of existing FHE GNN approaches, which often overlook\ninput data redundancy and apply uniform computational strategies. Our framework\nachieves significant performance gains through a hierarchical optimization\nstrategy executed entirely on the server: first, FHE-compatible node importance\nscores (based on encrypted degree statistics) are computed from the encrypted\ngraph. These scores then guide a homomorphic partitioning process, generating\nmulti-level importance masks directly under FHE. This dynamically generated\nmask facilitates both input graph pruning (by logically removing unimportant\nelements) and a novel adaptive polynomial activation scheme, where activation\ncomplexity is tailored to node importance levels. Empirical evaluations\ndemonstrate that DESIGN substantially accelerates FHE GNN inference compared to\nstate-of-the-art methods while maintaining competitive model accuracy,\npresenting a robust solution for secure graph analytics. Our implementation is\npublicly available at https://github.com/LabRAI/DESIGN."
                },
                "authors": [
                    {
                        "name": "Kaixiang Zhao"
                    },
                    {
                        "name": "Joseph Yousry Attalla"
                    },
                    {
                        "name": "Qian Lou"
                    },
                    {
                        "name": "Yushun Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yushun Dong"
                },
                "author": "Yushun Dong",
                "arxiv_comment": "Under Review in Conference on Neural Information Processing Systems\n  (NeurIPS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05649v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05649v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10304v2",
                "updated": "2025-07-15T09:06:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    9,
                    6,
                    56,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-14T14:08:37Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    14,
                    8,
                    37,
                    0,
                    195,
                    0
                ],
                "title": "Overlapping signals in next-generation gravitational wave observatories:\n  A recipe for selecting the best parameter estimation technique",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overlapping signals in next-generation gravitational wave observatories:\n  A recipe for selecting the best parameter estimation technique"
                },
                "summary": "Third-generation gravitational wave detectors such as Einstein Telescope and\nCosmic Explorer will have significantly better sensitivities than current\ndetectors, as well as a wider frequency bandwidth. This will increase the\nnumber and duration of the observed signals, leading to many signals\noverlapping in time. If not adequately accounted for, this can lead to biases\nin parameter estimation. In this work, we combine the joint parameter\nestimation method with relative binning to handle full parameter inference on\noverlapping signals from binary black holes, including precession effects and\nhigher-order mode content. As this method is computationally more expensive\nthan traditional single-signal parameter estimation, we test a prior-informed\nFisher matrix and a time-frequency overlap method for estimating expected bias\nto help us decide when joint parameter estimation is necessary over the simpler\nmethods. We improve upon previous Fisher matrix implementations by including\nthe prior information and performing an optimization routine to better locate\nthe maximum likelihood point point, but we still find the method unreliable.\nThe time-frequency method is accurate in 86% of close binary black hole\nmergers. We end by developing our own method of estimating bias due overlaps,\nwhere we reweight the single signal parameter estimation posterior to quantify\nhow much the overlapping signals affect it. We show it has 99% accuracy for\nzero noise injections (98% in Gaussian noise), at the cost of one additional\nstandard sampling run when joint parameter estimation proves to be necessary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Third-generation gravitational wave detectors such as Einstein Telescope and\nCosmic Explorer will have significantly better sensitivities than current\ndetectors, as well as a wider frequency bandwidth. This will increase the\nnumber and duration of the observed signals, leading to many signals\noverlapping in time. If not adequately accounted for, this can lead to biases\nin parameter estimation. In this work, we combine the joint parameter\nestimation method with relative binning to handle full parameter inference on\noverlapping signals from binary black holes, including precession effects and\nhigher-order mode content. As this method is computationally more expensive\nthan traditional single-signal parameter estimation, we test a prior-informed\nFisher matrix and a time-frequency overlap method for estimating expected bias\nto help us decide when joint parameter estimation is necessary over the simpler\nmethods. We improve upon previous Fisher matrix implementations by including\nthe prior information and performing an optimization routine to better locate\nthe maximum likelihood point point, but we still find the method unreliable.\nThe time-frequency method is accurate in 86% of close binary black hole\nmergers. We end by developing our own method of estimating bias due overlaps,\nwhere we reweight the single signal parameter estimation posterior to quantify\nhow much the overlapping signals affect it. We show it has 99% accuracy for\nzero noise injections (98% in Gaussian noise), at the cost of one additional\nstandard sampling run when joint parameter estimation proves to be necessary."
                },
                "authors": [
                    {
                        "name": "Tomasz Baka"
                    },
                    {
                        "name": "Harsh Narola"
                    },
                    {
                        "name": "Justin Janquart"
                    },
                    {
                        "name": "Anuradha Samajdar"
                    },
                    {
                        "name": "Tim Dietrich"
                    },
                    {
                        "name": "Chris Van Den Broeck"
                    }
                ],
                "author_detail": {
                    "name": "Chris Van Den Broeck"
                },
                "author": "Chris Van Den Broeck",
                "arxiv_comment": "18 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10302v1",
                "updated": "2025-07-14T14:05:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    14,
                    5,
                    19,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T14:05:19Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    14,
                    5,
                    19,
                    0,
                    195,
                    0
                ],
                "title": "DisCo: Towards Distinct and Coherent Visual Encapsulation in Video MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DisCo: Towards Distinct and Coherent Visual Encapsulation in Video MLLMs"
                },
                "summary": "In video Multimodal Large Language Models (video MLLMs), the visual\nencapsulation process plays a pivotal role in converting video contents into\nrepresentative tokens for LLM input. While linear projectors are widely\nemployed for encapsulation, they introduce semantic indistinctness and temporal\nincoherence when applied to videos. Conversely, the structure of resamplers\nshows promise in tackling these challenges, but an effective solution remains\nunexplored. Drawing inspiration from resampler structures, we introduce DisCo,\na novel visual encapsulation method designed to yield semantically distinct and\ntemporally coherent visual tokens for video MLLMs. DisCo integrates two key\ncomponents: (1) A Visual Concept Discriminator (VCD) module, assigning unique\nsemantics for visual tokens by associating them in pair with discriminative\nconcepts in the video. (2) A Temporal Focus Calibrator (TFC) module, ensuring\nconsistent temporal focus of visual tokens to video elements across every video\nframe. Through extensive experiments on multiple video MLLM frameworks, we\ndemonstrate that DisCo remarkably outperforms previous state-of-the-art methods\nacross a variety of video understanding benchmarks, while also achieving higher\ntoken efficiency thanks to the reduction of semantic indistinctness. The code:\nhttps://github.com/ZJHTerry18/DisCo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In video Multimodal Large Language Models (video MLLMs), the visual\nencapsulation process plays a pivotal role in converting video contents into\nrepresentative tokens for LLM input. While linear projectors are widely\nemployed for encapsulation, they introduce semantic indistinctness and temporal\nincoherence when applied to videos. Conversely, the structure of resamplers\nshows promise in tackling these challenges, but an effective solution remains\nunexplored. Drawing inspiration from resampler structures, we introduce DisCo,\na novel visual encapsulation method designed to yield semantically distinct and\ntemporally coherent visual tokens for video MLLMs. DisCo integrates two key\ncomponents: (1) A Visual Concept Discriminator (VCD) module, assigning unique\nsemantics for visual tokens by associating them in pair with discriminative\nconcepts in the video. (2) A Temporal Focus Calibrator (TFC) module, ensuring\nconsistent temporal focus of visual tokens to video elements across every video\nframe. Through extensive experiments on multiple video MLLM frameworks, we\ndemonstrate that DisCo remarkably outperforms previous state-of-the-art methods\nacross a variety of video understanding benchmarks, while also achieving higher\ntoken efficiency thanks to the reduction of semantic indistinctness. The code:\nhttps://github.com/ZJHTerry18/DisCo."
                },
                "authors": [
                    {
                        "name": "Jiahe Zhao"
                    },
                    {
                        "name": "Rongkun Zheng"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Helin Wang"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hengshuang Zhao"
                },
                "author": "Hengshuang Zhao",
                "arxiv_comment": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10284v1",
                "updated": "2025-07-14T13:51:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    13,
                    51,
                    28,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T13:51:28Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    13,
                    51,
                    28,
                    0,
                    195,
                    0
                ],
                "title": "Prompt Informed Reinforcement Learning for Visual Coverage Path Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Informed Reinforcement Learning for Visual Coverage Path Planning"
                },
                "summary": "Visual coverage path planning with unmanned aerial vehicles (UAVs) requires\nagents to strategically coordinate UAV motion and camera control to maximize\ncoverage, minimize redundancy, and maintain battery efficiency. Traditional\nreinforcement learning (RL) methods rely on environment-specific reward\nformulations that lack semantic adaptability. This study proposes\nPrompt-Informed Reinforcement Learning (PIRL), a novel approach that integrates\nthe zero-shot reasoning ability and in-context learning capability of large\nlanguage models with curiosity-driven RL. PIRL leverages semantic feedback from\nan LLM, GPT-3.5, to dynamically shape the reward function of the Proximal\nPolicy Optimization (PPO) RL policy guiding the agent in position and camera\nadjustments for optimal visual coverage. The PIRL agent is trained using OpenAI\nGym and evaluated in various environments. Furthermore, the sim-to-real-like\nability and zero-shot generalization of the agent are tested by operating the\nagent in Webots simulator which introduces realistic physical dynamics. Results\nshow that PIRL outperforms multiple learning-based baselines such as PPO with\nstatic rewards, PPO with exploratory weight initialization, imitation learning,\nand an LLM-only controller. Across different environments, PIRL outperforms the\nbest-performing baseline by achieving up to 14% higher visual coverage in\nOpenAI Gym and 27% higher in Webots, up to 25% higher battery efficiency, and\nup to 18\\% lower redundancy, depending on the environment. The results\nhighlight the effectiveness of LLM-guided reward shaping in complex spatial\nexploration tasks and suggest a promising direction for integrating natural\nlanguage priors into RL for robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual coverage path planning with unmanned aerial vehicles (UAVs) requires\nagents to strategically coordinate UAV motion and camera control to maximize\ncoverage, minimize redundancy, and maintain battery efficiency. Traditional\nreinforcement learning (RL) methods rely on environment-specific reward\nformulations that lack semantic adaptability. This study proposes\nPrompt-Informed Reinforcement Learning (PIRL), a novel approach that integrates\nthe zero-shot reasoning ability and in-context learning capability of large\nlanguage models with curiosity-driven RL. PIRL leverages semantic feedback from\nan LLM, GPT-3.5, to dynamically shape the reward function of the Proximal\nPolicy Optimization (PPO) RL policy guiding the agent in position and camera\nadjustments for optimal visual coverage. The PIRL agent is trained using OpenAI\nGym and evaluated in various environments. Furthermore, the sim-to-real-like\nability and zero-shot generalization of the agent are tested by operating the\nagent in Webots simulator which introduces realistic physical dynamics. Results\nshow that PIRL outperforms multiple learning-based baselines such as PPO with\nstatic rewards, PPO with exploratory weight initialization, imitation learning,\nand an LLM-only controller. Across different environments, PIRL outperforms the\nbest-performing baseline by achieving up to 14% higher visual coverage in\nOpenAI Gym and 27% higher in Webots, up to 25% higher battery efficiency, and\nup to 18\\% lower redundancy, depending on the environment. The results\nhighlight the effectiveness of LLM-guided reward shaping in complex spatial\nexploration tasks and suggest a promising direction for integrating natural\nlanguage priors into RL for robotics."
                },
                "authors": [
                    {
                        "name": "Venkat Margapuri"
                    }
                ],
                "author_detail": {
                    "name": "Venkat Margapuri"
                },
                "author": "Venkat Margapuri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10281v1",
                "updated": "2025-07-14T13:48:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    13,
                    48,
                    13,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T13:48:13Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    13,
                    48,
                    13,
                    0,
                    195,
                    0
                ],
                "title": "Toward Real-World Table Agents: Capabilities, Workflows, and Design\n  Principles for LLM-based Table Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Real-World Table Agents: Capabilities, Workflows, and Design\n  Principles for LLM-based Table Intelligence"
                },
                "summary": "Tables are fundamental in domains such as finance, healthcare, and public\nadministration, yet real-world table tasks often involve noise, structural\nheterogeneity, and semantic complexity--issues underexplored in existing\nresearch that primarily targets clean academic datasets. This survey focuses on\nLLM-based Table Agents, which aim to automate table-centric workflows by\nintegrating preprocessing, reasoning, and domain adaptation. We define five\ncore competencies--C1: Table Structure Understanding, C2: Table and Query\nSemantic Understanding, C3: Table Retrieval and Compression, C4: Executable\nReasoning with Traceability, and C5: Cross-Domain Generalization--to analyze\nand compare current approaches. In addition, a detailed examination of the\nText-to-SQL Agent reveals a performance gap between academic benchmarks and\nreal-world scenarios, especially for open-source models. Finally, we provide\nactionable insights to improve the robustness, generalization, and efficiency\nof LLM-based Table Agents in practical settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tables are fundamental in domains such as finance, healthcare, and public\nadministration, yet real-world table tasks often involve noise, structural\nheterogeneity, and semantic complexity--issues underexplored in existing\nresearch that primarily targets clean academic datasets. This survey focuses on\nLLM-based Table Agents, which aim to automate table-centric workflows by\nintegrating preprocessing, reasoning, and domain adaptation. We define five\ncore competencies--C1: Table Structure Understanding, C2: Table and Query\nSemantic Understanding, C3: Table Retrieval and Compression, C4: Executable\nReasoning with Traceability, and C5: Cross-Domain Generalization--to analyze\nand compare current approaches. In addition, a detailed examination of the\nText-to-SQL Agent reveals a performance gap between academic benchmarks and\nreal-world scenarios, especially for open-source models. Finally, we provide\nactionable insights to improve the robustness, generalization, and efficiency\nof LLM-based Table Agents in practical settings."
                },
                "authors": [
                    {
                        "name": "Jiaming Tian"
                    },
                    {
                        "name": "Liyao Li"
                    },
                    {
                        "name": "Wentao Ye"
                    },
                    {
                        "name": "Haobo Wang"
                    },
                    {
                        "name": "Lingxin Wang"
                    },
                    {
                        "name": "Lihua Yu"
                    },
                    {
                        "name": "Zujie Ren"
                    },
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Junbo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Junbo Zhao"
                },
                "author": "Junbo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10276v1",
                "updated": "2025-07-14T13:45:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    13,
                    45,
                    13,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T13:45:13Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    13,
                    45,
                    13,
                    0,
                    195,
                    0
                ],
                "title": "A Method for Testing Diffusive Shock Acceleration and Diffusion\n  Propagation of 1-100 TeV Cosmic Electron with Multi-wavelength Observation of\n  Geminga Halo and Pulsar Wind Nebula",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Method for Testing Diffusive Shock Acceleration and Diffusion\n  Propagation of 1-100 TeV Cosmic Electron with Multi-wavelength Observation of\n  Geminga Halo and Pulsar Wind Nebula"
                },
                "summary": "Diffusive shock acceleration and diffusion propagation are essential\ncomponents of the standard cosmic ray model. These theories are based on\nextensive observations of high-energy solar processes, providing substantial\ndirect evidence in the MeV energy range. Although the model is widely and\nsuccessfully used to explain high-energy cosmic phenomena, direct validation\nhas been elusive. The multi-wavelength spectra and angular profile measurements\nof the Geminga pulsar wind nebula and its pulsar halo, particularly the precise\nspectral observations by HAWC and LHAASO-KM2A in recent years, offer a rare\nopportunity to test these theories with cosmic rays energies between 1 TeV and\nseveral hundred TeV. These observations are expected to elevate the direct\ntesting of theoretical models from multi-MeV to sub-PeV energies. In this work,\na method is developed to test the diffusive shock acceleration and diffusion\npropagation model between one and several hundred TeV energies through the\nlatest spectral and morphological data of the Geminga region from HAWC and\nFermi-LAT. Our results show that the theories of diffusive shock acceleration\nand diffusion propagation are consistent with experimental observations.\nHowever, the published morphological data adopted rather wide energy bins and\ncurrently do not allow a high precision test of the inferred energy dependent\ndiffusion coefficient by observed energy spectra with DSA theory. It is\nanticipated that future HAWC and LHAASO-KM2A observations will yield\nhigher-precision results, and the confirmation of a rapidly increasing\ndiffusion coefficient above 100 TeV would serve as important evidence\nsupporting the diffusive shock acceleration and diffusion propagation theory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusive shock acceleration and diffusion propagation are essential\ncomponents of the standard cosmic ray model. These theories are based on\nextensive observations of high-energy solar processes, providing substantial\ndirect evidence in the MeV energy range. Although the model is widely and\nsuccessfully used to explain high-energy cosmic phenomena, direct validation\nhas been elusive. The multi-wavelength spectra and angular profile measurements\nof the Geminga pulsar wind nebula and its pulsar halo, particularly the precise\nspectral observations by HAWC and LHAASO-KM2A in recent years, offer a rare\nopportunity to test these theories with cosmic rays energies between 1 TeV and\nseveral hundred TeV. These observations are expected to elevate the direct\ntesting of theoretical models from multi-MeV to sub-PeV energies. In this work,\na method is developed to test the diffusive shock acceleration and diffusion\npropagation model between one and several hundred TeV energies through the\nlatest spectral and morphological data of the Geminga region from HAWC and\nFermi-LAT. Our results show that the theories of diffusive shock acceleration\nand diffusion propagation are consistent with experimental observations.\nHowever, the published morphological data adopted rather wide energy bins and\ncurrently do not allow a high precision test of the inferred energy dependent\ndiffusion coefficient by observed energy spectra with DSA theory. It is\nanticipated that future HAWC and LHAASO-KM2A observations will yield\nhigher-precision results, and the confirmation of a rapidly increasing\ndiffusion coefficient above 100 TeV would serve as important evidence\nsupporting the diffusive shock acceleration and diffusion propagation theory."
                },
                "authors": [
                    {
                        "name": "Weikang Gao"
                    },
                    {
                        "name": "Li-Zhuo Bao"
                    },
                    {
                        "name": "Kun Fang"
                    },
                    {
                        "name": "En-sheng Chen"
                    },
                    {
                        "name": "Siming Liu"
                    },
                    {
                        "name": "HongBo Hu"
                    }
                ],
                "author_detail": {
                    "name": "HongBo Hu"
                },
                "author": "HongBo Hu",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10259v1",
                "updated": "2025-07-14T13:33:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    13,
                    33,
                    30,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T13:33:30Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    13,
                    33,
                    30,
                    0,
                    195,
                    0
                ],
                "title": "Cross-Timeslot Optimization for Distributed GPU Inference Using\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Timeslot Optimization for Distributed GPU Inference Using\n  Reinforcement Learning"
                },
                "summary": "The rapid growth of large language model (LLM) services imposes increasing\ndemands on distributed GPU inference infrastructure. Most existing scheduling\nsystems rely on the current system state to make decisions, without considering\nhow task demand and resource availability evolve over time. This lack of\ntemporal awareness leads to inefficient GPU utilization, high task migration\noverhead, and poor system responsiveness under dynamic workloads. In this work,\nwe identify the fundamental limitations of these instantaneous-state-only\nscheduling approaches and propose Temporal Optimal Resource scheduling via\nTwo-layer Architecture (TORTA). TORTA introduces a spatiotemporal scheduling\nframework that captures both long-term workload patterns and short-term\nexecution constraints. It adopts a two-layer design: a macro-level scheduler\nleverages reinforcement learning and optimal transport to coordinate\ninter-region task distribution, while a micro-level allocator refines\ntask-to-server assignments within each region to reduce latency and switching\ncosts. Experimental results across multiple network topologies show that TORTA\nreduces average inference response time by up to 15\\%, improves load balance by\napproximately 4-5\\%, and cuts total operational cost by 10-20\\% compared to\nstate-of-the-art baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of large language model (LLM) services imposes increasing\ndemands on distributed GPU inference infrastructure. Most existing scheduling\nsystems rely on the current system state to make decisions, without considering\nhow task demand and resource availability evolve over time. This lack of\ntemporal awareness leads to inefficient GPU utilization, high task migration\noverhead, and poor system responsiveness under dynamic workloads. In this work,\nwe identify the fundamental limitations of these instantaneous-state-only\nscheduling approaches and propose Temporal Optimal Resource scheduling via\nTwo-layer Architecture (TORTA). TORTA introduces a spatiotemporal scheduling\nframework that captures both long-term workload patterns and short-term\nexecution constraints. It adopts a two-layer design: a macro-level scheduler\nleverages reinforcement learning and optimal transport to coordinate\ninter-region task distribution, while a micro-level allocator refines\ntask-to-server assignments within each region to reduce latency and switching\ncosts. Experimental results across multiple network topologies show that TORTA\nreduces average inference response time by up to 15\\%, improves load balance by\napproximately 4-5\\%, and cuts total operational cost by 10-20\\% compared to\nstate-of-the-art baseline methods."
                },
                "authors": [
                    {
                        "name": "Chengze Du"
                    },
                    {
                        "name": "Zhiwei Yu"
                    },
                    {
                        "name": "Heng Xu"
                    },
                    {
                        "name": "Haojie Wang"
                    },
                    {
                        "name": "Bo liu"
                    },
                    {
                        "name": "Jialong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jialong Li"
                },
                "author": "Jialong Li",
                "arxiv_comment": "17 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04095v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04095v4",
                "updated": "2025-07-14T12:57:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    12,
                    57,
                    44,
                    0,
                    195,
                    0
                ],
                "published": "2024-10-05T09:30:55Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    9,
                    30,
                    55,
                    5,
                    279,
                    0
                ],
                "title": "Sharp finite statistics for quantum key distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sharp finite statistics for quantum key distribution"
                },
                "summary": "The performance of quantum key distribution (QKD) heavily depends on\nstatistical inference. For a broad class of protocols, the central statistical\ntask is a random sampling problem, customarily addressed using a hypergeometric\ntail bound due to Serfling. Here, we provide an alternative solution for this\ntask of unprecedented tightness among QKD security analyses. As a by-product,\nconfidence intervals for the average of nonidentical Bernoulli parameters\nfollow too. These naturally fit in statistical analyses of decoy-state QKD and\nalso outperform standard tools. Last, we show that, in a vast parameter regime,\nthe use of tail bounds is not enforced because the cumulative mass function of\nthe hypergeometric distribution is accurately computable. This sharply\ndecreases the minimum block sizes necessary for QKD, and reveals the tightness\nof our analytical bounds when moderate-to-large blocks are considered.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of quantum key distribution (QKD) heavily depends on\nstatistical inference. For a broad class of protocols, the central statistical\ntask is a random sampling problem, customarily addressed using a hypergeometric\ntail bound due to Serfling. Here, we provide an alternative solution for this\ntask of unprecedented tightness among QKD security analyses. As a by-product,\nconfidence intervals for the average of nonidentical Bernoulli parameters\nfollow too. These naturally fit in statistical analyses of decoy-state QKD and\nalso outperform standard tools. Last, we show that, in a vast parameter regime,\nthe use of tail bounds is not enforced because the cumulative mass function of\nthe hypergeometric distribution is accurately computable. This sharply\ndecreases the minimum block sizes necessary for QKD, and reveals the tightness\nof our analytical bounds when moderate-to-large blocks are considered."
                },
                "authors": [
                    {
                        "name": "Vaisakh Mannalath"
                    },
                    {
                        "name": "V√≠ctor Zapatero"
                    },
                    {
                        "name": "Marcos Curty"
                    }
                ],
                "author_detail": {
                    "name": "Marcos Curty"
                },
                "author": "Marcos Curty",
                "arxiv_doi": "10.1103/l735-x48g",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/l735-x48g",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.04095v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04095v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "22 pages, 3 figures",
                "arxiv_journal_ref": "Physical Review Letters 135.2 (2025): 020803",
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10222v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10222v1",
                "updated": "2025-07-14T12:39:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    12,
                    39,
                    7,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T12:39:07Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    12,
                    39,
                    7,
                    0,
                    195,
                    0
                ],
                "title": "Spatial Lifting for Dense Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial Lifting for Dense Prediction"
                },
                "summary": "We present Spatial Lifting (SL), a novel methodology for dense prediction\ntasks. SL operates by lifting standard inputs, such as 2D images, into a\nhigher-dimensional space and subsequently processing them using networks\ndesigned for that higher dimension, such as a 3D U-Net. Counterintuitively,\nthis dimensionality lifting allows us to achieve good performance on benchmark\ntasks compared to conventional approaches, while reducing inference costs and\nsignificantly lowering the number of model parameters. The SL framework\nproduces intrinsically structured outputs along the lifted dimension. This\nemergent structure facilitates dense supervision during training and enables\nrobust, near-zero-additional-cost prediction quality assessment at test time.\nWe validate our approach across 19 benchmark datasets (13 for semantic\nsegmentation and 6 for depth estimation), demonstrating competitive dense\nprediction performance while reducing the model parameter count by over 98% (in\nthe U-Net case) and lowering inference costs. Spatial Lifting introduces a new\nvision modeling paradigm that offers a promising path toward more efficient,\naccurate, and reliable deep networks for dense prediction tasks in vision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Spatial Lifting (SL), a novel methodology for dense prediction\ntasks. SL operates by lifting standard inputs, such as 2D images, into a\nhigher-dimensional space and subsequently processing them using networks\ndesigned for that higher dimension, such as a 3D U-Net. Counterintuitively,\nthis dimensionality lifting allows us to achieve good performance on benchmark\ntasks compared to conventional approaches, while reducing inference costs and\nsignificantly lowering the number of model parameters. The SL framework\nproduces intrinsically structured outputs along the lifted dimension. This\nemergent structure facilitates dense supervision during training and enables\nrobust, near-zero-additional-cost prediction quality assessment at test time.\nWe validate our approach across 19 benchmark datasets (13 for semantic\nsegmentation and 6 for depth estimation), demonstrating competitive dense\nprediction performance while reducing the model parameter count by over 98% (in\nthe U-Net case) and lowering inference costs. Spatial Lifting introduces a new\nvision modeling paradigm that offers a promising path toward more efficient,\naccurate, and reliable deep networks for dense prediction tasks in vision."
                },
                "authors": [
                    {
                        "name": "Mingzhi Xu"
                    },
                    {
                        "name": "Yizhe Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yizhe Zhang"
                },
                "author": "Yizhe Zhang",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10222v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10222v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24391v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24391v2",
                "updated": "2025-07-14T12:35:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    12,
                    35,
                    53,
                    0,
                    195,
                    0
                ],
                "published": "2025-03-31T17:59:58Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    59,
                    58,
                    0,
                    90,
                    0
                ],
                "title": "Easi3R: Estimating Disentangled Motion from DUSt3R Without Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Easi3R: Estimating Disentangled Motion from DUSt3R Without Training"
                },
                "summary": "Recent advances in DUSt3R have enabled robust estimation of dense point\nclouds and camera parameters of static scenes, leveraging Transformer network\narchitectures and direct supervision on large-scale 3D datasets. In contrast,\nthe limited scale and diversity of available 4D datasets present a major\nbottleneck for training a highly generalizable 4D model. This constraint has\ndriven conventional 4D methods to fine-tune 3D models on scalable dynamic video\ndata with additional geometric priors such as optical flow and depths. In this\nwork, we take an opposite path and introduce Easi3R, a simple yet efficient\ntraining-free method for 4D reconstruction. Our approach applies attention\nadaptation during inference, eliminating the need for from-scratch pre-training\nor network fine-tuning. We find that the attention layers in DUSt3R inherently\nencode rich information about camera and object motion. By carefully\ndisentangling these attention maps, we achieve accurate dynamic region\nsegmentation, camera pose estimation, and 4D dense point map reconstruction.\nExtensive experiments on real-world dynamic videos demonstrate that our\nlightweight attention adaptation significantly outperforms previous\nstate-of-the-art methods that are trained or finetuned on extensive dynamic\ndatasets. Our code is publicly available for research purpose at\nhttps://easi3r.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in DUSt3R have enabled robust estimation of dense point\nclouds and camera parameters of static scenes, leveraging Transformer network\narchitectures and direct supervision on large-scale 3D datasets. In contrast,\nthe limited scale and diversity of available 4D datasets present a major\nbottleneck for training a highly generalizable 4D model. This constraint has\ndriven conventional 4D methods to fine-tune 3D models on scalable dynamic video\ndata with additional geometric priors such as optical flow and depths. In this\nwork, we take an opposite path and introduce Easi3R, a simple yet efficient\ntraining-free method for 4D reconstruction. Our approach applies attention\nadaptation during inference, eliminating the need for from-scratch pre-training\nor network fine-tuning. We find that the attention layers in DUSt3R inherently\nencode rich information about camera and object motion. By carefully\ndisentangling these attention maps, we achieve accurate dynamic region\nsegmentation, camera pose estimation, and 4D dense point map reconstruction.\nExtensive experiments on real-world dynamic videos demonstrate that our\nlightweight attention adaptation significantly outperforms previous\nstate-of-the-art methods that are trained or finetuned on extensive dynamic\ndatasets. Our code is publicly available for research purpose at\nhttps://easi3r.github.io/"
                },
                "authors": [
                    {
                        "name": "Xingyu Chen"
                    },
                    {
                        "name": "Yue Chen"
                    },
                    {
                        "name": "Yuliang Xiu"
                    },
                    {
                        "name": "Andreas Geiger"
                    },
                    {
                        "name": "Anpei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Anpei Chen"
                },
                "author": "Anpei Chen",
                "arxiv_comment": "Page: https://easi3r.github.io/ Code:\n  https://github.com/Inception3D/Easi3R",
                "arxiv_journal_ref": "IEEE/CVF International Conference on Computer Vision (ICCV), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24391v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24391v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10218v1",
                "updated": "2025-07-14T12:35:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    12,
                    35,
                    17,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T12:35:17Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    12,
                    35,
                    17,
                    0,
                    195,
                    0
                ],
                "title": "Straighten Viscous Rectified Flow via Noise Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Straighten Viscous Rectified Flow via Noise Optimization"
                },
                "summary": "The Reflow operation aims to straighten the inference trajectories of the\nrectified flow during training by constructing deterministic couplings between\nnoises and images, thereby improving the quality of generated images in\nsingle-step or few-step generation. However, we identify critical limitations\nin Reflow, particularly its inability to rapidly generate high-quality images\ndue to a distribution gap between images in its constructed deterministic\ncouplings and real images. To address these shortcomings, we propose a novel\nalternative called Straighten Viscous Rectified Flow via Noise Optimization\n(VRFNO), which is a joint training framework integrating an encoder and a\nneural velocity field. VRFNO introduces two key innovations: (1) a historical\nvelocity term that enhances trajectory distinction, enabling the model to more\naccurately predict the velocity of the current trajectory, and (2) the noise\noptimization through reparameterization to form optimized couplings with real\nimages which are then utilized for training, effectively mitigating errors\ncaused by Reflow's limitations. Comprehensive experiments on synthetic data and\nreal datasets with varying resolutions show that VRFNO significantly mitigates\nthe limitations of Reflow, achieving state-of-the-art performance in both\none-step and few-step generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Reflow operation aims to straighten the inference trajectories of the\nrectified flow during training by constructing deterministic couplings between\nnoises and images, thereby improving the quality of generated images in\nsingle-step or few-step generation. However, we identify critical limitations\nin Reflow, particularly its inability to rapidly generate high-quality images\ndue to a distribution gap between images in its constructed deterministic\ncouplings and real images. To address these shortcomings, we propose a novel\nalternative called Straighten Viscous Rectified Flow via Noise Optimization\n(VRFNO), which is a joint training framework integrating an encoder and a\nneural velocity field. VRFNO introduces two key innovations: (1) a historical\nvelocity term that enhances trajectory distinction, enabling the model to more\naccurately predict the velocity of the current trajectory, and (2) the noise\noptimization through reparameterization to form optimized couplings with real\nimages which are then utilized for training, effectively mitigating errors\ncaused by Reflow's limitations. Comprehensive experiments on synthetic data and\nreal datasets with varying resolutions show that VRFNO significantly mitigates\nthe limitations of Reflow, achieving state-of-the-art performance in both\none-step and few-step generation tasks."
                },
                "authors": [
                    {
                        "name": "Jimin Dai"
                    },
                    {
                        "name": "Jiexi Yan"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Lei Luo"
                    }
                ],
                "author_detail": {
                    "name": "Lei Luo"
                },
                "author": "Lei Luo",
                "arxiv_journal_ref": "International Conference on Computer Vision 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10217v1",
                "updated": "2025-07-14T12:34:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    12,
                    34,
                    25,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T12:34:25Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    12,
                    34,
                    25,
                    0,
                    195,
                    0
                ],
                "title": "From Wardrobe to Canvas: Wardrobe Polyptych LoRA for Part-level\n  Controllable Human Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Wardrobe to Canvas: Wardrobe Polyptych LoRA for Part-level\n  Controllable Human Image Generation"
                },
                "summary": "Recent diffusion models achieve personalization by learning specific\nsubjects, allowing learned attributes to be integrated into generated images.\nHowever, personalized human image generation remains challenging due to the\nneed for precise and consistent attribute preservation (e.g., identity,\nclothing details). Existing subject-driven image generation methods often\nrequire either (1) inference-time fine-tuning with few images for each new\nsubject or (2) large-scale dataset training for generalization. Both approaches\nare computationally expensive and impractical for real-time applications. To\naddress these limitations, we present Wardrobe Polyptych LoRA, a novel\npart-level controllable model for personalized human image generation. By\ntraining only LoRA layers, our method removes the computational burden at\ninference while ensuring high-fidelity synthesis of unseen subjects. Our key\nidea is to condition the generation on the subject's wardrobe and leverage\nspatial references to reduce information loss, thereby improving fidelity and\nconsistency. Additionally, we introduce a selective subject region loss, which\nencourages the model to disregard some of reference images during training. Our\nloss ensures that generated images better align with text prompts while\nmaintaining subject integrity. Notably, our Wardrobe Polyptych LoRA requires no\nadditional parameters at the inference stage and performs generation using a\nsingle model trained on a few training samples. We construct a new dataset and\nbenchmark tailored for personalized human image generation. Extensive\nexperiments show that our approach significantly outperforms existing\ntechniques in fidelity and consistency, enabling realistic and\nidentity-preserving full-body synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent diffusion models achieve personalization by learning specific\nsubjects, allowing learned attributes to be integrated into generated images.\nHowever, personalized human image generation remains challenging due to the\nneed for precise and consistent attribute preservation (e.g., identity,\nclothing details). Existing subject-driven image generation methods often\nrequire either (1) inference-time fine-tuning with few images for each new\nsubject or (2) large-scale dataset training for generalization. Both approaches\nare computationally expensive and impractical for real-time applications. To\naddress these limitations, we present Wardrobe Polyptych LoRA, a novel\npart-level controllable model for personalized human image generation. By\ntraining only LoRA layers, our method removes the computational burden at\ninference while ensuring high-fidelity synthesis of unseen subjects. Our key\nidea is to condition the generation on the subject's wardrobe and leverage\nspatial references to reduce information loss, thereby improving fidelity and\nconsistency. Additionally, we introduce a selective subject region loss, which\nencourages the model to disregard some of reference images during training. Our\nloss ensures that generated images better align with text prompts while\nmaintaining subject integrity. Notably, our Wardrobe Polyptych LoRA requires no\nadditional parameters at the inference stage and performs generation using a\nsingle model trained on a few training samples. We construct a new dataset and\nbenchmark tailored for personalized human image generation. Extensive\nexperiments show that our approach significantly outperforms existing\ntechniques in fidelity and consistency, enabling realistic and\nidentity-preserving full-body synthesis."
                },
                "authors": [
                    {
                        "name": "Jeongho Kim"
                    },
                    {
                        "name": "Sunghyun Park"
                    },
                    {
                        "name": "Hyoungwoo Park"
                    },
                    {
                        "name": "Sungrack Yun"
                    },
                    {
                        "name": "Jaegul Choo"
                    },
                    {
                        "name": "Seokeon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Seokeon Cho"
                },
                "author": "Seokeon Cho",
                "arxiv_comment": "10 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10216v1",
                "updated": "2025-07-14T12:33:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    12,
                    33,
                    7,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T12:33:07Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    12,
                    33,
                    7,
                    0,
                    195,
                    0
                ],
                "title": "Absher: A Benchmark for Evaluating Large Language Models Understanding\n  of Saudi Dialects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Absher: A Benchmark for Evaluating Large Language Models Understanding\n  of Saudi Dialects"
                },
                "summary": "As large language models (LLMs) become increasingly central to Arabic NLP\napplications, evaluating their understanding of regional dialects and cultural\nnuances is essential, particularly in linguistically diverse settings like\nSaudi Arabia. This paper introduces \\texttt{Absher}, a comprehensive benchmark\nspecifically designed to assess LLMs performance across major Saudi dialects.\n\\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six\ndistinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage,\nCultural Interpretation, and Location Recognition. These questions are derived\nfrom a curated dataset of dialectal words, phrases, and proverbs sourced from\nvarious regions of Saudi Arabia. We evaluate several state-of-the-art LLMs,\nincluding multilingual and Arabic-specific models. We also provide detailed\ninsights into their capabilities and limitations. Our results reveal notable\nperformance gaps, particularly in tasks requiring cultural inference or\ncontextual understanding. Our findings highlight the urgent need for\ndialect-aware training and culturally aligned evaluation methodologies to\nimprove LLMs performance in real-world Arabic applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become increasingly central to Arabic NLP\napplications, evaluating their understanding of regional dialects and cultural\nnuances is essential, particularly in linguistically diverse settings like\nSaudi Arabia. This paper introduces \\texttt{Absher}, a comprehensive benchmark\nspecifically designed to assess LLMs performance across major Saudi dialects.\n\\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six\ndistinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage,\nCultural Interpretation, and Location Recognition. These questions are derived\nfrom a curated dataset of dialectal words, phrases, and proverbs sourced from\nvarious regions of Saudi Arabia. We evaluate several state-of-the-art LLMs,\nincluding multilingual and Arabic-specific models. We also provide detailed\ninsights into their capabilities and limitations. Our results reveal notable\nperformance gaps, particularly in tasks requiring cultural inference or\ncontextual understanding. Our findings highlight the urgent need for\ndialect-aware training and culturally aligned evaluation methodologies to\nimprove LLMs performance in real-world Arabic applications."
                },
                "authors": [
                    {
                        "name": "Renad Al-Monef"
                    },
                    {
                        "name": "Hassan Alhuzali"
                    },
                    {
                        "name": "Nora Alturayeif"
                    },
                    {
                        "name": "Ashwag Alasmari"
                    }
                ],
                "author_detail": {
                    "name": "Ashwag Alasmari"
                },
                "author": "Ashwag Alasmari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08331v2",
                "updated": "2025-07-14T12:19:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    12,
                    19,
                    42,
                    0,
                    195,
                    0
                ],
                "published": "2024-12-11T12:18:30Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    12,
                    18,
                    30,
                    2,
                    346,
                    0
                ],
                "title": "SLGaussian: Fast Language Gaussian Splatting in Sparse Views",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLGaussian: Fast Language Gaussian Splatting in Sparse Views"
                },
                "summary": "3D semantic field learning is crucial for applications like autonomous\nnavigation, AR/VR, and robotics, where accurate comprehension of 3D scenes from\nlimited viewpoints is essential. Existing methods struggle under sparse view\nconditions, relying on inefficient per-scene multi-view optimizations, which\nare impractical for many real-world tasks. To address this, we propose\nSLGaussian, a feed-forward method for constructing 3D semantic fields from\nsparse viewpoints, allowing direct inference of 3DGS-based scenes. By ensuring\nconsistent SAM segmentations through video tracking and using low-dimensional\nindexing for high-dimensional CLIP features, SLGaussian efficiently embeds\nlanguage information in 3D space, offering a robust solution for accurate 3D\nscene understanding under sparse view conditions. In experiments on two-view\nsparse 3D object querying and segmentation in the LERF and 3D-OVS datasets,\nSLGaussian outperforms existing methods in chosen IoU, Localization Accuracy,\nand mIoU. Moreover, our model achieves scene inference in under 30 seconds and\nopen-vocabulary querying in just 0.011 seconds per query.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D semantic field learning is crucial for applications like autonomous\nnavigation, AR/VR, and robotics, where accurate comprehension of 3D scenes from\nlimited viewpoints is essential. Existing methods struggle under sparse view\nconditions, relying on inefficient per-scene multi-view optimizations, which\nare impractical for many real-world tasks. To address this, we propose\nSLGaussian, a feed-forward method for constructing 3D semantic fields from\nsparse viewpoints, allowing direct inference of 3DGS-based scenes. By ensuring\nconsistent SAM segmentations through video tracking and using low-dimensional\nindexing for high-dimensional CLIP features, SLGaussian efficiently embeds\nlanguage information in 3D space, offering a robust solution for accurate 3D\nscene understanding under sparse view conditions. In experiments on two-view\nsparse 3D object querying and segmentation in the LERF and 3D-OVS datasets,\nSLGaussian outperforms existing methods in chosen IoU, Localization Accuracy,\nand mIoU. Moreover, our model achieves scene inference in under 30 seconds and\nopen-vocabulary querying in just 0.011 seconds per query."
                },
                "authors": [
                    {
                        "name": "Kangjie Chen"
                    },
                    {
                        "name": "BingQuan Dai"
                    },
                    {
                        "name": "Minghan Qin"
                    },
                    {
                        "name": "Dongbin Zhang"
                    },
                    {
                        "name": "Peihao Li"
                    },
                    {
                        "name": "Yingshuang Zou"
                    },
                    {
                        "name": "Haoqian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoqian Wang"
                },
                "author": "Haoqian Wang",
                "arxiv_comment": "Accepted by ACM MM 2025. Project page:\n  https://chenkangjie1123.github.io/SLGaussian.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10200v1",
                "updated": "2025-07-14T12:13:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    12,
                    13,
                    50,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T12:13:50Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    12,
                    13,
                    50,
                    0,
                    195,
                    0
                ],
                "title": "Natural Language-based Assessment of L2 Oral Proficiency using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language-based Assessment of L2 Oral Proficiency using LLMs"
                },
                "summary": "Natural language-based assessment (NLA) is an approach to second language\nassessment that uses instructions - expressed in the form of can-do descriptors\n- originally intended for human examiners, aiming to determine whether large\nlanguage models (LLMs) can interpret and apply them in ways comparable to human\nassessment. In this work, we explore the use of such descriptors with an\nopen-source LLM, Qwen 2.5 72B, to assess responses from the publicly available\nS&I Corpus in a zero-shot setting. Our results show that this approach -\nrelying solely on textual information - achieves competitive performance: while\nit does not outperform state-of-the-art speech LLMs fine-tuned for the task, it\nsurpasses a BERT-based model trained specifically for this purpose. NLA proves\nparticularly effective in mismatched task settings, is generalisable to other\ndata types and languages, and offers greater interpretability, as it is\ngrounded in clearly explainable, widely applicable language descriptors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language-based assessment (NLA) is an approach to second language\nassessment that uses instructions - expressed in the form of can-do descriptors\n- originally intended for human examiners, aiming to determine whether large\nlanguage models (LLMs) can interpret and apply them in ways comparable to human\nassessment. In this work, we explore the use of such descriptors with an\nopen-source LLM, Qwen 2.5 72B, to assess responses from the publicly available\nS&I Corpus in a zero-shot setting. Our results show that this approach -\nrelying solely on textual information - achieves competitive performance: while\nit does not outperform state-of-the-art speech LLMs fine-tuned for the task, it\nsurpasses a BERT-based model trained specifically for this purpose. NLA proves\nparticularly effective in mismatched task settings, is generalisable to other\ndata types and languages, and offers greater interpretability, as it is\ngrounded in clearly explainable, widely applicable language descriptors."
                },
                "authors": [
                    {
                        "name": "Stefano Bann√≤"
                    },
                    {
                        "name": "Rao Ma"
                    },
                    {
                        "name": "Mengjie Qian"
                    },
                    {
                        "name": "Siyuan Tang"
                    },
                    {
                        "name": "Kate Knill"
                    },
                    {
                        "name": "Mark Gales"
                    }
                ],
                "author_detail": {
                    "name": "Mark Gales"
                },
                "author": "Mark Gales",
                "arxiv_comment": "Accepted for the 10th Workshop on Speech and Language Technology in\n  Education (SLaTE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10183v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10183v1",
                "updated": "2025-07-14T11:47:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    47,
                    43,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T11:47:43Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    47,
                    43,
                    0,
                    195,
                    0
                ],
                "title": "T-GRAB: A Synthetic Diagnostic Benchmark for Learning on Temporal Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T-GRAB: A Synthetic Diagnostic Benchmark for Learning on Temporal Graphs"
                },
                "summary": "Dynamic graph learning methods have recently emerged as powerful tools for\nmodelling relational data evolving through time. However, despite extensive\nbenchmarking efforts, it remains unclear whether current Temporal Graph Neural\nNetworks (TGNNs) effectively capture core temporal patterns such as\nperiodicity, cause-and-effect, and long-range dependencies. In this work, we\nintroduce the Temporal Graph Reasoning Benchmark (T-GRAB), a comprehensive set\nof synthetic tasks designed to systematically probe the capabilities of TGNNs\nto reason across time. T-GRAB provides controlled, interpretable tasks that\nisolate key temporal skills: counting/memorizing periodic repetitions,\ninferring delayed causal effects, and capturing long-range dependencies over\nboth spatial and temporal dimensions. We evaluate 11 temporal graph learning\nmethods on these tasks, revealing fundamental shortcomings in their ability to\ngeneralize temporal patterns. Our findings offer actionable insights into the\nlimitations of current models, highlight challenges hidden by traditional\nreal-world benchmarks, and motivate the development of architectures with\nstronger temporal reasoning abilities. The code for T-GRAB can be found at:\nhttps://github.com/alirezadizaji/T-GRAB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic graph learning methods have recently emerged as powerful tools for\nmodelling relational data evolving through time. However, despite extensive\nbenchmarking efforts, it remains unclear whether current Temporal Graph Neural\nNetworks (TGNNs) effectively capture core temporal patterns such as\nperiodicity, cause-and-effect, and long-range dependencies. In this work, we\nintroduce the Temporal Graph Reasoning Benchmark (T-GRAB), a comprehensive set\nof synthetic tasks designed to systematically probe the capabilities of TGNNs\nto reason across time. T-GRAB provides controlled, interpretable tasks that\nisolate key temporal skills: counting/memorizing periodic repetitions,\ninferring delayed causal effects, and capturing long-range dependencies over\nboth spatial and temporal dimensions. We evaluate 11 temporal graph learning\nmethods on these tasks, revealing fundamental shortcomings in their ability to\ngeneralize temporal patterns. Our findings offer actionable insights into the\nlimitations of current models, highlight challenges hidden by traditional\nreal-world benchmarks, and motivate the development of architectures with\nstronger temporal reasoning abilities. The code for T-GRAB can be found at:\nhttps://github.com/alirezadizaji/T-GRAB."
                },
                "authors": [
                    {
                        "name": "Alireza Dizaji"
                    },
                    {
                        "name": "Benedict Aaron Tjandra"
                    },
                    {
                        "name": "Mehrab Hamidi"
                    },
                    {
                        "name": "Shenyang Huang"
                    },
                    {
                        "name": "Guillaume Rabusseau"
                    }
                ],
                "author_detail": {
                    "name": "Guillaume Rabusseau"
                },
                "author": "Guillaume Rabusseau",
                "arxiv_comment": "Accepted to MLoG-GenAI Workshop @ KDD 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10183v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10183v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08017v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08017v2",
                "updated": "2025-07-14T11:46:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    46,
                    41,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-07T20:26:31Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    20,
                    26,
                    31,
                    0,
                    188,
                    0
                ],
                "title": "Mechanistic Indicators of Understanding in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mechanistic Indicators of Understanding in Large Language Models"
                },
                "summary": "Recent findings in mechanistic interpretability (MI), the field probing the\ninner workings of Large Language Models (LLMs), challenge the view that these\nmodels rely solely on superficial statistics. We offer an accessible synthesis\nof these findings that doubles as an introduction to MI while integrating these\nfindings within a novel theoretical framework for thinking about machine\nunderstanding. We argue that LLMs develop internal structures that are\nfunctionally analogous to the kind of understanding that consists in seeing\nconnections. To sharpen this idea, we propose a three-tiered conception of\nunderstanding. First, conceptual understanding emerges when a model forms\n\"features\" as directions in latent space, learning the connections between\ndiverse manifestations of something. Second, state-of-the-world understanding\nemerges when a model learns contingent factual connections between features and\ndynamically tracks changes in the world. Third, principled understanding\nemerges when a model ceases to rely on a collection of memorized facts and\ndiscovers a \"circuit\" connecting these facts. However, these forms of\nunderstanding remain radically different from human understanding, as the\nphenomenon of \"parallel mechanisms\" shows. We conclude that the debate should\nmove beyond the yes-or-no question of whether LLMs understand to investigate\nhow their strange minds work and forge conceptions that fit them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent findings in mechanistic interpretability (MI), the field probing the\ninner workings of Large Language Models (LLMs), challenge the view that these\nmodels rely solely on superficial statistics. We offer an accessible synthesis\nof these findings that doubles as an introduction to MI while integrating these\nfindings within a novel theoretical framework for thinking about machine\nunderstanding. We argue that LLMs develop internal structures that are\nfunctionally analogous to the kind of understanding that consists in seeing\nconnections. To sharpen this idea, we propose a three-tiered conception of\nunderstanding. First, conceptual understanding emerges when a model forms\n\"features\" as directions in latent space, learning the connections between\ndiverse manifestations of something. Second, state-of-the-world understanding\nemerges when a model learns contingent factual connections between features and\ndynamically tracks changes in the world. Third, principled understanding\nemerges when a model ceases to rely on a collection of memorized facts and\ndiscovers a \"circuit\" connecting these facts. However, these forms of\nunderstanding remain radically different from human understanding, as the\nphenomenon of \"parallel mechanisms\" shows. We conclude that the debate should\nmove beyond the yes-or-no question of whether LLMs understand to investigate\nhow their strange minds work and forge conceptions that fit them."
                },
                "authors": [
                    {
                        "name": "Pierre Beckmann"
                    },
                    {
                        "name": "Matthieu Queloz"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Queloz"
                },
                "author": "Matthieu Queloz",
                "arxiv_comment": "32 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08017v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08017v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10182v1",
                "updated": "2025-07-14T11:44:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    44,
                    4,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T11:44:04Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    44,
                    4,
                    0,
                    195,
                    0
                ],
                "title": "Breaking the Myth: Can Small Models Infer Postconditions Too?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Myth: Can Small Models Infer Postconditions Too?"
                },
                "summary": "Formal specifications are essential for ensuring software correctness, yet\nmanually writing them is tedious and error-prone. Large Language Models (LLMs)\nhave shown promise in generating such specifications from natural language\nintents, but the giant model size and high computational demands raise a\nfundamental question: Do we really need large models for this task? In this\npaper, we show that a small, fine-tuned language model can achieve high-quality\npostcondition generation with much lower computational costs. We construct a\nspecialized dataset of prompts, reasoning logs, and postconditions, then\nsupervise the fine-tuning of a $7$B-parameter code model. Our approach tackles\nreal-world repository dependencies and preserves pre-state information,\nallowing for expressive and accurate specifications. We evaluate the model on a\nbenchmark of real-world Java bugs (Defects4J) and compare against both\nproprietary giants (e.g., GPT-4o) and open-source large models. Empirical\nresults demonstrate that our compact model matches or outperforms significantly\nlarger counterparts in syntax correctness, semantic correctness, and\nbug-distinguishing capability. These findings highlight that targeted\nfine-tuning on a modest dataset can enable small models to achieve results\nformerly seen only in massive, resource-heavy LLMs, offering a practical and\nefficient path for the real-world adoption of automated specification\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal specifications are essential for ensuring software correctness, yet\nmanually writing them is tedious and error-prone. Large Language Models (LLMs)\nhave shown promise in generating such specifications from natural language\nintents, but the giant model size and high computational demands raise a\nfundamental question: Do we really need large models for this task? In this\npaper, we show that a small, fine-tuned language model can achieve high-quality\npostcondition generation with much lower computational costs. We construct a\nspecialized dataset of prompts, reasoning logs, and postconditions, then\nsupervise the fine-tuning of a $7$B-parameter code model. Our approach tackles\nreal-world repository dependencies and preserves pre-state information,\nallowing for expressive and accurate specifications. We evaluate the model on a\nbenchmark of real-world Java bugs (Defects4J) and compare against both\nproprietary giants (e.g., GPT-4o) and open-source large models. Empirical\nresults demonstrate that our compact model matches or outperforms significantly\nlarger counterparts in syntax correctness, semantic correctness, and\nbug-distinguishing capability. These findings highlight that targeted\nfine-tuning on a modest dataset can enable small models to achieve results\nformerly seen only in massive, resource-heavy LLMs, offering a practical and\nefficient path for the real-world adoption of automated specification\ngeneration."
                },
                "authors": [
                    {
                        "name": "Gehao Zhang"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Juan Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Juan Zhai"
                },
                "author": "Juan Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10178v1",
                "updated": "2025-07-14T11:40:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    40,
                    17,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T11:40:17Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    40,
                    17,
                    0,
                    195,
                    0
                ],
                "title": "Pimba: A Processing-in-Memory Acceleration for Post-Transformer Large\n  Language Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pimba: A Processing-in-Memory Acceleration for Post-Transformer Large\n  Language Model Serving"
                },
                "summary": "Transformers are the driving force behind today's Large Language Models\n(LLMs), serving as the foundation for their performance and versatility. Yet,\ntheir compute and memory costs grow with sequence length, posing scalability\nchallenges for long-context inferencing. In response, the algorithm community\nis exploring alternative architectures, such as state space models (SSMs),\nlinear attention, and recurrent neural networks (RNNs), which we refer to as\npost-transformers. This shift presents a key challenge: building a serving\nsystem that efficiently supports both transformer and post-transformer LLMs\nwithin a unified framework. To address this challenge, we analyze the\nperformance characteristics of transformer and post-transformer LLMs. Despite\ntheir algorithmic differences, both are fundamentally limited by memory\nbandwidth under batched inference due to attention in transformers and state\nupdates in post-transformers. Further analyses suggest two additional insights:\n(1) state update operations, unlike attention, incur high hardware cost, making\nper-bank PIM acceleration inefficient, and (2) different low-precision\narithmetic methods offer varying accuracy-area tradeoffs, while we identify\nMicrosoft's MX as the Pareto-optimal choice. Building on these insights, we\ndesign Pimba as an array of State-update Processing Units (SPUs), each shared\nbetween two banks to enable interleaved access to PIM. Each SPU includes a\nState-update Processing Engine (SPE) that comprises element-wise multipliers\nand adders using MX-based quantized arithmetic, enabling efficient execution of\nstate update and attention operations. Our evaluation shows that, compared to\nLLM-optimized GPU and GPU+PIM systems, Pimba achieves up to 3.2x and 2.1x\nhigher token generation throughput, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers are the driving force behind today's Large Language Models\n(LLMs), serving as the foundation for their performance and versatility. Yet,\ntheir compute and memory costs grow with sequence length, posing scalability\nchallenges for long-context inferencing. In response, the algorithm community\nis exploring alternative architectures, such as state space models (SSMs),\nlinear attention, and recurrent neural networks (RNNs), which we refer to as\npost-transformers. This shift presents a key challenge: building a serving\nsystem that efficiently supports both transformer and post-transformer LLMs\nwithin a unified framework. To address this challenge, we analyze the\nperformance characteristics of transformer and post-transformer LLMs. Despite\ntheir algorithmic differences, both are fundamentally limited by memory\nbandwidth under batched inference due to attention in transformers and state\nupdates in post-transformers. Further analyses suggest two additional insights:\n(1) state update operations, unlike attention, incur high hardware cost, making\nper-bank PIM acceleration inefficient, and (2) different low-precision\narithmetic methods offer varying accuracy-area tradeoffs, while we identify\nMicrosoft's MX as the Pareto-optimal choice. Building on these insights, we\ndesign Pimba as an array of State-update Processing Units (SPUs), each shared\nbetween two banks to enable interleaved access to PIM. Each SPU includes a\nState-update Processing Engine (SPE) that comprises element-wise multipliers\nand adders using MX-based quantized arithmetic, enabling efficient execution of\nstate update and attention operations. Our evaluation shows that, compared to\nLLM-optimized GPU and GPU+PIM systems, Pimba achieves up to 3.2x and 2.1x\nhigher token generation throughput, respectively."
                },
                "authors": [
                    {
                        "name": "Wonung Kim"
                    },
                    {
                        "name": "Yubin Lee"
                    },
                    {
                        "name": "Yoonsung Kim"
                    },
                    {
                        "name": "Jinwoo Hwang"
                    },
                    {
                        "name": "Seongryong Oh"
                    },
                    {
                        "name": "Jiyong Jung"
                    },
                    {
                        "name": "Aziz Huseynov"
                    },
                    {
                        "name": "Woong Gyu Park"
                    },
                    {
                        "name": "Chang Hyun Park"
                    },
                    {
                        "name": "Divya Mahajan"
                    },
                    {
                        "name": "Jongse Park"
                    }
                ],
                "author_detail": {
                    "name": "Jongse Park"
                },
                "author": "Jongse Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10177v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10177v1",
                "updated": "2025-07-14T11:39:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    39,
                    34,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T11:39:34Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    39,
                    34,
                    0,
                    195,
                    0
                ],
                "title": "Abusive text transformation using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abusive text transformation using LLMs"
                },
                "summary": "Although Large Language Models (LLMs) have demonstrated significant\nadvancements in natural language processing tasks, their effectiveness in the\nclassification and transformation of abusive text into non-abusive versions\nremains an area for exploration. In this study, we aim to use LLMs to transform\nabusive text (tweets and reviews) featuring hate speech and swear words into\nnon-abusive text, while retaining the intent of the text. We evaluate the\nperformance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and\nGroq, on their ability to identify abusive text. We them to transform and\nobtain a text that is clean from abusive and inappropriate content but\nmaintains a similar level of sentiment and semantics, i.e. the transformed text\nneeds to maintain its message. Afterwards, we evaluate the raw and transformed\ndatasets with sentiment analysis and semantic analysis. Our results show Groq\nprovides vastly different results when compared with other LLMs. We have\nidentified similarities between GPT-4o and DeepSeek-V3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) have demonstrated significant\nadvancements in natural language processing tasks, their effectiveness in the\nclassification and transformation of abusive text into non-abusive versions\nremains an area for exploration. In this study, we aim to use LLMs to transform\nabusive text (tweets and reviews) featuring hate speech and swear words into\nnon-abusive text, while retaining the intent of the text. We evaluate the\nperformance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and\nGroq, on their ability to identify abusive text. We them to transform and\nobtain a text that is clean from abusive and inappropriate content but\nmaintains a similar level of sentiment and semantics, i.e. the transformed text\nneeds to maintain its message. Afterwards, we evaluate the raw and transformed\ndatasets with sentiment analysis and semantic analysis. Our results show Groq\nprovides vastly different results when compared with other LLMs. We have\nidentified similarities between GPT-4o and DeepSeek-V3."
                },
                "authors": [
                    {
                        "name": "Rohitash Chandra"
                    },
                    {
                        "name": "Jiyong Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jiyong Choi"
                },
                "author": "Jiyong Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10177v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10177v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10170v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10170v1",
                "updated": "2025-07-14T11:33:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    33,
                    14,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T11:33:14Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    33,
                    14,
                    0,
                    195,
                    0
                ],
                "title": "Understanding the Rank of Tensor Networks via an Intuitive\n  Example-Driven Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Rank of Tensor Networks via an Intuitive\n  Example-Driven Approach"
                },
                "summary": "Tensor Network (TN) decompositions have emerged as an indispensable tool in\nBig Data analytics owing to their ability to provide compact low-rank\nrepresentations, thus alleviating the ``Curse of Dimensionality'' inherent in\nhandling higher-order data. At the heart of their success lies the concept of\nTN ranks, which governs the efficiency and expressivity of TN decompositions.\nHowever, unlike matrix ranks, TN ranks often lack a universal meaning and an\nintuitive interpretation, with their properties varying significantly across\ndifferent TN structures. Consequently, TN ranks are frequently treated as\nempirically tuned hyperparameters, rather than as key design parameters\ninferred from domain knowledge. The aim of this Lecture Note is therefore to\ndemystify the foundational yet frequently misunderstood concept of TN ranks\nthrough real-life examples and intuitive visualizations. We begin by\nillustrating how domain knowledge can guide the selection of TN ranks in\nwidely-used models such as the Canonical Polyadic (CP) and Tucker\ndecompositions. For more complex TN structures, we employ a self-explanatory\ngraphical approach that generalizes to tensors of arbitrary order. Such a\nperspective naturally reveals the relationship between TN ranks and the\ncorresponding ranks of tensor unfoldings (matrices), thereby circumventing\ncumbersome multi-index tensor algebra while facilitating domain-informed TN\ndesign. It is our hope that this Lecture Note will equip readers with a clear\nand unified understanding of the concept of TN rank, along with the necessary\nphysical insight and intuition to support the selection, explainability, and\ndeployment of tensor methods in both practical applications and educational\ncontexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Network (TN) decompositions have emerged as an indispensable tool in\nBig Data analytics owing to their ability to provide compact low-rank\nrepresentations, thus alleviating the ``Curse of Dimensionality'' inherent in\nhandling higher-order data. At the heart of their success lies the concept of\nTN ranks, which governs the efficiency and expressivity of TN decompositions.\nHowever, unlike matrix ranks, TN ranks often lack a universal meaning and an\nintuitive interpretation, with their properties varying significantly across\ndifferent TN structures. Consequently, TN ranks are frequently treated as\nempirically tuned hyperparameters, rather than as key design parameters\ninferred from domain knowledge. The aim of this Lecture Note is therefore to\ndemystify the foundational yet frequently misunderstood concept of TN ranks\nthrough real-life examples and intuitive visualizations. We begin by\nillustrating how domain knowledge can guide the selection of TN ranks in\nwidely-used models such as the Canonical Polyadic (CP) and Tucker\ndecompositions. For more complex TN structures, we employ a self-explanatory\ngraphical approach that generalizes to tensors of arbitrary order. Such a\nperspective naturally reveals the relationship between TN ranks and the\ncorresponding ranks of tensor unfoldings (matrices), thereby circumventing\ncumbersome multi-index tensor algebra while facilitating domain-informed TN\ndesign. It is our hope that this Lecture Note will equip readers with a clear\nand unified understanding of the concept of TN rank, along with the necessary\nphysical insight and intuition to support the selection, explainability, and\ndeployment of tensor methods in both practical applications and educational\ncontexts."
                },
                "authors": [
                    {
                        "name": "Wuyang Zhou"
                    },
                    {
                        "name": "Giorgos Iacovides"
                    },
                    {
                        "name": "Kriton Konstantinidis"
                    },
                    {
                        "name": "Ilya Kisil"
                    },
                    {
                        "name": "Danilo Mandic"
                    }
                ],
                "author_detail": {
                    "name": "Danilo Mandic"
                },
                "author": "Danilo Mandic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10170v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10170v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10162v1",
                "updated": "2025-07-14T11:22:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    22,
                    50,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T11:22:50Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    22,
                    50,
                    0,
                    195,
                    0
                ],
                "title": "HASSLE: A Self-Supervised Learning Enhanced Hijacking Attack on Vertical\n  Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HASSLE: A Self-Supervised Learning Enhanced Hijacking Attack on Vertical\n  Federated Learning"
                },
                "summary": "Vertical Federated Learning (VFL) enables an orchestrating active party to\nperform a machine learning task by cooperating with passive parties that\nprovide additional task-related features for the same training data entities.\nWhile prior research has leveraged the privacy vulnerability of VFL to\ncompromise its integrity through a combination of label inference and backdoor\nattacks, their effectiveness is constrained by the low label inference\nprecision and suboptimal backdoor injection conditions. To facilitate a more\nrigorous security evaluation on VFL without these limitations, we propose\nHASSLE, a hijacking attack framework composed of a gradient-direction-based\nlabel inference module and an adversarial embedding generation algorithm\nenhanced by self-supervised learning. HASSLE accurately identifies private\nsamples associated with a targeted label using only a single known instance of\nthat label. In the two-party scenario, it demonstrates strong performance with\nan attack success rate (ASR) of over 99% across four datasets, including both\nimage and tabular modalities, and achieves 85% ASR on the more complex\nCIFAR-100 dataset. Evaluation of HASSLE against 8 potential defenses further\nhighlights its significant threat while providing new insights into building a\ntrustworthy VFL system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vertical Federated Learning (VFL) enables an orchestrating active party to\nperform a machine learning task by cooperating with passive parties that\nprovide additional task-related features for the same training data entities.\nWhile prior research has leveraged the privacy vulnerability of VFL to\ncompromise its integrity through a combination of label inference and backdoor\nattacks, their effectiveness is constrained by the low label inference\nprecision and suboptimal backdoor injection conditions. To facilitate a more\nrigorous security evaluation on VFL without these limitations, we propose\nHASSLE, a hijacking attack framework composed of a gradient-direction-based\nlabel inference module and an adversarial embedding generation algorithm\nenhanced by self-supervised learning. HASSLE accurately identifies private\nsamples associated with a targeted label using only a single known instance of\nthat label. In the two-party scenario, it demonstrates strong performance with\nan attack success rate (ASR) of over 99% across four datasets, including both\nimage and tabular modalities, and achieves 85% ASR on the more complex\nCIFAR-100 dataset. Evaluation of HASSLE against 8 potential defenses further\nhighlights its significant threat while providing new insights into building a\ntrustworthy VFL system."
                },
                "authors": [
                    {
                        "name": "Weiyang He"
                    },
                    {
                        "name": "Chip-Hong Chang"
                    }
                ],
                "author_detail": {
                    "name": "Chip-Hong Chang"
                },
                "author": "Chip-Hong Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14403v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14403v3",
                "updated": "2025-07-14T11:21:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    21,
                    20,
                    0,
                    195,
                    0
                ],
                "published": "2025-05-20T14:16:49Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    16,
                    49,
                    1,
                    140,
                    0
                ],
                "title": "Unearthing Gems from Stones: Policy Optimization with Negative Sample\n  Augmentation for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unearthing Gems from Stones: Policy Optimization with Negative Sample\n  Augmentation for LLM Reasoning"
                },
                "summary": "Recent advances in reasoning language models have witnessed a paradigm shift\nfrom short to long CoT pattern. Given the substantial computational cost of\nrollouts in long CoT models, maximizing the utility of fixed training datasets\nbecomes crucial. Our analysis reveals that negative responses contain valuable\ncomponents such as self-reflection and error-correction steps, yet primary\nexisting methods either completely discard negative samples (RFT) or apply\nequal penalization across all tokens (RL), failing to leverage these potential\nlearning signals. In light of this, we propose Behavior Constrained Policy\nGradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline\nRL framework that encompasses three stages: 1) sample segmentation, 2)\nconsensus-based step correctness assessment combining LLM and PRM judgers, and\n3) policy optimization with NSA designed to effectively mine positive steps\nwithin negative samples. Experimental results show that BCPG-NSA outperforms\nbaselines on several challenging math/coding reasoning benchmarks using the\nsame training dataset, achieving improved sample efficiency and demonstrating\nrobustness and scalability when extended to multiple iterations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reasoning language models have witnessed a paradigm shift\nfrom short to long CoT pattern. Given the substantial computational cost of\nrollouts in long CoT models, maximizing the utility of fixed training datasets\nbecomes crucial. Our analysis reveals that negative responses contain valuable\ncomponents such as self-reflection and error-correction steps, yet primary\nexisting methods either completely discard negative samples (RFT) or apply\nequal penalization across all tokens (RL), failing to leverage these potential\nlearning signals. In light of this, we propose Behavior Constrained Policy\nGradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline\nRL framework that encompasses three stages: 1) sample segmentation, 2)\nconsensus-based step correctness assessment combining LLM and PRM judgers, and\n3) policy optimization with NSA designed to effectively mine positive steps\nwithin negative samples. Experimental results show that BCPG-NSA outperforms\nbaselines on several challenging math/coding reasoning benchmarks using the\nsame training dataset, achieving improved sample efficiency and demonstrating\nrobustness and scalability when extended to multiple iterations."
                },
                "authors": [
                    {
                        "name": "Zhaohui Yang"
                    },
                    {
                        "name": "Yuxiao Ye"
                    },
                    {
                        "name": "Shilei Jiang"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Linjing Li"
                    },
                    {
                        "name": "Shihong Deng"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14403v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14403v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10156v1",
                "updated": "2025-07-14T11:12:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    12,
                    30,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T11:12:30Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    12,
                    30,
                    0,
                    195,
                    0
                ],
                "title": "Introducing the Swiss Food Knowledge Graph: AI for Context-Aware\n  Nutrition Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introducing the Swiss Food Knowledge Graph: AI for Context-Aware\n  Nutrition Recommendation"
                },
                "summary": "AI has driven significant progress in the nutrition field, especially through\nmultimedia-based automatic dietary assessment. However, existing automatic\ndietary assessment systems often overlook critical non-visual factors, such as\nrecipe-specific ingredient substitutions that can significantly alter\nnutritional content, and rarely account for individual dietary needs, including\nallergies, restrictions, cultural practices, and personal preferences. In\nSwitzerland, while food-related information is available, it remains\nfragmented, and no centralized repository currently integrates all relevant\nnutrition-related aspects within a Swiss context. To bridge this divide, we\nintroduce the Swiss Food Knowledge Graph (SwissFKG), the first resource, to our\nbest knowledge, to unite recipes, ingredients, and their substitutions with\nnutrient data, dietary restrictions, allergen information, and national\nnutrition guidelines under one graph. We establish a LLM-powered enrichment\npipeline for populating the graph, whereby we further present the first\nbenchmark of four off-the-shelf (<70 B parameter) LLMs for food knowledge\naugmentation. Our results demonstrate that LLMs can effectively enrich the\ngraph with relevant nutritional information. Our SwissFKG goes beyond recipe\nrecommendations by offering ingredient-level information such as allergen and\ndietary restriction information, and guidance aligned with nutritional\nguidelines. Moreover, we implement a Graph-RAG application to showcase how the\nSwissFKG's rich natural-language data structure can help LLM answer\nuser-specific nutrition queries, and we evaluate LLM-embedding pairings by\ncomparing user-query responses against predefined expected answers. As such,\nour work lays the foundation for the next generation of dietary assessment\ntools that blend visual, contextual, and cultural dimensions of eating.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI has driven significant progress in the nutrition field, especially through\nmultimedia-based automatic dietary assessment. However, existing automatic\ndietary assessment systems often overlook critical non-visual factors, such as\nrecipe-specific ingredient substitutions that can significantly alter\nnutritional content, and rarely account for individual dietary needs, including\nallergies, restrictions, cultural practices, and personal preferences. In\nSwitzerland, while food-related information is available, it remains\nfragmented, and no centralized repository currently integrates all relevant\nnutrition-related aspects within a Swiss context. To bridge this divide, we\nintroduce the Swiss Food Knowledge Graph (SwissFKG), the first resource, to our\nbest knowledge, to unite recipes, ingredients, and their substitutions with\nnutrient data, dietary restrictions, allergen information, and national\nnutrition guidelines under one graph. We establish a LLM-powered enrichment\npipeline for populating the graph, whereby we further present the first\nbenchmark of four off-the-shelf (<70 B parameter) LLMs for food knowledge\naugmentation. Our results demonstrate that LLMs can effectively enrich the\ngraph with relevant nutritional information. Our SwissFKG goes beyond recipe\nrecommendations by offering ingredient-level information such as allergen and\ndietary restriction information, and guidance aligned with nutritional\nguidelines. Moreover, we implement a Graph-RAG application to showcase how the\nSwissFKG's rich natural-language data structure can help LLM answer\nuser-specific nutrition queries, and we evaluate LLM-embedding pairings by\ncomparing user-query responses against predefined expected answers. As such,\nour work lays the foundation for the next generation of dietary assessment\ntools that blend visual, contextual, and cultural dimensions of eating."
                },
                "authors": [
                    {
                        "name": "Lubnaa Abdur Rahman"
                    },
                    {
                        "name": "Ioannis Papathanail"
                    },
                    {
                        "name": "Stavroula Mougiakakou"
                    }
                ],
                "author_detail": {
                    "name": "Stavroula Mougiakakou"
                },
                "author": "Stavroula Mougiakakou",
                "arxiv_comment": "10 pages, 2 Figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10155v1",
                "updated": "2025-07-14T11:10:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    10,
                    2,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T11:10:02Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    10,
                    2,
                    0,
                    195,
                    0
                ],
                "title": "Task-Based Flexible Feature Distillation for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-Based Flexible Feature Distillation for LLMs"
                },
                "summary": "Knowledge Distillation (KD) in general and feature distillation in particular\nare promising techniques for reducing the high computational demand of large\nlanguage models (LLMs). However, traditional feature KD methods typically\nassume that the teacher and the student share the same hidden size, limiting\nthe flexibility of the student's architecture. A common solution to this\nproblem involves training a linear projector to align their feature spaces, but\nthis introduces additional parameters that must be learned from scratch and\noften degrades performance on downstream tasks, especially in generative\nsettings. To address this issue, in this work, we propose a novel task-based\nfeature distillation method that enables knowledge transfer between teacher and\nstudent models with different hidden layer dimensions, without introducing any\nnew parameters. Leveraging the insight that only a subset of LLM components\ncontribute significantly to a specific downstream task, our approach identifies\nthe most task-relevant hidden units in the teacher and directly distills their\nactivations to the student. Our method is flexible and easily integrates with\nother distillation frameworks. Empirical results show consistent improvements\nover prior approaches across diverse tasks, including classification,\ninstruction-following, and summarization, achieving up to a 3\\% performance\ngain over the linear projection baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Distillation (KD) in general and feature distillation in particular\nare promising techniques for reducing the high computational demand of large\nlanguage models (LLMs). However, traditional feature KD methods typically\nassume that the teacher and the student share the same hidden size, limiting\nthe flexibility of the student's architecture. A common solution to this\nproblem involves training a linear projector to align their feature spaces, but\nthis introduces additional parameters that must be learned from scratch and\noften degrades performance on downstream tasks, especially in generative\nsettings. To address this issue, in this work, we propose a novel task-based\nfeature distillation method that enables knowledge transfer between teacher and\nstudent models with different hidden layer dimensions, without introducing any\nnew parameters. Leveraging the insight that only a subset of LLM components\ncontribute significantly to a specific downstream task, our approach identifies\nthe most task-relevant hidden units in the teacher and directly distills their\nactivations to the student. Our method is flexible and easily integrates with\nother distillation frameworks. Empirical results show consistent improvements\nover prior approaches across diverse tasks, including classification,\ninstruction-following, and summarization, achieving up to a 3\\% performance\ngain over the linear projection baseline."
                },
                "authors": [
                    {
                        "name": "Khouloud Saadi"
                    },
                    {
                        "name": "Di Wang"
                    }
                ],
                "author_detail": {
                    "name": "Di Wang"
                },
                "author": "Di Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10150v1",
                "updated": "2025-07-14T10:53:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    10,
                    53,
                    47,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T10:53:47Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    10,
                    53,
                    47,
                    0,
                    195,
                    0
                ],
                "title": "Past-Future Scheduler for LLM Serving under SLA Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Past-Future Scheduler for LLM Serving under SLA Guarantees"
                },
                "summary": "The exploration and application of Large Language Models (LLMs) is thriving.\nTo reduce deployment costs, continuous batching has become an essential feature\nin current service frameworks. The effectiveness of continuous batching relies\non an accurate estimate of the memory requirements of requests. However, due to\nthe diversity in request output lengths, existing frameworks tend to adopt\naggressive or conservative schedulers, which often result in significant\noverestimation or underestimation of memory consumption. Consequently, they\nsuffer from harmful request evictions or prolonged queuing times, failing to\nachieve satisfactory throughput under strict Service Level Agreement (SLA)\nguarantees (a.k.a. goodput), across various LLM application scenarios with\ndiffering input-output length distributions. To address this issue, we propose\na novel Past-Future scheduler that precisely estimates the peak memory\nresources required by the running batch via considering the historical\ndistribution of request output lengths and calculating memory occupancy at each\nfuture time point. It adapts to applications with all types of input-output\nlength distributions, balancing the trade-off between request queuing and\nharmful evictions, thereby consistently achieving better goodput. Furthermore,\nto validate the effectiveness of the proposed scheduler, we developed a\nhigh-performance LLM serving framework, LightLLM, that implements the\nPast-Future scheduler. Compared to existing aggressive or conservative\nschedulers, LightLLM demonstrates superior goodput, achieving up to 2-3$\\times$\nhigher goodput than other schedulers under heavy loads. LightLLM is open source\nto boost the research in such direction (https://github.com/ModelTC/lightllm).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exploration and application of Large Language Models (LLMs) is thriving.\nTo reduce deployment costs, continuous batching has become an essential feature\nin current service frameworks. The effectiveness of continuous batching relies\non an accurate estimate of the memory requirements of requests. However, due to\nthe diversity in request output lengths, existing frameworks tend to adopt\naggressive or conservative schedulers, which often result in significant\noverestimation or underestimation of memory consumption. Consequently, they\nsuffer from harmful request evictions or prolonged queuing times, failing to\nachieve satisfactory throughput under strict Service Level Agreement (SLA)\nguarantees (a.k.a. goodput), across various LLM application scenarios with\ndiffering input-output length distributions. To address this issue, we propose\na novel Past-Future scheduler that precisely estimates the peak memory\nresources required by the running batch via considering the historical\ndistribution of request output lengths and calculating memory occupancy at each\nfuture time point. It adapts to applications with all types of input-output\nlength distributions, balancing the trade-off between request queuing and\nharmful evictions, thereby consistently achieving better goodput. Furthermore,\nto validate the effectiveness of the proposed scheduler, we developed a\nhigh-performance LLM serving framework, LightLLM, that implements the\nPast-Future scheduler. Compared to existing aggressive or conservative\nschedulers, LightLLM demonstrates superior goodput, achieving up to 2-3$\\times$\nhigher goodput than other schedulers under heavy loads. LightLLM is open source\nto boost the research in such direction (https://github.com/ModelTC/lightllm)."
                },
                "authors": [
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Shihao Bai"
                    },
                    {
                        "name": "Siyu Wu"
                    },
                    {
                        "name": "Yunqian Fan"
                    },
                    {
                        "name": "Zaijun Wang"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Hailong Yang"
                    },
                    {
                        "name": "Xianglong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xianglong Liu"
                },
                "author": "Xianglong Liu",
                "arxiv_doi": "10.1145/3676641.3716011",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716011",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.10150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to ASPLOS 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10134v1",
                "updated": "2025-07-14T10:24:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    10,
                    24,
                    43,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T10:24:43Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    10,
                    24,
                    43,
                    0,
                    195,
                    0
                ],
                "title": "FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for\n  Fresh Data Collection in UAV-Assisted Wildfire Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for\n  Fresh Data Collection in UAV-Assisted Wildfire Monitoring"
                },
                "summary": "Unmanned Aerial Vehicles (UAVs) are vital for public safety, particularly in\nwildfire monitoring, where early detection minimizes environmental impact. In\nUAV-Assisted Wildfire Monitoring (UAWM) systems, joint optimization of sensor\ntransmission scheduling and velocity is critical for minimizing Age of\nInformation (AoI) from stale sensor data. Deep Reinforcement Learning (DRL) has\nbeen used for such optimization; however, its limitations such as low sampling\nefficiency, simulation-to-reality gaps, and complex training render it\nunsuitable for time-critical applications like wildfire monitoring. This paper\nintroduces a new online Flight Resource Allocation scheme based on LLM-Enabled\nIn-Context Learning (FRSICL) to jointly optimize the UAV's flight control and\ndata collection schedule along the trajectory in real time, thereby\nasymptotically minimizing the average AoI across ground sensors. In contrast to\nDRL, FRSICL generates data collection schedules and controls velocity using\nnatural language task descriptions and feedback from the environment, enabling\ndynamic decision-making without extensive retraining. Simulation results\nconfirm the effectiveness of the proposed FRSICL compared to Proximal Policy\nOptimization (PPO) and Nearest-Neighbor baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned Aerial Vehicles (UAVs) are vital for public safety, particularly in\nwildfire monitoring, where early detection minimizes environmental impact. In\nUAV-Assisted Wildfire Monitoring (UAWM) systems, joint optimization of sensor\ntransmission scheduling and velocity is critical for minimizing Age of\nInformation (AoI) from stale sensor data. Deep Reinforcement Learning (DRL) has\nbeen used for such optimization; however, its limitations such as low sampling\nefficiency, simulation-to-reality gaps, and complex training render it\nunsuitable for time-critical applications like wildfire monitoring. This paper\nintroduces a new online Flight Resource Allocation scheme based on LLM-Enabled\nIn-Context Learning (FRSICL) to jointly optimize the UAV's flight control and\ndata collection schedule along the trajectory in real time, thereby\nasymptotically minimizing the average AoI across ground sensors. In contrast to\nDRL, FRSICL generates data collection schedules and controls velocity using\nnatural language task descriptions and feedback from the environment, enabling\ndynamic decision-making without extensive retraining. Simulation results\nconfirm the effectiveness of the proposed FRSICL compared to Proximal Policy\nOptimization (PPO) and Nearest-Neighbor baselines."
                },
                "authors": [
                    {
                        "name": "Yousef Emami"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Miguel Gutierrez Gaitan"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Luis Almeida"
                    }
                ],
                "author_detail": {
                    "name": "Luis Almeida"
                },
                "author": "Luis Almeida",
                "arxiv_comment": "8 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "53-01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10131v1",
                "updated": "2025-07-14T10:21:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    10,
                    21,
                    27,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T10:21:27Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    10,
                    21,
                    27,
                    0,
                    195,
                    0
                ],
                "title": "Probabilistic Human Intent Prediction for Mobile Manipulation: An\n  Evaluation with Human-Inspired Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic Human Intent Prediction for Mobile Manipulation: An\n  Evaluation with Human-Inspired Constraints"
                },
                "summary": "Accurate inference of human intent enables human-robot collaboration without\nconstraining human control or causing conflicts between humans and robots. We\npresent GUIDER (Global User Intent Dual-phase Estimation for Robots), a\nprobabilistic framework that enables a robot to estimate the intent of human\noperators. GUIDER maintains two coupled belief layers, one tracking navigation\ngoals and the other manipulation goals. In the Navigation phase, a Synergy Map\nblends controller velocity with an occupancy grid to rank interaction areas.\nUpon arrival at a goal, an autonomous multi-view scan builds a local 3D cloud.\nThe Manipulation phase combines U2Net saliency, FastSAM instance saliency, and\nthree geometric grasp-feasibility tests, with an end-effector kinematics-aware\nupdate rule that evolves object probabilities in real-time. GUIDER can\nrecognize areas and objects of intent without predefined goals. We evaluated\nGUIDER on 25 trials (five participants x five task variants) in Isaac Sim, and\ncompared it with two baselines, one for navigation and one for manipulation.\nAcross the 25 trials, GUIDER achieved a median stability of 93-100% during\nnavigation, compared with 60-100% for the BOIR baseline, with an improvement of\n39.5% in a redirection scenario (T5). During manipulation, stability reached\n94-100% (versus 69-100% for Trajectron), with a 31.4% difference in a\nredirection task (T3). In geometry-constrained trials (manipulation), GUIDER\nrecognized the object intent three times earlier than Trajectron (median\nremaining time to confident prediction 23.6 s vs 7.8 s). These results validate\nour dual-phase framework and show improvements in intent inference in both\nphases of mobile manipulation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate inference of human intent enables human-robot collaboration without\nconstraining human control or causing conflicts between humans and robots. We\npresent GUIDER (Global User Intent Dual-phase Estimation for Robots), a\nprobabilistic framework that enables a robot to estimate the intent of human\noperators. GUIDER maintains two coupled belief layers, one tracking navigation\ngoals and the other manipulation goals. In the Navigation phase, a Synergy Map\nblends controller velocity with an occupancy grid to rank interaction areas.\nUpon arrival at a goal, an autonomous multi-view scan builds a local 3D cloud.\nThe Manipulation phase combines U2Net saliency, FastSAM instance saliency, and\nthree geometric grasp-feasibility tests, with an end-effector kinematics-aware\nupdate rule that evolves object probabilities in real-time. GUIDER can\nrecognize areas and objects of intent without predefined goals. We evaluated\nGUIDER on 25 trials (five participants x five task variants) in Isaac Sim, and\ncompared it with two baselines, one for navigation and one for manipulation.\nAcross the 25 trials, GUIDER achieved a median stability of 93-100% during\nnavigation, compared with 60-100% for the BOIR baseline, with an improvement of\n39.5% in a redirection scenario (T5). During manipulation, stability reached\n94-100% (versus 69-100% for Trajectron), with a 31.4% difference in a\nredirection task (T3). In geometry-constrained trials (manipulation), GUIDER\nrecognized the object intent three times earlier than Trajectron (median\nremaining time to confident prediction 23.6 s vs 7.8 s). These results validate\nour dual-phase framework and show improvements in intent inference in both\nphases of mobile manipulation tasks."
                },
                "authors": [
                    {
                        "name": "Cesar Alan Contreras"
                    },
                    {
                        "name": "Manolis Chiou"
                    },
                    {
                        "name": "Alireza Rastegarpanah"
                    },
                    {
                        "name": "Michal Szulik"
                    },
                    {
                        "name": "Rustam Stolkin"
                    }
                ],
                "author_detail": {
                    "name": "Rustam Stolkin"
                },
                "author": "Rustam Stolkin",
                "arxiv_comment": "Submitted to Journal of Intelligent & Robotic Systems (Under Review)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08307v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08307v2",
                "updated": "2025-07-14T10:16:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    10,
                    16,
                    32,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-11T04:48:12Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    4,
                    48,
                    12,
                    4,
                    192,
                    0
                ],
                "title": "M2DAO-Talker: Harmonizing Multi-granular Motion Decoupling and\n  Alternating Optimization for Talking-head Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M2DAO-Talker: Harmonizing Multi-granular Motion Decoupling and\n  Alternating Optimization for Talking-head Generation"
                },
                "summary": "Audio-driven talking head generation holds significant potential for film\nproduction. While existing 3D methods have advanced motion modeling and content\nsynthesis, they often produce rendering artifacts, such as motion blur,\ntemporal jitter, and local penetration, due to limitations in representing\nstable, fine-grained motion fields. Through systematic analysis, we reformulate\ntalking head generation into a unified framework comprising three steps: video\npreprocessing, motion representation, and rendering reconstruction. This\nframework underpins our proposed M2DAO-Talker, which addresses current\nlimitations via multi-granular motion decoupling and alternating optimization.\nSpecifically, we devise a novel 2D portrait preprocessing pipeline to extract\nframe-wise deformation control conditions (motion region segmentation masks,\nand camera parameters) to facilitate motion representation. To ameliorate\nmotion modeling, we elaborate a multi-granular motion decoupling strategy,\nwhich independently models non-rigid (oral and facial) and rigid (head) motions\nfor improved reconstruction accuracy. Meanwhile, a motion consistency\nconstraint is developed to ensure head-torso kinematic consistency, thereby\nmitigating penetration artifacts caused by motion aliasing. In addition, an\nalternating optimization strategy is designed to iteratively refine facial and\noral motion parameters, enabling more realistic video generation. Experiments\nacross multiple datasets show that M2DAO-Talker achieves state-of-the-art\nperformance, with the 2.43 dB PSNR improvement in generation quality and 0.64\ngain in user-evaluated video realness versus TalkingGaussian while with 150 FPS\ninference speed. Our project homepage is\nhttps://m2dao-talker.github.io/M2DAO-Talk.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio-driven talking head generation holds significant potential for film\nproduction. While existing 3D methods have advanced motion modeling and content\nsynthesis, they often produce rendering artifacts, such as motion blur,\ntemporal jitter, and local penetration, due to limitations in representing\nstable, fine-grained motion fields. Through systematic analysis, we reformulate\ntalking head generation into a unified framework comprising three steps: video\npreprocessing, motion representation, and rendering reconstruction. This\nframework underpins our proposed M2DAO-Talker, which addresses current\nlimitations via multi-granular motion decoupling and alternating optimization.\nSpecifically, we devise a novel 2D portrait preprocessing pipeline to extract\nframe-wise deformation control conditions (motion region segmentation masks,\nand camera parameters) to facilitate motion representation. To ameliorate\nmotion modeling, we elaborate a multi-granular motion decoupling strategy,\nwhich independently models non-rigid (oral and facial) and rigid (head) motions\nfor improved reconstruction accuracy. Meanwhile, a motion consistency\nconstraint is developed to ensure head-torso kinematic consistency, thereby\nmitigating penetration artifacts caused by motion aliasing. In addition, an\nalternating optimization strategy is designed to iteratively refine facial and\noral motion parameters, enabling more realistic video generation. Experiments\nacross multiple datasets show that M2DAO-Talker achieves state-of-the-art\nperformance, with the 2.43 dB PSNR improvement in generation quality and 0.64\ngain in user-evaluated video realness versus TalkingGaussian while with 150 FPS\ninference speed. Our project homepage is\nhttps://m2dao-talker.github.io/M2DAO-Talk.github.io."
                },
                "authors": [
                    {
                        "name": "Kui Jiang"
                    },
                    {
                        "name": "Shiyu Liu"
                    },
                    {
                        "name": "Junjun Jiang"
                    },
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Hongxun Yao"
                    },
                    {
                        "name": "Xiaopeng Fan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Fan"
                },
                "author": "Xiaopeng Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08307v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08307v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10124v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10124v1",
                "updated": "2025-07-14T10:09:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    10,
                    9,
                    46,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T10:09:46Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    10,
                    9,
                    46,
                    0,
                    195,
                    0
                ],
                "title": "Could you be wrong: Debiasing LLMs using a metacognitive prompt for\n  improving human decision making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Could you be wrong: Debiasing LLMs using a metacognitive prompt for\n  improving human decision making"
                },
                "summary": "Identifying bias in LLMs is ongoing. Because they are still in development,\nwhat is true today may be false tomorrow. We therefore need general strategies\nfor debiasing that will outlive current models. Strategies developed for\ndebiasing human decision making offer one promising approach as they\nincorporate an LLM-style prompt intervention designed to bring latent knowledge\ninto awareness during decision making. LLMs trained on vast amounts of\ninformation contain information about potential biases, counter-arguments, and\ncontradictory evidence, but that information may only be brought to bear if\nprompted. Metacognitive prompts developed in the human decision making\nliterature are designed to achieve this, and as I demonstrate here, they show\npromise with LLMs. The prompt I focus on here is \"could you be wrong?\"\nFollowing an LLM response, this prompt leads LLMs to produce additional\ninformation, including why they answered as they did, errors, biases,\ncontradictory evidence, and alternatives, none of which were apparent in their\ninitial response. Indeed, this metaknowledge often reveals that how LLMs and\nusers interpret prompts are not aligned. Here I demonstrate this prompt using a\nset of questions taken from recent articles about LLM biases, including\nimplicit discriminatory biases and failures of metacognition. \"Could you be\nwrong\" prompts the LLM to identify its own biases and produce cogent\nmetacognitive reflection. I also present another example involving convincing\nbut incomplete information, which is readily corrected by the metacognitive\nprompt. In sum, this work argues that human psychology offers a new avenue for\nprompt engineering, leveraging a long history of effective prompt-based\nimprovements to human decision making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying bias in LLMs is ongoing. Because they are still in development,\nwhat is true today may be false tomorrow. We therefore need general strategies\nfor debiasing that will outlive current models. Strategies developed for\ndebiasing human decision making offer one promising approach as they\nincorporate an LLM-style prompt intervention designed to bring latent knowledge\ninto awareness during decision making. LLMs trained on vast amounts of\ninformation contain information about potential biases, counter-arguments, and\ncontradictory evidence, but that information may only be brought to bear if\nprompted. Metacognitive prompts developed in the human decision making\nliterature are designed to achieve this, and as I demonstrate here, they show\npromise with LLMs. The prompt I focus on here is \"could you be wrong?\"\nFollowing an LLM response, this prompt leads LLMs to produce additional\ninformation, including why they answered as they did, errors, biases,\ncontradictory evidence, and alternatives, none of which were apparent in their\ninitial response. Indeed, this metaknowledge often reveals that how LLMs and\nusers interpret prompts are not aligned. Here I demonstrate this prompt using a\nset of questions taken from recent articles about LLM biases, including\nimplicit discriminatory biases and failures of metacognition. \"Could you be\nwrong\" prompts the LLM to identify its own biases and produce cogent\nmetacognitive reflection. I also present another example involving convincing\nbut incomplete information, which is readily corrected by the metacognitive\nprompt. In sum, this work argues that human psychology offers a new avenue for\nprompt engineering, leveraging a long history of effective prompt-based\nimprovements to human decision making."
                },
                "authors": [
                    {
                        "name": "Thomas T. Hills"
                    }
                ],
                "author_detail": {
                    "name": "Thomas T. Hills"
                },
                "author": "Thomas T. Hills",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10124v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10124v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09000v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09000v2",
                "updated": "2025-07-14T10:02:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    10,
                    2,
                    7,
                    0,
                    195,
                    0
                ],
                "published": "2025-01-15T18:29:32Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    18,
                    29,
                    32,
                    2,
                    15,
                    0
                ],
                "title": "Bayesian analysis of analog gravity systems with the Rezzolla-Zhidenko\n  metric",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian analysis of analog gravity systems with the Rezzolla-Zhidenko\n  metric"
                },
                "summary": "Analog gravity systems have the unique opportunity to probe theoretical\naspects of black hole physics in a controlled laboratory environment that one\ncannot easily observe for astrophysical black holes. In this work, we address\nthe question of whether one could use controlled initial perturbations to\nexcite the black hole ringdown and infer the effective black hole metric. Using\na theory-agnostic ansatz for the effective metric described by the\nRezzolla-Zhidenko metric and evolving perturbations on that background, we\nquantify with Bayesian analysis what regions of the effective spacetime could\nbe constrained in experiments. In contrast to standard ringdown analyses based\non quasi-normal mode extraction, a laboratory-controlled setup, in combination\nwith our framework, allows one to model the entire signal, including the prompt\nresponse and possible effects of late-time tails. Therefore, it has the\nintriguing advantage of not relying on start and end times when the\nsuperposition of quasi-normal modes is a good signal approximation. It also\navoids the non-trivial question of how many modes are present. We demonstrate\nthat this approach is feasible in principle and discuss opportunities beyond\nthis study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog gravity systems have the unique opportunity to probe theoretical\naspects of black hole physics in a controlled laboratory environment that one\ncannot easily observe for astrophysical black holes. In this work, we address\nthe question of whether one could use controlled initial perturbations to\nexcite the black hole ringdown and infer the effective black hole metric. Using\na theory-agnostic ansatz for the effective metric described by the\nRezzolla-Zhidenko metric and evolving perturbations on that background, we\nquantify with Bayesian analysis what regions of the effective spacetime could\nbe constrained in experiments. In contrast to standard ringdown analyses based\non quasi-normal mode extraction, a laboratory-controlled setup, in combination\nwith our framework, allows one to model the entire signal, including the prompt\nresponse and possible effects of late-time tails. Therefore, it has the\nintriguing advantage of not relying on start and end times when the\nsuperposition of quasi-normal modes is a good signal approximation. It also\navoids the non-trivial question of how many modes are present. We demonstrate\nthat this approach is feasible in principle and discuss opportunities beyond\nthis study."
                },
                "authors": [
                    {
                        "name": "Saulo Albuquerque"
                    },
                    {
                        "name": "Sebastian H. V√∂lkel"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian H. V√∂lkel"
                },
                "author": "Sebastian H. V√∂lkel",
                "arxiv_doi": "10.1103/kwrg-rs71",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/kwrg-rs71",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.09000v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09000v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "13 pages, 11 figures",
                "arxiv_journal_ref": "Phys. Rev. D 111, 124020 (2025)",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02882v2",
                "updated": "2025-07-14T09:56:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    56,
                    47,
                    0,
                    195,
                    0
                ],
                "published": "2025-04-02T05:47:28Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    5,
                    47,
                    28,
                    2,
                    92,
                    0
                ],
                "title": "DiaTool-DPO: Multi-Turn Direct Preference Optimization for\n  Tool-Augmented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiaTool-DPO: Multi-Turn Direct Preference Optimization for\n  Tool-Augmented Large Language Models"
                },
                "summary": "Tool-Augmented Larage Language Models (TA-LLMs) have shown promise in\nreal-world applications, but face challenges in handling incomplete queries and\nout-of-scope requests. While existing approaches rely mainly on Supervised\nFine-Tuning with expert trajectories, we propose DiaTool-DPO, a novel method\nthat enhances TA-LLM's dialogue capabilities through Direct Preference\nOptimization. We model TA-LLM interactions as a Markov Decision Process with 5\ndistinct dialogue states and categorize user queries into 3 types based on\ntheir state transition trajectories. We automatically construct paired\ntrajectory datasets of correct and incorrect dialogue flows and introduce a\nspecialized objective loss for dialogue control. Our comprehensive evaluation\ndemonstrates that DiaTool-DPO approaches GPT-4o's performance (94.8% in\ninformation gathering, 91% in tool call rejection) with substantial\nimprovements over baseline (44% and 9.6% respectively) while maintaining core\nfunctionality. Our approach opens new possibilities for developing TA-LLMs that\ncan handle diverse real-world scenarios without requiring additional expert\ndemonstrations or human labeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-Augmented Larage Language Models (TA-LLMs) have shown promise in\nreal-world applications, but face challenges in handling incomplete queries and\nout-of-scope requests. While existing approaches rely mainly on Supervised\nFine-Tuning with expert trajectories, we propose DiaTool-DPO, a novel method\nthat enhances TA-LLM's dialogue capabilities through Direct Preference\nOptimization. We model TA-LLM interactions as a Markov Decision Process with 5\ndistinct dialogue states and categorize user queries into 3 types based on\ntheir state transition trajectories. We automatically construct paired\ntrajectory datasets of correct and incorrect dialogue flows and introduce a\nspecialized objective loss for dialogue control. Our comprehensive evaluation\ndemonstrates that DiaTool-DPO approaches GPT-4o's performance (94.8% in\ninformation gathering, 91% in tool call rejection) with substantial\nimprovements over baseline (44% and 9.6% respectively) while maintaining core\nfunctionality. Our approach opens new possibilities for developing TA-LLMs that\ncan handle diverse real-world scenarios without requiring additional expert\ndemonstrations or human labeling."
                },
                "authors": [
                    {
                        "name": "Sunghee Jung"
                    },
                    {
                        "name": "Donghun Lee"
                    },
                    {
                        "name": "Shinbok Lee"
                    },
                    {
                        "name": "Gaeun Seo"
                    },
                    {
                        "name": "Daniel Lee"
                    },
                    {
                        "name": "Byeongil Ko"
                    },
                    {
                        "name": "Junrae Cho"
                    },
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Eunggyun Kim"
                    },
                    {
                        "name": "Myeongcheol Shin"
                    }
                ],
                "author_detail": {
                    "name": "Myeongcheol Shin"
                },
                "author": "Myeongcheol Shin",
                "arxiv_comment": "Accepted to SIGDIAL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06272v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06272v2",
                "updated": "2025-07-14T09:49:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    49,
                    47,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-08T07:46:26Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    7,
                    46,
                    26,
                    1,
                    189,
                    0
                ],
                "title": "LIRA: Inferring Segmentation in Large Multi-modal Models with Local\n  Interleaved Region Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LIRA: Inferring Segmentation in Large Multi-modal Models with Local\n  Interleaved Region Assistance"
                },
                "summary": "While large multi-modal models (LMMs) demonstrate promising capabilities in\nsegmentation and comprehension, they still struggle with two limitations:\ninaccurate segmentation and hallucinated comprehension. These challenges stem\nprimarily from constraints in weak visual comprehension and a lack of\nfine-grained perception. To alleviate these limitations, we propose LIRA, a\nframework that capitalizes on the complementary relationship between visual\ncomprehension and segmentation via two key components: (1) Semantic-Enhanced\nFeature Extractor (SEFE) improves object attribute inference by fusing semantic\nand pixel-level features, leading to more accurate segmentation; (2)\nInterleaved Local Visual Coupling (ILVC) autoregressively generates local\ndescriptions after extracting local features based on segmentation masks,\noffering fine-grained supervision to mitigate hallucinations. Furthermore, we\nfind that the precision of object segmentation is positively correlated with\nthe latent related semantics of the <seg> token. To quantify this relationship\nand the model's potential semantic inferring ability, we introduce the\nAttributes Evaluation (AttrEval) dataset. Our experiments show that LIRA\nachieves state-of-the-art performance in both segmentation and comprehension\ntasks. Code will be available at https://github.com/echo840/LIRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large multi-modal models (LMMs) demonstrate promising capabilities in\nsegmentation and comprehension, they still struggle with two limitations:\ninaccurate segmentation and hallucinated comprehension. These challenges stem\nprimarily from constraints in weak visual comprehension and a lack of\nfine-grained perception. To alleviate these limitations, we propose LIRA, a\nframework that capitalizes on the complementary relationship between visual\ncomprehension and segmentation via two key components: (1) Semantic-Enhanced\nFeature Extractor (SEFE) improves object attribute inference by fusing semantic\nand pixel-level features, leading to more accurate segmentation; (2)\nInterleaved Local Visual Coupling (ILVC) autoregressively generates local\ndescriptions after extracting local features based on segmentation masks,\noffering fine-grained supervision to mitigate hallucinations. Furthermore, we\nfind that the precision of object segmentation is positively correlated with\nthe latent related semantics of the <seg> token. To quantify this relationship\nand the model's potential semantic inferring ability, we introduce the\nAttributes Evaluation (AttrEval) dataset. Our experiments show that LIRA\nachieves state-of-the-art performance in both segmentation and comprehension\ntasks. Code will be available at https://github.com/echo840/LIRA."
                },
                "authors": [
                    {
                        "name": "Zhang Li"
                    },
                    {
                        "name": "Biao Yang"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Shuo Zhang"
                    },
                    {
                        "name": "Zhiyin Ma"
                    },
                    {
                        "name": "Shuo Zhang"
                    },
                    {
                        "name": "Liang Yin"
                    },
                    {
                        "name": "Linger Deng"
                    },
                    {
                        "name": "Yabo Sun"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "arxiv_comment": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06272v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06272v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10103v1",
                "updated": "2025-07-14T09:41:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    41,
                    51,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T09:41:51Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    41,
                    51,
                    0,
                    195,
                    0
                ],
                "title": "Accelerating Automatic Program Repair with Dual Retrieval-Augmented\n  Fine-Tuning and Patch Generation on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Automatic Program Repair with Dual Retrieval-Augmented\n  Fine-Tuning and Patch Generation on Large Language Models"
                },
                "summary": "Automated Program Repair (APR) is essential for ensuring software reliability\nand quality while enhancing efficiency and reducing developers' workload.\nAlthough rule-based and learning-based APR methods have demonstrated their\neffectiveness, their performance was constrained by the defect type of repair,\nthe quality of training data, and the size of model parameters. Recently, Large\nLanguage Models (LLMs) combined with Retrieval-Augmented-Generation (RAG) have\nbeen increasingly adopted in APR tasks. However, current code LLMs and RAG\ndesigns neither fully address code repair tasks nor consider code-specific\nfeatures. To overcome these limitations, we propose SelRepair, a novel APR\napproach with integration of a fine-tuned LLM with a newly-designed dual RAG\nmodule. This approach uses a bug-fix pair dataset for fine-tuning and\nincorporates semantic and syntactic/structural similarity information through\nan RAG selection gate. This design ensures relevant information is retrieved\nefficiently, thereby reducing token length and inference time. Evaluations on\nJava datasets show SelRepair outperforms other APR methods, achieving 26.29%\nand 17.64% in terms of exact match (EM) on different datasets while reducing\ninference time by at least 6.42% with controlled input lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Program Repair (APR) is essential for ensuring software reliability\nand quality while enhancing efficiency and reducing developers' workload.\nAlthough rule-based and learning-based APR methods have demonstrated their\neffectiveness, their performance was constrained by the defect type of repair,\nthe quality of training data, and the size of model parameters. Recently, Large\nLanguage Models (LLMs) combined with Retrieval-Augmented-Generation (RAG) have\nbeen increasingly adopted in APR tasks. However, current code LLMs and RAG\ndesigns neither fully address code repair tasks nor consider code-specific\nfeatures. To overcome these limitations, we propose SelRepair, a novel APR\napproach with integration of a fine-tuned LLM with a newly-designed dual RAG\nmodule. This approach uses a bug-fix pair dataset for fine-tuning and\nincorporates semantic and syntactic/structural similarity information through\nan RAG selection gate. This design ensures relevant information is retrieved\nefficiently, thereby reducing token length and inference time. Evaluations on\nJava datasets show SelRepair outperforms other APR methods, achieving 26.29%\nand 17.64% in terms of exact match (EM) on different datasets while reducing\ninference time by at least 6.42% with controlled input lengths."
                },
                "authors": [
                    {
                        "name": "Hanyang Guo"
                    },
                    {
                        "name": "Xiaoheng Xie"
                    },
                    {
                        "name": "Hong-Ning Dai"
                    },
                    {
                        "name": "Peng Di"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Bishenghui Tao"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10099v1",
                "updated": "2025-07-14T09:34:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    34,
                    33,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T09:34:33Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    34,
                    33,
                    0,
                    195,
                    0
                ],
                "title": "ReDemon UI: Reactive Synthesis by Demonstration for Web UI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReDemon UI: Reactive Synthesis by Demonstration for Web UI"
                },
                "summary": "ReDemon UI synthesizes React applications from user demonstrations, enabling\ndesigners and non-expert programmers to create UIs that integrate with standard\nUI prototyping workflows. Users provide a static mockup sketch with event\nhandler holes and demonstrate desired runtime behaviors by interacting with the\nrendered mockup and editing the sketch. ReDemon UI identifies reactive data and\nsynthesizes a React program with correct state update logic. We utilize\nenumerative synthesis for simple UIs and LLMs for more complex UIs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReDemon UI synthesizes React applications from user demonstrations, enabling\ndesigners and non-expert programmers to create UIs that integrate with standard\nUI prototyping workflows. Users provide a static mockup sketch with event\nhandler holes and demonstrate desired runtime behaviors by interacting with the\nrendered mockup and editing the sketch. ReDemon UI identifies reactive data and\nsynthesizes a React program with correct state update logic. We utilize\nenumerative synthesis for simple UIs and LLMs for more complex UIs."
                },
                "authors": [
                    {
                        "name": "Jay Lee"
                    },
                    {
                        "name": "Gyuhyeok Oh"
                    },
                    {
                        "name": "Joongwon Ahn"
                    },
                    {
                        "name": "Xiaokang Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Qiu"
                },
                "author": "Xiaokang Qiu",
                "arxiv_comment": "Submitted to UIST 2025 Posters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10098v1",
                "updated": "2025-07-14T09:33:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    33,
                    40,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T09:33:40Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    33,
                    40,
                    0,
                    195,
                    0
                ],
                "title": "Fusing Large Language Models with Temporal Transformers for Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusing Large Language Models with Temporal Transformers for Time Series\n  Forecasting"
                },
                "summary": "Recently, large language models (LLMs) have demonstrated powerful\ncapabilities in performing various tasks and thus are applied by recent studies\nto time series forecasting (TSF) tasks, which predict future values with the\ngiven historical time series. Existing LLM-based approaches transfer knowledge\nlearned from text data to time series prediction using prompting or fine-tuning\nstrategies. However, LLMs are proficient at reasoning over discrete tokens and\nsemantic patterns but are not initially designed to model continuous numerical\ntime series data. The gaps between text and time series data lead LLMs to\nachieve inferior performance to a vanilla Transformer model that is directly\ntrained on TSF data. However, the vanilla Transformers often struggle to learn\nhigh-level semantic patterns. In this paper, we design a novel\nTransformer-based architecture that complementarily leverages LLMs and vanilla\nTransformers, so as to integrate the high-level semantic representations\nlearned by LLMs into the temporal information encoded by time series\nTransformers, where a hybrid representation is obtained by fusing the\nrepresentations from the LLM and the Transformer. The resulting fused\nrepresentation contains both historical temporal dynamics and semantic\nvariation patterns, allowing our model to predict more accurate future values.\nExperiments on benchmark datasets demonstrate the effectiveness of the proposed\napproach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have demonstrated powerful\ncapabilities in performing various tasks and thus are applied by recent studies\nto time series forecasting (TSF) tasks, which predict future values with the\ngiven historical time series. Existing LLM-based approaches transfer knowledge\nlearned from text data to time series prediction using prompting or fine-tuning\nstrategies. However, LLMs are proficient at reasoning over discrete tokens and\nsemantic patterns but are not initially designed to model continuous numerical\ntime series data. The gaps between text and time series data lead LLMs to\nachieve inferior performance to a vanilla Transformer model that is directly\ntrained on TSF data. However, the vanilla Transformers often struggle to learn\nhigh-level semantic patterns. In this paper, we design a novel\nTransformer-based architecture that complementarily leverages LLMs and vanilla\nTransformers, so as to integrate the high-level semantic representations\nlearned by LLMs into the temporal information encoded by time series\nTransformers, where a hybrid representation is obtained by fusing the\nrepresentations from the LLM and the Transformer. The resulting fused\nrepresentation contains both historical temporal dynamics and semantic\nvariation patterns, allowing our model to predict more accurate future values.\nExperiments on benchmark datasets demonstrate the effectiveness of the proposed\napproach."
                },
                "authors": [
                    {
                        "name": "Chen Su"
                    },
                    {
                        "name": "Yuanhe Tian"
                    },
                    {
                        "name": "Qinyu Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Yan Song"
                    }
                ],
                "author_detail": {
                    "name": "Yan Song"
                },
                "author": "Yan Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15595v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15595v3",
                "updated": "2025-07-14T09:25:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    25,
                    43,
                    0,
                    195,
                    0
                ],
                "published": "2024-10-21T02:27:24Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    2,
                    27,
                    24,
                    0,
                    295,
                    0
                ],
                "title": "A Comprehensive Survey of Direct Preference Optimization: Datasets,\n  Theories, Variants, and Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey of Direct Preference Optimization: Datasets,\n  Theories, Variants, and Applications"
                },
                "summary": "With the rapid advancement of large language models (LLMs), aligning policy\nmodels with human preferences has become increasingly critical. Direct\nPreference Optimization (DPO) has emerged as a promising approach for\nalignment, acting as an RL-free alternative to Reinforcement Learning from\nHuman Feedback (RLHF). Despite DPO's various advancements and inherent\nlimitations, an in-depth review of these aspects is currently lacking in the\nliterature. In this work, we present a comprehensive review of the challenges\nand opportunities in DPO, covering theoretical analyses, variants, relevant\npreference datasets, and applications. Specifically, we categorize recent\nstudies on DPO based on key research questions to provide a thorough\nunderstanding of DPO's current landscape. Additionally, we propose several\nfuture research directions to offer insights on model alignment for the\nresearch community. An updated collection of relevant papers can be found on\nhttps://github.com/Mr-Loevan/DPO-Survey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of large language models (LLMs), aligning policy\nmodels with human preferences has become increasingly critical. Direct\nPreference Optimization (DPO) has emerged as a promising approach for\nalignment, acting as an RL-free alternative to Reinforcement Learning from\nHuman Feedback (RLHF). Despite DPO's various advancements and inherent\nlimitations, an in-depth review of these aspects is currently lacking in the\nliterature. In this work, we present a comprehensive review of the challenges\nand opportunities in DPO, covering theoretical analyses, variants, relevant\npreference datasets, and applications. Specifically, we categorize recent\nstudies on DPO based on key research questions to provide a thorough\nunderstanding of DPO's current landscape. Additionally, we propose several\nfuture research directions to offer insights on model alignment for the\nresearch community. An updated collection of relevant papers can be found on\nhttps://github.com/Mr-Loevan/DPO-Survey."
                },
                "authors": [
                    {
                        "name": "Wenyi Xiao"
                    },
                    {
                        "name": "Zechuan Wang"
                    },
                    {
                        "name": "Leilei Gan"
                    },
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Zongrui Li"
                    },
                    {
                        "name": "Ruirui Lei"
                    },
                    {
                        "name": "Wanggui He"
                    },
                    {
                        "name": "Luu Anh Tuan"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Hao Jiang"
                    },
                    {
                        "name": "Zhou Zhao"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "arxiv_comment": "45 pages, 12 Figures. Project page:\n  https://github.com/Mr-Loevan/DPO-Survey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15595v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15595v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00200v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00200v2",
                "updated": "2025-07-14T09:19:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    19,
                    59,
                    0,
                    195,
                    0
                ],
                "published": "2025-05-30T20:12:51Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    20,
                    12,
                    51,
                    4,
                    150,
                    0
                ],
                "title": "Structuring Radiology Reports: Challenging LLMs with Lightweight Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structuring Radiology Reports: Challenging LLMs with Lightweight Models"
                },
                "summary": "Radiology reports are critical for clinical decision-making but often lack a\nstandardized format, limiting both human interpretability and machine learning\n(ML) applications. While large language models (LLMs) have shown strong\ncapabilities in reformatting clinical text, their high computational\nrequirements, lack of transparency, and data privacy concerns hinder practical\ndeployment. To address these challenges, we explore lightweight encoder-decoder\nmodels (<300M parameters)-specifically T5 and BERT2BERT-for structuring\nradiology reports from the MIMIC-CXR and CheXpert Plus datasets. We benchmark\nthese models against eight open-source LLMs (1B-70B), adapted using prefix\nprompting, in-context learning (ICL), and low-rank adaptation (LoRA)\nfinetuning. Our best-performing lightweight model outperforms all LLMs adapted\nusing prompt-based techniques on a human-annotated test set. While some\nLoRA-finetuned LLMs achieve modest gains over the lightweight model on the\nFindings section (BLEU 6.4%, ROUGE-L 4.8%, BERTScore 3.6%, F1-RadGraph 1.1%,\nGREEN 3.6%, and F1-SRR-BERT 4.3%), these improvements come at the cost of\nsubstantially greater computational resources. For example, LLaMA-3-70B\nincurred more than 400 times the inference time, cost, and carbon emissions\ncompared to the lightweight model. These results underscore the potential of\nlightweight, task-specific models as sustainable and privacy-preserving\nsolutions for structuring clinical text in resource-constrained healthcare\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radiology reports are critical for clinical decision-making but often lack a\nstandardized format, limiting both human interpretability and machine learning\n(ML) applications. While large language models (LLMs) have shown strong\ncapabilities in reformatting clinical text, their high computational\nrequirements, lack of transparency, and data privacy concerns hinder practical\ndeployment. To address these challenges, we explore lightweight encoder-decoder\nmodels (<300M parameters)-specifically T5 and BERT2BERT-for structuring\nradiology reports from the MIMIC-CXR and CheXpert Plus datasets. We benchmark\nthese models against eight open-source LLMs (1B-70B), adapted using prefix\nprompting, in-context learning (ICL), and low-rank adaptation (LoRA)\nfinetuning. Our best-performing lightweight model outperforms all LLMs adapted\nusing prompt-based techniques on a human-annotated test set. While some\nLoRA-finetuned LLMs achieve modest gains over the lightweight model on the\nFindings section (BLEU 6.4%, ROUGE-L 4.8%, BERTScore 3.6%, F1-RadGraph 1.1%,\nGREEN 3.6%, and F1-SRR-BERT 4.3%), these improvements come at the cost of\nsubstantially greater computational resources. For example, LLaMA-3-70B\nincurred more than 400 times the inference time, cost, and carbon emissions\ncompared to the lightweight model. These results underscore the potential of\nlightweight, task-specific models as sustainable and privacy-preserving\nsolutions for structuring clinical text in resource-constrained healthcare\nsettings."
                },
                "authors": [
                    {
                        "name": "Johannes Moll"
                    },
                    {
                        "name": "Louisa Fay"
                    },
                    {
                        "name": "Asfandyar Azhar"
                    },
                    {
                        "name": "Sophie Ostmeier"
                    },
                    {
                        "name": "Tim Lueth"
                    },
                    {
                        "name": "Sergios Gatidis"
                    },
                    {
                        "name": "Curtis Langlotz"
                    },
                    {
                        "name": "Jean-Benoit Delbrouck"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Benoit Delbrouck"
                },
                "author": "Jean-Benoit Delbrouck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00200v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00200v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10087v1",
                "updated": "2025-07-14T09:13:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    13,
                    7,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T09:13:07Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    13,
                    7,
                    0,
                    195,
                    0
                ],
                "title": "Foundation Model Driven Robotics: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Model Driven Robotics: A Comprehensive Review"
                },
                "summary": "The rapid emergence of foundation models, particularly Large Language Models\n(LLMs) and Vision-Language Models (VLMs), has introduced a transformative\nparadigm in robotics. These models offer powerful capabilities in semantic\nunderstanding, high-level reasoning, and cross-modal generalization, enabling\nsignificant advances in perception, planning, control, and human-robot\ninteraction. This critical review provides a structured synthesis of recent\ndevelopments, categorizing applications across simulation-driven design,\nopen-world execution, sim-to-real transfer, and adaptable robotics. Unlike\nexisting surveys that emphasize isolated capabilities, this work highlights\nintegrated, system-level strategies and evaluates their practical feasibility\nin real-world environments. Key enabling trends such as procedural scene\ngeneration, policy generalization, and multimodal reasoning are discussed\nalongside core bottlenecks, including limited embodiment, lack of multimodal\ndata, safety risks, and computational constraints. Through this lens, this\npaper identifies both the architectural strengths and critical limitations of\nfoundation model-based robotics, highlighting open challenges in real-time\noperation, grounding, resilience, and trust. The review concludes with a\nroadmap for future research aimed at bridging semantic reasoning and physical\nintelligence through more robust, interpretable, and embodied models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid emergence of foundation models, particularly Large Language Models\n(LLMs) and Vision-Language Models (VLMs), has introduced a transformative\nparadigm in robotics. These models offer powerful capabilities in semantic\nunderstanding, high-level reasoning, and cross-modal generalization, enabling\nsignificant advances in perception, planning, control, and human-robot\ninteraction. This critical review provides a structured synthesis of recent\ndevelopments, categorizing applications across simulation-driven design,\nopen-world execution, sim-to-real transfer, and adaptable robotics. Unlike\nexisting surveys that emphasize isolated capabilities, this work highlights\nintegrated, system-level strategies and evaluates their practical feasibility\nin real-world environments. Key enabling trends such as procedural scene\ngeneration, policy generalization, and multimodal reasoning are discussed\nalongside core bottlenecks, including limited embodiment, lack of multimodal\ndata, safety risks, and computational constraints. Through this lens, this\npaper identifies both the architectural strengths and critical limitations of\nfoundation model-based robotics, highlighting open challenges in real-time\noperation, grounding, resilience, and trust. The review concludes with a\nroadmap for future research aimed at bridging semantic reasoning and physical\nintelligence through more robust, interpretable, and embodied models."
                },
                "authors": [
                    {
                        "name": "Muhammad Tayyab Khan"
                    },
                    {
                        "name": "Ammar Waheed"
                    }
                ],
                "author_detail": {
                    "name": "Ammar Waheed"
                },
                "author": "Ammar Waheed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10074v1",
                "updated": "2025-07-14T09:00:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    0,
                    27,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T09:00:27Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    0,
                    27,
                    0,
                    195,
                    0
                ],
                "title": "Learning-Aided Iterative Receiver for Superimposed Pilots: Design and\n  Experimental Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-Aided Iterative Receiver for Superimposed Pilots: Design and\n  Experimental Evaluation"
                },
                "summary": "The superimposed pilot transmission scheme offers substantial potential for\nimproving spectral efficiency in MIMO-OFDM systems, but it presents significant\nchallenges for receiver design due to pilot contamination and data\ninterference. To address these issues, we propose an advanced iterative\nreceiver based on joint channel estimation, detection, and decoding, which\nrefines the receiver outputs through iterative feedback. The proposed receiver\nincorporates two adaptive channel estimation strategies to enhance robustness\nunder time-varying and mismatched channel conditions. First, a variational\nmessage passing (VMP) method and its low-complexity variant (VMP-L) are\nintroduced to perform inference without relying on time-domain correlation.\nSecond, a deep learning (DL) based estimator is developed, featuring a\nconvolutional neural network with a despreading module and an attention\nmechanism to extract and fuse relevant channel features. Extensive simulations\nunder multi-stream and high-mobility scenarios demonstrate that the proposed\nreceiver consistently outperforms conventional orthogonal pilot baselines in\nboth throughput and block error rate. Moreover, over-the-air experiments\nvalidate the practical effectiveness of the proposed design. Among the methods,\nthe DL based estimator achieves a favorable trade-off between performance and\ncomplexity, highlighting its suitability for real-world deployment in dynamic\nwireless environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The superimposed pilot transmission scheme offers substantial potential for\nimproving spectral efficiency in MIMO-OFDM systems, but it presents significant\nchallenges for receiver design due to pilot contamination and data\ninterference. To address these issues, we propose an advanced iterative\nreceiver based on joint channel estimation, detection, and decoding, which\nrefines the receiver outputs through iterative feedback. The proposed receiver\nincorporates two adaptive channel estimation strategies to enhance robustness\nunder time-varying and mismatched channel conditions. First, a variational\nmessage passing (VMP) method and its low-complexity variant (VMP-L) are\nintroduced to perform inference without relying on time-domain correlation.\nSecond, a deep learning (DL) based estimator is developed, featuring a\nconvolutional neural network with a despreading module and an attention\nmechanism to extract and fuse relevant channel features. Extensive simulations\nunder multi-stream and high-mobility scenarios demonstrate that the proposed\nreceiver consistently outperforms conventional orthogonal pilot baselines in\nboth throughput and block error rate. Moreover, over-the-air experiments\nvalidate the practical effectiveness of the proposed design. Among the methods,\nthe DL based estimator achieves a favorable trade-off between performance and\ncomplexity, highlighting its suitability for real-world deployment in dynamic\nwireless environments."
                },
                "authors": [
                    {
                        "name": "Xinjie Li"
                    },
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Yixiao Cao"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Chao-Kai Wen"
                    },
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Shi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Shi Jin"
                },
                "author": "Shi Jin",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10073v1",
                "updated": "2025-07-14T08:59:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    59,
                    26,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T08:59:26Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    59,
                    26,
                    0,
                    195,
                    0
                ],
                "title": "Cultural Bias in Large Language Models: Evaluating AI Agents through\n  Moral Questionnaires",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cultural Bias in Large Language Models: Evaluating AI Agents through\n  Moral Questionnaires"
                },
                "summary": "Are AI systems truly representing human values, or merely averaging across\nthem? Our study suggests a concerning reality: Large Language Models (LLMs)\nfail to represent diverse cultural moral frameworks despite their linguistic\ncapabilities. We expose significant gaps between AI-generated and human moral\nintuitions by applying the Moral Foundations Questionnaire across 19 cultural\ncontexts. Comparing multiple state-of-the-art LLMs' origins against human\nbaseline data, we find these models systematically homogenize moral diversity.\nSurprisingly, increased model size doesn't consistently improve cultural\nrepresentation fidelity. Our findings challenge the growing use of LLMs as\nsynthetic populations in social science research and highlight a fundamental\nlimitation in current AI alignment approaches. Without data-driven alignment\nbeyond prompting, these systems cannot capture the nuanced, culturally-specific\nmoral intuitions. Our results call for more grounded alignment objectives and\nevaluation metrics to ensure AI systems represent diverse human values rather\nthan flattening the moral landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are AI systems truly representing human values, or merely averaging across\nthem? Our study suggests a concerning reality: Large Language Models (LLMs)\nfail to represent diverse cultural moral frameworks despite their linguistic\ncapabilities. We expose significant gaps between AI-generated and human moral\nintuitions by applying the Moral Foundations Questionnaire across 19 cultural\ncontexts. Comparing multiple state-of-the-art LLMs' origins against human\nbaseline data, we find these models systematically homogenize moral diversity.\nSurprisingly, increased model size doesn't consistently improve cultural\nrepresentation fidelity. Our findings challenge the growing use of LLMs as\nsynthetic populations in social science research and highlight a fundamental\nlimitation in current AI alignment approaches. Without data-driven alignment\nbeyond prompting, these systems cannot capture the nuanced, culturally-specific\nmoral intuitions. Our results call for more grounded alignment objectives and\nevaluation metrics to ensure AI systems represent diverse human values rather\nthan flattening the moral landscape."
                },
                "authors": [
                    {
                        "name": "Simon M√ºnker"
                    }
                ],
                "author_detail": {
                    "name": "Simon M√ºnker"
                },
                "author": "Simon M√ºnker",
                "arxiv_comment": "15pages, 1 figure, 2 tables",
                "arxiv_journal_ref": "Proceedings of 0th Symposium on Moral and Legal AI Alignment of\n  the IACAP/AISB Conference, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10069v1",
                "updated": "2025-07-14T08:53:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T08:53:48Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism"
                },
                "summary": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we propose Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we propose Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs)."
                },
                "authors": [
                    {
                        "name": "Zedong Liu"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Guangming Tan"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Dingwen Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dingwen Tao"
                },
                "author": "Dingwen Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10064v1",
                "updated": "2025-07-14T08:49:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    49,
                    16,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T08:49:16Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    49,
                    16,
                    0,
                    195,
                    0
                ],
                "title": "Precision Thermometry of Flat Flames Using Spatially Resolved\n  Multi-Color Laser Absorption Spectroscopy of Carbon Dioxide",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precision Thermometry of Flat Flames Using Spatially Resolved\n  Multi-Color Laser Absorption Spectroscopy of Carbon Dioxide"
                },
                "summary": "This work developed an accurate and robust absorption-based method for\nspatially resolved measurements of gas temperatures in flames and reacting\nflows, with typical single-measurement uncertainties on the order of 1\\%. This\nmethod exploits narrow-linewidth laser absorption of hot CO$_2$ molecules,\nwhich can be generated from combustion or artificially seeded into the flow. A\ncollinear dual-laser setup allowed for periodic scans over tens of CO$_2$\nabsorption transitions near the $\\nu_3$ bandhead every 100 $\\mu s$, from which\ngas temperatures (as well as CO$_2$ concentrations) were determined with high\nsensitivity and robustness. Spatially resolved measurements were achieved using\nan electrically driven high-speed beam scanning system consisting of a 2-D\ngalvo scanner and a pair of off-axis parabolic mirrors. An effective spatial\nresolution of 1 mm was achieved at a planar field measurement speed of 200 Hz\nand a volumetric field measurement speed of 2 Hz, with further improvements in\nspeed anticipated by another order of magnitude. A physically constrained\nnonlinear inference framework was also developed for the quantitative analysis\nof the measurement data. Proof-of-concept experiments were performed on\naxisymmetric flames stabilized on a Mckenna burner at various equivalence\nratios and flow rates, and the results agreed asymptotically with the\ntheoretical value of the adiabatic flame temperature. An additional experiment\non a flame of complex geometry demonstrated an excellent level of resolution,\nprecision, and contrast achieved by the current thermometry method. This method\npromises to provide good utility in future combustion studies due to its high\nperformance metrics and relative ease of use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work developed an accurate and robust absorption-based method for\nspatially resolved measurements of gas temperatures in flames and reacting\nflows, with typical single-measurement uncertainties on the order of 1\\%. This\nmethod exploits narrow-linewidth laser absorption of hot CO$_2$ molecules,\nwhich can be generated from combustion or artificially seeded into the flow. A\ncollinear dual-laser setup allowed for periodic scans over tens of CO$_2$\nabsorption transitions near the $\\nu_3$ bandhead every 100 $\\mu s$, from which\ngas temperatures (as well as CO$_2$ concentrations) were determined with high\nsensitivity and robustness. Spatially resolved measurements were achieved using\nan electrically driven high-speed beam scanning system consisting of a 2-D\ngalvo scanner and a pair of off-axis parabolic mirrors. An effective spatial\nresolution of 1 mm was achieved at a planar field measurement speed of 200 Hz\nand a volumetric field measurement speed of 2 Hz, with further improvements in\nspeed anticipated by another order of magnitude. A physically constrained\nnonlinear inference framework was also developed for the quantitative analysis\nof the measurement data. Proof-of-concept experiments were performed on\naxisymmetric flames stabilized on a Mckenna burner at various equivalence\nratios and flow rates, and the results agreed asymptotically with the\ntheoretical value of the adiabatic flame temperature. An additional experiment\non a flame of complex geometry demonstrated an excellent level of resolution,\nprecision, and contrast achieved by the current thermometry method. This method\npromises to provide good utility in future combustion studies due to its high\nperformance metrics and relative ease of use."
                },
                "authors": [
                    {
                        "name": "Shuoxun Zhang"
                    },
                    {
                        "name": "Shengkai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shengkai Wang"
                },
                "author": "Shengkai Wang",
                "arxiv_comment": "Manuscript submitted to Combustion and Flame on Nov. 24, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10062v1",
                "updated": "2025-07-14T08:47:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    47,
                    19,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T08:47:19Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    47,
                    19,
                    0,
                    195,
                    0
                ],
                "title": "LLMShot: Reducing snapshot testing maintenance via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMShot: Reducing snapshot testing maintenance via LLMs"
                },
                "summary": "Snapshot testing has emerged as a critical technique for UI validation in\nmodern software development, yet it suffers from substantial maintenance\noverhead due to frequent UI changes causing test failures that require manual\ninspection to distinguish between genuine regressions and intentional design\nchanges. This manual triage process becomes increasingly burdensome as\napplications evolve, creating a need for automated analysis solutions. This\npaper introduces LLMShot, a novel framework that leverages vision-based Large\nLanguage Models to automatically analyze snapshot test failures through\nhierarchical classification of UI changes. To evaluate LLMShot's effectiveness,\nwe developed a comprehensive dataset using a feature-rich iOS application with\nconfigurable feature flags, creating realistic scenarios that produce authentic\nsnapshot differences representative of real development workflows. Our\nevaluation using Gemma3 models demonstrates strong classification performance,\nwith the 12B variant achieving over 84% recall in identifying failure root\ncauses while the 4B model offers practical deployment advantages with\nacceptable performance for continuous integration environments. However, our\nexploration of selective ignore mechanisms revealed significant limitations in\ncurrent prompting-based approaches for controllable visual reasoning. LLMShot\nrepresents the first automated approach to semantic snapshot test analysis,\noffering developers structured insights that can substantially reduce manual\ntriage effort and advance toward more intelligent UI testing paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Snapshot testing has emerged as a critical technique for UI validation in\nmodern software development, yet it suffers from substantial maintenance\noverhead due to frequent UI changes causing test failures that require manual\ninspection to distinguish between genuine regressions and intentional design\nchanges. This manual triage process becomes increasingly burdensome as\napplications evolve, creating a need for automated analysis solutions. This\npaper introduces LLMShot, a novel framework that leverages vision-based Large\nLanguage Models to automatically analyze snapshot test failures through\nhierarchical classification of UI changes. To evaluate LLMShot's effectiveness,\nwe developed a comprehensive dataset using a feature-rich iOS application with\nconfigurable feature flags, creating realistic scenarios that produce authentic\nsnapshot differences representative of real development workflows. Our\nevaluation using Gemma3 models demonstrates strong classification performance,\nwith the 12B variant achieving over 84% recall in identifying failure root\ncauses while the 4B model offers practical deployment advantages with\nacceptable performance for continuous integration environments. However, our\nexploration of selective ignore mechanisms revealed significant limitations in\ncurrent prompting-based approaches for controllable visual reasoning. LLMShot\nrepresents the first automated approach to semantic snapshot test analysis,\noffering developers structured insights that can substantially reduce manual\ntriage effort and advance toward more intelligent UI testing paradigms."
                },
                "authors": [
                    {
                        "name": "Erg√ºn Batuhan Kaynak"
                    },
                    {
                        "name": "Mayasah Lami"
                    },
                    {
                        "name": "Sahand Moslemi"
                    },
                    {
                        "name": "Anil Koyuncu"
                    }
                ],
                "author_detail": {
                    "name": "Anil Koyuncu"
                },
                "author": "Anil Koyuncu",
                "arxiv_comment": "Accepted to ICSME 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10060v1",
                "updated": "2025-07-14T08:46:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    46,
                    38,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T08:46:38Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    46,
                    38,
                    0,
                    195,
                    0
                ],
                "title": "SRG/ART-XC All-Sky X-ray Survey: Sensitivity Assessment Based on\n  Aperture Photometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SRG/ART-XC All-Sky X-ray Survey: Sensitivity Assessment Based on\n  Aperture Photometry"
                },
                "summary": "The Spectrum-Roentgen-Gamma (SRG) observatory continues to operate\nsuccessfully in orbit at the Lagrange point L2. The Mikhail Pavlinsky ART-XC\ntelescope has demonstrated high efficiency in conducting X-ray surveys both\nover large sky regions and the entire celestial sphere. A recently published\nsource catalog, based on data from the first four and partially completed fifth\nsky scans, contains 1,545 objects detected in the 4-12 keV energy range. In\nthis work, using the same sky survey data, we assess the sensitivity to point\nsource detection across the celestial sphere based on X-ray aperture photometry\n- that is, we calculate the upper flux limit in the 4-12 keV band at any given\nsignificance level. The method is implemented using both Poisson statistics and\nBayesian inference, with consistent results between the two approaches. This\ninformation is important for studying variable and transient X-ray sources, as\nwell as sources that are not detected with sufficient statistical significance\nin the ART-XC all-sky survey. The ART-XC upper limit service is available at\nhttps://www.srg.cosmos.ru/uplim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Spectrum-Roentgen-Gamma (SRG) observatory continues to operate\nsuccessfully in orbit at the Lagrange point L2. The Mikhail Pavlinsky ART-XC\ntelescope has demonstrated high efficiency in conducting X-ray surveys both\nover large sky regions and the entire celestial sphere. A recently published\nsource catalog, based on data from the first four and partially completed fifth\nsky scans, contains 1,545 objects detected in the 4-12 keV energy range. In\nthis work, using the same sky survey data, we assess the sensitivity to point\nsource detection across the celestial sphere based on X-ray aperture photometry\n- that is, we calculate the upper flux limit in the 4-12 keV band at any given\nsignificance level. The method is implemented using both Poisson statistics and\nBayesian inference, with consistent results between the two approaches. This\ninformation is important for studying variable and transient X-ray sources, as\nwell as sources that are not detected with sufficient statistical significance\nin the ART-XC all-sky survey. The ART-XC upper limit service is available at\nhttps://www.srg.cosmos.ru/uplim."
                },
                "authors": [
                    {
                        "name": "N. Y. Tyrin"
                    },
                    {
                        "name": "R. A. Krivonos"
                    },
                    {
                        "name": "V. A. Arefiev"
                    },
                    {
                        "name": "R. A. Burenin"
                    },
                    {
                        "name": "E. I. Zakharov"
                    },
                    {
                        "name": "A. A. Lutovinov"
                    },
                    {
                        "name": "S. Y. Sazonov"
                    },
                    {
                        "name": "A. D. Samorodova"
                    },
                    {
                        "name": "E. V. Filippova"
                    },
                    {
                        "name": "A. A. Abbakumov"
                    },
                    {
                        "name": "V. V. Konoplev"
                    },
                    {
                        "name": "F. V. Korotkov"
                    },
                    {
                        "name": "V. N. Nazarov"
                    }
                ],
                "author_detail": {
                    "name": "V. N. Nazarov"
                },
                "author": "V. N. Nazarov",
                "arxiv_comment": "4 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10059v1",
                "updated": "2025-07-14T08:44:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    44,
                    59,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T08:44:59Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    44,
                    59,
                    0,
                    195,
                    0
                ],
                "title": "GeLaCo: An Evolutionary Approach to Layer Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeLaCo: An Evolutionary Approach to Layer Compression"
                },
                "summary": "Large Language Models (LLM) have achieved remarkable performance across a\nlarge number of tasks, but face critical deployment and usage barriers due to\nsubstantial computational requirements. Model compression methods, which aim to\nreduce model size while preserving its capacity, are an important means to\nmitigate these issues. Promising approaches along these lines, such as\nstructured pruning, typically require costly empirical search for optimal\nvariants and may run the risk of ignoring better solutions. In this work we\nintroduce GeLaCo, an evolutionary approach to LLM compression via layer\ncollapse. Our approach supports an efficient exploration of the compression\nsolution space via population-based search and a module-wise similarity fitness\nfunction capturing attention, feed-forward, and hidden state representations.\nGeLaCo also supports both single and multi-objective evolutionary compression\nsearch, establishing the first Pareto frontier along compression and quality\naxes. We evaluate GeLaCo solutions via both perplexity-based and generative\nevaluations over foundational and instruction-tuned models, outperforming\nstate-of-the-art alternatives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) have achieved remarkable performance across a\nlarge number of tasks, but face critical deployment and usage barriers due to\nsubstantial computational requirements. Model compression methods, which aim to\nreduce model size while preserving its capacity, are an important means to\nmitigate these issues. Promising approaches along these lines, such as\nstructured pruning, typically require costly empirical search for optimal\nvariants and may run the risk of ignoring better solutions. In this work we\nintroduce GeLaCo, an evolutionary approach to LLM compression via layer\ncollapse. Our approach supports an efficient exploration of the compression\nsolution space via population-based search and a module-wise similarity fitness\nfunction capturing attention, feed-forward, and hidden state representations.\nGeLaCo also supports both single and multi-objective evolutionary compression\nsearch, establishing the first Pareto frontier along compression and quality\naxes. We evaluate GeLaCo solutions via both perplexity-based and generative\nevaluations over foundational and instruction-tuned models, outperforming\nstate-of-the-art alternatives."
                },
                "authors": [
                    {
                        "name": "David Ponce"
                    },
                    {
                        "name": "Thierry Etchegoyhen"
                    },
                    {
                        "name": "Javier Del Ser"
                    }
                ],
                "author_detail": {
                    "name": "Javier Del Ser"
                },
                "author": "Javier Del Ser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10054v1",
                "updated": "2025-07-14T08:36:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    36,
                    26,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T08:36:26Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    36,
                    26,
                    0,
                    195,
                    0
                ],
                "title": "Explicit Vulnerability Generation with LLMs: An Investigation Beyond\n  Adversarial Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explicit Vulnerability Generation with LLMs: An Investigation Beyond\n  Adversarial Attacks"
                },
                "summary": "Large Language Models (LLMs) are increasingly used as code assistants, yet\ntheir behavior when explicitly asked to generate insecure code remains poorly\nunderstood. While prior research has focused on unintended vulnerabilities or\nadversarial prompting techniques, this study examines a more direct threat\nscenario: open-source LLMs generating vulnerable code when prompted either\ndirectly or indirectly. We propose a dual experimental design: (1) Dynamic\nPrompting, which systematically varies vulnerability type, user persona, and\ndirectness across structured templates; and (2) Reverse Prompting, which\nderives prompts from real vulnerable code samples to assess vulnerability\nreproduction accuracy. We evaluate three open-source 7B-parameter models\n(Qwen2, Mistral, and Gemma) using ESBMC static analysis to assess both the\npresence of vulnerabilities and the correctness of the generated vulnerability\ntype. Results show all models frequently produce vulnerable outputs, with Qwen2\nachieving highest correctness rates. User persona significantly affects\nsuccess, where student personas achieved higher vulnerability rates than\nprofessional roles, while direct prompts were marginally more effective.\nVulnerability reproduction followed an inverted-U pattern with cyclomatic\ncomplexity, peaking at moderate ranges. Our findings expose limitations of\nsafety mechanisms in open-source models, particularly for seemingly benign\neducational requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used as code assistants, yet\ntheir behavior when explicitly asked to generate insecure code remains poorly\nunderstood. While prior research has focused on unintended vulnerabilities or\nadversarial prompting techniques, this study examines a more direct threat\nscenario: open-source LLMs generating vulnerable code when prompted either\ndirectly or indirectly. We propose a dual experimental design: (1) Dynamic\nPrompting, which systematically varies vulnerability type, user persona, and\ndirectness across structured templates; and (2) Reverse Prompting, which\nderives prompts from real vulnerable code samples to assess vulnerability\nreproduction accuracy. We evaluate three open-source 7B-parameter models\n(Qwen2, Mistral, and Gemma) using ESBMC static analysis to assess both the\npresence of vulnerabilities and the correctness of the generated vulnerability\ntype. Results show all models frequently produce vulnerable outputs, with Qwen2\nachieving highest correctness rates. User persona significantly affects\nsuccess, where student personas achieved higher vulnerability rates than\nprofessional roles, while direct prompts were marginally more effective.\nVulnerability reproduction followed an inverted-U pattern with cyclomatic\ncomplexity, peaking at moderate ranges. Our findings expose limitations of\nsafety mechanisms in open-source models, particularly for seemingly benign\neducational requests."
                },
                "authors": [
                    {
                        "name": "Emir Bosnak"
                    },
                    {
                        "name": "Sahand Moslemi"
                    },
                    {
                        "name": "Mayasah Lami"
                    },
                    {
                        "name": "Anil Koyuncu"
                    }
                ],
                "author_detail": {
                    "name": "Anil Koyuncu"
                },
                "author": "Anil Koyuncu",
                "arxiv_comment": "Accepted to ICSME 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11415v2",
                "updated": "2025-07-14T08:34:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    34,
                    57,
                    0,
                    195,
                    0
                ],
                "published": "2024-08-21T08:20:41Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    8,
                    20,
                    41,
                    2,
                    234,
                    0
                ],
                "title": "Political Bias in LLMs: Unaligned Moral Values in Agent-centric\n  Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Political Bias in LLMs: Unaligned Moral Values in Agent-centric\n  Simulations"
                },
                "summary": "Contemporary research in social sciences increasingly utilizes\nstate-of-the-art generative language models to annotate or generate content.\nWhile these models achieve benchmark-leading performance on common language\ntasks, their application to novel out-of-domain tasks remains insufficiently\nexplored. To address this gap, we investigate how personalized language models\nalign with human responses on the Moral Foundation Theory Questionnaire. We\nadapt open-source generative language models to different political personas\nand repeatedly survey these models to generate synthetic data sets where\nmodel-persona combinations define our sub-populations. Our analysis reveals\nthat models produce inconsistent results across multiple repetitions, yielding\nhigh response variance. Furthermore, the alignment between synthetic data and\ncorresponding human data from psychological studies shows a weak correlation,\nwith conservative persona-prompted models particularly failing to align with\nactual conservative populations. These results suggest that language models\nstruggle to coherently represent ideologies through in-context prompting due to\ntheir alignment process. Thus, using language models to simulate social\ninteractions requires measurable improvements in in-context optimization or\nparameter manipulation to align with psychological and sociological stereotypes\nproperly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary research in social sciences increasingly utilizes\nstate-of-the-art generative language models to annotate or generate content.\nWhile these models achieve benchmark-leading performance on common language\ntasks, their application to novel out-of-domain tasks remains insufficiently\nexplored. To address this gap, we investigate how personalized language models\nalign with human responses on the Moral Foundation Theory Questionnaire. We\nadapt open-source generative language models to different political personas\nand repeatedly survey these models to generate synthetic data sets where\nmodel-persona combinations define our sub-populations. Our analysis reveals\nthat models produce inconsistent results across multiple repetitions, yielding\nhigh response variance. Furthermore, the alignment between synthetic data and\ncorresponding human data from psychological studies shows a weak correlation,\nwith conservative persona-prompted models particularly failing to align with\nactual conservative populations. These results suggest that language models\nstruggle to coherently represent ideologies through in-context prompting due to\ntheir alignment process. Thus, using language models to simulate social\ninteractions requires measurable improvements in in-context optimization or\nparameter manipulation to align with psychological and sociological stereotypes\nproperly."
                },
                "authors": [
                    {
                        "name": "Simon M√ºnker"
                    }
                ],
                "author_detail": {
                    "name": "Simon M√ºnker"
                },
                "author": "Simon M√ºnker",
                "arxiv_doi": "10.21248/jlcl.38.2025.289",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.21248/jlcl.38.2025.289",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.11415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 2 tables",
                "arxiv_journal_ref": "JLCL 2025, Band 38(2)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15902v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15902v2",
                "updated": "2025-07-14T08:34:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    34,
                    3,
                    0,
                    195,
                    0
                ],
                "published": "2025-02-21T19:41:32Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    19,
                    41,
                    32,
                    4,
                    52,
                    0
                ],
                "title": "IPAD: Inverse Prompt for AI Detection -- A Robust and Explainable\n  LLM-Generated Text Detector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IPAD: Inverse Prompt for AI Detection -- A Robust and Explainable\n  LLM-Generated Text Detector"
                },
                "summary": "Large Language Models (LLMs) have attained human-level fluency in text\ngeneration, which complicates the distinction between human-written and\nLLM-generated texts. This increases the risk of misuse and highlights the need\nfor reliable detectors. Yet, existing detectors exhibit poor robustness on\nout-of-distribution (OOD) data and attacked data, which is critical for\nreal-world scenarios. Also, they struggle to provide interpretable evidence to\nsupport their decisions, thus undermining the reliability. In light of these\nchallenges, we propose IPAD (Inverse Prompt for AI Detection), a novel\nframework consisting of a Prompt Inverter that identifies predicted prompts\nthat could have generated the input text, and two Distinguishers that examine\nthe probability that the input texts align with the predicted prompts.\nEmpirical evaluations demonstrate that IPAD outperforms the strongest baselines\nby 9.05% (Average Recall) on in-distribution data, 12.93% (AUROC) on\nout-of-distribution (OOD) data, and 5.48% (AUROC) on attacked data. IPAD also\nperforms robustly on structured datasets. Furthermore, an interpretability\nassessment is conducted to illustrate that IPAD enhances the AI detection\ntrustworthiness by allowing users to directly examine the decision-making\nevidence, which provides interpretable support for its state-of-the-art\ndetection results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have attained human-level fluency in text\ngeneration, which complicates the distinction between human-written and\nLLM-generated texts. This increases the risk of misuse and highlights the need\nfor reliable detectors. Yet, existing detectors exhibit poor robustness on\nout-of-distribution (OOD) data and attacked data, which is critical for\nreal-world scenarios. Also, they struggle to provide interpretable evidence to\nsupport their decisions, thus undermining the reliability. In light of these\nchallenges, we propose IPAD (Inverse Prompt for AI Detection), a novel\nframework consisting of a Prompt Inverter that identifies predicted prompts\nthat could have generated the input text, and two Distinguishers that examine\nthe probability that the input texts align with the predicted prompts.\nEmpirical evaluations demonstrate that IPAD outperforms the strongest baselines\nby 9.05% (Average Recall) on in-distribution data, 12.93% (AUROC) on\nout-of-distribution (OOD) data, and 5.48% (AUROC) on attacked data. IPAD also\nperforms robustly on structured datasets. Furthermore, an interpretability\nassessment is conducted to illustrate that IPAD enhances the AI detection\ntrustworthiness by allowing users to directly examine the decision-making\nevidence, which provides interpretable support for its state-of-the-art\ndetection results."
                },
                "authors": [
                    {
                        "name": "Zheng Chen"
                    },
                    {
                        "name": "Yushi Feng"
                    },
                    {
                        "name": "Changyang He"
                    },
                    {
                        "name": "Yue Deng"
                    },
                    {
                        "name": "Hongxi Pu"
                    },
                    {
                        "name": "Bo Li"
                    }
                ],
                "author_detail": {
                    "name": "Bo Li"
                },
                "author": "Bo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15902v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15902v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10047v1",
                "updated": "2025-07-14T08:26:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    26,
                    41,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T08:26:41Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    26,
                    41,
                    0,
                    195,
                    0
                ],
                "title": "MP-RBFN: Learning-based Vehicle Motion Primitives using Radial Basis\n  Function Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MP-RBFN: Learning-based Vehicle Motion Primitives using Radial Basis\n  Function Networks"
                },
                "summary": "This research introduces MP-RBFN, a novel formulation leveraging Radial Basis\nFunction Networks for efficiently learning Motion Primitives derived from\noptimal control problems for autonomous driving. While traditional motion\nplanning approaches based on optimization are highly accurate, they are often\ncomputationally prohibitive. In contrast, sampling-based methods demonstrate\nhigh performance but impose constraints on the geometric shape of trajectories.\nMP-RBFN combines the strengths of both by coupling the high-fidelity trajectory\ngeneration of sampling-based methods with an accurate description of vehicle\ndynamics. Empirical results show compelling performance compared to previous\nmethods, achieving a precise description of motion primitives at low inference\ntimes. MP-RBFN yields a seven times higher accuracy in generating optimized\nmotion primitives compared to existing semi-analytic approaches. We demonstrate\nthe practical applicability of MP-RBFN for motion planning by integrating the\nmethod into a sampling-based trajectory planner. MP-RBFN is available as\nopen-source software at https://github.com/TUM-AVS/RBFN-Motion-Primitives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research introduces MP-RBFN, a novel formulation leveraging Radial Basis\nFunction Networks for efficiently learning Motion Primitives derived from\noptimal control problems for autonomous driving. While traditional motion\nplanning approaches based on optimization are highly accurate, they are often\ncomputationally prohibitive. In contrast, sampling-based methods demonstrate\nhigh performance but impose constraints on the geometric shape of trajectories.\nMP-RBFN combines the strengths of both by coupling the high-fidelity trajectory\ngeneration of sampling-based methods with an accurate description of vehicle\ndynamics. Empirical results show compelling performance compared to previous\nmethods, achieving a precise description of motion primitives at low inference\ntimes. MP-RBFN yields a seven times higher accuracy in generating optimized\nmotion primitives compared to existing semi-analytic approaches. We demonstrate\nthe practical applicability of MP-RBFN for motion planning by integrating the\nmethod into a sampling-based trajectory planner. MP-RBFN is available as\nopen-source software at https://github.com/TUM-AVS/RBFN-Motion-Primitives."
                },
                "authors": [
                    {
                        "name": "Marc Kaufeld"
                    },
                    {
                        "name": "Mattia Piccinini"
                    },
                    {
                        "name": "Johannes Betz"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Betz"
                },
                "author": "Johannes Betz",
                "arxiv_comment": "8 pages, Submitted to the IEEE International Conference on\n  Intelligent Transportation Systems (ITSC 2025), Australia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08505v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08505v2",
                "updated": "2025-07-14T08:25:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    25,
                    14,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-11T11:30:57Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    11,
                    30,
                    57,
                    4,
                    192,
                    0
                ],
                "title": "Efficient Deployment of Vision-Language Models on Mobile Devices: A Case\n  Study on OnePlus 13R",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Deployment of Vision-Language Models on Mobile Devices: A Case\n  Study on OnePlus 13R"
                },
                "summary": "Vision-Language Models (VLMs) offer promising capabilities for mobile\ndevices, but their deployment faces significant challenges due to computational\nlimitations and energy inefficiency, especially for real-time applications.\nThis study provides a comprehensive survey of deployment frameworks for VLMs on\nmobile devices, evaluating llama.cpp, MLC-Imp, and mllm in the context of\nrunning LLaVA-1.5 7B, MobileVLM-3B, and Imp-v1.5 3B as representative workloads\non a OnePlus 13R. Each deployment framework was evaluated on the OnePlus 13R\nwhile running VLMs, with measurements covering CPU, GPU, and NPU utilization,\ntemperature, inference time, power consumption, and user experience.\nBenchmarking revealed critical performance bottlenecks across frameworks: CPU\nresources were consistently over-utilized during token generation, while GPU\nand NPU accelerators were largely unused. When the GPU was used, primarily for\nimage feature extraction, it was saturated, leading to degraded device\nresponsiveness. The study contributes framework-level benchmarks, practical\nprofiling tools, and an in-depth analysis of hardware utilization bottlenecks,\nhighlighting the consistent overuse of CPUs and the ineffective or unstable use\nof GPUs and NPUs in current deployment frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) offer promising capabilities for mobile\ndevices, but their deployment faces significant challenges due to computational\nlimitations and energy inefficiency, especially for real-time applications.\nThis study provides a comprehensive survey of deployment frameworks for VLMs on\nmobile devices, evaluating llama.cpp, MLC-Imp, and mllm in the context of\nrunning LLaVA-1.5 7B, MobileVLM-3B, and Imp-v1.5 3B as representative workloads\non a OnePlus 13R. Each deployment framework was evaluated on the OnePlus 13R\nwhile running VLMs, with measurements covering CPU, GPU, and NPU utilization,\ntemperature, inference time, power consumption, and user experience.\nBenchmarking revealed critical performance bottlenecks across frameworks: CPU\nresources were consistently over-utilized during token generation, while GPU\nand NPU accelerators were largely unused. When the GPU was used, primarily for\nimage feature extraction, it was saturated, leading to degraded device\nresponsiveness. The study contributes framework-level benchmarks, practical\nprofiling tools, and an in-depth analysis of hardware utilization bottlenecks,\nhighlighting the consistent overuse of CPUs and the ineffective or unstable use\nof GPUs and NPUs in current deployment frameworks."
                },
                "authors": [
                    {
                        "name": "Pablo Robin Guerrero"
                    },
                    {
                        "name": "Yueyang Pan"
                    },
                    {
                        "name": "Sanidhya Kashyap"
                    }
                ],
                "author_detail": {
                    "name": "Sanidhya Kashyap"
                },
                "author": "Sanidhya Kashyap",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08505v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08505v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10045v1",
                "updated": "2025-07-14T08:23:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    23,
                    25,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T08:23:25Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    23,
                    25,
                    0,
                    195,
                    0
                ],
                "title": "Automating SPARQL Query Translations between DBpedia and Wikidata",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating SPARQL Query Translations between DBpedia and Wikidata"
                },
                "summary": "This paper investigates whether state-of-the-art Large Language Models (LLMs)\ncan automatically translate SPARQL between popular Knowledge Graph (KG)\nschemas. We focus on translations between the DBpedia and Wikidata KG, and\nlater on DBLP and OpenAlex KG. This study addresses a notable gap in KG\ninteroperability research by rigorously evaluating LLM performance on\nSPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first\nalign 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100\nDBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic\nKGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and\nMistral-Large-Instruct-2407 are selected based on their sizes and architectures\nand tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs\nwere compared with gold answers, and resulting errors were categorized. We find\nthat the performance varies markedly across models and prompting strategies,\nand that translations for Wikidata to DBpedia work far better than translations\nfor DBpedia to Wikidata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates whether state-of-the-art Large Language Models (LLMs)\ncan automatically translate SPARQL between popular Knowledge Graph (KG)\nschemas. We focus on translations between the DBpedia and Wikidata KG, and\nlater on DBLP and OpenAlex KG. This study addresses a notable gap in KG\ninteroperability research by rigorously evaluating LLM performance on\nSPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first\nalign 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100\nDBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic\nKGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and\nMistral-Large-Instruct-2407 are selected based on their sizes and architectures\nand tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs\nwere compared with gold answers, and resulting errors were categorized. We find\nthat the performance varies markedly across models and prompting strategies,\nand that translations for Wikidata to DBpedia work far better than translations\nfor DBpedia to Wikidata."
                },
                "authors": [
                    {
                        "name": "Malte Christian Bartels"
                    },
                    {
                        "name": "Debayan Banerjee"
                    },
                    {
                        "name": "Ricardo Usbeck"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Usbeck"
                },
                "author": "Ricardo Usbeck",
                "arxiv_comment": "18 pages, 2 figues. Paper accepted at SEMANTiCS 2025 conference\n  happening on September 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.10541v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10541v2",
                "updated": "2025-07-15T06:16:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    6,
                    16,
                    53,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-14T17:58:47Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    58,
                    47,
                    0,
                    195,
                    0
                ],
                "title": "REST: Stress Testing Large Reasoning Models by Asking Multiple Problems\n  at Once",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REST: Stress Testing Large Reasoning Models by Asking Multiple Problems\n  at Once"
                },
                "summary": "Recent Large Reasoning Models (LRMs) have achieved remarkable progress on\ntask-specific benchmarks, yet their evaluation methods remain constrained by\nisolated problem-solving paradigms. Existing benchmarks predominantly assess\nsingle-question reasoning through sequential testing, resulting critical\nlimitations: (1) vulnerability to data contamination and less challenging\n(e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly creation of new\nquestions with large human efforts, (2) failure to evaluate models under\nmulti-context pressure, a key requirement for real-world deployment. To bridge\nthis gap, we present REST (Reasoning Evaluation through Simultaneous Testing),\na stress-testing framework that exposes LRMs to multiple problems\nsimultaneously. Beyond basic reasoning, REST evaluates several under-tested\ncapabilities: contextual priority allocation, cross-problem interference\nresistance, and dynamic cognitive load management. Our evaluation reveals\nseveral striking findings: Even state-of-the-art (SOTA) models like DeepSeek-R1\nexhibit substantial performance degradation under stress testing. Crucially,\nREST demonstrates stronger discriminative power than existing benchmarks,\nrevealing pronounced performance differences among models that exhibit similar,\nnear-ceiling performance under single-question evaluations. Some key insights\nemerge from our analysis: (1) the \"overthinking trap\" is a critical factor\ncontributing to the performance degradation; (2) the models trained with\n\"long2short\" technique preserve more accuracy of their single-problem\nperformance under REST, outperforming standard-trained counterparts. These\nresults establish REST as a cost-efficient, future-proof evaluation paradigm\nthat better reflects real-world reasoning demands while reducing reliance on\ncontinuous human annotation. Code and results are available at\nhttps://opendatalab.github.io/REST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Large Reasoning Models (LRMs) have achieved remarkable progress on\ntask-specific benchmarks, yet their evaluation methods remain constrained by\nisolated problem-solving paradigms. Existing benchmarks predominantly assess\nsingle-question reasoning through sequential testing, resulting critical\nlimitations: (1) vulnerability to data contamination and less challenging\n(e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly creation of new\nquestions with large human efforts, (2) failure to evaluate models under\nmulti-context pressure, a key requirement for real-world deployment. To bridge\nthis gap, we present REST (Reasoning Evaluation through Simultaneous Testing),\na stress-testing framework that exposes LRMs to multiple problems\nsimultaneously. Beyond basic reasoning, REST evaluates several under-tested\ncapabilities: contextual priority allocation, cross-problem interference\nresistance, and dynamic cognitive load management. Our evaluation reveals\nseveral striking findings: Even state-of-the-art (SOTA) models like DeepSeek-R1\nexhibit substantial performance degradation under stress testing. Crucially,\nREST demonstrates stronger discriminative power than existing benchmarks,\nrevealing pronounced performance differences among models that exhibit similar,\nnear-ceiling performance under single-question evaluations. Some key insights\nemerge from our analysis: (1) the \"overthinking trap\" is a critical factor\ncontributing to the performance degradation; (2) the models trained with\n\"long2short\" technique preserve more accuracy of their single-problem\nperformance under REST, outperforming standard-trained counterparts. These\nresults establish REST as a cost-efficient, future-proof evaluation paradigm\nthat better reflects real-world reasoning demands while reducing reliance on\ncontinuous human annotation. Code and results are available at\nhttps://opendatalab.github.io/REST."
                },
                "authors": [
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Qizhi Pei"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Qiyao Sun"
                    },
                    {
                        "name": "Zinan Tang"
                    },
                    {
                        "name": "H. Vicky Zhao"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Lijun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Lijun Wu"
                },
                "author": "Lijun Wu",
                "arxiv_comment": "REST (Reasoning Evaluation through Simultaneous Testing), a\n  stress-testing framework that concurrently exposes LRMs to multiple problems\n  simultaneously",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10541v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10541v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10540v1",
                "updated": "2025-07-14T17:58:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    58,
                    2,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T17:58:02Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    58,
                    2,
                    0,
                    195,
                    0
                ],
                "title": "Fusing LLM Capabilities with Routing Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusing LLM Capabilities with Routing Data"
                },
                "summary": "The rapid advancement of large language models (LLMs) has created a vibrant\necosystem of diverse architectures, each with unique strengths due to\ndifferences in design, training data, and objectives. However, most\napplications still rely on a single backend model, limiting coverage of\ncapabilities and leading to inefficiencies in performance and token cost when\ntackling complex tasks. We highlight an underexploited opportunity: LLM routing\ndata, produced when hosting platforms route diverse queries to different\nmodels, which can reveal comparative strengths across tasks. To address this,\nwe propose FusionBench, a comprehensive routing benchmark covering 14 tasks\nacross five domains with 20 open-source LLMs (8B to 671B parameters), capturing\n103M tokens and summarizing reusable thought templates from top models.\nBuilding on this, we introduce FusionFactory, a systematic fusion framework\nwith three levels: (1) query-level fusion, tailoring routers for each query\nusing both direct responses and reasoning-augmented outputs; (2) thought-level\nfusion, leveraging abstract templates derived from top-performing LLMs' answers\nto similar queries; and (3) model-level fusion, transferring capabilities\nbetween models via distillation, using top responses or highest judge scores as\ntraining data. Experiments show FusionFactory consistently outperforms the best\nindividual LLM across all 14 benchmarks, with optimal fusion configurations\nvarying by benchmark, demonstrating the value of systematic LLM fusion in\nharnessing complementary strengths and improving overall performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has created a vibrant\necosystem of diverse architectures, each with unique strengths due to\ndifferences in design, training data, and objectives. However, most\napplications still rely on a single backend model, limiting coverage of\ncapabilities and leading to inefficiencies in performance and token cost when\ntackling complex tasks. We highlight an underexploited opportunity: LLM routing\ndata, produced when hosting platforms route diverse queries to different\nmodels, which can reveal comparative strengths across tasks. To address this,\nwe propose FusionBench, a comprehensive routing benchmark covering 14 tasks\nacross five domains with 20 open-source LLMs (8B to 671B parameters), capturing\n103M tokens and summarizing reusable thought templates from top models.\nBuilding on this, we introduce FusionFactory, a systematic fusion framework\nwith three levels: (1) query-level fusion, tailoring routers for each query\nusing both direct responses and reasoning-augmented outputs; (2) thought-level\nfusion, leveraging abstract templates derived from top-performing LLMs' answers\nto similar queries; and (3) model-level fusion, transferring capabilities\nbetween models via distillation, using top responses or highest judge scores as\ntraining data. Experiments show FusionFactory consistently outperforms the best\nindividual LLM across all 14 benchmarks, with optimal fusion configurations\nvarying by benchmark, demonstrating the value of systematic LLM fusion in\nharnessing complementary strengths and improving overall performance."
                },
                "authors": [
                    {
                        "name": "Tao Feng"
                    },
                    {
                        "name": "Haozhen Zhang"
                    },
                    {
                        "name": "Zijie Lei"
                    },
                    {
                        "name": "Pengrui Han"
                    },
                    {
                        "name": "Mostofa Patwary"
                    },
                    {
                        "name": "Mohammad Shoeybi"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Jiaxuan You"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxuan You"
                },
                "author": "Jiaxuan You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10535v1",
                "updated": "2025-07-14T17:56:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    56,
                    29,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T17:56:29Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    56,
                    29,
                    0,
                    195,
                    0
                ],
                "title": "CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks"
                },
                "summary": "Large Language Models (LLMs) have significantly advanced the state-of-the-art\nin various coding tasks. Beyond directly answering user queries, LLMs can also\nserve as judges, assessing and comparing the quality of responses generated by\nother models. Such an evaluation capability is crucial both for benchmarking\ndifferent LLMs and for improving response quality through response ranking.\nHowever, despite the growing adoption of the LLM-as-a-Judge paradigm, its\neffectiveness in coding scenarios remains underexplored due to the absence of\ndedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a\nbenchmark explicitly designed to evaluate the performance of LLM-as-a-Judge\nmodels across three critical coding tasks: code generation, code repair, and\nunit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge\nmodels, we find that recent thinking models significantly outperform\nnon-thinking models on our carefully designed code judging tasks. Notably, even\nrelatively small thinking models, such as Qwen3-8B, can outperform specially\ntrained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still\nexhibit significant randomness in their judgment of coding tasks. For pairwise\njudging tasks, simply changing the order in which responses are presented can\nsubstantially impact accuracy. In addition, when judging code and unit tests\nwritten by different LLMs, LLM-as-a-Judge models also show variance in\nperformance. This sensitivity raises concerns about the reliability and\nconsistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal\nprompting strategies for LLM-as-a-Judge. We find that using pair-wise\ncomparison outperforms scalar point-wise judging. Furthermore, retaining\ncomments and reasoning in the full, unprocessed LLM response leads to improved\njudge performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly advanced the state-of-the-art\nin various coding tasks. Beyond directly answering user queries, LLMs can also\nserve as judges, assessing and comparing the quality of responses generated by\nother models. Such an evaluation capability is crucial both for benchmarking\ndifferent LLMs and for improving response quality through response ranking.\nHowever, despite the growing adoption of the LLM-as-a-Judge paradigm, its\neffectiveness in coding scenarios remains underexplored due to the absence of\ndedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a\nbenchmark explicitly designed to evaluate the performance of LLM-as-a-Judge\nmodels across three critical coding tasks: code generation, code repair, and\nunit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge\nmodels, we find that recent thinking models significantly outperform\nnon-thinking models on our carefully designed code judging tasks. Notably, even\nrelatively small thinking models, such as Qwen3-8B, can outperform specially\ntrained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still\nexhibit significant randomness in their judgment of coding tasks. For pairwise\njudging tasks, simply changing the order in which responses are presented can\nsubstantially impact accuracy. In addition, when judging code and unit tests\nwritten by different LLMs, LLM-as-a-Judge models also show variance in\nperformance. This sensitivity raises concerns about the reliability and\nconsistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal\nprompting strategies for LLM-as-a-Judge. We find that using pair-wise\ncomparison outperforms scalar point-wise judging. Furthermore, retaining\ncomments and reasoning in the full, unprocessed LLM response leads to improved\njudge performance."
                },
                "authors": [
                    {
                        "name": "Hongchao Jiang"
                    },
                    {
                        "name": "Yiming Chen"
                    },
                    {
                        "name": "Yushi Cao"
                    },
                    {
                        "name": "Hung-yi Lee"
                    },
                    {
                        "name": "Robby T. Tan"
                    }
                ],
                "author_detail": {
                    "name": "Robby T. Tan"
                },
                "author": "Robby T. Tan",
                "arxiv_comment": "Dataset is available at\n  https://huggingface.co/datasets/mattymchen/codejudgebench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10532v1",
                "updated": "2025-07-14T17:55:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    55,
                    15,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T17:55:15Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    55,
                    15,
                    0,
                    195,
                    0
                ],
                "title": "Reasoning or Memorization? Unreliable Results of Reinforcement Learning\n  Due to Data Contamination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning or Memorization? Unreliable Results of Reinforcement Learning\n  Due to Data Contamination"
                },
                "summary": "The reasoning capabilities of large language models (LLMs) have been a\nlongstanding focus of research. Recent works have further enhanced these\ncapabilities using reinforcement learning (RL), with many new methods claiming\nsignificant improvements with minimal or no external supervision. Surprisingly,\nsome studies even suggest that random or incorrect reward signals can enhance\nreasoning performance. However, these breakthroughs are mostly reported on the\nQwen2.5 model family and evaluated on well-known benchmarks such as MATH-500,\nAMC, and AIME, while failing to achieve similar gains on other models like\nLlama, which warrants further investigation. Our analysis shows that although\nQwen2.5 achieves strong mathematical reasoning performance, its pretraining on\nlarge-scale web corpora makes it vulnerable to data contamination in popular\nbenchmarks. As a result, results derived from these benchmarks may be\nunreliable. To address this, we introduce a generator that produces fully\nsynthetic arithmetic problems of arbitrary length and difficulty, yielding a\nclean dataset we call RandomCalculation. Using these leakage-free datasets, we\nshow that only accurate reward signals consistently improve performance, while\nnoisy or incorrect signals do not. We advocate for evaluating RL methods on\nuncontaminated benchmarks and across diverse model families to ensure\ntrustworthy conclusions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning capabilities of large language models (LLMs) have been a\nlongstanding focus of research. Recent works have further enhanced these\ncapabilities using reinforcement learning (RL), with many new methods claiming\nsignificant improvements with minimal or no external supervision. Surprisingly,\nsome studies even suggest that random or incorrect reward signals can enhance\nreasoning performance. However, these breakthroughs are mostly reported on the\nQwen2.5 model family and evaluated on well-known benchmarks such as MATH-500,\nAMC, and AIME, while failing to achieve similar gains on other models like\nLlama, which warrants further investigation. Our analysis shows that although\nQwen2.5 achieves strong mathematical reasoning performance, its pretraining on\nlarge-scale web corpora makes it vulnerable to data contamination in popular\nbenchmarks. As a result, results derived from these benchmarks may be\nunreliable. To address this, we introduce a generator that produces fully\nsynthetic arithmetic problems of arbitrary length and difficulty, yielding a\nclean dataset we call RandomCalculation. Using these leakage-free datasets, we\nshow that only accurate reward signals consistently improve performance, while\nnoisy or incorrect signals do not. We advocate for evaluating RL methods on\nuncontaminated benchmarks and across diverse model families to ensure\ntrustworthy conclusions."
                },
                "authors": [
                    {
                        "name": "Mingqi Wu"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Qiaole Dong"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Senjie Jin"
                    },
                    {
                        "name": "Xiaoran Fan"
                    },
                    {
                        "name": "Yuhao Zhou"
                    },
                    {
                        "name": "Yanwei Fu"
                    },
                    {
                        "name": "Qin Liu"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10524v1",
                "updated": "2025-07-14T17:49:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    49,
                    0,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T17:49:00Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    49,
                    0,
                    0,
                    195,
                    0
                ],
                "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation"
                },
                "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost."
                },
                "authors": [
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Yujin Kim"
                    },
                    {
                        "name": "Reza Bayat"
                    },
                    {
                        "name": "Sungnyun Kim"
                    },
                    {
                        "name": "Jiyoun Ha"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "Hrayr Harutyunyan"
                    },
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Aaron Courville"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "36 pages, 9 figures, 14 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10522v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10522v1",
                "updated": "2025-07-14T17:47:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    47,
                    28,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T17:47:28Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    47,
                    28,
                    0,
                    195,
                    0
                ],
                "title": "DeepResearch$^{\\text{Eco}}$: A Recursive Agentic Workflow for Complex\n  Scientific Question Answering in Ecology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepResearch$^{\\text{Eco}}$: A Recursive Agentic Workflow for Complex\n  Scientific Question Answering in Ecology"
                },
                "summary": "We introduce DeepResearch$^{\\text{Eco}}$, a novel agentic LLM-based system\nfor automated scientific synthesis that supports recursive, depth- and\nbreadth-controlled exploration of original research questions -- enhancing\nsearch diversity and nuance in the retrieval of relevant scientific literature.\nUnlike conventional retrieval-augmented generation pipelines, DeepResearch\nenables user-controllable synthesis with transparent reasoning and\nparameter-driven configurability, facilitating high-throughput integration of\ndomain-specific evidence while maintaining analytical rigor. Applied to 49\necological research questions, DeepResearch achieves up to a 21-fold increase\nin source integration and a 14.9-fold rise in sources integrated per 1,000\nwords. High-parameter settings yield expert-level analytical depth and\ncontextual diversity.\n  Source code available at: https://github.com/sciknoworg/deep-research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce DeepResearch$^{\\text{Eco}}$, a novel agentic LLM-based system\nfor automated scientific synthesis that supports recursive, depth- and\nbreadth-controlled exploration of original research questions -- enhancing\nsearch diversity and nuance in the retrieval of relevant scientific literature.\nUnlike conventional retrieval-augmented generation pipelines, DeepResearch\nenables user-controllable synthesis with transparent reasoning and\nparameter-driven configurability, facilitating high-throughput integration of\ndomain-specific evidence while maintaining analytical rigor. Applied to 49\necological research questions, DeepResearch achieves up to a 21-fold increase\nin source integration and a 14.9-fold rise in sources integrated per 1,000\nwords. High-parameter settings yield expert-level analytical depth and\ncontextual diversity.\n  Source code available at: https://github.com/sciknoworg/deep-research."
                },
                "authors": [
                    {
                        "name": "Jennifer D'Souza"
                    },
                    {
                        "name": "Endres Keno Sander"
                    },
                    {
                        "name": "Andrei Aioanei"
                    }
                ],
                "author_detail": {
                    "name": "Andrei Aioanei"
                },
                "author": "Andrei Aioanei",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10522v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10522v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21628v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21628v2",
                "updated": "2025-07-14T17:46:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    46,
                    29,
                    0,
                    195,
                    0
                ],
                "published": "2025-06-24T20:23:39Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    20,
                    23,
                    39,
                    1,
                    175,
                    0
                ],
                "title": "Ark: An Open-source Python-based Framework for Robot Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ark: An Open-source Python-based Framework for Robot Learning"
                },
                "summary": "Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics\nChallenges to the first humanoid-robot kickboxing tournament-yet commercial\nautonomy still lags behind progress in machine learning. A major bottleneck is\nsoftware: current robot stacks demand steep learning curves, low-level C/C++\nexpertise, fragmented tooling, and intricate hardware integration, in stark\ncontrast to the Python-centric, well-documented ecosystems that propelled\nmodern AI. We introduce ARK, an open-source, Python-first robotics framework\ndesigned to close that gap. ARK presents a Gym-style environment interface that\nallows users to collect data, preprocess it, and train policies using\nstate-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy)\nwhile seamlessly toggling between high-fidelity simulation and physical robots.\nA lightweight client-server architecture provides networked\npublisher-subscriber communication, and optional C/C++ bindings ensure\nreal-time performance when needed. ARK ships with reusable modules for control,\nSLAM, motion planning, system identification, and visualization, along with\nnative ROS interoperability. Comprehensive documentation and case studies-from\nmanipulation to mobile navigation-demonstrate rapid prototyping, effortless\nhardware swapping, and end-to-end pipelines that rival the convenience of\nmainstream machine-learning workflows. By unifying robotics and AI practices\nunder a common Python umbrella, ARK lowers entry barriers and accelerates\nresearch and commercial deployment of autonomous robots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics\nChallenges to the first humanoid-robot kickboxing tournament-yet commercial\nautonomy still lags behind progress in machine learning. A major bottleneck is\nsoftware: current robot stacks demand steep learning curves, low-level C/C++\nexpertise, fragmented tooling, and intricate hardware integration, in stark\ncontrast to the Python-centric, well-documented ecosystems that propelled\nmodern AI. We introduce ARK, an open-source, Python-first robotics framework\ndesigned to close that gap. ARK presents a Gym-style environment interface that\nallows users to collect data, preprocess it, and train policies using\nstate-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy)\nwhile seamlessly toggling between high-fidelity simulation and physical robots.\nA lightweight client-server architecture provides networked\npublisher-subscriber communication, and optional C/C++ bindings ensure\nreal-time performance when needed. ARK ships with reusable modules for control,\nSLAM, motion planning, system identification, and visualization, along with\nnative ROS interoperability. Comprehensive documentation and case studies-from\nmanipulation to mobile navigation-demonstrate rapid prototyping, effortless\nhardware swapping, and end-to-end pipelines that rival the convenience of\nmainstream machine-learning workflows. By unifying robotics and AI practices\nunder a common Python umbrella, ARK lowers entry barriers and accelerates\nresearch and commercial deployment of autonomous robots."
                },
                "authors": [
                    {
                        "name": "Magnus Dierking"
                    },
                    {
                        "name": "Christopher E. Mower"
                    },
                    {
                        "name": "Sarthak Das"
                    },
                    {
                        "name": "Huang Helong"
                    },
                    {
                        "name": "Jiacheng Qiu"
                    },
                    {
                        "name": "Cody Reading"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Huidong Liang"
                    },
                    {
                        "name": "Huang Guowei"
                    },
                    {
                        "name": "Jan Peters"
                    },
                    {
                        "name": "Quan Xingyue"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    }
                ],
                "author_detail": {
                    "name": "Haitham Bou-Ammar"
                },
                "author": "Haitham Bou-Ammar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21628v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21628v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14836v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14836v2",
                "updated": "2025-07-14T17:14:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    14,
                    45,
                    0,
                    195,
                    0
                ],
                "published": "2025-03-19T02:35:01Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    2,
                    35,
                    1,
                    2,
                    78,
                    0
                ],
                "title": "On the Robustness Tradeoff in Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Robustness Tradeoff in Fine-Tuning"
                },
                "summary": "Fine-tuning has become the standard practice for adapting pre-trained models\nto downstream tasks. However, the impact on model robustness is not well\nunderstood. In this work, we characterize the robustness-accuracy trade-off in\nfine-tuning. We evaluate the robustness and accuracy of fine-tuned models over\n6 benchmark datasets and 7 different fine-tuning strategies. We observe a\nconsistent trade-off between adversarial robustness and accuracy. Peripheral\nupdates such as BitFit are more effective for simple tasks -- over 75% above\nthe average measured by the area under the Pareto frontiers on CIFAR-10 and\nCIFAR-100. In contrast, fine-tuning information-heavy layers, such as attention\nlayers via Compacter, achieves a better Pareto frontier on more complex tasks\n-- 57.5% and 34.6% above the average on Caltech-256 and CUB-200, respectively.\nLastly, we observe that the robustness of fine-tuning against\nout-of-distribution data closely tracks accuracy. These insights emphasize the\nneed for robustness-aware fine-tuning to ensure reliable real-world\ndeployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning has become the standard practice for adapting pre-trained models\nto downstream tasks. However, the impact on model robustness is not well\nunderstood. In this work, we characterize the robustness-accuracy trade-off in\nfine-tuning. We evaluate the robustness and accuracy of fine-tuned models over\n6 benchmark datasets and 7 different fine-tuning strategies. We observe a\nconsistent trade-off between adversarial robustness and accuracy. Peripheral\nupdates such as BitFit are more effective for simple tasks -- over 75% above\nthe average measured by the area under the Pareto frontiers on CIFAR-10 and\nCIFAR-100. In contrast, fine-tuning information-heavy layers, such as attention\nlayers via Compacter, achieves a better Pareto frontier on more complex tasks\n-- 57.5% and 34.6% above the average on Caltech-256 and CUB-200, respectively.\nLastly, we observe that the robustness of fine-tuning against\nout-of-distribution data closely tracks accuracy. These insights emphasize the\nneed for robustness-aware fine-tuning to ensure reliable real-world\ndeployments."
                },
                "authors": [
                    {
                        "name": "Kunyang Li"
                    },
                    {
                        "name": "Jean-Charles Noirot Ferrand"
                    },
                    {
                        "name": "Ryan Sheatsley"
                    },
                    {
                        "name": "Blaine Hoak"
                    },
                    {
                        "name": "Yohan Beugin"
                    },
                    {
                        "name": "Eric Pauley"
                    },
                    {
                        "name": "Patrick McDaniel"
                    }
                ],
                "author_detail": {
                    "name": "Patrick McDaniel"
                },
                "author": "Patrick McDaniel",
                "arxiv_comment": "Accepted to International Conference on Computer Vision, ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14836v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14836v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10475v1",
                "updated": "2025-07-14T16:55:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    55,
                    57,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T16:55:57Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    55,
                    57,
                    0,
                    195,
                    0
                ],
                "title": "Can You Detect the Difference?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can You Detect the Difference?"
                },
                "summary": "The rapid advancement of large language models (LLMs) has raised concerns\nabout reliably detecting AI-generated text. Stylometric metrics work well on\nautoregressive (AR) outputs, but their effectiveness on diffusion-based models\nis unknown. We present the first systematic comparison of diffusion-generated\ntext (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity,\nburstiness, lexical diversity, readability, and BLEU/ROUGE scores show that\nLLaDA closely mimics human text in perplexity and burstiness, yielding high\nfalse-negative rates for AR-oriented detectors. LLaMA shows much lower\nperplexity but reduced lexical fidelity. Relying on any single metric fails to\nseparate diffusion outputs from human writing. We highlight the need for\ndiffusion-aware detectors and outline directions such as hybrid models,\ndiffusion-specific stylometric signatures, and robust watermarking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has raised concerns\nabout reliably detecting AI-generated text. Stylometric metrics work well on\nautoregressive (AR) outputs, but their effectiveness on diffusion-based models\nis unknown. We present the first systematic comparison of diffusion-generated\ntext (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity,\nburstiness, lexical diversity, readability, and BLEU/ROUGE scores show that\nLLaDA closely mimics human text in perplexity and burstiness, yielding high\nfalse-negative rates for AR-oriented detectors. LLaMA shows much lower\nperplexity but reduced lexical fidelity. Relying on any single metric fails to\nseparate diffusion outputs from human writing. We highlight the need for\ndiffusion-aware detectors and outline directions such as hybrid models,\ndiffusion-specific stylometric signatures, and robust watermarking."
                },
                "authors": [
                    {
                        "name": "ƒ∞smail Tarƒ±m"
                    },
                    {
                        "name": "Aytuƒü Onan"
                    }
                ],
                "author_detail": {
                    "name": "Aytuƒü Onan"
                },
                "author": "Aytuƒü Onan",
                "arxiv_comment": "11 pages, 3 figures, 2 tables. Code and data:\n  https://github.com/ismailtrm/ceng_404. Cross-list requested to cs.AI for\n  AI-safety relevance",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10472v1",
                "updated": "2025-07-14T16:53:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    53,
                    19,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T16:53:19Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    53,
                    19,
                    0,
                    195,
                    0
                ],
                "title": "MLAR: Multi-layer Large Language Model-based Robotic Process Automation\n  Applicant Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLAR: Multi-layer Large Language Model-based Robotic Process Automation\n  Applicant Tracking"
                },
                "summary": "This paper introduces an innovative Applicant Tracking System (ATS) enhanced\nby a novel Robotic process automation (RPA) framework or as further referred to\nas MLAR. Traditional recruitment processes often encounter bottlenecks in\nresume screening and candidate shortlisting due to time and resource\nconstraints. MLAR addresses these challenges employing Large Language Models\n(LLMs) in three distinct layers: extracting key characteristics from job\npostings in the first layer, parsing applicant resume to identify education,\nexperience, skills in the second layer, and similarity matching in the third\nlayer. These features are then matched through advanced semantic algorithms to\nidentify the best candidates efficiently. Our approach integrates seamlessly\ninto existing RPA pipelines, automating resume parsing, job matching, and\ncandidate notifications. Extensive performance benchmarking shows that MLAR\noutperforms the leading RPA platforms, including UiPath and Automation\nAnywhere, in high-volume resume-processing tasks. When processing 2,400\nresumes, MLAR achieved an average processing time of 5.4 seconds per resume,\nreducing processing time by approximately 16.9% compared to Automation Anywhere\nand 17.1% compared to UiPath. These results highlight the potential of MLAR to\ntransform recruitment workflows by providing an efficient, accurate, and\nscalable solution tailored to modern hiring needs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an innovative Applicant Tracking System (ATS) enhanced\nby a novel Robotic process automation (RPA) framework or as further referred to\nas MLAR. Traditional recruitment processes often encounter bottlenecks in\nresume screening and candidate shortlisting due to time and resource\nconstraints. MLAR addresses these challenges employing Large Language Models\n(LLMs) in three distinct layers: extracting key characteristics from job\npostings in the first layer, parsing applicant resume to identify education,\nexperience, skills in the second layer, and similarity matching in the third\nlayer. These features are then matched through advanced semantic algorithms to\nidentify the best candidates efficiently. Our approach integrates seamlessly\ninto existing RPA pipelines, automating resume parsing, job matching, and\ncandidate notifications. Extensive performance benchmarking shows that MLAR\noutperforms the leading RPA platforms, including UiPath and Automation\nAnywhere, in high-volume resume-processing tasks. When processing 2,400\nresumes, MLAR achieved an average processing time of 5.4 seconds per resume,\nreducing processing time by approximately 16.9% compared to Automation Anywhere\nand 17.1% compared to UiPath. These results highlight the potential of MLAR to\ntransform recruitment workflows by providing an efficient, accurate, and\nscalable solution tailored to modern hiring needs."
                },
                "authors": [
                    {
                        "name": "Mohamed T. Younes"
                    },
                    {
                        "name": "Omar Walid"
                    },
                    {
                        "name": "Mai Hassan"
                    },
                    {
                        "name": "Ali Hamdi"
                    }
                ],
                "author_detail": {
                    "name": "Ali Hamdi"
                },
                "author": "Ali Hamdi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10468v1",
                "updated": "2025-07-14T16:46:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    46,
                    30,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T16:46:30Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    46,
                    30,
                    0,
                    195,
                    0
                ],
                "title": "From BERT to Qwen: Hate Detection across architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From BERT to Qwen: Hate Detection across architectures"
                },
                "summary": "Online platforms struggle to curb hate speech without over-censoring\nlegitimate discourse. Early bidirectional transformer encoders made big\nstrides, but the arrival of ultra-large autoregressive LLMs promises deeper\ncontext-awareness. Whether this extra scale actually improves practical\nhate-speech detection on real-world text remains unverified. Our study puts\nthis question to the test by benchmarking both model families, classic encoders\nand next-generation LLMs, on curated corpora of online interactions for\nhate-speech detection (Hate or No Hate).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online platforms struggle to curb hate speech without over-censoring\nlegitimate discourse. Early bidirectional transformer encoders made big\nstrides, but the arrival of ultra-large autoregressive LLMs promises deeper\ncontext-awareness. Whether this extra scale actually improves practical\nhate-speech detection on real-world text remains unverified. Our study puts\nthis question to the test by benchmarking both model families, classic encoders\nand next-generation LLMs, on curated corpora of online interactions for\nhate-speech detection (Hate or No Hate)."
                },
                "authors": [
                    {
                        "name": "Ariadna Mon"
                    },
                    {
                        "name": "Sa√∫l Fenollosa"
                    },
                    {
                        "name": "Jon Lecumberri"
                    }
                ],
                "author_detail": {
                    "name": "Jon Lecumberri"
                },
                "author": "Jon Lecumberri",
                "arxiv_comment": "4 pages, 5 figures. EE-559 Deep Learning course project (Group 11)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10457v1",
                "updated": "2025-07-14T16:37:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    37,
                    5,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T16:37:05Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    37,
                    5,
                    0,
                    195,
                    0
                ],
                "title": "Logic layer Prompt Control Injection (LPCI): A Novel Security\n  Vulnerability Class in Agentic Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logic layer Prompt Control Injection (LPCI): A Novel Security\n  Vulnerability Class in Agentic Systems"
                },
                "summary": "The integration of large language models (LLMs) into enterprise systems has\ncreated a new class of covert security vulnerabilities, particularly within\nlogic-execution layers and persistent-memory contexts. In this paper, we\nintroduce Logic-Layer Prompt Control Injection (LPCI), a novel attack category\nin which encoded, delayed, and conditionally triggered payloads are embedded in\nmemory, vector stores, or tool outputs. These payloads can bypass conventional\ninput filters and trigger unauthorised behaviour across sessions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language models (LLMs) into enterprise systems has\ncreated a new class of covert security vulnerabilities, particularly within\nlogic-execution layers and persistent-memory contexts. In this paper, we\nintroduce Logic-Layer Prompt Control Injection (LPCI), a novel attack category\nin which encoded, delayed, and conditionally triggered payloads are embedded in\nmemory, vector stores, or tool outputs. These payloads can bypass conventional\ninput filters and trigger unauthorised behaviour across sessions."
                },
                "authors": [
                    {
                        "name": "Hammad Atta"
                    },
                    {
                        "name": "Ken Huang"
                    },
                    {
                        "name": "Manish Bhatt"
                    },
                    {
                        "name": "Kamal Ahmed"
                    },
                    {
                        "name": "Muhammad Aziz Ul Haq"
                    },
                    {
                        "name": "Yasir Mehmood"
                    }
                ],
                "author_detail": {
                    "name": "Yasir Mehmood"
                },
                "author": "Yasir Mehmood",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12355v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12355v3",
                "updated": "2025-07-15T05:21:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    5,
                    21,
                    49,
                    1,
                    196,
                    0
                ],
                "published": "2025-04-16T02:33:19Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    2,
                    33,
                    19,
                    2,
                    106,
                    0
                ],
                "title": "Leveraging Large Language Models for Multi-Class and Multi-Label\n  Detection of Drug Use and Overdose Symptoms on Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Multi-Class and Multi-Label\n  Detection of Drug Use and Overdose Symptoms on Social Media"
                },
                "summary": "Drug overdose remains a critical global health issue, often driven by misuse\nof opioids, painkillers, and psychiatric medications. Traditional research\nmethods face limitations, whereas social media offers real-time insights into\nself-reported substance use and overdose symptoms. This study proposes an\nAI-driven NLP framework trained on annotated social media data to detect\ncommonly used drugs and associated overdose symptoms. Using a hybrid annotation\nstrategy with LLMs and human annotators, we applied traditional ML models,\nneural networks, and advanced transformer-based models. Our framework achieved\n98% accuracy in multi-class and 97% in multi-label classification,\noutperforming baseline models by up to 8%. These findings highlight the\npotential of AI for supporting public health surveillance and personalized\nintervention strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drug overdose remains a critical global health issue, often driven by misuse\nof opioids, painkillers, and psychiatric medications. Traditional research\nmethods face limitations, whereas social media offers real-time insights into\nself-reported substance use and overdose symptoms. This study proposes an\nAI-driven NLP framework trained on annotated social media data to detect\ncommonly used drugs and associated overdose symptoms. Using a hybrid annotation\nstrategy with LLMs and human annotators, we applied traditional ML models,\nneural networks, and advanced transformer-based models. Our framework achieved\n98% accuracy in multi-class and 97% in multi-label classification,\noutperforming baseline models by up to 8%. These findings highlight the\npotential of AI for supporting public health surveillance and personalized\nintervention strategies."
                },
                "authors": [
                    {
                        "name": "Muhammad Ahmad"
                    },
                    {
                        "name": "Fida Ullah"
                    },
                    {
                        "name": "Muhammad Usman"
                    },
                    {
                        "name": "Umyh Habiba"
                    },
                    {
                        "name": "ldar Batyrshin"
                    },
                    {
                        "name": "Grigori Sidorov"
                    }
                ],
                "author_detail": {
                    "name": "Grigori Sidorov"
                },
                "author": "Grigori Sidorov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12355v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12355v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10445v1",
                "updated": "2025-07-14T16:28:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    28,
                    0,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T16:28:00Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    28,
                    0,
                    0,
                    195,
                    0
                ],
                "title": "Referential ambiguity and clarification requests: comparing human and\n  LLM behaviour",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Referential ambiguity and clarification requests: comparing human and\n  LLM behaviour"
                },
                "summary": "In this work we examine LLMs' ability to ask clarification questions in\ntask-oriented dialogues that follow the asynchronous\ninstruction-giver/instruction-follower format. We present a new corpus that\ncombines two existing annotations of the Minecraft Dialogue Corpus -- one for\nreference and ambiguity in reference, and one for SDRT including clarifications\n-- into a single common format providing the necessary information to\nexperiment with clarifications and their relation to ambiguity. With this\ncorpus we compare LLM actions with original human-generated clarification\nquestions, examining how both humans and LLMs act in the case of ambiguity. We\nfind that there is only a weak link between ambiguity and humans producing\nclarification questions in these dialogues, and low correlation between humans\nand LLMs. Humans hardly ever produce clarification questions for referential\nambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce\nmore clarification questions for referential ambiguity, but less so for task\nuncertainty. We question if LLMs' ability to ask clarification questions is\npredicated on their recent ability to simulate reasoning, and test this with\ndifferent reasoning approaches, finding that reasoning does appear to increase\nquestion frequency and relevancy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we examine LLMs' ability to ask clarification questions in\ntask-oriented dialogues that follow the asynchronous\ninstruction-giver/instruction-follower format. We present a new corpus that\ncombines two existing annotations of the Minecraft Dialogue Corpus -- one for\nreference and ambiguity in reference, and one for SDRT including clarifications\n-- into a single common format providing the necessary information to\nexperiment with clarifications and their relation to ambiguity. With this\ncorpus we compare LLM actions with original human-generated clarification\nquestions, examining how both humans and LLMs act in the case of ambiguity. We\nfind that there is only a weak link between ambiguity and humans producing\nclarification questions in these dialogues, and low correlation between humans\nand LLMs. Humans hardly ever produce clarification questions for referential\nambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce\nmore clarification questions for referential ambiguity, but less so for task\nuncertainty. We question if LLMs' ability to ask clarification questions is\npredicated on their recent ability to simulate reasoning, and test this with\ndifferent reasoning approaches, finding that reasoning does appear to increase\nquestion frequency and relevancy."
                },
                "authors": [
                    {
                        "name": "Chris Madge"
                    },
                    {
                        "name": "Matthew Purver"
                    },
                    {
                        "name": "Massimo Poesio"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Poesio"
                },
                "author": "Massimo Poesio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10427v1",
                "updated": "2025-07-14T16:16:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    16,
                    12,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T16:16:12Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    16,
                    12,
                    0,
                    195,
                    0
                ],
                "title": "Towards Emotion Co-regulation with LLM-powered Socially Assistive\n  Robots: Integrating LLM Prompts and Robotic Behaviors to Support\n  Parent-Neurodivergent Child Dyads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Emotion Co-regulation with LLM-powered Socially Assistive\n  Robots: Integrating LLM Prompts and Robotic Behaviors to Support\n  Parent-Neurodivergent Child Dyads"
                },
                "summary": "Socially Assistive Robotics (SAR) has shown promise in supporting emotion\nregulation for neurodivergent children. Recently, there has been increasing\ninterest in leveraging advanced technologies to assist parents in co-regulating\nemotions with their children. However, limited research has explored the\nintegration of large language models (LLMs) with SAR to facilitate emotion\nco-regulation between parents and children with neurodevelopmental disorders.\nTo address this gap, we developed an LLM-powered social robot by deploying a\nspeech communication module on the MiRo-E robotic platform. This supervised\nautonomous system integrates LLM prompts and robotic behaviors to deliver\ntailored interventions for both parents and neurodivergent children. Pilot\ntests were conducted with two parent-child dyads, followed by a qualitative\nanalysis. The findings reveal MiRo-E's positive impacts on interaction dynamics\nand its potential to facilitate emotion regulation, along with identified\ndesign and technical challenges. Based on these insights, we provide design\nimplications to advance the future development of LLM-powered SAR for mental\nhealth applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Socially Assistive Robotics (SAR) has shown promise in supporting emotion\nregulation for neurodivergent children. Recently, there has been increasing\ninterest in leveraging advanced technologies to assist parents in co-regulating\nemotions with their children. However, limited research has explored the\nintegration of large language models (LLMs) with SAR to facilitate emotion\nco-regulation between parents and children with neurodevelopmental disorders.\nTo address this gap, we developed an LLM-powered social robot by deploying a\nspeech communication module on the MiRo-E robotic platform. This supervised\nautonomous system integrates LLM prompts and robotic behaviors to deliver\ntailored interventions for both parents and neurodivergent children. Pilot\ntests were conducted with two parent-child dyads, followed by a qualitative\nanalysis. The findings reveal MiRo-E's positive impacts on interaction dynamics\nand its potential to facilitate emotion regulation, along with identified\ndesign and technical challenges. Based on these insights, we provide design\nimplications to advance the future development of LLM-powered SAR for mental\nhealth applications."
                },
                "authors": [
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Felix Schijve"
                    },
                    {
                        "name": "Sheng Li"
                    },
                    {
                        "name": "Yuye Yang"
                    },
                    {
                        "name": "Jun Hu"
                    },
                    {
                        "name": "Emilia Barakova"
                    }
                ],
                "author_detail": {
                    "name": "Emilia Barakova"
                },
                "author": "Emilia Barakova",
                "arxiv_comment": "Submission for the IROS 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v3",
                "updated": "2025-07-14T16:14:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    14,
                    49,
                    0,
                    195,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at https://github.com/THU-MIG/PrefixKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at https://github.com/THU-MIG/PrefixKV."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jiaxin Li"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "12 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10392v1",
                "updated": "2025-07-14T15:31:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    31,
                    31,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T15:31:31Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    31,
                    31,
                    0,
                    195,
                    0
                ],
                "title": "Zorse: Optimizing LLM Training Efficiency on Heterogeneous GPU Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zorse: Optimizing LLM Training Efficiency on Heterogeneous GPU Clusters"
                },
                "summary": "Large language models (LLMs) require vast amounts of GPU compute to train,\nbut limited availability and high costs of GPUs make homogeneous clusters\nimpractical for many organizations. Instead, assembling heterogeneous clusters\nby pooling together GPUs of different generations allows them to achieve higher\naggregate compute and make use of all available GPUs. However, training on\nheterogeneous clusters presents several challenges, including load balancing\nacross GPUs, optimizing memory usage to accommodate varying memory capacities,\nand ensuring communication-efficient training over diverse network\ninterconnects potentially spanning multiple datacenters. In this paper, we make\nthe case that efficient training on heterogeneous clusters requires (1) the\nintegration of pipeline parallelism and data parallelism in a manner that is\nboth communication- and memory-efficient, and (2) a more adaptable\nconfiguration of pipeline and data parallelism, which includes the capability\nto flexibly partition GPUs into asymmetric pipeline parallel stages and to\nincorporate heterogeneous GPUs within the same data parallelism group. We\npropose Zorse, the first system to unify all these capabilities while\nincorporating a planner that automatically configures training strategies for a\ngiven workload. Our evaluation shows that Zorse significantly outperforms\nstate-of-the-art systems in heterogeneous training scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) require vast amounts of GPU compute to train,\nbut limited availability and high costs of GPUs make homogeneous clusters\nimpractical for many organizations. Instead, assembling heterogeneous clusters\nby pooling together GPUs of different generations allows them to achieve higher\naggregate compute and make use of all available GPUs. However, training on\nheterogeneous clusters presents several challenges, including load balancing\nacross GPUs, optimizing memory usage to accommodate varying memory capacities,\nand ensuring communication-efficient training over diverse network\ninterconnects potentially spanning multiple datacenters. In this paper, we make\nthe case that efficient training on heterogeneous clusters requires (1) the\nintegration of pipeline parallelism and data parallelism in a manner that is\nboth communication- and memory-efficient, and (2) a more adaptable\nconfiguration of pipeline and data parallelism, which includes the capability\nto flexibly partition GPUs into asymmetric pipeline parallel stages and to\nincorporate heterogeneous GPUs within the same data parallelism group. We\npropose Zorse, the first system to unify all these capabilities while\nincorporating a planner that automatically configures training strategies for a\ngiven workload. Our evaluation shows that Zorse significantly outperforms\nstate-of-the-art systems in heterogeneous training scenarios."
                },
                "authors": [
                    {
                        "name": "Runsheng Benson Guo"
                    },
                    {
                        "name": "Utkarsh Anand"
                    },
                    {
                        "name": "Khuzaima Daudjee"
                    },
                    {
                        "name": "Rathijit Sen"
                    }
                ],
                "author_detail": {
                    "name": "Rathijit Sen"
                },
                "author": "Rathijit Sen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11168v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11168v3",
                "updated": "2025-07-14T15:27:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    27,
                    11,
                    0,
                    195,
                    0
                ],
                "published": "2025-04-15T13:16:02Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    16,
                    2,
                    1,
                    105,
                    0
                ],
                "title": "Bypassing LLM Guardrails: An Empirical Analysis of Evasion Attacks\n  against Prompt Injection and Jailbreak Detection Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bypassing LLM Guardrails: An Empirical Analysis of Evasion Attacks\n  against Prompt Injection and Jailbreak Detection Systems"
                },
                "summary": "Large Language Models (LLMs) guardrail systems are designed to protect\nagainst prompt injection and jailbreak attacks. However, they remain vulnerable\nto evasion techniques. We demonstrate two approaches for bypassing LLM prompt\ninjection and jailbreak detection systems via traditional character injection\nmethods and algorithmic Adversarial Machine Learning (AML) evasion techniques.\nThrough testing against six prominent protection systems, including Microsoft's\nAzure Prompt Shield and Meta's Prompt Guard, we show that both methods can be\nused to evade detection while maintaining adversarial utility achieving in some\ninstances up to 100% evasion success. Furthermore, we demonstrate that\nadversaries can enhance Attack Success Rates (ASR) against black-box targets by\nleveraging word importance ranking computed by offline white-box models. Our\nfindings reveal vulnerabilities within current LLM protection mechanisms and\nhighlight the need for more robust guardrail systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) guardrail systems are designed to protect\nagainst prompt injection and jailbreak attacks. However, they remain vulnerable\nto evasion techniques. We demonstrate two approaches for bypassing LLM prompt\ninjection and jailbreak detection systems via traditional character injection\nmethods and algorithmic Adversarial Machine Learning (AML) evasion techniques.\nThrough testing against six prominent protection systems, including Microsoft's\nAzure Prompt Shield and Meta's Prompt Guard, we show that both methods can be\nused to evade detection while maintaining adversarial utility achieving in some\ninstances up to 100% evasion success. Furthermore, we demonstrate that\nadversaries can enhance Attack Success Rates (ASR) against black-box targets by\nleveraging word importance ranking computed by offline white-box models. Our\nfindings reveal vulnerabilities within current LLM protection mechanisms and\nhighlight the need for more robust guardrail systems."
                },
                "authors": [
                    {
                        "name": "William Hackett"
                    },
                    {
                        "name": "Lewis Birch"
                    },
                    {
                        "name": "Stefan Trawicki"
                    },
                    {
                        "name": "Neeraj Suri"
                    },
                    {
                        "name": "Peter Garraghan"
                    }
                ],
                "author_detail": {
                    "name": "Peter Garraghan"
                },
                "author": "Peter Garraghan",
                "arxiv_comment": "14 pages, 5 figures, 11 tables. To be published in LLMSec 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11168v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11168v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10382v1",
                "updated": "2025-07-14T15:23:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    23,
                    11,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T15:23:11Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    23,
                    11,
                    0,
                    195,
                    0
                ],
                "title": "Leveraging RAG-LLMs for Urban Mobility Simulation and Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging RAG-LLMs for Urban Mobility Simulation and Analysis"
                },
                "summary": "With the rise of smart mobility and shared e-mobility services, numerous\nadvanced technologies have been applied to this field. Cloud-based traffic\nsimulation solutions have flourished, offering increasingly realistic\nrepresentations of the evolving mobility landscape. LLMs have emerged as\npioneering tools, providing robust support for various applications, including\nintelligent decision-making, user interaction, and real-time traffic analysis.\nAs user demand for e-mobility continues to grow, delivering comprehensive\nend-to-end solutions has become crucial. In this paper, we present a\ncloud-based, LLM-powered shared e-mobility platform, integrated with a mobile\napplication for personalized route recommendations. The optimization module is\nevaluated based on travel time and cost across different traffic scenarios.\nAdditionally, the LLM-powered RAG framework is evaluated at the schema level\nfor different users, using various evaluation methods. Schema-level RAG with\nXiYanSQL achieves an average execution accuracy of 0.81 on system operator\nqueries and 0.98 on user queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of smart mobility and shared e-mobility services, numerous\nadvanced technologies have been applied to this field. Cloud-based traffic\nsimulation solutions have flourished, offering increasingly realistic\nrepresentations of the evolving mobility landscape. LLMs have emerged as\npioneering tools, providing robust support for various applications, including\nintelligent decision-making, user interaction, and real-time traffic analysis.\nAs user demand for e-mobility continues to grow, delivering comprehensive\nend-to-end solutions has become crucial. In this paper, we present a\ncloud-based, LLM-powered shared e-mobility platform, integrated with a mobile\napplication for personalized route recommendations. The optimization module is\nevaluated based on travel time and cost across different traffic scenarios.\nAdditionally, the LLM-powered RAG framework is evaluated at the schema level\nfor different users, using various evaluation methods. Schema-level RAG with\nXiYanSQL achieves an average execution accuracy of 0.81 on system operator\nqueries and 0.98 on user queries."
                },
                "authors": [
                    {
                        "name": "Yue Ding"
                    },
                    {
                        "name": "Conor McCarthy"
                    },
                    {
                        "name": "Kevin O'Shea"
                    },
                    {
                        "name": "Mingming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Mingming Liu"
                },
                "author": "Mingming Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02083v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02083v2",
                "updated": "2025-07-14T15:17:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    17,
                    16,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-02T18:41:44Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    18,
                    41,
                    44,
                    2,
                    183,
                    0
                ],
                "title": "Measuring Scientific Capabilities of Language Models with a Systems\n  Biology Dry Lab",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Scientific Capabilities of Language Models with a Systems\n  Biology Dry Lab"
                },
                "summary": "Designing experiments and result interpretations are core scientific\ncompetencies, particularly in biology, where researchers perturb complex\nsystems to uncover the underlying systems. Recent efforts to evaluate the\nscientific capabilities of large language models (LLMs) fail to test these\ncompetencies because wet-lab experimentation is prohibitively expensive: in\nexpertise, time and equipment. We introduce SciGym, a first-in-class benchmark\nthat assesses LLMs' iterative experiment design and analysis abilities in\nopen-ended scientific discovery tasks. SciGym overcomes the challenge of\nwet-lab costs by running a dry lab of biological systems. These models, encoded\nin Systems Biology Markup Language, are efficient for generating simulated\ndata, making them ideal testbeds for experimentation on realistically complex\nsystems. We evaluated six frontier LLMs on 137 small systems, and released a\ntotal of 350 systems. Our evaluation shows that while more capable models\ndemonstrated superior performance, all models' performance declined\nsignificantly as system complexity increased, suggesting substantial room for\nimprovement in the scientific capabilities of LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing experiments and result interpretations are core scientific\ncompetencies, particularly in biology, where researchers perturb complex\nsystems to uncover the underlying systems. Recent efforts to evaluate the\nscientific capabilities of large language models (LLMs) fail to test these\ncompetencies because wet-lab experimentation is prohibitively expensive: in\nexpertise, time and equipment. We introduce SciGym, a first-in-class benchmark\nthat assesses LLMs' iterative experiment design and analysis abilities in\nopen-ended scientific discovery tasks. SciGym overcomes the challenge of\nwet-lab costs by running a dry lab of biological systems. These models, encoded\nin Systems Biology Markup Language, are efficient for generating simulated\ndata, making them ideal testbeds for experimentation on realistically complex\nsystems. We evaluated six frontier LLMs on 137 small systems, and released a\ntotal of 350 systems. Our evaluation shows that while more capable models\ndemonstrated superior performance, all models' performance declined\nsignificantly as system complexity increased, suggesting substantial room for\nimprovement in the scientific capabilities of LLM agents."
                },
                "authors": [
                    {
                        "name": "Haonan Duan"
                    },
                    {
                        "name": "Stephen Zhewen Lu"
                    },
                    {
                        "name": "Caitlin Fiona Harrigan"
                    },
                    {
                        "name": "Nishkrit Desai"
                    },
                    {
                        "name": "Jiarui Lu"
                    },
                    {
                        "name": "Micha≈Ç Koziarski"
                    },
                    {
                        "name": "Leonardo Cotta"
                    },
                    {
                        "name": "Chris J. Maddison"
                    }
                ],
                "author_detail": {
                    "name": "Chris J. Maddison"
                },
                "author": "Chris J. Maddison",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02083v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02083v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06238v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06238v2",
                "updated": "2025-07-14T15:16:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    16,
                    18,
                    0,
                    195,
                    0
                ],
                "published": "2024-10-08T17:54:03Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    17,
                    54,
                    3,
                    1,
                    282,
                    0
                ],
                "title": "EVOLvE: Evaluating and Optimizing LLMs For In-Context Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOLvE: Evaluating and Optimizing LLMs For In-Context Exploration"
                },
                "summary": "Despite their success in many domains, large language models (LLMs) remain\nunder-studied in scenarios requiring optimal decision-making under uncertainty.\nThis is crucial as many real-world applications, ranging from personalized\nrecommendations to healthcare interventions, demand that LLMs not only predict\nbut also actively learn to make optimal decisions through exploration. In this\nwork, we measure LLMs' (in)ability to make optimal decisions in bandits, a\nstate-less reinforcement learning setting relevant to many applications. We\ndevelop a comprehensive suite of environments, including both context-free and\ncontextual bandits with varying task difficulties, to benchmark LLMs'\nperformance. Motivated by the existence of optimal exploration algorithms, we\npropose efficient ways to integrate this algorithmic knowledge into LLMs: by\nproviding explicit algorithm-guided support during inference; and through\nalgorithm distillation via in-context demonstrations and fine-tuning, using\nsynthetic data generated from these algorithms. Impressively, these techniques\nallow us to achieve superior exploration performance with smaller models,\nsurpassing larger models on various tasks. We conducted an extensive ablation\nstudy to shed light on various factors, such as task difficulty and data\nrepresentation, that influence the efficiency of LLM exploration. Additionally,\nwe conduct a rigorous analysis of the LLM's exploration efficiency using the\nconcept of regret, linking its ability to explore to the model size and\nunderlying algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their success in many domains, large language models (LLMs) remain\nunder-studied in scenarios requiring optimal decision-making under uncertainty.\nThis is crucial as many real-world applications, ranging from personalized\nrecommendations to healthcare interventions, demand that LLMs not only predict\nbut also actively learn to make optimal decisions through exploration. In this\nwork, we measure LLMs' (in)ability to make optimal decisions in bandits, a\nstate-less reinforcement learning setting relevant to many applications. We\ndevelop a comprehensive suite of environments, including both context-free and\ncontextual bandits with varying task difficulties, to benchmark LLMs'\nperformance. Motivated by the existence of optimal exploration algorithms, we\npropose efficient ways to integrate this algorithmic knowledge into LLMs: by\nproviding explicit algorithm-guided support during inference; and through\nalgorithm distillation via in-context demonstrations and fine-tuning, using\nsynthetic data generated from these algorithms. Impressively, these techniques\nallow us to achieve superior exploration performance with smaller models,\nsurpassing larger models on various tasks. We conducted an extensive ablation\nstudy to shed light on various factors, such as task difficulty and data\nrepresentation, that influence the efficiency of LLM exploration. Additionally,\nwe conduct a rigorous analysis of the LLM's exploration efficiency using the\nconcept of regret, linking its ability to explore to the model size and\nunderlying algorithm."
                },
                "authors": [
                    {
                        "name": "Allen Nie"
                    },
                    {
                        "name": "Yi Su"
                    },
                    {
                        "name": "Bo Chang"
                    },
                    {
                        "name": "Jonathan N. Lee"
                    },
                    {
                        "name": "Ed H. Chi"
                    },
                    {
                        "name": "Quoc V. Le"
                    },
                    {
                        "name": "Minmin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Minmin Chen"
                },
                "author": "Minmin Chen",
                "arxiv_comment": "28 pages. Published at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06238v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10367v1",
                "updated": "2025-07-14T15:09:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T15:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline"
                },
                "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junbin Kang"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Shaohong Guo"
                    },
                    {
                        "name": "Ziyan Qiu"
                    },
                    {
                        "name": "Mingzhen You"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Anqi Yu"
                    },
                    {
                        "name": "Tianhong Ding"
                    },
                    {
                        "name": "Xinwei Hu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by NSDI'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02620v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02620v2",
                "updated": "2025-07-14T14:55:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    14,
                    55,
                    53,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-03T13:47:42Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    47,
                    42,
                    3,
                    184,
                    0
                ],
                "title": "FlowSpec: Continuous Pipelined Speculative Decoding for Efficient\n  Distributed LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowSpec: Continuous Pipelined Speculative Decoding for Efficient\n  Distributed LLM Inference"
                },
                "summary": "Distributed inference serves as a promising approach to enabling the\ninference of large language models (LLMs) at the network edge. It distributes\nthe inference process to multiple devices to ensure that the LLMs can fit into\nthe device memory. Recent pipeline-based approaches have the potential to\nparallelize communication and computation, which helps reduce inference\nlatency. However, the benefit diminishes when the inference request at the\nnetwork edge is sparse, where pipeline is typically at low utilization. To\nenable efficient distributed LLM inference at the edge, we propose\n\\textbf{FlowSpec}, a pipeline-parallel tree-based speculative decoding\nframework. FlowSpec incorporates three key mechanisms to improve decoding\nefficiency: 1) score-based step-wise verification prioritizes more important\ndraft tokens to bring earlier accpeted tokens; 2) efficient draft management to\nprune invalid tokens while maintaining correct causal relationship during\nverification; 3) dynamic draft expansion strategies to supply high-quality\nspeculative inputs. These techniques work in concert to enhance both pipeline\nutilization and speculative efficiency. We evaluate FlowSpec on a real-world\ntestbed with other baselines. Experimental results demonstrate that our\nproposed framework significantly improves inference speed across diverse models\nand configurations, achieving speedup ratios 1.28$\\times$-1.79$\\times$ compared\nto baselines. Our code is publicly available at\n\\href{https://github.com/Leosang-lx/FlowSpec#}{https://github.com/Leosang-lx/FlowSpec\\#}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed inference serves as a promising approach to enabling the\ninference of large language models (LLMs) at the network edge. It distributes\nthe inference process to multiple devices to ensure that the LLMs can fit into\nthe device memory. Recent pipeline-based approaches have the potential to\nparallelize communication and computation, which helps reduce inference\nlatency. However, the benefit diminishes when the inference request at the\nnetwork edge is sparse, where pipeline is typically at low utilization. To\nenable efficient distributed LLM inference at the edge, we propose\n\\textbf{FlowSpec}, a pipeline-parallel tree-based speculative decoding\nframework. FlowSpec incorporates three key mechanisms to improve decoding\nefficiency: 1) score-based step-wise verification prioritizes more important\ndraft tokens to bring earlier accpeted tokens; 2) efficient draft management to\nprune invalid tokens while maintaining correct causal relationship during\nverification; 3) dynamic draft expansion strategies to supply high-quality\nspeculative inputs. These techniques work in concert to enhance both pipeline\nutilization and speculative efficiency. We evaluate FlowSpec on a real-world\ntestbed with other baselines. Experimental results demonstrate that our\nproposed framework significantly improves inference speed across diverse models\nand configurations, achieving speedup ratios 1.28$\\times$-1.79$\\times$ compared\nto baselines. Our code is publicly available at\n\\href{https://github.com/Leosang-lx/FlowSpec#}{https://github.com/Leosang-lx/FlowSpec\\#}"
                },
                "authors": [
                    {
                        "name": "Xing Liu"
                    },
                    {
                        "name": "Lizhuo Luo"
                    },
                    {
                        "name": "Ming Tang"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "16 pages, and the last 3 are appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02620v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02620v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10342v1",
                "updated": "2025-07-14T14:47:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    14,
                    47,
                    1,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T14:47:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    14,
                    47,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "Using AI to replicate human experimental results: a motion study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using AI to replicate human experimental results: a motion study"
                },
                "summary": "This paper explores the potential of large language models (LLMs) as reliable\nanalytical tools in linguistic research, focusing on the emergence of affective\nmeanings in temporal expressions involving manner-of-motion verbs. While LLMs\nlike GPT-4 have shown promise across a range of tasks, their ability to\nreplicate nuanced human judgements remains under scrutiny. We conducted four\npsycholinguistic studies (on emergent meanings, valence shifts, verb choice in\nemotional contexts, and sentence-emoji associations) first with human\nparticipants and then replicated the same tasks using an LLM. Results across\nall studies show a striking convergence between human and AI responses, with\nstatistical analyses (e.g., Spearman's rho = .73-.96) indicating strong\ncorrelations in both rating patterns and categorical choices. While minor\ndivergences were observed in some cases, these did not alter the overall\ninterpretative outcomes. These findings offer compelling evidence that LLMs can\naugment traditional human-based experimentation, enabling broader-scale studies\nwithout compromising interpretative validity. This convergence not only\nstrengthens the empirical foundation of prior human-based findings but also\nopens possibilities for hypothesis generation and data expansion through AI.\nUltimately, our study supports the use of LLMs as credible and informative\ncollaborators in linguistic inquiry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of large language models (LLMs) as reliable\nanalytical tools in linguistic research, focusing on the emergence of affective\nmeanings in temporal expressions involving manner-of-motion verbs. While LLMs\nlike GPT-4 have shown promise across a range of tasks, their ability to\nreplicate nuanced human judgements remains under scrutiny. We conducted four\npsycholinguistic studies (on emergent meanings, valence shifts, verb choice in\nemotional contexts, and sentence-emoji associations) first with human\nparticipants and then replicated the same tasks using an LLM. Results across\nall studies show a striking convergence between human and AI responses, with\nstatistical analyses (e.g., Spearman's rho = .73-.96) indicating strong\ncorrelations in both rating patterns and categorical choices. While minor\ndivergences were observed in some cases, these did not alter the overall\ninterpretative outcomes. These findings offer compelling evidence that LLMs can\naugment traditional human-based experimentation, enabling broader-scale studies\nwithout compromising interpretative validity. This convergence not only\nstrengthens the empirical foundation of prior human-based findings but also\nopens possibilities for hypothesis generation and data expansion through AI.\nUltimately, our study supports the use of LLMs as credible and informative\ncollaborators in linguistic inquiry."
                },
                "authors": [
                    {
                        "name": "Rosa Illan Castillo"
                    },
                    {
                        "name": "Javier Valenzuela"
                    }
                ],
                "author_detail": {
                    "name": "Javier Valenzuela"
                },
                "author": "Javier Valenzuela",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10338v1",
                "updated": "2025-07-14T14:43:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    14,
                    43,
                    14,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T14:43:14Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    14,
                    43,
                    14,
                    0,
                    195,
                    0
                ],
                "title": "AssertCoder: LLM-Based Assertion Generation via Multimodal Specification\n  Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AssertCoder: LLM-Based Assertion Generation via Multimodal Specification\n  Extraction"
                },
                "summary": "Assertion-Based Verification (ABV) is critical for ensuring functional\ncorrectness in modern hardware systems. However, manually writing high-quality\nSVAs remains labor-intensive and error-prone. To bridge this gap, we propose\nAssertCoder, a novel unified framework that automatically generates\nhigh-quality SVAs directly from multimodal hardware design specifications.\nAssertCoder employs a modality-sensitive preprocessing to parse heterogeneous\nspecification formats (text, tables, diagrams, and formulas), followed by a set\nof dedicated semantic analyzers that extract structured representations aligned\nwith signal-level semantics. These representations are utilized to drive\nassertion synthesis via multi-step chain-of-thought (CoT) prompting. The\nframework incorporates a mutation-based evaluation approach to assess assertion\nquality via model checking and further refine the generated assertions.\nExperimental evaluation across three real-world Register-Transfer Level (RTL)\ndesigns demonstrates AssertCoder's superior performance, achieving an average\nincrease of 8.4% in functional correctness and 5.8% in mutation detection\ncompared to existing state-of-the-art approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assertion-Based Verification (ABV) is critical for ensuring functional\ncorrectness in modern hardware systems. However, manually writing high-quality\nSVAs remains labor-intensive and error-prone. To bridge this gap, we propose\nAssertCoder, a novel unified framework that automatically generates\nhigh-quality SVAs directly from multimodal hardware design specifications.\nAssertCoder employs a modality-sensitive preprocessing to parse heterogeneous\nspecification formats (text, tables, diagrams, and formulas), followed by a set\nof dedicated semantic analyzers that extract structured representations aligned\nwith signal-level semantics. These representations are utilized to drive\nassertion synthesis via multi-step chain-of-thought (CoT) prompting. The\nframework incorporates a mutation-based evaluation approach to assess assertion\nquality via model checking and further refine the generated assertions.\nExperimental evaluation across three real-world Register-Transfer Level (RTL)\ndesigns demonstrates AssertCoder's superior performance, achieving an average\nincrease of 8.4% in functional correctness and 5.8% in mutation detection\ncompared to existing state-of-the-art approaches."
                },
                "authors": [
                    {
                        "name": "Enyuan Tian"
                    },
                    {
                        "name": "Yiwei Ci"
                    },
                    {
                        "name": "Qiusong Yang"
                    },
                    {
                        "name": "Yufeng Li"
                    },
                    {
                        "name": "Zhichao Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Lyu"
                },
                "author": "Zhichao Lyu",
                "arxiv_comment": "7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22791v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22791v3",
                "updated": "2025-07-15T12:59:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    59,
                    47,
                    1,
                    196,
                    0
                ],
                "published": "2025-06-28T07:25:12Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    7,
                    25,
                    12,
                    5,
                    179,
                    0
                ],
                "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models"
                },
                "summary": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications."
                },
                "authors": [
                    {
                        "name": "Jianxin Yan"
                    },
                    {
                        "name": "Wangze Ni"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22791v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22791v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10326v1",
                "updated": "2025-07-14T14:34:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    14,
                    34,
                    15,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T14:34:15Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    14,
                    34,
                    15,
                    0,
                    195,
                    0
                ],
                "title": "Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation"
                },
                "summary": "Prompt engineering has proven to be a crucial step in leveraging pretrained\nlarge language models (LLMs) in solving various real-world tasks. Numerous\nsolutions have been proposed that seek to automate prompt engineering by using\nthe model itself to edit prompts. However, the majority of state-of-the-art\napproaches are evaluated on tasks that require minimal prompt templates and on\nvery large and highly capable LLMs. In contrast, solving complex tasks that\nrequire detailed information to be included in the prompt increases the amount\nof text that needs to be optimised. Furthermore, smaller models have been shown\nto be more sensitive to prompt design. To address these challenges, we propose\nan evolutionary search approach to automated discrete prompt optimisation\nconsisting of two phases. In the first phase, grammar-guided genetic\nprogramming is invoked to synthesise prompt-creating programmes by searching\nthe space of programmes populated by function compositions of syntactic,\ndictionary-based and LLM-based prompt-editing functions. In the second phase,\nlocal search is applied to explore the neighbourhoods of best-performing\nprogrammes in an attempt to further fine-tune their performance. Our approach\noutperforms three state-of-the-art prompt optimisation approaches,\nPromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose\nLLMs in four domain-specific challenging tasks. We also illustrate several\nexamples where these benchmark methods suffer relatively severe performance\ndegradation, while our approach improves performance in almost all task-model\ncombinations, only incurring minimal degradation when it does not.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt engineering has proven to be a crucial step in leveraging pretrained\nlarge language models (LLMs) in solving various real-world tasks. Numerous\nsolutions have been proposed that seek to automate prompt engineering by using\nthe model itself to edit prompts. However, the majority of state-of-the-art\napproaches are evaluated on tasks that require minimal prompt templates and on\nvery large and highly capable LLMs. In contrast, solving complex tasks that\nrequire detailed information to be included in the prompt increases the amount\nof text that needs to be optimised. Furthermore, smaller models have been shown\nto be more sensitive to prompt design. To address these challenges, we propose\nan evolutionary search approach to automated discrete prompt optimisation\nconsisting of two phases. In the first phase, grammar-guided genetic\nprogramming is invoked to synthesise prompt-creating programmes by searching\nthe space of programmes populated by function compositions of syntactic,\ndictionary-based and LLM-based prompt-editing functions. In the second phase,\nlocal search is applied to explore the neighbourhoods of best-performing\nprogrammes in an attempt to further fine-tune their performance. Our approach\noutperforms three state-of-the-art prompt optimisation approaches,\nPromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose\nLLMs in four domain-specific challenging tasks. We also illustrate several\nexamples where these benchmark methods suffer relatively severe performance\ndegradation, while our approach improves performance in almost all task-model\ncombinations, only incurring minimal degradation when it does not."
                },
                "authors": [
                    {
                        "name": "Muzhaffar Hazman"
                    },
                    {
                        "name": "Minh-Khoi Pham"
                    },
                    {
                        "name": "Shweta Soundararajan"
                    },
                    {
                        "name": "Goncalo Mordido"
                    },
                    {
                        "name": "Leonardo Custode"
                    },
                    {
                        "name": "David Lynch"
                    },
                    {
                        "name": "Giorgio Cruciata"
                    },
                    {
                        "name": "Yucheng Shi"
                    },
                    {
                        "name": "Hongmeng Song"
                    },
                    {
                        "name": "Wang Chao"
                    },
                    {
                        "name": "Pan Yue"
                    },
                    {
                        "name": "Aleksandar Milenovic"
                    },
                    {
                        "name": "Alexandros Agapitos"
                    }
                ],
                "author_detail": {
                    "name": "Alexandros Agapitos"
                },
                "author": "Alexandros Agapitos",
                "arxiv_comment": "Accepted for Publication at ECAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10325v1",
                "updated": "2025-07-14T14:32:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    14,
                    32,
                    46,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T14:32:46Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    14,
                    32,
                    46,
                    0,
                    195,
                    0
                ],
                "title": "Convergence of Agnostic Federated Averaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convergence of Agnostic Federated Averaging"
                },
                "summary": "Federated learning (FL) enables decentralized model training without\ncentralizing raw data. However, practical FL deployments often face a key\nrealistic challenge: Clients participate intermittently in server aggregation\nand with unknown, possibly biased participation probabilities. Most existing\nconvergence results either assume full-device participation, or rely on\nknowledge of (in fact uniform) client availability distributions -- assumptions\nthat rarely hold in practice. In this work, we characterize the optimization\nproblem that consistently adheres to the stochastic dynamics of the well-known\n\\emph{agnostic Federated Averaging (FedAvg)} algorithm under random (and\nvariably-sized) client availability, and rigorously establish its convergence\nfor convex, possibly nonsmooth losses, achieving a standard rate of order\n$\\mathcal{O}(1/\\sqrt{T})$, where $T$ denotes the aggregation horizon. Our\nanalysis provides the first convergence guarantees for agnostic FedAvg under\ngeneral, non-uniform, stochastic client participation, without knowledge of the\nparticipation distribution. We also empirically demonstrate that agnostic\nFedAvg in fact outperforms common (and suboptimal) weighted aggregation FedAvg\nvariants, even with server-side knowledge of participation weights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) enables decentralized model training without\ncentralizing raw data. However, practical FL deployments often face a key\nrealistic challenge: Clients participate intermittently in server aggregation\nand with unknown, possibly biased participation probabilities. Most existing\nconvergence results either assume full-device participation, or rely on\nknowledge of (in fact uniform) client availability distributions -- assumptions\nthat rarely hold in practice. In this work, we characterize the optimization\nproblem that consistently adheres to the stochastic dynamics of the well-known\n\\emph{agnostic Federated Averaging (FedAvg)} algorithm under random (and\nvariably-sized) client availability, and rigorously establish its convergence\nfor convex, possibly nonsmooth losses, achieving a standard rate of order\n$\\mathcal{O}(1/\\sqrt{T})$, where $T$ denotes the aggregation horizon. Our\nanalysis provides the first convergence guarantees for agnostic FedAvg under\ngeneral, non-uniform, stochastic client participation, without knowledge of the\nparticipation distribution. We also empirically demonstrate that agnostic\nFedAvg in fact outperforms common (and suboptimal) weighted aggregation FedAvg\nvariants, even with server-side knowledge of participation weights."
                },
                "authors": [
                    {
                        "name": "Herlock"
                    },
                    {
                        "name": "Rahimi"
                    },
                    {
                        "name": "Dionysis Kalogerias"
                    }
                ],
                "author_detail": {
                    "name": "Dionysis Kalogerias"
                },
                "arxiv_affiliation": "SeyedAbolfazl",
                "author": "Dionysis Kalogerias",
                "arxiv_comment": "5 pages, 2 figurres, CAMSAP conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12864v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12864v3",
                "updated": "2025-07-14T14:30:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    14,
                    30,
                    57,
                    0,
                    195,
                    0
                ],
                "published": "2025-05-19T08:48:12Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    8,
                    48,
                    12,
                    0,
                    139,
                    0
                ],
                "title": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams"
                },
                "summary": "Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a\nnovel benchmark derived from 340 law exams spanning 116 law school courses\nacross a range of subjects and degree levels. The dataset comprises 4,886 law\nexam questions in English and German, including 2,841 long-form, open-ended\nquestions and 2,045 multiple-choice questions. Besides reference answers, the\nopen questions are also accompanied by explicit guidance outlining the expected\nlegal reasoning approach such as issue spotting, rule recall, or rule\napplication. Our evaluation on both open-ended and multiple-choice questions\npresent significant challenges for current LLMs; in particular, they notably\nstruggle with open questions that require structured, multi-step legal\nreasoning. Moreover, our results underscore the effectiveness of the dataset in\ndifferentiating between models with varying capabilities. Adopting an\nLLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate\nhow model-generated reasoning steps can be evaluated consistently and\naccurately. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. Project page:\nhttps://lexam-benchmark.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a\nnovel benchmark derived from 340 law exams spanning 116 law school courses\nacross a range of subjects and degree levels. The dataset comprises 4,886 law\nexam questions in English and German, including 2,841 long-form, open-ended\nquestions and 2,045 multiple-choice questions. Besides reference answers, the\nopen questions are also accompanied by explicit guidance outlining the expected\nlegal reasoning approach such as issue spotting, rule recall, or rule\napplication. Our evaluation on both open-ended and multiple-choice questions\npresent significant challenges for current LLMs; in particular, they notably\nstruggle with open questions that require structured, multi-step legal\nreasoning. Moreover, our results underscore the effectiveness of the dataset in\ndifferentiating between models with varying capabilities. Adopting an\nLLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate\nhow model-generated reasoning steps can be evaluated consistently and\naccurately. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. Project page:\nhttps://lexam-benchmark.github.io/"
                },
                "authors": [
                    {
                        "name": "Yu Fan"
                    },
                    {
                        "name": "Jingwei Ni"
                    },
                    {
                        "name": "Jakob Merane"
                    },
                    {
                        "name": "Etienne Salimbeni"
                    },
                    {
                        "name": "Yang Tian"
                    },
                    {
                        "name": "Yoan Hermstr√ºwer"
                    },
                    {
                        "name": "Yinya Huang"
                    },
                    {
                        "name": "Mubashara Akhtar"
                    },
                    {
                        "name": "Florian Geering"
                    },
                    {
                        "name": "Oliver Dreyer"
                    },
                    {
                        "name": "Daniel Brunner"
                    },
                    {
                        "name": "Markus Leippold"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Alexander Stremitzer"
                    },
                    {
                        "name": "Christoph Engel"
                    },
                    {
                        "name": "Elliott Ash"
                    },
                    {
                        "name": "Joel Niklaus"
                    }
                ],
                "author_detail": {
                    "name": "Joel Niklaus"
                },
                "author": "Joel Niklaus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12864v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12864v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10302v1",
                "updated": "2025-07-14T14:05:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    14,
                    5,
                    19,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T14:05:19Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    14,
                    5,
                    19,
                    0,
                    195,
                    0
                ],
                "title": "DisCo: Towards Distinct and Coherent Visual Encapsulation in Video MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DisCo: Towards Distinct and Coherent Visual Encapsulation in Video MLLMs"
                },
                "summary": "In video Multimodal Large Language Models (video MLLMs), the visual\nencapsulation process plays a pivotal role in converting video contents into\nrepresentative tokens for LLM input. While linear projectors are widely\nemployed for encapsulation, they introduce semantic indistinctness and temporal\nincoherence when applied to videos. Conversely, the structure of resamplers\nshows promise in tackling these challenges, but an effective solution remains\nunexplored. Drawing inspiration from resampler structures, we introduce DisCo,\na novel visual encapsulation method designed to yield semantically distinct and\ntemporally coherent visual tokens for video MLLMs. DisCo integrates two key\ncomponents: (1) A Visual Concept Discriminator (VCD) module, assigning unique\nsemantics for visual tokens by associating them in pair with discriminative\nconcepts in the video. (2) A Temporal Focus Calibrator (TFC) module, ensuring\nconsistent temporal focus of visual tokens to video elements across every video\nframe. Through extensive experiments on multiple video MLLM frameworks, we\ndemonstrate that DisCo remarkably outperforms previous state-of-the-art methods\nacross a variety of video understanding benchmarks, while also achieving higher\ntoken efficiency thanks to the reduction of semantic indistinctness. The code:\nhttps://github.com/ZJHTerry18/DisCo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In video Multimodal Large Language Models (video MLLMs), the visual\nencapsulation process plays a pivotal role in converting video contents into\nrepresentative tokens for LLM input. While linear projectors are widely\nemployed for encapsulation, they introduce semantic indistinctness and temporal\nincoherence when applied to videos. Conversely, the structure of resamplers\nshows promise in tackling these challenges, but an effective solution remains\nunexplored. Drawing inspiration from resampler structures, we introduce DisCo,\na novel visual encapsulation method designed to yield semantically distinct and\ntemporally coherent visual tokens for video MLLMs. DisCo integrates two key\ncomponents: (1) A Visual Concept Discriminator (VCD) module, assigning unique\nsemantics for visual tokens by associating them in pair with discriminative\nconcepts in the video. (2) A Temporal Focus Calibrator (TFC) module, ensuring\nconsistent temporal focus of visual tokens to video elements across every video\nframe. Through extensive experiments on multiple video MLLM frameworks, we\ndemonstrate that DisCo remarkably outperforms previous state-of-the-art methods\nacross a variety of video understanding benchmarks, while also achieving higher\ntoken efficiency thanks to the reduction of semantic indistinctness. The code:\nhttps://github.com/ZJHTerry18/DisCo."
                },
                "authors": [
                    {
                        "name": "Jiahe Zhao"
                    },
                    {
                        "name": "Rongkun Zheng"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Helin Wang"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hengshuang Zhao"
                },
                "author": "Hengshuang Zhao",
                "arxiv_comment": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10284v1",
                "updated": "2025-07-14T13:51:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    13,
                    51,
                    28,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T13:51:28Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    13,
                    51,
                    28,
                    0,
                    195,
                    0
                ],
                "title": "Prompt Informed Reinforcement Learning for Visual Coverage Path Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Informed Reinforcement Learning for Visual Coverage Path Planning"
                },
                "summary": "Visual coverage path planning with unmanned aerial vehicles (UAVs) requires\nagents to strategically coordinate UAV motion and camera control to maximize\ncoverage, minimize redundancy, and maintain battery efficiency. Traditional\nreinforcement learning (RL) methods rely on environment-specific reward\nformulations that lack semantic adaptability. This study proposes\nPrompt-Informed Reinforcement Learning (PIRL), a novel approach that integrates\nthe zero-shot reasoning ability and in-context learning capability of large\nlanguage models with curiosity-driven RL. PIRL leverages semantic feedback from\nan LLM, GPT-3.5, to dynamically shape the reward function of the Proximal\nPolicy Optimization (PPO) RL policy guiding the agent in position and camera\nadjustments for optimal visual coverage. The PIRL agent is trained using OpenAI\nGym and evaluated in various environments. Furthermore, the sim-to-real-like\nability and zero-shot generalization of the agent are tested by operating the\nagent in Webots simulator which introduces realistic physical dynamics. Results\nshow that PIRL outperforms multiple learning-based baselines such as PPO with\nstatic rewards, PPO with exploratory weight initialization, imitation learning,\nand an LLM-only controller. Across different environments, PIRL outperforms the\nbest-performing baseline by achieving up to 14% higher visual coverage in\nOpenAI Gym and 27% higher in Webots, up to 25% higher battery efficiency, and\nup to 18\\% lower redundancy, depending on the environment. The results\nhighlight the effectiveness of LLM-guided reward shaping in complex spatial\nexploration tasks and suggest a promising direction for integrating natural\nlanguage priors into RL for robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual coverage path planning with unmanned aerial vehicles (UAVs) requires\nagents to strategically coordinate UAV motion and camera control to maximize\ncoverage, minimize redundancy, and maintain battery efficiency. Traditional\nreinforcement learning (RL) methods rely on environment-specific reward\nformulations that lack semantic adaptability. This study proposes\nPrompt-Informed Reinforcement Learning (PIRL), a novel approach that integrates\nthe zero-shot reasoning ability and in-context learning capability of large\nlanguage models with curiosity-driven RL. PIRL leverages semantic feedback from\nan LLM, GPT-3.5, to dynamically shape the reward function of the Proximal\nPolicy Optimization (PPO) RL policy guiding the agent in position and camera\nadjustments for optimal visual coverage. The PIRL agent is trained using OpenAI\nGym and evaluated in various environments. Furthermore, the sim-to-real-like\nability and zero-shot generalization of the agent are tested by operating the\nagent in Webots simulator which introduces realistic physical dynamics. Results\nshow that PIRL outperforms multiple learning-based baselines such as PPO with\nstatic rewards, PPO with exploratory weight initialization, imitation learning,\nand an LLM-only controller. Across different environments, PIRL outperforms the\nbest-performing baseline by achieving up to 14% higher visual coverage in\nOpenAI Gym and 27% higher in Webots, up to 25% higher battery efficiency, and\nup to 18\\% lower redundancy, depending on the environment. The results\nhighlight the effectiveness of LLM-guided reward shaping in complex spatial\nexploration tasks and suggest a promising direction for integrating natural\nlanguage priors into RL for robotics."
                },
                "authors": [
                    {
                        "name": "Venkat Margapuri"
                    }
                ],
                "author_detail": {
                    "name": "Venkat Margapuri"
                },
                "author": "Venkat Margapuri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10281v1",
                "updated": "2025-07-14T13:48:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    13,
                    48,
                    13,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T13:48:13Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    13,
                    48,
                    13,
                    0,
                    195,
                    0
                ],
                "title": "Toward Real-World Table Agents: Capabilities, Workflows, and Design\n  Principles for LLM-based Table Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Real-World Table Agents: Capabilities, Workflows, and Design\n  Principles for LLM-based Table Intelligence"
                },
                "summary": "Tables are fundamental in domains such as finance, healthcare, and public\nadministration, yet real-world table tasks often involve noise, structural\nheterogeneity, and semantic complexity--issues underexplored in existing\nresearch that primarily targets clean academic datasets. This survey focuses on\nLLM-based Table Agents, which aim to automate table-centric workflows by\nintegrating preprocessing, reasoning, and domain adaptation. We define five\ncore competencies--C1: Table Structure Understanding, C2: Table and Query\nSemantic Understanding, C3: Table Retrieval and Compression, C4: Executable\nReasoning with Traceability, and C5: Cross-Domain Generalization--to analyze\nand compare current approaches. In addition, a detailed examination of the\nText-to-SQL Agent reveals a performance gap between academic benchmarks and\nreal-world scenarios, especially for open-source models. Finally, we provide\nactionable insights to improve the robustness, generalization, and efficiency\nof LLM-based Table Agents in practical settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tables are fundamental in domains such as finance, healthcare, and public\nadministration, yet real-world table tasks often involve noise, structural\nheterogeneity, and semantic complexity--issues underexplored in existing\nresearch that primarily targets clean academic datasets. This survey focuses on\nLLM-based Table Agents, which aim to automate table-centric workflows by\nintegrating preprocessing, reasoning, and domain adaptation. We define five\ncore competencies--C1: Table Structure Understanding, C2: Table and Query\nSemantic Understanding, C3: Table Retrieval and Compression, C4: Executable\nReasoning with Traceability, and C5: Cross-Domain Generalization--to analyze\nand compare current approaches. In addition, a detailed examination of the\nText-to-SQL Agent reveals a performance gap between academic benchmarks and\nreal-world scenarios, especially for open-source models. Finally, we provide\nactionable insights to improve the robustness, generalization, and efficiency\nof LLM-based Table Agents in practical settings."
                },
                "authors": [
                    {
                        "name": "Jiaming Tian"
                    },
                    {
                        "name": "Liyao Li"
                    },
                    {
                        "name": "Wentao Ye"
                    },
                    {
                        "name": "Haobo Wang"
                    },
                    {
                        "name": "Lingxin Wang"
                    },
                    {
                        "name": "Lihua Yu"
                    },
                    {
                        "name": "Zujie Ren"
                    },
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Junbo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Junbo Zhao"
                },
                "author": "Junbo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10259v1",
                "updated": "2025-07-14T13:33:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    13,
                    33,
                    30,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T13:33:30Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    13,
                    33,
                    30,
                    0,
                    195,
                    0
                ],
                "title": "Cross-Timeslot Optimization for Distributed GPU Inference Using\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Timeslot Optimization for Distributed GPU Inference Using\n  Reinforcement Learning"
                },
                "summary": "The rapid growth of large language model (LLM) services imposes increasing\ndemands on distributed GPU inference infrastructure. Most existing scheduling\nsystems rely on the current system state to make decisions, without considering\nhow task demand and resource availability evolve over time. This lack of\ntemporal awareness leads to inefficient GPU utilization, high task migration\noverhead, and poor system responsiveness under dynamic workloads. In this work,\nwe identify the fundamental limitations of these instantaneous-state-only\nscheduling approaches and propose Temporal Optimal Resource scheduling via\nTwo-layer Architecture (TORTA). TORTA introduces a spatiotemporal scheduling\nframework that captures both long-term workload patterns and short-term\nexecution constraints. It adopts a two-layer design: a macro-level scheduler\nleverages reinforcement learning and optimal transport to coordinate\ninter-region task distribution, while a micro-level allocator refines\ntask-to-server assignments within each region to reduce latency and switching\ncosts. Experimental results across multiple network topologies show that TORTA\nreduces average inference response time by up to 15\\%, improves load balance by\napproximately 4-5\\%, and cuts total operational cost by 10-20\\% compared to\nstate-of-the-art baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of large language model (LLM) services imposes increasing\ndemands on distributed GPU inference infrastructure. Most existing scheduling\nsystems rely on the current system state to make decisions, without considering\nhow task demand and resource availability evolve over time. This lack of\ntemporal awareness leads to inefficient GPU utilization, high task migration\noverhead, and poor system responsiveness under dynamic workloads. In this work,\nwe identify the fundamental limitations of these instantaneous-state-only\nscheduling approaches and propose Temporal Optimal Resource scheduling via\nTwo-layer Architecture (TORTA). TORTA introduces a spatiotemporal scheduling\nframework that captures both long-term workload patterns and short-term\nexecution constraints. It adopts a two-layer design: a macro-level scheduler\nleverages reinforcement learning and optimal transport to coordinate\ninter-region task distribution, while a micro-level allocator refines\ntask-to-server assignments within each region to reduce latency and switching\ncosts. Experimental results across multiple network topologies show that TORTA\nreduces average inference response time by up to 15\\%, improves load balance by\napproximately 4-5\\%, and cuts total operational cost by 10-20\\% compared to\nstate-of-the-art baseline methods."
                },
                "authors": [
                    {
                        "name": "Chengze Du"
                    },
                    {
                        "name": "Zhiwei Yu"
                    },
                    {
                        "name": "Heng Xu"
                    },
                    {
                        "name": "Haojie Wang"
                    },
                    {
                        "name": "Bo liu"
                    },
                    {
                        "name": "Jialong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jialong Li"
                },
                "author": "Jialong Li",
                "arxiv_comment": "17 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10250v1",
                "updated": "2025-07-14T13:17:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    13,
                    17,
                    46,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T13:17:46Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    13,
                    17,
                    46,
                    0,
                    195,
                    0
                ],
                "title": "DepViT-CAD: Deployable Vision Transformer-Based Cancer Diagnosis in\n  Histopathology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DepViT-CAD: Deployable Vision Transformer-Based Cancer Diagnosis in\n  Histopathology"
                },
                "summary": "Accurate and timely cancer diagnosis from histopathological slides is vital\nfor effective clinical decision-making. This paper introduces DepViT-CAD, a\ndeployable AI system for multi-class cancer diagnosis in histopathology. At its\ncore is MAViT, a novel Multi-Attention Vision Transformer designed to capture\nfine-grained morphological patterns across diverse tumor types. MAViT was\ntrained on expert-annotated patches from 1008 whole-slide images, covering 11\ndiagnostic categories, including 10 major cancers and non-tumor tissue.\nDepViT-CAD was validated on two independent cohorts: 275 WSIs from The Cancer\nGenome Atlas and 50 routine clinical cases from pathology labs, achieving\ndiagnostic sensitivities of 94.11% and 92%, respectively. By combining\nstate-of-the-art transformer architecture with large-scale real-world\nvalidation, DepViT-CAD offers a robust and scalable approach for AI-assisted\ncancer diagnostics. To support transparency and reproducibility, software and\ncode will be made publicly available at GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and timely cancer diagnosis from histopathological slides is vital\nfor effective clinical decision-making. This paper introduces DepViT-CAD, a\ndeployable AI system for multi-class cancer diagnosis in histopathology. At its\ncore is MAViT, a novel Multi-Attention Vision Transformer designed to capture\nfine-grained morphological patterns across diverse tumor types. MAViT was\ntrained on expert-annotated patches from 1008 whole-slide images, covering 11\ndiagnostic categories, including 10 major cancers and non-tumor tissue.\nDepViT-CAD was validated on two independent cohorts: 275 WSIs from The Cancer\nGenome Atlas and 50 routine clinical cases from pathology labs, achieving\ndiagnostic sensitivities of 94.11% and 92%, respectively. By combining\nstate-of-the-art transformer architecture with large-scale real-world\nvalidation, DepViT-CAD offers a robust and scalable approach for AI-assisted\ncancer diagnostics. To support transparency and reproducibility, software and\ncode will be made publicly available at GitHub."
                },
                "authors": [
                    {
                        "name": "Ashkan Shakarami"
                    },
                    {
                        "name": "Lorenzo Nicole"
                    },
                    {
                        "name": "Rocco Cappellesso"
                    },
                    {
                        "name": "Angelo Paolo Dei Tos"
                    },
                    {
                        "name": "Stefano Ghidoni"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Ghidoni"
                },
                "author": "Stefano Ghidoni",
                "arxiv_comment": "25 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10216v1",
                "updated": "2025-07-14T12:33:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    12,
                    33,
                    7,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T12:33:07Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    12,
                    33,
                    7,
                    0,
                    195,
                    0
                ],
                "title": "Absher: A Benchmark for Evaluating Large Language Models Understanding\n  of Saudi Dialects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Absher: A Benchmark for Evaluating Large Language Models Understanding\n  of Saudi Dialects"
                },
                "summary": "As large language models (LLMs) become increasingly central to Arabic NLP\napplications, evaluating their understanding of regional dialects and cultural\nnuances is essential, particularly in linguistically diverse settings like\nSaudi Arabia. This paper introduces \\texttt{Absher}, a comprehensive benchmark\nspecifically designed to assess LLMs performance across major Saudi dialects.\n\\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six\ndistinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage,\nCultural Interpretation, and Location Recognition. These questions are derived\nfrom a curated dataset of dialectal words, phrases, and proverbs sourced from\nvarious regions of Saudi Arabia. We evaluate several state-of-the-art LLMs,\nincluding multilingual and Arabic-specific models. We also provide detailed\ninsights into their capabilities and limitations. Our results reveal notable\nperformance gaps, particularly in tasks requiring cultural inference or\ncontextual understanding. Our findings highlight the urgent need for\ndialect-aware training and culturally aligned evaluation methodologies to\nimprove LLMs performance in real-world Arabic applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become increasingly central to Arabic NLP\napplications, evaluating their understanding of regional dialects and cultural\nnuances is essential, particularly in linguistically diverse settings like\nSaudi Arabia. This paper introduces \\texttt{Absher}, a comprehensive benchmark\nspecifically designed to assess LLMs performance across major Saudi dialects.\n\\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six\ndistinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage,\nCultural Interpretation, and Location Recognition. These questions are derived\nfrom a curated dataset of dialectal words, phrases, and proverbs sourced from\nvarious regions of Saudi Arabia. We evaluate several state-of-the-art LLMs,\nincluding multilingual and Arabic-specific models. We also provide detailed\ninsights into their capabilities and limitations. Our results reveal notable\nperformance gaps, particularly in tasks requiring cultural inference or\ncontextual understanding. Our findings highlight the urgent need for\ndialect-aware training and culturally aligned evaluation methodologies to\nimprove LLMs performance in real-world Arabic applications."
                },
                "authors": [
                    {
                        "name": "Renad Al-Monef"
                    },
                    {
                        "name": "Hassan Alhuzali"
                    },
                    {
                        "name": "Nora Alturayeif"
                    },
                    {
                        "name": "Ashwag Alasmari"
                    }
                ],
                "author_detail": {
                    "name": "Ashwag Alasmari"
                },
                "author": "Ashwag Alasmari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10210v1",
                "updated": "2025-07-14T12:28:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    12,
                    28,
                    1,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T12:28:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    12,
                    28,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "Fine-Grained Coordinated OFDMA With Fiber Backhaul Enabled by openwifi\n  and White Rabbit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained Coordinated OFDMA With Fiber Backhaul Enabled by openwifi\n  and White Rabbit"
                },
                "summary": "Proper coordination is needed to guarantee the performance of wireless\nnetworks in dense deployments. Contention-based systems suffer badly in terms\nof latency when multiple devices compete for the same resources. Coordinated\nOrthogonal Frequency Division Multiple Access (Co-OFDMA) is proposed for Wi-Fi\n8 to remedy this, as it enables multiple Access Points (APs) to share spectrum\nmore efficiently. However, fine-grained resource allocation, namely within\n20MHz bandwidth, is argued to be impractical due to the over-the-air scheduling\noverhead and complexity in terms of physical layer signaling. A wired backhaul\nmitigates the need for over-the-air scheduling and synchronization, and it\nallows for coordination even if APs are not in each others' range. Furthermore,\nit forms the basis for more advanced multi-AP coordination schemes like\ncoordinated beamforming and joint transmission. In this work we demonstrate the\nrealization of Wi-Fi 6 compliant fine-grained Co-OFDMA using a fiber backhaul,\nenabled by the open-source platforms openwifi and White Rabbit. We show that\nthe performance in terms of carrier frequency offset pre-compensation and time\nsynchronization between two APs exceeds related wireless standard requirements.\nFurthermore, the quality of the received constellation of the Co-OFDMA frame as\nreported by a wireless connectivity tester is better than individual frames\nsent by the APs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proper coordination is needed to guarantee the performance of wireless\nnetworks in dense deployments. Contention-based systems suffer badly in terms\nof latency when multiple devices compete for the same resources. Coordinated\nOrthogonal Frequency Division Multiple Access (Co-OFDMA) is proposed for Wi-Fi\n8 to remedy this, as it enables multiple Access Points (APs) to share spectrum\nmore efficiently. However, fine-grained resource allocation, namely within\n20MHz bandwidth, is argued to be impractical due to the over-the-air scheduling\noverhead and complexity in terms of physical layer signaling. A wired backhaul\nmitigates the need for over-the-air scheduling and synchronization, and it\nallows for coordination even if APs are not in each others' range. Furthermore,\nit forms the basis for more advanced multi-AP coordination schemes like\ncoordinated beamforming and joint transmission. In this work we demonstrate the\nrealization of Wi-Fi 6 compliant fine-grained Co-OFDMA using a fiber backhaul,\nenabled by the open-source platforms openwifi and White Rabbit. We show that\nthe performance in terms of carrier frequency offset pre-compensation and time\nsynchronization between two APs exceeds related wireless standard requirements.\nFurthermore, the quality of the received constellation of the Co-OFDMA frame as\nreported by a wireless connectivity tester is better than individual frames\nsent by the APs."
                },
                "authors": [
                    {
                        "name": "Thijs Havinga"
                    },
                    {
                        "name": "Xianjun Jiao"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Baiheng Chen"
                    },
                    {
                        "name": "Robbe Gaeremynck"
                    },
                    {
                        "name": "Ingrid Moerman"
                    }
                ],
                "author_detail": {
                    "name": "Ingrid Moerman"
                },
                "author": "Ingrid Moerman",
                "arxiv_comment": "6 pages, 7 figures. Submitted to GLOBECOM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10200v1",
                "updated": "2025-07-14T12:13:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    12,
                    13,
                    50,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T12:13:50Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    12,
                    13,
                    50,
                    0,
                    195,
                    0
                ],
                "title": "Natural Language-based Assessment of L2 Oral Proficiency using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language-based Assessment of L2 Oral Proficiency using LLMs"
                },
                "summary": "Natural language-based assessment (NLA) is an approach to second language\nassessment that uses instructions - expressed in the form of can-do descriptors\n- originally intended for human examiners, aiming to determine whether large\nlanguage models (LLMs) can interpret and apply them in ways comparable to human\nassessment. In this work, we explore the use of such descriptors with an\nopen-source LLM, Qwen 2.5 72B, to assess responses from the publicly available\nS&I Corpus in a zero-shot setting. Our results show that this approach -\nrelying solely on textual information - achieves competitive performance: while\nit does not outperform state-of-the-art speech LLMs fine-tuned for the task, it\nsurpasses a BERT-based model trained specifically for this purpose. NLA proves\nparticularly effective in mismatched task settings, is generalisable to other\ndata types and languages, and offers greater interpretability, as it is\ngrounded in clearly explainable, widely applicable language descriptors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language-based assessment (NLA) is an approach to second language\nassessment that uses instructions - expressed in the form of can-do descriptors\n- originally intended for human examiners, aiming to determine whether large\nlanguage models (LLMs) can interpret and apply them in ways comparable to human\nassessment. In this work, we explore the use of such descriptors with an\nopen-source LLM, Qwen 2.5 72B, to assess responses from the publicly available\nS&I Corpus in a zero-shot setting. Our results show that this approach -\nrelying solely on textual information - achieves competitive performance: while\nit does not outperform state-of-the-art speech LLMs fine-tuned for the task, it\nsurpasses a BERT-based model trained specifically for this purpose. NLA proves\nparticularly effective in mismatched task settings, is generalisable to other\ndata types and languages, and offers greater interpretability, as it is\ngrounded in clearly explainable, widely applicable language descriptors."
                },
                "authors": [
                    {
                        "name": "Stefano Bann√≤"
                    },
                    {
                        "name": "Rao Ma"
                    },
                    {
                        "name": "Mengjie Qian"
                    },
                    {
                        "name": "Siyuan Tang"
                    },
                    {
                        "name": "Kate Knill"
                    },
                    {
                        "name": "Mark Gales"
                    }
                ],
                "author_detail": {
                    "name": "Mark Gales"
                },
                "author": "Mark Gales",
                "arxiv_comment": "Accepted for the 10th Workshop on Speech and Language Technology in\n  Education (SLaTE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08017v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08017v2",
                "updated": "2025-07-14T11:46:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    46,
                    41,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-07T20:26:31Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    20,
                    26,
                    31,
                    0,
                    188,
                    0
                ],
                "title": "Mechanistic Indicators of Understanding in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mechanistic Indicators of Understanding in Large Language Models"
                },
                "summary": "Recent findings in mechanistic interpretability (MI), the field probing the\ninner workings of Large Language Models (LLMs), challenge the view that these\nmodels rely solely on superficial statistics. We offer an accessible synthesis\nof these findings that doubles as an introduction to MI while integrating these\nfindings within a novel theoretical framework for thinking about machine\nunderstanding. We argue that LLMs develop internal structures that are\nfunctionally analogous to the kind of understanding that consists in seeing\nconnections. To sharpen this idea, we propose a three-tiered conception of\nunderstanding. First, conceptual understanding emerges when a model forms\n\"features\" as directions in latent space, learning the connections between\ndiverse manifestations of something. Second, state-of-the-world understanding\nemerges when a model learns contingent factual connections between features and\ndynamically tracks changes in the world. Third, principled understanding\nemerges when a model ceases to rely on a collection of memorized facts and\ndiscovers a \"circuit\" connecting these facts. However, these forms of\nunderstanding remain radically different from human understanding, as the\nphenomenon of \"parallel mechanisms\" shows. We conclude that the debate should\nmove beyond the yes-or-no question of whether LLMs understand to investigate\nhow their strange minds work and forge conceptions that fit them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent findings in mechanistic interpretability (MI), the field probing the\ninner workings of Large Language Models (LLMs), challenge the view that these\nmodels rely solely on superficial statistics. We offer an accessible synthesis\nof these findings that doubles as an introduction to MI while integrating these\nfindings within a novel theoretical framework for thinking about machine\nunderstanding. We argue that LLMs develop internal structures that are\nfunctionally analogous to the kind of understanding that consists in seeing\nconnections. To sharpen this idea, we propose a three-tiered conception of\nunderstanding. First, conceptual understanding emerges when a model forms\n\"features\" as directions in latent space, learning the connections between\ndiverse manifestations of something. Second, state-of-the-world understanding\nemerges when a model learns contingent factual connections between features and\ndynamically tracks changes in the world. Third, principled understanding\nemerges when a model ceases to rely on a collection of memorized facts and\ndiscovers a \"circuit\" connecting these facts. However, these forms of\nunderstanding remain radically different from human understanding, as the\nphenomenon of \"parallel mechanisms\" shows. We conclude that the debate should\nmove beyond the yes-or-no question of whether LLMs understand to investigate\nhow their strange minds work and forge conceptions that fit them."
                },
                "authors": [
                    {
                        "name": "Pierre Beckmann"
                    },
                    {
                        "name": "Matthieu Queloz"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Queloz"
                },
                "author": "Matthieu Queloz",
                "arxiv_comment": "32 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08017v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08017v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10182v1",
                "updated": "2025-07-14T11:44:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    44,
                    4,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T11:44:04Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    44,
                    4,
                    0,
                    195,
                    0
                ],
                "title": "Breaking the Myth: Can Small Models Infer Postconditions Too?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Myth: Can Small Models Infer Postconditions Too?"
                },
                "summary": "Formal specifications are essential for ensuring software correctness, yet\nmanually writing them is tedious and error-prone. Large Language Models (LLMs)\nhave shown promise in generating such specifications from natural language\nintents, but the giant model size and high computational demands raise a\nfundamental question: Do we really need large models for this task? In this\npaper, we show that a small, fine-tuned language model can achieve high-quality\npostcondition generation with much lower computational costs. We construct a\nspecialized dataset of prompts, reasoning logs, and postconditions, then\nsupervise the fine-tuning of a $7$B-parameter code model. Our approach tackles\nreal-world repository dependencies and preserves pre-state information,\nallowing for expressive and accurate specifications. We evaluate the model on a\nbenchmark of real-world Java bugs (Defects4J) and compare against both\nproprietary giants (e.g., GPT-4o) and open-source large models. Empirical\nresults demonstrate that our compact model matches or outperforms significantly\nlarger counterparts in syntax correctness, semantic correctness, and\nbug-distinguishing capability. These findings highlight that targeted\nfine-tuning on a modest dataset can enable small models to achieve results\nformerly seen only in massive, resource-heavy LLMs, offering a practical and\nefficient path for the real-world adoption of automated specification\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal specifications are essential for ensuring software correctness, yet\nmanually writing them is tedious and error-prone. Large Language Models (LLMs)\nhave shown promise in generating such specifications from natural language\nintents, but the giant model size and high computational demands raise a\nfundamental question: Do we really need large models for this task? In this\npaper, we show that a small, fine-tuned language model can achieve high-quality\npostcondition generation with much lower computational costs. We construct a\nspecialized dataset of prompts, reasoning logs, and postconditions, then\nsupervise the fine-tuning of a $7$B-parameter code model. Our approach tackles\nreal-world repository dependencies and preserves pre-state information,\nallowing for expressive and accurate specifications. We evaluate the model on a\nbenchmark of real-world Java bugs (Defects4J) and compare against both\nproprietary giants (e.g., GPT-4o) and open-source large models. Empirical\nresults demonstrate that our compact model matches or outperforms significantly\nlarger counterparts in syntax correctness, semantic correctness, and\nbug-distinguishing capability. These findings highlight that targeted\nfine-tuning on a modest dataset can enable small models to achieve results\nformerly seen only in massive, resource-heavy LLMs, offering a practical and\nefficient path for the real-world adoption of automated specification\ngeneration."
                },
                "authors": [
                    {
                        "name": "Gehao Zhang"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Juan Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Juan Zhai"
                },
                "author": "Juan Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10178v1",
                "updated": "2025-07-14T11:40:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    40,
                    17,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T11:40:17Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    40,
                    17,
                    0,
                    195,
                    0
                ],
                "title": "Pimba: A Processing-in-Memory Acceleration for Post-Transformer Large\n  Language Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pimba: A Processing-in-Memory Acceleration for Post-Transformer Large\n  Language Model Serving"
                },
                "summary": "Transformers are the driving force behind today's Large Language Models\n(LLMs), serving as the foundation for their performance and versatility. Yet,\ntheir compute and memory costs grow with sequence length, posing scalability\nchallenges for long-context inferencing. In response, the algorithm community\nis exploring alternative architectures, such as state space models (SSMs),\nlinear attention, and recurrent neural networks (RNNs), which we refer to as\npost-transformers. This shift presents a key challenge: building a serving\nsystem that efficiently supports both transformer and post-transformer LLMs\nwithin a unified framework. To address this challenge, we analyze the\nperformance characteristics of transformer and post-transformer LLMs. Despite\ntheir algorithmic differences, both are fundamentally limited by memory\nbandwidth under batched inference due to attention in transformers and state\nupdates in post-transformers. Further analyses suggest two additional insights:\n(1) state update operations, unlike attention, incur high hardware cost, making\nper-bank PIM acceleration inefficient, and (2) different low-precision\narithmetic methods offer varying accuracy-area tradeoffs, while we identify\nMicrosoft's MX as the Pareto-optimal choice. Building on these insights, we\ndesign Pimba as an array of State-update Processing Units (SPUs), each shared\nbetween two banks to enable interleaved access to PIM. Each SPU includes a\nState-update Processing Engine (SPE) that comprises element-wise multipliers\nand adders using MX-based quantized arithmetic, enabling efficient execution of\nstate update and attention operations. Our evaluation shows that, compared to\nLLM-optimized GPU and GPU+PIM systems, Pimba achieves up to 3.2x and 2.1x\nhigher token generation throughput, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers are the driving force behind today's Large Language Models\n(LLMs), serving as the foundation for their performance and versatility. Yet,\ntheir compute and memory costs grow with sequence length, posing scalability\nchallenges for long-context inferencing. In response, the algorithm community\nis exploring alternative architectures, such as state space models (SSMs),\nlinear attention, and recurrent neural networks (RNNs), which we refer to as\npost-transformers. This shift presents a key challenge: building a serving\nsystem that efficiently supports both transformer and post-transformer LLMs\nwithin a unified framework. To address this challenge, we analyze the\nperformance characteristics of transformer and post-transformer LLMs. Despite\ntheir algorithmic differences, both are fundamentally limited by memory\nbandwidth under batched inference due to attention in transformers and state\nupdates in post-transformers. Further analyses suggest two additional insights:\n(1) state update operations, unlike attention, incur high hardware cost, making\nper-bank PIM acceleration inefficient, and (2) different low-precision\narithmetic methods offer varying accuracy-area tradeoffs, while we identify\nMicrosoft's MX as the Pareto-optimal choice. Building on these insights, we\ndesign Pimba as an array of State-update Processing Units (SPUs), each shared\nbetween two banks to enable interleaved access to PIM. Each SPU includes a\nState-update Processing Engine (SPE) that comprises element-wise multipliers\nand adders using MX-based quantized arithmetic, enabling efficient execution of\nstate update and attention operations. Our evaluation shows that, compared to\nLLM-optimized GPU and GPU+PIM systems, Pimba achieves up to 3.2x and 2.1x\nhigher token generation throughput, respectively."
                },
                "authors": [
                    {
                        "name": "Wonung Kim"
                    },
                    {
                        "name": "Yubin Lee"
                    },
                    {
                        "name": "Yoonsung Kim"
                    },
                    {
                        "name": "Jinwoo Hwang"
                    },
                    {
                        "name": "Seongryong Oh"
                    },
                    {
                        "name": "Jiyong Jung"
                    },
                    {
                        "name": "Aziz Huseynov"
                    },
                    {
                        "name": "Woong Gyu Park"
                    },
                    {
                        "name": "Chang Hyun Park"
                    },
                    {
                        "name": "Divya Mahajan"
                    },
                    {
                        "name": "Jongse Park"
                    }
                ],
                "author_detail": {
                    "name": "Jongse Park"
                },
                "author": "Jongse Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10177v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10177v1",
                "updated": "2025-07-14T11:39:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    39,
                    34,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T11:39:34Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    39,
                    34,
                    0,
                    195,
                    0
                ],
                "title": "Abusive text transformation using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abusive text transformation using LLMs"
                },
                "summary": "Although Large Language Models (LLMs) have demonstrated significant\nadvancements in natural language processing tasks, their effectiveness in the\nclassification and transformation of abusive text into non-abusive versions\nremains an area for exploration. In this study, we aim to use LLMs to transform\nabusive text (tweets and reviews) featuring hate speech and swear words into\nnon-abusive text, while retaining the intent of the text. We evaluate the\nperformance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and\nGroq, on their ability to identify abusive text. We them to transform and\nobtain a text that is clean from abusive and inappropriate content but\nmaintains a similar level of sentiment and semantics, i.e. the transformed text\nneeds to maintain its message. Afterwards, we evaluate the raw and transformed\ndatasets with sentiment analysis and semantic analysis. Our results show Groq\nprovides vastly different results when compared with other LLMs. We have\nidentified similarities between GPT-4o and DeepSeek-V3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) have demonstrated significant\nadvancements in natural language processing tasks, their effectiveness in the\nclassification and transformation of abusive text into non-abusive versions\nremains an area for exploration. In this study, we aim to use LLMs to transform\nabusive text (tweets and reviews) featuring hate speech and swear words into\nnon-abusive text, while retaining the intent of the text. We evaluate the\nperformance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and\nGroq, on their ability to identify abusive text. We them to transform and\nobtain a text that is clean from abusive and inappropriate content but\nmaintains a similar level of sentiment and semantics, i.e. the transformed text\nneeds to maintain its message. Afterwards, we evaluate the raw and transformed\ndatasets with sentiment analysis and semantic analysis. Our results show Groq\nprovides vastly different results when compared with other LLMs. We have\nidentified similarities between GPT-4o and DeepSeek-V3."
                },
                "authors": [
                    {
                        "name": "Rohitash Chandra"
                    },
                    {
                        "name": "Jiyong Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jiyong Choi"
                },
                "author": "Jiyong Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10177v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10177v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10171v1",
                "updated": "2025-07-14T11:33:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    33,
                    47,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T11:33:47Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    33,
                    47,
                    0,
                    195,
                    0
                ],
                "title": "SlumpGuard: An AI-Powered Real-Time System for Automated Concrete Slump\n  Prediction via Video Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlumpGuard: An AI-Powered Real-Time System for Automated Concrete Slump\n  Prediction via Video Analysis"
                },
                "summary": "Concrete workability is essential for construction quality, with the slump\ntest being the most common on-site method for its assessment. However,\ntraditional slump testing is manual, time-consuming, and prone to\ninconsistency, limiting its applicability for real-time monitoring. To address\nthese challenges, we propose SlumpGuard, an AI-powered, video-based system that\nautomatically analyzes concrete flow from the truck chute to assess workability\nin real time. Our system enables full-batch inspection without manual\nintervention, improving both the accuracy and efficiency of quality control. We\npresent the system design, a the construction of a dedicated dataset, and\nempirical results from real-world deployment, demonstrating the effectiveness\nof SlumpGuard as a practical solution for modern concrete quality assurance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concrete workability is essential for construction quality, with the slump\ntest being the most common on-site method for its assessment. However,\ntraditional slump testing is manual, time-consuming, and prone to\ninconsistency, limiting its applicability for real-time monitoring. To address\nthese challenges, we propose SlumpGuard, an AI-powered, video-based system that\nautomatically analyzes concrete flow from the truck chute to assess workability\nin real time. Our system enables full-batch inspection without manual\nintervention, improving both the accuracy and efficiency of quality control. We\npresent the system design, a the construction of a dedicated dataset, and\nempirical results from real-world deployment, demonstrating the effectiveness\nof SlumpGuard as a practical solution for modern concrete quality assurance."
                },
                "authors": [
                    {
                        "name": "Youngmin Kim"
                    },
                    {
                        "name": "Giyeong Oh"
                    },
                    {
                        "name": "Kwangsoo Youm"
                    },
                    {
                        "name": "Youngjae Yu"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Yu"
                },
                "author": "Youngjae Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10170v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10170v1",
                "updated": "2025-07-14T11:33:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    33,
                    14,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T11:33:14Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    33,
                    14,
                    0,
                    195,
                    0
                ],
                "title": "Understanding the Rank of Tensor Networks via an Intuitive\n  Example-Driven Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Rank of Tensor Networks via an Intuitive\n  Example-Driven Approach"
                },
                "summary": "Tensor Network (TN) decompositions have emerged as an indispensable tool in\nBig Data analytics owing to their ability to provide compact low-rank\nrepresentations, thus alleviating the ``Curse of Dimensionality'' inherent in\nhandling higher-order data. At the heart of their success lies the concept of\nTN ranks, which governs the efficiency and expressivity of TN decompositions.\nHowever, unlike matrix ranks, TN ranks often lack a universal meaning and an\nintuitive interpretation, with their properties varying significantly across\ndifferent TN structures. Consequently, TN ranks are frequently treated as\nempirically tuned hyperparameters, rather than as key design parameters\ninferred from domain knowledge. The aim of this Lecture Note is therefore to\ndemystify the foundational yet frequently misunderstood concept of TN ranks\nthrough real-life examples and intuitive visualizations. We begin by\nillustrating how domain knowledge can guide the selection of TN ranks in\nwidely-used models such as the Canonical Polyadic (CP) and Tucker\ndecompositions. For more complex TN structures, we employ a self-explanatory\ngraphical approach that generalizes to tensors of arbitrary order. Such a\nperspective naturally reveals the relationship between TN ranks and the\ncorresponding ranks of tensor unfoldings (matrices), thereby circumventing\ncumbersome multi-index tensor algebra while facilitating domain-informed TN\ndesign. It is our hope that this Lecture Note will equip readers with a clear\nand unified understanding of the concept of TN rank, along with the necessary\nphysical insight and intuition to support the selection, explainability, and\ndeployment of tensor methods in both practical applications and educational\ncontexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Network (TN) decompositions have emerged as an indispensable tool in\nBig Data analytics owing to their ability to provide compact low-rank\nrepresentations, thus alleviating the ``Curse of Dimensionality'' inherent in\nhandling higher-order data. At the heart of their success lies the concept of\nTN ranks, which governs the efficiency and expressivity of TN decompositions.\nHowever, unlike matrix ranks, TN ranks often lack a universal meaning and an\nintuitive interpretation, with their properties varying significantly across\ndifferent TN structures. Consequently, TN ranks are frequently treated as\nempirically tuned hyperparameters, rather than as key design parameters\ninferred from domain knowledge. The aim of this Lecture Note is therefore to\ndemystify the foundational yet frequently misunderstood concept of TN ranks\nthrough real-life examples and intuitive visualizations. We begin by\nillustrating how domain knowledge can guide the selection of TN ranks in\nwidely-used models such as the Canonical Polyadic (CP) and Tucker\ndecompositions. For more complex TN structures, we employ a self-explanatory\ngraphical approach that generalizes to tensors of arbitrary order. Such a\nperspective naturally reveals the relationship between TN ranks and the\ncorresponding ranks of tensor unfoldings (matrices), thereby circumventing\ncumbersome multi-index tensor algebra while facilitating domain-informed TN\ndesign. It is our hope that this Lecture Note will equip readers with a clear\nand unified understanding of the concept of TN rank, along with the necessary\nphysical insight and intuition to support the selection, explainability, and\ndeployment of tensor methods in both practical applications and educational\ncontexts."
                },
                "authors": [
                    {
                        "name": "Wuyang Zhou"
                    },
                    {
                        "name": "Giorgos Iacovides"
                    },
                    {
                        "name": "Kriton Konstantinidis"
                    },
                    {
                        "name": "Ilya Kisil"
                    },
                    {
                        "name": "Danilo Mandic"
                    }
                ],
                "author_detail": {
                    "name": "Danilo Mandic"
                },
                "author": "Danilo Mandic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10170v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10170v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14403v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14403v3",
                "updated": "2025-07-14T11:21:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    21,
                    20,
                    0,
                    195,
                    0
                ],
                "published": "2025-05-20T14:16:49Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    16,
                    49,
                    1,
                    140,
                    0
                ],
                "title": "Unearthing Gems from Stones: Policy Optimization with Negative Sample\n  Augmentation for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unearthing Gems from Stones: Policy Optimization with Negative Sample\n  Augmentation for LLM Reasoning"
                },
                "summary": "Recent advances in reasoning language models have witnessed a paradigm shift\nfrom short to long CoT pattern. Given the substantial computational cost of\nrollouts in long CoT models, maximizing the utility of fixed training datasets\nbecomes crucial. Our analysis reveals that negative responses contain valuable\ncomponents such as self-reflection and error-correction steps, yet primary\nexisting methods either completely discard negative samples (RFT) or apply\nequal penalization across all tokens (RL), failing to leverage these potential\nlearning signals. In light of this, we propose Behavior Constrained Policy\nGradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline\nRL framework that encompasses three stages: 1) sample segmentation, 2)\nconsensus-based step correctness assessment combining LLM and PRM judgers, and\n3) policy optimization with NSA designed to effectively mine positive steps\nwithin negative samples. Experimental results show that BCPG-NSA outperforms\nbaselines on several challenging math/coding reasoning benchmarks using the\nsame training dataset, achieving improved sample efficiency and demonstrating\nrobustness and scalability when extended to multiple iterations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reasoning language models have witnessed a paradigm shift\nfrom short to long CoT pattern. Given the substantial computational cost of\nrollouts in long CoT models, maximizing the utility of fixed training datasets\nbecomes crucial. Our analysis reveals that negative responses contain valuable\ncomponents such as self-reflection and error-correction steps, yet primary\nexisting methods either completely discard negative samples (RFT) or apply\nequal penalization across all tokens (RL), failing to leverage these potential\nlearning signals. In light of this, we propose Behavior Constrained Policy\nGradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline\nRL framework that encompasses three stages: 1) sample segmentation, 2)\nconsensus-based step correctness assessment combining LLM and PRM judgers, and\n3) policy optimization with NSA designed to effectively mine positive steps\nwithin negative samples. Experimental results show that BCPG-NSA outperforms\nbaselines on several challenging math/coding reasoning benchmarks using the\nsame training dataset, achieving improved sample efficiency and demonstrating\nrobustness and scalability when extended to multiple iterations."
                },
                "authors": [
                    {
                        "name": "Zhaohui Yang"
                    },
                    {
                        "name": "Yuxiao Ye"
                    },
                    {
                        "name": "Shilei Jiang"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Linjing Li"
                    },
                    {
                        "name": "Shihong Deng"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14403v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14403v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10160v1",
                "updated": "2025-07-14T11:18:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    18,
                    33,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T11:18:33Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    18,
                    33,
                    0,
                    195,
                    0
                ],
                "title": "Domain Borders Are There to Be Crossed With Federated Few-Shot\n  Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain Borders Are There to Be Crossed With Federated Few-Shot\n  Adaptation"
                },
                "summary": "Federated Learning has emerged as a leading paradigm for decentralized,\nprivacy-preserving learning, particularly relevant in the era of interconnected\nedge devices equipped with sensors. However, the practical implementation of\nFederated Learning faces three primary challenges: the need for human\ninvolvement in costly data labelling processes for target adaptation, covariate\nshift in client device data collection due to environmental factors affecting\nsensors, leading to discrepancies between source and target samples, and the\nimpracticality of continuous or regular model updates in resource-constrained\nenvironments due to limited data transmission capabilities and technical\nconstraints on channel availability and energy efficiency. To tackle these\nissues, we expand upon an efficient and scalable Federated Learning framework\ntailored for real-world client adaptation in industrial settings. This\nframework leverages a pre-trained source model comprising a deep backbone, an\nadaptation module, and a classifier running on a powerful server. By freezing\nthe backbone and classifier during client adaptation on resource-constrained\ndevices, we allow the domain adaptive linear layer to handle target domain\nadaptation, thus minimizing overall computational overhead. Furthermore, this\nsetup, designated as FedAcross+, is extended to encompass the processing of\nstreaming data, thereby rendering the solution suitable for non-stationary\nenvironments. Extensive experimental results demonstrate the effectiveness of\nFedAcross+ in achieving competitive adaptation on low-end client devices with\nlimited target samples, successfully addressing the challenge of domain shift.\nMoreover, our framework accommodates sporadic model updates within\nresource-constrained environments, ensuring practical and seamless deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning has emerged as a leading paradigm for decentralized,\nprivacy-preserving learning, particularly relevant in the era of interconnected\nedge devices equipped with sensors. However, the practical implementation of\nFederated Learning faces three primary challenges: the need for human\ninvolvement in costly data labelling processes for target adaptation, covariate\nshift in client device data collection due to environmental factors affecting\nsensors, leading to discrepancies between source and target samples, and the\nimpracticality of continuous or regular model updates in resource-constrained\nenvironments due to limited data transmission capabilities and technical\nconstraints on channel availability and energy efficiency. To tackle these\nissues, we expand upon an efficient and scalable Federated Learning framework\ntailored for real-world client adaptation in industrial settings. This\nframework leverages a pre-trained source model comprising a deep backbone, an\nadaptation module, and a classifier running on a powerful server. By freezing\nthe backbone and classifier during client adaptation on resource-constrained\ndevices, we allow the domain adaptive linear layer to handle target domain\nadaptation, thus minimizing overall computational overhead. Furthermore, this\nsetup, designated as FedAcross+, is extended to encompass the processing of\nstreaming data, thereby rendering the solution suitable for non-stationary\nenvironments. Extensive experimental results demonstrate the effectiveness of\nFedAcross+ in achieving competitive adaptation on low-end client devices with\nlimited target samples, successfully addressing the challenge of domain shift.\nMoreover, our framework accommodates sporadic model updates within\nresource-constrained environments, ensuring practical and seamless deployment."
                },
                "authors": [
                    {
                        "name": "Manuel R√∂der"
                    },
                    {
                        "name": "Christoph Raab"
                    },
                    {
                        "name": "Frank-Michael Schleif"
                    }
                ],
                "author_detail": {
                    "name": "Frank-Michael Schleif"
                },
                "author": "Frank-Michael Schleif",
                "arxiv_comment": "Extension of http://dx.doi.org/10.5220/0012351900003654",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10156v1",
                "updated": "2025-07-14T11:12:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    12,
                    30,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T11:12:30Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    12,
                    30,
                    0,
                    195,
                    0
                ],
                "title": "Introducing the Swiss Food Knowledge Graph: AI for Context-Aware\n  Nutrition Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introducing the Swiss Food Knowledge Graph: AI for Context-Aware\n  Nutrition Recommendation"
                },
                "summary": "AI has driven significant progress in the nutrition field, especially through\nmultimedia-based automatic dietary assessment. However, existing automatic\ndietary assessment systems often overlook critical non-visual factors, such as\nrecipe-specific ingredient substitutions that can significantly alter\nnutritional content, and rarely account for individual dietary needs, including\nallergies, restrictions, cultural practices, and personal preferences. In\nSwitzerland, while food-related information is available, it remains\nfragmented, and no centralized repository currently integrates all relevant\nnutrition-related aspects within a Swiss context. To bridge this divide, we\nintroduce the Swiss Food Knowledge Graph (SwissFKG), the first resource, to our\nbest knowledge, to unite recipes, ingredients, and their substitutions with\nnutrient data, dietary restrictions, allergen information, and national\nnutrition guidelines under one graph. We establish a LLM-powered enrichment\npipeline for populating the graph, whereby we further present the first\nbenchmark of four off-the-shelf (<70 B parameter) LLMs for food knowledge\naugmentation. Our results demonstrate that LLMs can effectively enrich the\ngraph with relevant nutritional information. Our SwissFKG goes beyond recipe\nrecommendations by offering ingredient-level information such as allergen and\ndietary restriction information, and guidance aligned with nutritional\nguidelines. Moreover, we implement a Graph-RAG application to showcase how the\nSwissFKG's rich natural-language data structure can help LLM answer\nuser-specific nutrition queries, and we evaluate LLM-embedding pairings by\ncomparing user-query responses against predefined expected answers. As such,\nour work lays the foundation for the next generation of dietary assessment\ntools that blend visual, contextual, and cultural dimensions of eating.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI has driven significant progress in the nutrition field, especially through\nmultimedia-based automatic dietary assessment. However, existing automatic\ndietary assessment systems often overlook critical non-visual factors, such as\nrecipe-specific ingredient substitutions that can significantly alter\nnutritional content, and rarely account for individual dietary needs, including\nallergies, restrictions, cultural practices, and personal preferences. In\nSwitzerland, while food-related information is available, it remains\nfragmented, and no centralized repository currently integrates all relevant\nnutrition-related aspects within a Swiss context. To bridge this divide, we\nintroduce the Swiss Food Knowledge Graph (SwissFKG), the first resource, to our\nbest knowledge, to unite recipes, ingredients, and their substitutions with\nnutrient data, dietary restrictions, allergen information, and national\nnutrition guidelines under one graph. We establish a LLM-powered enrichment\npipeline for populating the graph, whereby we further present the first\nbenchmark of four off-the-shelf (<70 B parameter) LLMs for food knowledge\naugmentation. Our results demonstrate that LLMs can effectively enrich the\ngraph with relevant nutritional information. Our SwissFKG goes beyond recipe\nrecommendations by offering ingredient-level information such as allergen and\ndietary restriction information, and guidance aligned with nutritional\nguidelines. Moreover, we implement a Graph-RAG application to showcase how the\nSwissFKG's rich natural-language data structure can help LLM answer\nuser-specific nutrition queries, and we evaluate LLM-embedding pairings by\ncomparing user-query responses against predefined expected answers. As such,\nour work lays the foundation for the next generation of dietary assessment\ntools that blend visual, contextual, and cultural dimensions of eating."
                },
                "authors": [
                    {
                        "name": "Lubnaa Abdur Rahman"
                    },
                    {
                        "name": "Ioannis Papathanail"
                    },
                    {
                        "name": "Stavroula Mougiakakou"
                    }
                ],
                "author_detail": {
                    "name": "Stavroula Mougiakakou"
                },
                "author": "Stavroula Mougiakakou",
                "arxiv_comment": "10 pages, 2 Figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10155v1",
                "updated": "2025-07-14T11:10:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    10,
                    2,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T11:10:02Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    10,
                    2,
                    0,
                    195,
                    0
                ],
                "title": "Task-Based Flexible Feature Distillation for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-Based Flexible Feature Distillation for LLMs"
                },
                "summary": "Knowledge Distillation (KD) in general and feature distillation in particular\nare promising techniques for reducing the high computational demand of large\nlanguage models (LLMs). However, traditional feature KD methods typically\nassume that the teacher and the student share the same hidden size, limiting\nthe flexibility of the student's architecture. A common solution to this\nproblem involves training a linear projector to align their feature spaces, but\nthis introduces additional parameters that must be learned from scratch and\noften degrades performance on downstream tasks, especially in generative\nsettings. To address this issue, in this work, we propose a novel task-based\nfeature distillation method that enables knowledge transfer between teacher and\nstudent models with different hidden layer dimensions, without introducing any\nnew parameters. Leveraging the insight that only a subset of LLM components\ncontribute significantly to a specific downstream task, our approach identifies\nthe most task-relevant hidden units in the teacher and directly distills their\nactivations to the student. Our method is flexible and easily integrates with\nother distillation frameworks. Empirical results show consistent improvements\nover prior approaches across diverse tasks, including classification,\ninstruction-following, and summarization, achieving up to a 3\\% performance\ngain over the linear projection baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Distillation (KD) in general and feature distillation in particular\nare promising techniques for reducing the high computational demand of large\nlanguage models (LLMs). However, traditional feature KD methods typically\nassume that the teacher and the student share the same hidden size, limiting\nthe flexibility of the student's architecture. A common solution to this\nproblem involves training a linear projector to align their feature spaces, but\nthis introduces additional parameters that must be learned from scratch and\noften degrades performance on downstream tasks, especially in generative\nsettings. To address this issue, in this work, we propose a novel task-based\nfeature distillation method that enables knowledge transfer between teacher and\nstudent models with different hidden layer dimensions, without introducing any\nnew parameters. Leveraging the insight that only a subset of LLM components\ncontribute significantly to a specific downstream task, our approach identifies\nthe most task-relevant hidden units in the teacher and directly distills their\nactivations to the student. Our method is flexible and easily integrates with\nother distillation frameworks. Empirical results show consistent improvements\nover prior approaches across diverse tasks, including classification,\ninstruction-following, and summarization, achieving up to a 3\\% performance\ngain over the linear projection baseline."
                },
                "authors": [
                    {
                        "name": "Khouloud Saadi"
                    },
                    {
                        "name": "Di Wang"
                    }
                ],
                "author_detail": {
                    "name": "Di Wang"
                },
                "author": "Di Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10150v1",
                "updated": "2025-07-14T10:53:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    10,
                    53,
                    47,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T10:53:47Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    10,
                    53,
                    47,
                    0,
                    195,
                    0
                ],
                "title": "Past-Future Scheduler for LLM Serving under SLA Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Past-Future Scheduler for LLM Serving under SLA Guarantees"
                },
                "summary": "The exploration and application of Large Language Models (LLMs) is thriving.\nTo reduce deployment costs, continuous batching has become an essential feature\nin current service frameworks. The effectiveness of continuous batching relies\non an accurate estimate of the memory requirements of requests. However, due to\nthe diversity in request output lengths, existing frameworks tend to adopt\naggressive or conservative schedulers, which often result in significant\noverestimation or underestimation of memory consumption. Consequently, they\nsuffer from harmful request evictions or prolonged queuing times, failing to\nachieve satisfactory throughput under strict Service Level Agreement (SLA)\nguarantees (a.k.a. goodput), across various LLM application scenarios with\ndiffering input-output length distributions. To address this issue, we propose\na novel Past-Future scheduler that precisely estimates the peak memory\nresources required by the running batch via considering the historical\ndistribution of request output lengths and calculating memory occupancy at each\nfuture time point. It adapts to applications with all types of input-output\nlength distributions, balancing the trade-off between request queuing and\nharmful evictions, thereby consistently achieving better goodput. Furthermore,\nto validate the effectiveness of the proposed scheduler, we developed a\nhigh-performance LLM serving framework, LightLLM, that implements the\nPast-Future scheduler. Compared to existing aggressive or conservative\nschedulers, LightLLM demonstrates superior goodput, achieving up to 2-3$\\times$\nhigher goodput than other schedulers under heavy loads. LightLLM is open source\nto boost the research in such direction (https://github.com/ModelTC/lightllm).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exploration and application of Large Language Models (LLMs) is thriving.\nTo reduce deployment costs, continuous batching has become an essential feature\nin current service frameworks. The effectiveness of continuous batching relies\non an accurate estimate of the memory requirements of requests. However, due to\nthe diversity in request output lengths, existing frameworks tend to adopt\naggressive or conservative schedulers, which often result in significant\noverestimation or underestimation of memory consumption. Consequently, they\nsuffer from harmful request evictions or prolonged queuing times, failing to\nachieve satisfactory throughput under strict Service Level Agreement (SLA)\nguarantees (a.k.a. goodput), across various LLM application scenarios with\ndiffering input-output length distributions. To address this issue, we propose\na novel Past-Future scheduler that precisely estimates the peak memory\nresources required by the running batch via considering the historical\ndistribution of request output lengths and calculating memory occupancy at each\nfuture time point. It adapts to applications with all types of input-output\nlength distributions, balancing the trade-off between request queuing and\nharmful evictions, thereby consistently achieving better goodput. Furthermore,\nto validate the effectiveness of the proposed scheduler, we developed a\nhigh-performance LLM serving framework, LightLLM, that implements the\nPast-Future scheduler. Compared to existing aggressive or conservative\nschedulers, LightLLM demonstrates superior goodput, achieving up to 2-3$\\times$\nhigher goodput than other schedulers under heavy loads. LightLLM is open source\nto boost the research in such direction (https://github.com/ModelTC/lightllm)."
                },
                "authors": [
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Shihao Bai"
                    },
                    {
                        "name": "Siyu Wu"
                    },
                    {
                        "name": "Yunqian Fan"
                    },
                    {
                        "name": "Zaijun Wang"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Hailong Yang"
                    },
                    {
                        "name": "Xianglong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xianglong Liu"
                },
                "author": "Xianglong Liu",
                "arxiv_doi": "10.1145/3676641.3716011",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716011",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.10150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to ASPLOS 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10142v1",
                "updated": "2025-07-14T10:39:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    10,
                    39,
                    17,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T10:39:17Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    10,
                    39,
                    17,
                    0,
                    195,
                    0
                ],
                "title": "Adaptability in Multi-Agent Reinforcement Learning: A Framework and\n  Unified Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptability in Multi-Agent Reinforcement Learning: A Framework and\n  Unified Review"
                },
                "summary": "Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in\ncoordinating multiple agents across simulated benchmarks and constrained\nscenarios. However, its deployment in real-world multi-agent systems (MAS)\nremains limited, primarily due to the complex and dynamic nature of such\nenvironments. These challenges arise from multiple interacting sources of\nvariability, including fluctuating agent populations, evolving task goals, and\ninconsistent execution conditions. Together, these factors demand that MARL\nalgorithms remain effective under continuously changing system configurations\nand operational demands. To better capture and assess this capacity for\nadjustment, we introduce the concept of \\textit{adaptability} as a unified and\npractically grounded lens through which to evaluate the reliability of MARL\nalgorithms under shifting conditions, broadly referring to any changes in the\nenvironment dynamics that may occur during learning or execution. Centred on\nthe notion of adaptability, we propose a structured framework comprising three\nkey dimensions: learning adaptability, policy adaptability, and scenario-driven\nadaptability. By adopting this adaptability perspective, we aim to support more\nprincipled assessments of MARL performance beyond narrowly defined benchmarks.\nUltimately, this survey contributes to the development of algorithms that are\nbetter suited for deployment in dynamic, real-world multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in\ncoordinating multiple agents across simulated benchmarks and constrained\nscenarios. However, its deployment in real-world multi-agent systems (MAS)\nremains limited, primarily due to the complex and dynamic nature of such\nenvironments. These challenges arise from multiple interacting sources of\nvariability, including fluctuating agent populations, evolving task goals, and\ninconsistent execution conditions. Together, these factors demand that MARL\nalgorithms remain effective under continuously changing system configurations\nand operational demands. To better capture and assess this capacity for\nadjustment, we introduce the concept of \\textit{adaptability} as a unified and\npractically grounded lens through which to evaluate the reliability of MARL\nalgorithms under shifting conditions, broadly referring to any changes in the\nenvironment dynamics that may occur during learning or execution. Centred on\nthe notion of adaptability, we propose a structured framework comprising three\nkey dimensions: learning adaptability, policy adaptability, and scenario-driven\nadaptability. By adopting this adaptability perspective, we aim to support more\nprincipled assessments of MARL performance beyond narrowly defined benchmarks.\nUltimately, this survey contributes to the development of algorithms that are\nbetter suited for deployment in dynamic, real-world multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Siyi Hu"
                    },
                    {
                        "name": "Mohamad A Hady"
                    },
                    {
                        "name": "Jianglin Qiao"
                    },
                    {
                        "name": "Jimmy Cao"
                    },
                    {
                        "name": "Mahardhika Pratama"
                    },
                    {
                        "name": "Ryszard Kowalczyk"
                    }
                ],
                "author_detail": {
                    "name": "Ryszard Kowalczyk"
                },
                "author": "Ryszard Kowalczyk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10139v1",
                "updated": "2025-07-14T10:36:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    10,
                    36,
                    15,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T10:36:15Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    10,
                    36,
                    15,
                    0,
                    195,
                    0
                ],
                "title": "Large-Scale Graph Building in Dynamic Environments: Low Latency and High\n  Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-Scale Graph Building in Dynamic Environments: Low Latency and High\n  Quality"
                },
                "summary": "Learning and constructing large-scale graphs has attracted attention in\nrecent decades, resulting in a rich literature that introduced various systems,\ntools, and algorithms. Grale is one of such tools that is designed for offline\nenvironments and is deployed in more than 50 different industrial settings at\nGoogle. Grale is widely applicable because of its ability to efficiently learn\nand construct a graph on datasets with multiple types of features. However, it\nis often the case that applications require the underlying data to evolve\ncontinuously and rapidly and the updated graph needs to be available with low\nlatency. Such setting make the use of Grale prohibitive. While there are\nApproximate Nearest Neighbor (ANN) systems that handle dynamic updates with low\nlatency, they are mostly limited to similarities over a single embedding.\n  In this work, we introduce a system that inherits the advantages and the\nquality of Grale, and maintains a graph construction in a dynamic setting with\ntens of milliseconds of latency per request. We call the system Dynamic Grale\nUsing ScaNN (Dynamic GUS). Our system has a wide range of applications with\nover 10 deployments at Google. One of the applications is in Android Security\nand Privacy, where Dynamic Grale Using ScaNN enables capturing harmful\napplications 4 times faster, before they can reach users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning and constructing large-scale graphs has attracted attention in\nrecent decades, resulting in a rich literature that introduced various systems,\ntools, and algorithms. Grale is one of such tools that is designed for offline\nenvironments and is deployed in more than 50 different industrial settings at\nGoogle. Grale is widely applicable because of its ability to efficiently learn\nand construct a graph on datasets with multiple types of features. However, it\nis often the case that applications require the underlying data to evolve\ncontinuously and rapidly and the updated graph needs to be available with low\nlatency. Such setting make the use of Grale prohibitive. While there are\nApproximate Nearest Neighbor (ANN) systems that handle dynamic updates with low\nlatency, they are mostly limited to similarities over a single embedding.\n  In this work, we introduce a system that inherits the advantages and the\nquality of Grale, and maintains a graph construction in a dynamic setting with\ntens of milliseconds of latency per request. We call the system Dynamic Grale\nUsing ScaNN (Dynamic GUS). Our system has a wide range of applications with\nover 10 deployments at Google. One of the applications is in Android Security\nand Privacy, where Dynamic Grale Using ScaNN enables capturing harmful\napplications 4 times faster, before they can reach users."
                },
                "authors": [
                    {
                        "name": "Filipe Miguel Gon√ßalves de Almeida"
                    },
                    {
                        "name": "CJ Carey"
                    },
                    {
                        "name": "Hendrik Fichtenberger"
                    },
                    {
                        "name": "Jonathan Halcrow"
                    },
                    {
                        "name": "Silvio Lattanzi"
                    },
                    {
                        "name": "Andr√© Linhares"
                    },
                    {
                        "name": "Tao Meng"
                    },
                    {
                        "name": "Ashkan Norouzi-Fard"
                    },
                    {
                        "name": "Nikos Parotsidis"
                    },
                    {
                        "name": "Bryan Perozzi"
                    },
                    {
                        "name": "David Simcha"
                    }
                ],
                "author_detail": {
                    "name": "David Simcha"
                },
                "author": "David Simcha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10134v1",
                "updated": "2025-07-14T10:24:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    10,
                    24,
                    43,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T10:24:43Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    10,
                    24,
                    43,
                    0,
                    195,
                    0
                ],
                "title": "FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for\n  Fresh Data Collection in UAV-Assisted Wildfire Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for\n  Fresh Data Collection in UAV-Assisted Wildfire Monitoring"
                },
                "summary": "Unmanned Aerial Vehicles (UAVs) are vital for public safety, particularly in\nwildfire monitoring, where early detection minimizes environmental impact. In\nUAV-Assisted Wildfire Monitoring (UAWM) systems, joint optimization of sensor\ntransmission scheduling and velocity is critical for minimizing Age of\nInformation (AoI) from stale sensor data. Deep Reinforcement Learning (DRL) has\nbeen used for such optimization; however, its limitations such as low sampling\nefficiency, simulation-to-reality gaps, and complex training render it\nunsuitable for time-critical applications like wildfire monitoring. This paper\nintroduces a new online Flight Resource Allocation scheme based on LLM-Enabled\nIn-Context Learning (FRSICL) to jointly optimize the UAV's flight control and\ndata collection schedule along the trajectory in real time, thereby\nasymptotically minimizing the average AoI across ground sensors. In contrast to\nDRL, FRSICL generates data collection schedules and controls velocity using\nnatural language task descriptions and feedback from the environment, enabling\ndynamic decision-making without extensive retraining. Simulation results\nconfirm the effectiveness of the proposed FRSICL compared to Proximal Policy\nOptimization (PPO) and Nearest-Neighbor baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned Aerial Vehicles (UAVs) are vital for public safety, particularly in\nwildfire monitoring, where early detection minimizes environmental impact. In\nUAV-Assisted Wildfire Monitoring (UAWM) systems, joint optimization of sensor\ntransmission scheduling and velocity is critical for minimizing Age of\nInformation (AoI) from stale sensor data. Deep Reinforcement Learning (DRL) has\nbeen used for such optimization; however, its limitations such as low sampling\nefficiency, simulation-to-reality gaps, and complex training render it\nunsuitable for time-critical applications like wildfire monitoring. This paper\nintroduces a new online Flight Resource Allocation scheme based on LLM-Enabled\nIn-Context Learning (FRSICL) to jointly optimize the UAV's flight control and\ndata collection schedule along the trajectory in real time, thereby\nasymptotically minimizing the average AoI across ground sensors. In contrast to\nDRL, FRSICL generates data collection schedules and controls velocity using\nnatural language task descriptions and feedback from the environment, enabling\ndynamic decision-making without extensive retraining. Simulation results\nconfirm the effectiveness of the proposed FRSICL compared to Proximal Policy\nOptimization (PPO) and Nearest-Neighbor baselines."
                },
                "authors": [
                    {
                        "name": "Yousef Emami"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Miguel Gutierrez Gaitan"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Luis Almeida"
                    }
                ],
                "author_detail": {
                    "name": "Luis Almeida"
                },
                "author": "Luis Almeida",
                "arxiv_comment": "8 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "53-01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10124v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10124v1",
                "updated": "2025-07-14T10:09:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    10,
                    9,
                    46,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T10:09:46Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    10,
                    9,
                    46,
                    0,
                    195,
                    0
                ],
                "title": "Could you be wrong: Debiasing LLMs using a metacognitive prompt for\n  improving human decision making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Could you be wrong: Debiasing LLMs using a metacognitive prompt for\n  improving human decision making"
                },
                "summary": "Identifying bias in LLMs is ongoing. Because they are still in development,\nwhat is true today may be false tomorrow. We therefore need general strategies\nfor debiasing that will outlive current models. Strategies developed for\ndebiasing human decision making offer one promising approach as they\nincorporate an LLM-style prompt intervention designed to bring latent knowledge\ninto awareness during decision making. LLMs trained on vast amounts of\ninformation contain information about potential biases, counter-arguments, and\ncontradictory evidence, but that information may only be brought to bear if\nprompted. Metacognitive prompts developed in the human decision making\nliterature are designed to achieve this, and as I demonstrate here, they show\npromise with LLMs. The prompt I focus on here is \"could you be wrong?\"\nFollowing an LLM response, this prompt leads LLMs to produce additional\ninformation, including why they answered as they did, errors, biases,\ncontradictory evidence, and alternatives, none of which were apparent in their\ninitial response. Indeed, this metaknowledge often reveals that how LLMs and\nusers interpret prompts are not aligned. Here I demonstrate this prompt using a\nset of questions taken from recent articles about LLM biases, including\nimplicit discriminatory biases and failures of metacognition. \"Could you be\nwrong\" prompts the LLM to identify its own biases and produce cogent\nmetacognitive reflection. I also present another example involving convincing\nbut incomplete information, which is readily corrected by the metacognitive\nprompt. In sum, this work argues that human psychology offers a new avenue for\nprompt engineering, leveraging a long history of effective prompt-based\nimprovements to human decision making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying bias in LLMs is ongoing. Because they are still in development,\nwhat is true today may be false tomorrow. We therefore need general strategies\nfor debiasing that will outlive current models. Strategies developed for\ndebiasing human decision making offer one promising approach as they\nincorporate an LLM-style prompt intervention designed to bring latent knowledge\ninto awareness during decision making. LLMs trained on vast amounts of\ninformation contain information about potential biases, counter-arguments, and\ncontradictory evidence, but that information may only be brought to bear if\nprompted. Metacognitive prompts developed in the human decision making\nliterature are designed to achieve this, and as I demonstrate here, they show\npromise with LLMs. The prompt I focus on here is \"could you be wrong?\"\nFollowing an LLM response, this prompt leads LLMs to produce additional\ninformation, including why they answered as they did, errors, biases,\ncontradictory evidence, and alternatives, none of which were apparent in their\ninitial response. Indeed, this metaknowledge often reveals that how LLMs and\nusers interpret prompts are not aligned. Here I demonstrate this prompt using a\nset of questions taken from recent articles about LLM biases, including\nimplicit discriminatory biases and failures of metacognition. \"Could you be\nwrong\" prompts the LLM to identify its own biases and produce cogent\nmetacognitive reflection. I also present another example involving convincing\nbut incomplete information, which is readily corrected by the metacognitive\nprompt. In sum, this work argues that human psychology offers a new avenue for\nprompt engineering, leveraging a long history of effective prompt-based\nimprovements to human decision making."
                },
                "authors": [
                    {
                        "name": "Thomas T. Hills"
                    }
                ],
                "author_detail": {
                    "name": "Thomas T. Hills"
                },
                "author": "Thomas T. Hills",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10124v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10124v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02882v2",
                "updated": "2025-07-14T09:56:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    56,
                    47,
                    0,
                    195,
                    0
                ],
                "published": "2025-04-02T05:47:28Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    5,
                    47,
                    28,
                    2,
                    92,
                    0
                ],
                "title": "DiaTool-DPO: Multi-Turn Direct Preference Optimization for\n  Tool-Augmented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiaTool-DPO: Multi-Turn Direct Preference Optimization for\n  Tool-Augmented Large Language Models"
                },
                "summary": "Tool-Augmented Larage Language Models (TA-LLMs) have shown promise in\nreal-world applications, but face challenges in handling incomplete queries and\nout-of-scope requests. While existing approaches rely mainly on Supervised\nFine-Tuning with expert trajectories, we propose DiaTool-DPO, a novel method\nthat enhances TA-LLM's dialogue capabilities through Direct Preference\nOptimization. We model TA-LLM interactions as a Markov Decision Process with 5\ndistinct dialogue states and categorize user queries into 3 types based on\ntheir state transition trajectories. We automatically construct paired\ntrajectory datasets of correct and incorrect dialogue flows and introduce a\nspecialized objective loss for dialogue control. Our comprehensive evaluation\ndemonstrates that DiaTool-DPO approaches GPT-4o's performance (94.8% in\ninformation gathering, 91% in tool call rejection) with substantial\nimprovements over baseline (44% and 9.6% respectively) while maintaining core\nfunctionality. Our approach opens new possibilities for developing TA-LLMs that\ncan handle diverse real-world scenarios without requiring additional expert\ndemonstrations or human labeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-Augmented Larage Language Models (TA-LLMs) have shown promise in\nreal-world applications, but face challenges in handling incomplete queries and\nout-of-scope requests. While existing approaches rely mainly on Supervised\nFine-Tuning with expert trajectories, we propose DiaTool-DPO, a novel method\nthat enhances TA-LLM's dialogue capabilities through Direct Preference\nOptimization. We model TA-LLM interactions as a Markov Decision Process with 5\ndistinct dialogue states and categorize user queries into 3 types based on\ntheir state transition trajectories. We automatically construct paired\ntrajectory datasets of correct and incorrect dialogue flows and introduce a\nspecialized objective loss for dialogue control. Our comprehensive evaluation\ndemonstrates that DiaTool-DPO approaches GPT-4o's performance (94.8% in\ninformation gathering, 91% in tool call rejection) with substantial\nimprovements over baseline (44% and 9.6% respectively) while maintaining core\nfunctionality. Our approach opens new possibilities for developing TA-LLMs that\ncan handle diverse real-world scenarios without requiring additional expert\ndemonstrations or human labeling."
                },
                "authors": [
                    {
                        "name": "Sunghee Jung"
                    },
                    {
                        "name": "Donghun Lee"
                    },
                    {
                        "name": "Shinbok Lee"
                    },
                    {
                        "name": "Gaeun Seo"
                    },
                    {
                        "name": "Daniel Lee"
                    },
                    {
                        "name": "Byeongil Ko"
                    },
                    {
                        "name": "Junrae Cho"
                    },
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Eunggyun Kim"
                    },
                    {
                        "name": "Myeongcheol Shin"
                    }
                ],
                "author_detail": {
                    "name": "Myeongcheol Shin"
                },
                "author": "Myeongcheol Shin",
                "arxiv_comment": "Accepted to SIGDIAL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01465v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01465v2",
                "updated": "2025-07-14T09:45:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    45,
                    34,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-02T08:24:50Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    24,
                    50,
                    2,
                    183,
                    0
                ],
                "title": "Pruning the Tree: Rethinking RPKI Architecture From The Ground Up",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning the Tree: Rethinking RPKI Architecture From The Ground Up"
                },
                "summary": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nwhich introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70\\% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nwhich introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70\\% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations."
                },
                "authors": [
                    {
                        "name": "Haya Schulmann"
                    },
                    {
                        "name": "Niklas Vogel"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Vogel"
                },
                "author": "Niklas Vogel",
                "arxiv_comment": "Accepted for publication at NDSS2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01465v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01465v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10103v1",
                "updated": "2025-07-14T09:41:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    41,
                    51,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T09:41:51Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    41,
                    51,
                    0,
                    195,
                    0
                ],
                "title": "Accelerating Automatic Program Repair with Dual Retrieval-Augmented\n  Fine-Tuning and Patch Generation on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Automatic Program Repair with Dual Retrieval-Augmented\n  Fine-Tuning and Patch Generation on Large Language Models"
                },
                "summary": "Automated Program Repair (APR) is essential for ensuring software reliability\nand quality while enhancing efficiency and reducing developers' workload.\nAlthough rule-based and learning-based APR methods have demonstrated their\neffectiveness, their performance was constrained by the defect type of repair,\nthe quality of training data, and the size of model parameters. Recently, Large\nLanguage Models (LLMs) combined with Retrieval-Augmented-Generation (RAG) have\nbeen increasingly adopted in APR tasks. However, current code LLMs and RAG\ndesigns neither fully address code repair tasks nor consider code-specific\nfeatures. To overcome these limitations, we propose SelRepair, a novel APR\napproach with integration of a fine-tuned LLM with a newly-designed dual RAG\nmodule. This approach uses a bug-fix pair dataset for fine-tuning and\nincorporates semantic and syntactic/structural similarity information through\nan RAG selection gate. This design ensures relevant information is retrieved\nefficiently, thereby reducing token length and inference time. Evaluations on\nJava datasets show SelRepair outperforms other APR methods, achieving 26.29%\nand 17.64% in terms of exact match (EM) on different datasets while reducing\ninference time by at least 6.42% with controlled input lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Program Repair (APR) is essential for ensuring software reliability\nand quality while enhancing efficiency and reducing developers' workload.\nAlthough rule-based and learning-based APR methods have demonstrated their\neffectiveness, their performance was constrained by the defect type of repair,\nthe quality of training data, and the size of model parameters. Recently, Large\nLanguage Models (LLMs) combined with Retrieval-Augmented-Generation (RAG) have\nbeen increasingly adopted in APR tasks. However, current code LLMs and RAG\ndesigns neither fully address code repair tasks nor consider code-specific\nfeatures. To overcome these limitations, we propose SelRepair, a novel APR\napproach with integration of a fine-tuned LLM with a newly-designed dual RAG\nmodule. This approach uses a bug-fix pair dataset for fine-tuning and\nincorporates semantic and syntactic/structural similarity information through\nan RAG selection gate. This design ensures relevant information is retrieved\nefficiently, thereby reducing token length and inference time. Evaluations on\nJava datasets show SelRepair outperforms other APR methods, achieving 26.29%\nand 17.64% in terms of exact match (EM) on different datasets while reducing\ninference time by at least 6.42% with controlled input lengths."
                },
                "authors": [
                    {
                        "name": "Hanyang Guo"
                    },
                    {
                        "name": "Xiaoheng Xie"
                    },
                    {
                        "name": "Hong-Ning Dai"
                    },
                    {
                        "name": "Peng Di"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Bishenghui Tao"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10099v1",
                "updated": "2025-07-14T09:34:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    34,
                    33,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T09:34:33Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    34,
                    33,
                    0,
                    195,
                    0
                ],
                "title": "ReDemon UI: Reactive Synthesis by Demonstration for Web UI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReDemon UI: Reactive Synthesis by Demonstration for Web UI"
                },
                "summary": "ReDemon UI synthesizes React applications from user demonstrations, enabling\ndesigners and non-expert programmers to create UIs that integrate with standard\nUI prototyping workflows. Users provide a static mockup sketch with event\nhandler holes and demonstrate desired runtime behaviors by interacting with the\nrendered mockup and editing the sketch. ReDemon UI identifies reactive data and\nsynthesizes a React program with correct state update logic. We utilize\nenumerative synthesis for simple UIs and LLMs for more complex UIs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReDemon UI synthesizes React applications from user demonstrations, enabling\ndesigners and non-expert programmers to create UIs that integrate with standard\nUI prototyping workflows. Users provide a static mockup sketch with event\nhandler holes and demonstrate desired runtime behaviors by interacting with the\nrendered mockup and editing the sketch. ReDemon UI identifies reactive data and\nsynthesizes a React program with correct state update logic. We utilize\nenumerative synthesis for simple UIs and LLMs for more complex UIs."
                },
                "authors": [
                    {
                        "name": "Jay Lee"
                    },
                    {
                        "name": "Gyuhyeok Oh"
                    },
                    {
                        "name": "Joongwon Ahn"
                    },
                    {
                        "name": "Xiaokang Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Qiu"
                },
                "author": "Xiaokang Qiu",
                "arxiv_comment": "Submitted to UIST 2025 Posters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10098v1",
                "updated": "2025-07-14T09:33:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    33,
                    40,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T09:33:40Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    33,
                    40,
                    0,
                    195,
                    0
                ],
                "title": "Fusing Large Language Models with Temporal Transformers for Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusing Large Language Models with Temporal Transformers for Time Series\n  Forecasting"
                },
                "summary": "Recently, large language models (LLMs) have demonstrated powerful\ncapabilities in performing various tasks and thus are applied by recent studies\nto time series forecasting (TSF) tasks, which predict future values with the\ngiven historical time series. Existing LLM-based approaches transfer knowledge\nlearned from text data to time series prediction using prompting or fine-tuning\nstrategies. However, LLMs are proficient at reasoning over discrete tokens and\nsemantic patterns but are not initially designed to model continuous numerical\ntime series data. The gaps between text and time series data lead LLMs to\nachieve inferior performance to a vanilla Transformer model that is directly\ntrained on TSF data. However, the vanilla Transformers often struggle to learn\nhigh-level semantic patterns. In this paper, we design a novel\nTransformer-based architecture that complementarily leverages LLMs and vanilla\nTransformers, so as to integrate the high-level semantic representations\nlearned by LLMs into the temporal information encoded by time series\nTransformers, where a hybrid representation is obtained by fusing the\nrepresentations from the LLM and the Transformer. The resulting fused\nrepresentation contains both historical temporal dynamics and semantic\nvariation patterns, allowing our model to predict more accurate future values.\nExperiments on benchmark datasets demonstrate the effectiveness of the proposed\napproach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have demonstrated powerful\ncapabilities in performing various tasks and thus are applied by recent studies\nto time series forecasting (TSF) tasks, which predict future values with the\ngiven historical time series. Existing LLM-based approaches transfer knowledge\nlearned from text data to time series prediction using prompting or fine-tuning\nstrategies. However, LLMs are proficient at reasoning over discrete tokens and\nsemantic patterns but are not initially designed to model continuous numerical\ntime series data. The gaps between text and time series data lead LLMs to\nachieve inferior performance to a vanilla Transformer model that is directly\ntrained on TSF data. However, the vanilla Transformers often struggle to learn\nhigh-level semantic patterns. In this paper, we design a novel\nTransformer-based architecture that complementarily leverages LLMs and vanilla\nTransformers, so as to integrate the high-level semantic representations\nlearned by LLMs into the temporal information encoded by time series\nTransformers, where a hybrid representation is obtained by fusing the\nrepresentations from the LLM and the Transformer. The resulting fused\nrepresentation contains both historical temporal dynamics and semantic\nvariation patterns, allowing our model to predict more accurate future values.\nExperiments on benchmark datasets demonstrate the effectiveness of the proposed\napproach."
                },
                "authors": [
                    {
                        "name": "Chen Su"
                    },
                    {
                        "name": "Yuanhe Tian"
                    },
                    {
                        "name": "Qinyu Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Yan Song"
                    }
                ],
                "author_detail": {
                    "name": "Yan Song"
                },
                "author": "Yan Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15595v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15595v3",
                "updated": "2025-07-14T09:25:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    25,
                    43,
                    0,
                    195,
                    0
                ],
                "published": "2024-10-21T02:27:24Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    2,
                    27,
                    24,
                    0,
                    295,
                    0
                ],
                "title": "A Comprehensive Survey of Direct Preference Optimization: Datasets,\n  Theories, Variants, and Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey of Direct Preference Optimization: Datasets,\n  Theories, Variants, and Applications"
                },
                "summary": "With the rapid advancement of large language models (LLMs), aligning policy\nmodels with human preferences has become increasingly critical. Direct\nPreference Optimization (DPO) has emerged as a promising approach for\nalignment, acting as an RL-free alternative to Reinforcement Learning from\nHuman Feedback (RLHF). Despite DPO's various advancements and inherent\nlimitations, an in-depth review of these aspects is currently lacking in the\nliterature. In this work, we present a comprehensive review of the challenges\nand opportunities in DPO, covering theoretical analyses, variants, relevant\npreference datasets, and applications. Specifically, we categorize recent\nstudies on DPO based on key research questions to provide a thorough\nunderstanding of DPO's current landscape. Additionally, we propose several\nfuture research directions to offer insights on model alignment for the\nresearch community. An updated collection of relevant papers can be found on\nhttps://github.com/Mr-Loevan/DPO-Survey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of large language models (LLMs), aligning policy\nmodels with human preferences has become increasingly critical. Direct\nPreference Optimization (DPO) has emerged as a promising approach for\nalignment, acting as an RL-free alternative to Reinforcement Learning from\nHuman Feedback (RLHF). Despite DPO's various advancements and inherent\nlimitations, an in-depth review of these aspects is currently lacking in the\nliterature. In this work, we present a comprehensive review of the challenges\nand opportunities in DPO, covering theoretical analyses, variants, relevant\npreference datasets, and applications. Specifically, we categorize recent\nstudies on DPO based on key research questions to provide a thorough\nunderstanding of DPO's current landscape. Additionally, we propose several\nfuture research directions to offer insights on model alignment for the\nresearch community. An updated collection of relevant papers can be found on\nhttps://github.com/Mr-Loevan/DPO-Survey."
                },
                "authors": [
                    {
                        "name": "Wenyi Xiao"
                    },
                    {
                        "name": "Zechuan Wang"
                    },
                    {
                        "name": "Leilei Gan"
                    },
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Zongrui Li"
                    },
                    {
                        "name": "Ruirui Lei"
                    },
                    {
                        "name": "Wanggui He"
                    },
                    {
                        "name": "Luu Anh Tuan"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Hao Jiang"
                    },
                    {
                        "name": "Zhou Zhao"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "arxiv_comment": "45 pages, 12 Figures. Project page:\n  https://github.com/Mr-Loevan/DPO-Survey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15595v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15595v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00200v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00200v2",
                "updated": "2025-07-14T09:19:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    19,
                    59,
                    0,
                    195,
                    0
                ],
                "published": "2025-05-30T20:12:51Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    20,
                    12,
                    51,
                    4,
                    150,
                    0
                ],
                "title": "Structuring Radiology Reports: Challenging LLMs with Lightweight Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structuring Radiology Reports: Challenging LLMs with Lightweight Models"
                },
                "summary": "Radiology reports are critical for clinical decision-making but often lack a\nstandardized format, limiting both human interpretability and machine learning\n(ML) applications. While large language models (LLMs) have shown strong\ncapabilities in reformatting clinical text, their high computational\nrequirements, lack of transparency, and data privacy concerns hinder practical\ndeployment. To address these challenges, we explore lightweight encoder-decoder\nmodels (<300M parameters)-specifically T5 and BERT2BERT-for structuring\nradiology reports from the MIMIC-CXR and CheXpert Plus datasets. We benchmark\nthese models against eight open-source LLMs (1B-70B), adapted using prefix\nprompting, in-context learning (ICL), and low-rank adaptation (LoRA)\nfinetuning. Our best-performing lightweight model outperforms all LLMs adapted\nusing prompt-based techniques on a human-annotated test set. While some\nLoRA-finetuned LLMs achieve modest gains over the lightweight model on the\nFindings section (BLEU 6.4%, ROUGE-L 4.8%, BERTScore 3.6%, F1-RadGraph 1.1%,\nGREEN 3.6%, and F1-SRR-BERT 4.3%), these improvements come at the cost of\nsubstantially greater computational resources. For example, LLaMA-3-70B\nincurred more than 400 times the inference time, cost, and carbon emissions\ncompared to the lightweight model. These results underscore the potential of\nlightweight, task-specific models as sustainable and privacy-preserving\nsolutions for structuring clinical text in resource-constrained healthcare\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radiology reports are critical for clinical decision-making but often lack a\nstandardized format, limiting both human interpretability and machine learning\n(ML) applications. While large language models (LLMs) have shown strong\ncapabilities in reformatting clinical text, their high computational\nrequirements, lack of transparency, and data privacy concerns hinder practical\ndeployment. To address these challenges, we explore lightweight encoder-decoder\nmodels (<300M parameters)-specifically T5 and BERT2BERT-for structuring\nradiology reports from the MIMIC-CXR and CheXpert Plus datasets. We benchmark\nthese models against eight open-source LLMs (1B-70B), adapted using prefix\nprompting, in-context learning (ICL), and low-rank adaptation (LoRA)\nfinetuning. Our best-performing lightweight model outperforms all LLMs adapted\nusing prompt-based techniques on a human-annotated test set. While some\nLoRA-finetuned LLMs achieve modest gains over the lightweight model on the\nFindings section (BLEU 6.4%, ROUGE-L 4.8%, BERTScore 3.6%, F1-RadGraph 1.1%,\nGREEN 3.6%, and F1-SRR-BERT 4.3%), these improvements come at the cost of\nsubstantially greater computational resources. For example, LLaMA-3-70B\nincurred more than 400 times the inference time, cost, and carbon emissions\ncompared to the lightweight model. These results underscore the potential of\nlightweight, task-specific models as sustainable and privacy-preserving\nsolutions for structuring clinical text in resource-constrained healthcare\nsettings."
                },
                "authors": [
                    {
                        "name": "Johannes Moll"
                    },
                    {
                        "name": "Louisa Fay"
                    },
                    {
                        "name": "Asfandyar Azhar"
                    },
                    {
                        "name": "Sophie Ostmeier"
                    },
                    {
                        "name": "Tim Lueth"
                    },
                    {
                        "name": "Sergios Gatidis"
                    },
                    {
                        "name": "Curtis Langlotz"
                    },
                    {
                        "name": "Jean-Benoit Delbrouck"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Benoit Delbrouck"
                },
                "author": "Jean-Benoit Delbrouck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00200v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00200v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10087v1",
                "updated": "2025-07-14T09:13:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    13,
                    7,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T09:13:07Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    13,
                    7,
                    0,
                    195,
                    0
                ],
                "title": "Foundation Model Driven Robotics: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Model Driven Robotics: A Comprehensive Review"
                },
                "summary": "The rapid emergence of foundation models, particularly Large Language Models\n(LLMs) and Vision-Language Models (VLMs), has introduced a transformative\nparadigm in robotics. These models offer powerful capabilities in semantic\nunderstanding, high-level reasoning, and cross-modal generalization, enabling\nsignificant advances in perception, planning, control, and human-robot\ninteraction. This critical review provides a structured synthesis of recent\ndevelopments, categorizing applications across simulation-driven design,\nopen-world execution, sim-to-real transfer, and adaptable robotics. Unlike\nexisting surveys that emphasize isolated capabilities, this work highlights\nintegrated, system-level strategies and evaluates their practical feasibility\nin real-world environments. Key enabling trends such as procedural scene\ngeneration, policy generalization, and multimodal reasoning are discussed\nalongside core bottlenecks, including limited embodiment, lack of multimodal\ndata, safety risks, and computational constraints. Through this lens, this\npaper identifies both the architectural strengths and critical limitations of\nfoundation model-based robotics, highlighting open challenges in real-time\noperation, grounding, resilience, and trust. The review concludes with a\nroadmap for future research aimed at bridging semantic reasoning and physical\nintelligence through more robust, interpretable, and embodied models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid emergence of foundation models, particularly Large Language Models\n(LLMs) and Vision-Language Models (VLMs), has introduced a transformative\nparadigm in robotics. These models offer powerful capabilities in semantic\nunderstanding, high-level reasoning, and cross-modal generalization, enabling\nsignificant advances in perception, planning, control, and human-robot\ninteraction. This critical review provides a structured synthesis of recent\ndevelopments, categorizing applications across simulation-driven design,\nopen-world execution, sim-to-real transfer, and adaptable robotics. Unlike\nexisting surveys that emphasize isolated capabilities, this work highlights\nintegrated, system-level strategies and evaluates their practical feasibility\nin real-world environments. Key enabling trends such as procedural scene\ngeneration, policy generalization, and multimodal reasoning are discussed\nalongside core bottlenecks, including limited embodiment, lack of multimodal\ndata, safety risks, and computational constraints. Through this lens, this\npaper identifies both the architectural strengths and critical limitations of\nfoundation model-based robotics, highlighting open challenges in real-time\noperation, grounding, resilience, and trust. The review concludes with a\nroadmap for future research aimed at bridging semantic reasoning and physical\nintelligence through more robust, interpretable, and embodied models."
                },
                "authors": [
                    {
                        "name": "Muhammad Tayyab Khan"
                    },
                    {
                        "name": "Ammar Waheed"
                    }
                ],
                "author_detail": {
                    "name": "Ammar Waheed"
                },
                "author": "Ammar Waheed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10078v1",
                "updated": "2025-07-14T09:03:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    3,
                    44,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T09:03:44Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    3,
                    44,
                    0,
                    195,
                    0
                ],
                "title": "Compression Method for Deep Diagonal State Space Model Based on $H^2$\n  Optimal Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compression Method for Deep Diagonal State Space Model Based on $H^2$\n  Optimal Reduction"
                },
                "summary": "Deep learning models incorporating linear SSMs have gained attention for\ncapturing long-range dependencies in sequential data. However, their large\nparameter sizes pose challenges for deployment on resource-constrained devices.\nIn this study, we propose an efficient parameter reduction method for these\nmodels by applying $H^{2}$ model order reduction techniques from control theory\nto their linear SSM components. In experiments, the LRA benchmark results show\nthat the model compression based on our proposed method outperforms an existing\nmethod using the Balanced Truncation, while successfully reducing the number of\nparameters in the SSMs to $1/32$ without sacrificing the performance of the\noriginal models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models incorporating linear SSMs have gained attention for\ncapturing long-range dependencies in sequential data. However, their large\nparameter sizes pose challenges for deployment on resource-constrained devices.\nIn this study, we propose an efficient parameter reduction method for these\nmodels by applying $H^{2}$ model order reduction techniques from control theory\nto their linear SSM components. In experiments, the LRA benchmark results show\nthat the model compression based on our proposed method outperforms an existing\nmethod using the Balanced Truncation, while successfully reducing the number of\nparameters in the SSMs to $1/32$ without sacrificing the performance of the\noriginal models."
                },
                "authors": [
                    {
                        "name": "Hiroki Sakamoto"
                    },
                    {
                        "name": "Kazuhiro Sato"
                    }
                ],
                "author_detail": {
                    "name": "Kazuhiro Sato"
                },
                "author": "Kazuhiro Sato",
                "arxiv_comment": "Accepted to IEEE Control Systems Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10074v1",
                "updated": "2025-07-14T09:00:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    0,
                    27,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T09:00:27Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    0,
                    27,
                    0,
                    195,
                    0
                ],
                "title": "Learning-Aided Iterative Receiver for Superimposed Pilots: Design and\n  Experimental Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-Aided Iterative Receiver for Superimposed Pilots: Design and\n  Experimental Evaluation"
                },
                "summary": "The superimposed pilot transmission scheme offers substantial potential for\nimproving spectral efficiency in MIMO-OFDM systems, but it presents significant\nchallenges for receiver design due to pilot contamination and data\ninterference. To address these issues, we propose an advanced iterative\nreceiver based on joint channel estimation, detection, and decoding, which\nrefines the receiver outputs through iterative feedback. The proposed receiver\nincorporates two adaptive channel estimation strategies to enhance robustness\nunder time-varying and mismatched channel conditions. First, a variational\nmessage passing (VMP) method and its low-complexity variant (VMP-L) are\nintroduced to perform inference without relying on time-domain correlation.\nSecond, a deep learning (DL) based estimator is developed, featuring a\nconvolutional neural network with a despreading module and an attention\nmechanism to extract and fuse relevant channel features. Extensive simulations\nunder multi-stream and high-mobility scenarios demonstrate that the proposed\nreceiver consistently outperforms conventional orthogonal pilot baselines in\nboth throughput and block error rate. Moreover, over-the-air experiments\nvalidate the practical effectiveness of the proposed design. Among the methods,\nthe DL based estimator achieves a favorable trade-off between performance and\ncomplexity, highlighting its suitability for real-world deployment in dynamic\nwireless environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The superimposed pilot transmission scheme offers substantial potential for\nimproving spectral efficiency in MIMO-OFDM systems, but it presents significant\nchallenges for receiver design due to pilot contamination and data\ninterference. To address these issues, we propose an advanced iterative\nreceiver based on joint channel estimation, detection, and decoding, which\nrefines the receiver outputs through iterative feedback. The proposed receiver\nincorporates two adaptive channel estimation strategies to enhance robustness\nunder time-varying and mismatched channel conditions. First, a variational\nmessage passing (VMP) method and its low-complexity variant (VMP-L) are\nintroduced to perform inference without relying on time-domain correlation.\nSecond, a deep learning (DL) based estimator is developed, featuring a\nconvolutional neural network with a despreading module and an attention\nmechanism to extract and fuse relevant channel features. Extensive simulations\nunder multi-stream and high-mobility scenarios demonstrate that the proposed\nreceiver consistently outperforms conventional orthogonal pilot baselines in\nboth throughput and block error rate. Moreover, over-the-air experiments\nvalidate the practical effectiveness of the proposed design. Among the methods,\nthe DL based estimator achieves a favorable trade-off between performance and\ncomplexity, highlighting its suitability for real-world deployment in dynamic\nwireless environments."
                },
                "authors": [
                    {
                        "name": "Xinjie Li"
                    },
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Yixiao Cao"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Chao-Kai Wen"
                    },
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Shi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Shi Jin"
                },
                "author": "Shi Jin",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10073v1",
                "updated": "2025-07-14T08:59:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    59,
                    26,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T08:59:26Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    59,
                    26,
                    0,
                    195,
                    0
                ],
                "title": "Cultural Bias in Large Language Models: Evaluating AI Agents through\n  Moral Questionnaires",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cultural Bias in Large Language Models: Evaluating AI Agents through\n  Moral Questionnaires"
                },
                "summary": "Are AI systems truly representing human values, or merely averaging across\nthem? Our study suggests a concerning reality: Large Language Models (LLMs)\nfail to represent diverse cultural moral frameworks despite their linguistic\ncapabilities. We expose significant gaps between AI-generated and human moral\nintuitions by applying the Moral Foundations Questionnaire across 19 cultural\ncontexts. Comparing multiple state-of-the-art LLMs' origins against human\nbaseline data, we find these models systematically homogenize moral diversity.\nSurprisingly, increased model size doesn't consistently improve cultural\nrepresentation fidelity. Our findings challenge the growing use of LLMs as\nsynthetic populations in social science research and highlight a fundamental\nlimitation in current AI alignment approaches. Without data-driven alignment\nbeyond prompting, these systems cannot capture the nuanced, culturally-specific\nmoral intuitions. Our results call for more grounded alignment objectives and\nevaluation metrics to ensure AI systems represent diverse human values rather\nthan flattening the moral landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are AI systems truly representing human values, or merely averaging across\nthem? Our study suggests a concerning reality: Large Language Models (LLMs)\nfail to represent diverse cultural moral frameworks despite their linguistic\ncapabilities. We expose significant gaps between AI-generated and human moral\nintuitions by applying the Moral Foundations Questionnaire across 19 cultural\ncontexts. Comparing multiple state-of-the-art LLMs' origins against human\nbaseline data, we find these models systematically homogenize moral diversity.\nSurprisingly, increased model size doesn't consistently improve cultural\nrepresentation fidelity. Our findings challenge the growing use of LLMs as\nsynthetic populations in social science research and highlight a fundamental\nlimitation in current AI alignment approaches. Without data-driven alignment\nbeyond prompting, these systems cannot capture the nuanced, culturally-specific\nmoral intuitions. Our results call for more grounded alignment objectives and\nevaluation metrics to ensure AI systems represent diverse human values rather\nthan flattening the moral landscape."
                },
                "authors": [
                    {
                        "name": "Simon M√ºnker"
                    }
                ],
                "author_detail": {
                    "name": "Simon M√ºnker"
                },
                "author": "Simon M√ºnker",
                "arxiv_comment": "15pages, 1 figure, 2 tables",
                "arxiv_journal_ref": "Proceedings of 0th Symposium on Moral and Legal AI Alignment of\n  the IACAP/AISB Conference, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10069v1",
                "updated": "2025-07-14T08:53:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T08:53:48Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism"
                },
                "summary": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we propose Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we propose Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs)."
                },
                "authors": [
                    {
                        "name": "Zedong Liu"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Guangming Tan"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Dingwen Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dingwen Tao"
                },
                "author": "Dingwen Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10062v1",
                "updated": "2025-07-14T08:47:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    47,
                    19,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T08:47:19Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    47,
                    19,
                    0,
                    195,
                    0
                ],
                "title": "LLMShot: Reducing snapshot testing maintenance via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMShot: Reducing snapshot testing maintenance via LLMs"
                },
                "summary": "Snapshot testing has emerged as a critical technique for UI validation in\nmodern software development, yet it suffers from substantial maintenance\noverhead due to frequent UI changes causing test failures that require manual\ninspection to distinguish between genuine regressions and intentional design\nchanges. This manual triage process becomes increasingly burdensome as\napplications evolve, creating a need for automated analysis solutions. This\npaper introduces LLMShot, a novel framework that leverages vision-based Large\nLanguage Models to automatically analyze snapshot test failures through\nhierarchical classification of UI changes. To evaluate LLMShot's effectiveness,\nwe developed a comprehensive dataset using a feature-rich iOS application with\nconfigurable feature flags, creating realistic scenarios that produce authentic\nsnapshot differences representative of real development workflows. Our\nevaluation using Gemma3 models demonstrates strong classification performance,\nwith the 12B variant achieving over 84% recall in identifying failure root\ncauses while the 4B model offers practical deployment advantages with\nacceptable performance for continuous integration environments. However, our\nexploration of selective ignore mechanisms revealed significant limitations in\ncurrent prompting-based approaches for controllable visual reasoning. LLMShot\nrepresents the first automated approach to semantic snapshot test analysis,\noffering developers structured insights that can substantially reduce manual\ntriage effort and advance toward more intelligent UI testing paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Snapshot testing has emerged as a critical technique for UI validation in\nmodern software development, yet it suffers from substantial maintenance\noverhead due to frequent UI changes causing test failures that require manual\ninspection to distinguish between genuine regressions and intentional design\nchanges. This manual triage process becomes increasingly burdensome as\napplications evolve, creating a need for automated analysis solutions. This\npaper introduces LLMShot, a novel framework that leverages vision-based Large\nLanguage Models to automatically analyze snapshot test failures through\nhierarchical classification of UI changes. To evaluate LLMShot's effectiveness,\nwe developed a comprehensive dataset using a feature-rich iOS application with\nconfigurable feature flags, creating realistic scenarios that produce authentic\nsnapshot differences representative of real development workflows. Our\nevaluation using Gemma3 models demonstrates strong classification performance,\nwith the 12B variant achieving over 84% recall in identifying failure root\ncauses while the 4B model offers practical deployment advantages with\nacceptable performance for continuous integration environments. However, our\nexploration of selective ignore mechanisms revealed significant limitations in\ncurrent prompting-based approaches for controllable visual reasoning. LLMShot\nrepresents the first automated approach to semantic snapshot test analysis,\noffering developers structured insights that can substantially reduce manual\ntriage effort and advance toward more intelligent UI testing paradigms."
                },
                "authors": [
                    {
                        "name": "Erg√ºn Batuhan Kaynak"
                    },
                    {
                        "name": "Mayasah Lami"
                    },
                    {
                        "name": "Sahand Moslemi"
                    },
                    {
                        "name": "Anil Koyuncu"
                    }
                ],
                "author_detail": {
                    "name": "Anil Koyuncu"
                },
                "author": "Anil Koyuncu",
                "arxiv_comment": "Accepted to ICSME 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10059v1",
                "updated": "2025-07-14T08:44:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    44,
                    59,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T08:44:59Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    44,
                    59,
                    0,
                    195,
                    0
                ],
                "title": "GeLaCo: An Evolutionary Approach to Layer Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeLaCo: An Evolutionary Approach to Layer Compression"
                },
                "summary": "Large Language Models (LLM) have achieved remarkable performance across a\nlarge number of tasks, but face critical deployment and usage barriers due to\nsubstantial computational requirements. Model compression methods, which aim to\nreduce model size while preserving its capacity, are an important means to\nmitigate these issues. Promising approaches along these lines, such as\nstructured pruning, typically require costly empirical search for optimal\nvariants and may run the risk of ignoring better solutions. In this work we\nintroduce GeLaCo, an evolutionary approach to LLM compression via layer\ncollapse. Our approach supports an efficient exploration of the compression\nsolution space via population-based search and a module-wise similarity fitness\nfunction capturing attention, feed-forward, and hidden state representations.\nGeLaCo also supports both single and multi-objective evolutionary compression\nsearch, establishing the first Pareto frontier along compression and quality\naxes. We evaluate GeLaCo solutions via both perplexity-based and generative\nevaluations over foundational and instruction-tuned models, outperforming\nstate-of-the-art alternatives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) have achieved remarkable performance across a\nlarge number of tasks, but face critical deployment and usage barriers due to\nsubstantial computational requirements. Model compression methods, which aim to\nreduce model size while preserving its capacity, are an important means to\nmitigate these issues. Promising approaches along these lines, such as\nstructured pruning, typically require costly empirical search for optimal\nvariants and may run the risk of ignoring better solutions. In this work we\nintroduce GeLaCo, an evolutionary approach to LLM compression via layer\ncollapse. Our approach supports an efficient exploration of the compression\nsolution space via population-based search and a module-wise similarity fitness\nfunction capturing attention, feed-forward, and hidden state representations.\nGeLaCo also supports both single and multi-objective evolutionary compression\nsearch, establishing the first Pareto frontier along compression and quality\naxes. We evaluate GeLaCo solutions via both perplexity-based and generative\nevaluations over foundational and instruction-tuned models, outperforming\nstate-of-the-art alternatives."
                },
                "authors": [
                    {
                        "name": "David Ponce"
                    },
                    {
                        "name": "Thierry Etchegoyhen"
                    },
                    {
                        "name": "Javier Del Ser"
                    }
                ],
                "author_detail": {
                    "name": "Javier Del Ser"
                },
                "author": "Javier Del Ser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10055v1",
                "updated": "2025-07-14T08:40:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    40,
                    24,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T08:40:24Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    40,
                    24,
                    0,
                    195,
                    0
                ],
                "title": "Hand Gesture Recognition for Collaborative Robots Using Lightweight Deep\n  Learning in Real-Time Robotic Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hand Gesture Recognition for Collaborative Robots Using Lightweight Deep\n  Learning in Real-Time Robotic Systems"
                },
                "summary": "Direct and natural interaction is essential for intuitive human-robot\ncollaboration, eliminating the need for additional devices such as joysticks,\ntablets, or wearable sensors. In this paper, we present a lightweight deep\nlearning-based hand gesture recognition system that enables humans to control\ncollaborative robots naturally and efficiently. This model recognizes eight\ndistinct hand gestures with only 1,103 parameters and a compact size of 22 KB,\nachieving an accuracy of 93.5%. To further optimize the model for real-world\ndeployment on edge devices, we applied quantization and pruning using\nTensorFlow Lite, reducing the final model size to just 7 KB. The system was\nsuccessfully implemented and tested on a Universal Robot UR5 collaborative\nrobot within a real-time robotic framework based on ROS2. The results\ndemonstrate that even extremely lightweight models can deliver accurate and\nresponsive hand gesture-based control for collaborative robots, opening new\npossibilities for natural human-robot interaction in constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct and natural interaction is essential for intuitive human-robot\ncollaboration, eliminating the need for additional devices such as joysticks,\ntablets, or wearable sensors. In this paper, we present a lightweight deep\nlearning-based hand gesture recognition system that enables humans to control\ncollaborative robots naturally and efficiently. This model recognizes eight\ndistinct hand gestures with only 1,103 parameters and a compact size of 22 KB,\nachieving an accuracy of 93.5%. To further optimize the model for real-world\ndeployment on edge devices, we applied quantization and pruning using\nTensorFlow Lite, reducing the final model size to just 7 KB. The system was\nsuccessfully implemented and tested on a Universal Robot UR5 collaborative\nrobot within a real-time robotic framework based on ROS2. The results\ndemonstrate that even extremely lightweight models can deliver accurate and\nresponsive hand gesture-based control for collaborative robots, opening new\npossibilities for natural human-robot interaction in constrained environments."
                },
                "authors": [
                    {
                        "name": "Muhtadin"
                    },
                    {
                        "name": "I Wayan Agus Darmawan"
                    },
                    {
                        "name": "Muhammad Hilmi Rusydiansyah"
                    },
                    {
                        "name": "I Ketut Eddy Purnama"
                    },
                    {
                        "name": "Chastine Fatichah"
                    },
                    {
                        "name": "Mauridhi Hery Purnomo"
                    }
                ],
                "author_detail": {
                    "name": "Mauridhi Hery Purnomo"
                },
                "author": "Mauridhi Hery Purnomo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10054v1",
                "updated": "2025-07-14T08:36:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    36,
                    26,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T08:36:26Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    36,
                    26,
                    0,
                    195,
                    0
                ],
                "title": "Explicit Vulnerability Generation with LLMs: An Investigation Beyond\n  Adversarial Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explicit Vulnerability Generation with LLMs: An Investigation Beyond\n  Adversarial Attacks"
                },
                "summary": "Large Language Models (LLMs) are increasingly used as code assistants, yet\ntheir behavior when explicitly asked to generate insecure code remains poorly\nunderstood. While prior research has focused on unintended vulnerabilities or\nadversarial prompting techniques, this study examines a more direct threat\nscenario: open-source LLMs generating vulnerable code when prompted either\ndirectly or indirectly. We propose a dual experimental design: (1) Dynamic\nPrompting, which systematically varies vulnerability type, user persona, and\ndirectness across structured templates; and (2) Reverse Prompting, which\nderives prompts from real vulnerable code samples to assess vulnerability\nreproduction accuracy. We evaluate three open-source 7B-parameter models\n(Qwen2, Mistral, and Gemma) using ESBMC static analysis to assess both the\npresence of vulnerabilities and the correctness of the generated vulnerability\ntype. Results show all models frequently produce vulnerable outputs, with Qwen2\nachieving highest correctness rates. User persona significantly affects\nsuccess, where student personas achieved higher vulnerability rates than\nprofessional roles, while direct prompts were marginally more effective.\nVulnerability reproduction followed an inverted-U pattern with cyclomatic\ncomplexity, peaking at moderate ranges. Our findings expose limitations of\nsafety mechanisms in open-source models, particularly for seemingly benign\neducational requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used as code assistants, yet\ntheir behavior when explicitly asked to generate insecure code remains poorly\nunderstood. While prior research has focused on unintended vulnerabilities or\nadversarial prompting techniques, this study examines a more direct threat\nscenario: open-source LLMs generating vulnerable code when prompted either\ndirectly or indirectly. We propose a dual experimental design: (1) Dynamic\nPrompting, which systematically varies vulnerability type, user persona, and\ndirectness across structured templates; and (2) Reverse Prompting, which\nderives prompts from real vulnerable code samples to assess vulnerability\nreproduction accuracy. We evaluate three open-source 7B-parameter models\n(Qwen2, Mistral, and Gemma) using ESBMC static analysis to assess both the\npresence of vulnerabilities and the correctness of the generated vulnerability\ntype. Results show all models frequently produce vulnerable outputs, with Qwen2\nachieving highest correctness rates. User persona significantly affects\nsuccess, where student personas achieved higher vulnerability rates than\nprofessional roles, while direct prompts were marginally more effective.\nVulnerability reproduction followed an inverted-U pattern with cyclomatic\ncomplexity, peaking at moderate ranges. Our findings expose limitations of\nsafety mechanisms in open-source models, particularly for seemingly benign\neducational requests."
                },
                "authors": [
                    {
                        "name": "Emir Bosnak"
                    },
                    {
                        "name": "Sahand Moslemi"
                    },
                    {
                        "name": "Mayasah Lami"
                    },
                    {
                        "name": "Anil Koyuncu"
                    }
                ],
                "author_detail": {
                    "name": "Anil Koyuncu"
                },
                "author": "Anil Koyuncu",
                "arxiv_comment": "Accepted to ICSME 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11415v2",
                "updated": "2025-07-14T08:34:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    34,
                    57,
                    0,
                    195,
                    0
                ],
                "published": "2024-08-21T08:20:41Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    8,
                    20,
                    41,
                    2,
                    234,
                    0
                ],
                "title": "Political Bias in LLMs: Unaligned Moral Values in Agent-centric\n  Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Political Bias in LLMs: Unaligned Moral Values in Agent-centric\n  Simulations"
                },
                "summary": "Contemporary research in social sciences increasingly utilizes\nstate-of-the-art generative language models to annotate or generate content.\nWhile these models achieve benchmark-leading performance on common language\ntasks, their application to novel out-of-domain tasks remains insufficiently\nexplored. To address this gap, we investigate how personalized language models\nalign with human responses on the Moral Foundation Theory Questionnaire. We\nadapt open-source generative language models to different political personas\nand repeatedly survey these models to generate synthetic data sets where\nmodel-persona combinations define our sub-populations. Our analysis reveals\nthat models produce inconsistent results across multiple repetitions, yielding\nhigh response variance. Furthermore, the alignment between synthetic data and\ncorresponding human data from psychological studies shows a weak correlation,\nwith conservative persona-prompted models particularly failing to align with\nactual conservative populations. These results suggest that language models\nstruggle to coherently represent ideologies through in-context prompting due to\ntheir alignment process. Thus, using language models to simulate social\ninteractions requires measurable improvements in in-context optimization or\nparameter manipulation to align with psychological and sociological stereotypes\nproperly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary research in social sciences increasingly utilizes\nstate-of-the-art generative language models to annotate or generate content.\nWhile these models achieve benchmark-leading performance on common language\ntasks, their application to novel out-of-domain tasks remains insufficiently\nexplored. To address this gap, we investigate how personalized language models\nalign with human responses on the Moral Foundation Theory Questionnaire. We\nadapt open-source generative language models to different political personas\nand repeatedly survey these models to generate synthetic data sets where\nmodel-persona combinations define our sub-populations. Our analysis reveals\nthat models produce inconsistent results across multiple repetitions, yielding\nhigh response variance. Furthermore, the alignment between synthetic data and\ncorresponding human data from psychological studies shows a weak correlation,\nwith conservative persona-prompted models particularly failing to align with\nactual conservative populations. These results suggest that language models\nstruggle to coherently represent ideologies through in-context prompting due to\ntheir alignment process. Thus, using language models to simulate social\ninteractions requires measurable improvements in in-context optimization or\nparameter manipulation to align with psychological and sociological stereotypes\nproperly."
                },
                "authors": [
                    {
                        "name": "Simon M√ºnker"
                    }
                ],
                "author_detail": {
                    "name": "Simon M√ºnker"
                },
                "author": "Simon M√ºnker",
                "arxiv_doi": "10.21248/jlcl.38.2025.289",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.21248/jlcl.38.2025.289",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.11415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 2 tables",
                "arxiv_journal_ref": "JLCL 2025, Band 38(2)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15902v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15902v2",
                "updated": "2025-07-14T08:34:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    34,
                    3,
                    0,
                    195,
                    0
                ],
                "published": "2025-02-21T19:41:32Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    19,
                    41,
                    32,
                    4,
                    52,
                    0
                ],
                "title": "IPAD: Inverse Prompt for AI Detection -- A Robust and Explainable\n  LLM-Generated Text Detector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IPAD: Inverse Prompt for AI Detection -- A Robust and Explainable\n  LLM-Generated Text Detector"
                },
                "summary": "Large Language Models (LLMs) have attained human-level fluency in text\ngeneration, which complicates the distinction between human-written and\nLLM-generated texts. This increases the risk of misuse and highlights the need\nfor reliable detectors. Yet, existing detectors exhibit poor robustness on\nout-of-distribution (OOD) data and attacked data, which is critical for\nreal-world scenarios. Also, they struggle to provide interpretable evidence to\nsupport their decisions, thus undermining the reliability. In light of these\nchallenges, we propose IPAD (Inverse Prompt for AI Detection), a novel\nframework consisting of a Prompt Inverter that identifies predicted prompts\nthat could have generated the input text, and two Distinguishers that examine\nthe probability that the input texts align with the predicted prompts.\nEmpirical evaluations demonstrate that IPAD outperforms the strongest baselines\nby 9.05% (Average Recall) on in-distribution data, 12.93% (AUROC) on\nout-of-distribution (OOD) data, and 5.48% (AUROC) on attacked data. IPAD also\nperforms robustly on structured datasets. Furthermore, an interpretability\nassessment is conducted to illustrate that IPAD enhances the AI detection\ntrustworthiness by allowing users to directly examine the decision-making\nevidence, which provides interpretable support for its state-of-the-art\ndetection results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have attained human-level fluency in text\ngeneration, which complicates the distinction between human-written and\nLLM-generated texts. This increases the risk of misuse and highlights the need\nfor reliable detectors. Yet, existing detectors exhibit poor robustness on\nout-of-distribution (OOD) data and attacked data, which is critical for\nreal-world scenarios. Also, they struggle to provide interpretable evidence to\nsupport their decisions, thus undermining the reliability. In light of these\nchallenges, we propose IPAD (Inverse Prompt for AI Detection), a novel\nframework consisting of a Prompt Inverter that identifies predicted prompts\nthat could have generated the input text, and two Distinguishers that examine\nthe probability that the input texts align with the predicted prompts.\nEmpirical evaluations demonstrate that IPAD outperforms the strongest baselines\nby 9.05% (Average Recall) on in-distribution data, 12.93% (AUROC) on\nout-of-distribution (OOD) data, and 5.48% (AUROC) on attacked data. IPAD also\nperforms robustly on structured datasets. Furthermore, an interpretability\nassessment is conducted to illustrate that IPAD enhances the AI detection\ntrustworthiness by allowing users to directly examine the decision-making\nevidence, which provides interpretable support for its state-of-the-art\ndetection results."
                },
                "authors": [
                    {
                        "name": "Zheng Chen"
                    },
                    {
                        "name": "Yushi Feng"
                    },
                    {
                        "name": "Changyang He"
                    },
                    {
                        "name": "Yue Deng"
                    },
                    {
                        "name": "Hongxi Pu"
                    },
                    {
                        "name": "Bo Li"
                    }
                ],
                "author_detail": {
                    "name": "Bo Li"
                },
                "author": "Bo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15902v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15902v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08505v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08505v2",
                "updated": "2025-07-14T08:25:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    25,
                    14,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-11T11:30:57Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    11,
                    30,
                    57,
                    4,
                    192,
                    0
                ],
                "title": "Efficient Deployment of Vision-Language Models on Mobile Devices: A Case\n  Study on OnePlus 13R",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Deployment of Vision-Language Models on Mobile Devices: A Case\n  Study on OnePlus 13R"
                },
                "summary": "Vision-Language Models (VLMs) offer promising capabilities for mobile\ndevices, but their deployment faces significant challenges due to computational\nlimitations and energy inefficiency, especially for real-time applications.\nThis study provides a comprehensive survey of deployment frameworks for VLMs on\nmobile devices, evaluating llama.cpp, MLC-Imp, and mllm in the context of\nrunning LLaVA-1.5 7B, MobileVLM-3B, and Imp-v1.5 3B as representative workloads\non a OnePlus 13R. Each deployment framework was evaluated on the OnePlus 13R\nwhile running VLMs, with measurements covering CPU, GPU, and NPU utilization,\ntemperature, inference time, power consumption, and user experience.\nBenchmarking revealed critical performance bottlenecks across frameworks: CPU\nresources were consistently over-utilized during token generation, while GPU\nand NPU accelerators were largely unused. When the GPU was used, primarily for\nimage feature extraction, it was saturated, leading to degraded device\nresponsiveness. The study contributes framework-level benchmarks, practical\nprofiling tools, and an in-depth analysis of hardware utilization bottlenecks,\nhighlighting the consistent overuse of CPUs and the ineffective or unstable use\nof GPUs and NPUs in current deployment frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) offer promising capabilities for mobile\ndevices, but their deployment faces significant challenges due to computational\nlimitations and energy inefficiency, especially for real-time applications.\nThis study provides a comprehensive survey of deployment frameworks for VLMs on\nmobile devices, evaluating llama.cpp, MLC-Imp, and mllm in the context of\nrunning LLaVA-1.5 7B, MobileVLM-3B, and Imp-v1.5 3B as representative workloads\non a OnePlus 13R. Each deployment framework was evaluated on the OnePlus 13R\nwhile running VLMs, with measurements covering CPU, GPU, and NPU utilization,\ntemperature, inference time, power consumption, and user experience.\nBenchmarking revealed critical performance bottlenecks across frameworks: CPU\nresources were consistently over-utilized during token generation, while GPU\nand NPU accelerators were largely unused. When the GPU was used, primarily for\nimage feature extraction, it was saturated, leading to degraded device\nresponsiveness. The study contributes framework-level benchmarks, practical\nprofiling tools, and an in-depth analysis of hardware utilization bottlenecks,\nhighlighting the consistent overuse of CPUs and the ineffective or unstable use\nof GPUs and NPUs in current deployment frameworks."
                },
                "authors": [
                    {
                        "name": "Pablo Robin Guerrero"
                    },
                    {
                        "name": "Yueyang Pan"
                    },
                    {
                        "name": "Sanidhya Kashyap"
                    }
                ],
                "author_detail": {
                    "name": "Sanidhya Kashyap"
                },
                "author": "Sanidhya Kashyap",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08505v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08505v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10045v1",
                "updated": "2025-07-14T08:23:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    23,
                    25,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T08:23:25Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    23,
                    25,
                    0,
                    195,
                    0
                ],
                "title": "Automating SPARQL Query Translations between DBpedia and Wikidata",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating SPARQL Query Translations between DBpedia and Wikidata"
                },
                "summary": "This paper investigates whether state-of-the-art Large Language Models (LLMs)\ncan automatically translate SPARQL between popular Knowledge Graph (KG)\nschemas. We focus on translations between the DBpedia and Wikidata KG, and\nlater on DBLP and OpenAlex KG. This study addresses a notable gap in KG\ninteroperability research by rigorously evaluating LLM performance on\nSPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first\nalign 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100\nDBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic\nKGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and\nMistral-Large-Instruct-2407 are selected based on their sizes and architectures\nand tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs\nwere compared with gold answers, and resulting errors were categorized. We find\nthat the performance varies markedly across models and prompting strategies,\nand that translations for Wikidata to DBpedia work far better than translations\nfor DBpedia to Wikidata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates whether state-of-the-art Large Language Models (LLMs)\ncan automatically translate SPARQL between popular Knowledge Graph (KG)\nschemas. We focus on translations between the DBpedia and Wikidata KG, and\nlater on DBLP and OpenAlex KG. This study addresses a notable gap in KG\ninteroperability research by rigorously evaluating LLM performance on\nSPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first\nalign 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100\nDBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic\nKGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and\nMistral-Large-Instruct-2407 are selected based on their sizes and architectures\nand tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs\nwere compared with gold answers, and resulting errors were categorized. We find\nthat the performance varies markedly across models and prompting strategies,\nand that translations for Wikidata to DBpedia work far better than translations\nfor DBpedia to Wikidata."
                },
                "authors": [
                    {
                        "name": "Malte Christian Bartels"
                    },
                    {
                        "name": "Debayan Banerjee"
                    },
                    {
                        "name": "Ricardo Usbeck"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Usbeck"
                },
                "author": "Ricardo Usbeck",
                "arxiv_comment": "18 pages, 2 figues. Paper accepted at SEMANTiCS 2025 conference\n  happening on September 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10039v1",
                "updated": "2025-07-14T08:16:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    16,
                    58,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T08:16:58Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    16,
                    58,
                    0,
                    195,
                    0
                ],
                "title": "Towards Applying Large Language Models to Complement Single-Cell\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Applying Large Language Models to Complement Single-Cell\n  Foundation Models"
                },
                "summary": "Single-cell foundation models such as scGPT represent a significant\nadvancement in single-cell omics, with an ability to achieve state-of-the-art\nperformance on various downstream biological tasks. However, these models are\ninherently limited in that a vast amount of information in biology exists as\ntext, which they are unable to leverage. There have therefore been several\nrecent works that propose the use of LLMs as an alternative to single-cell\nfoundation models, achieving competitive results. However, there is little\nunderstanding of what factors drive this performance, along with a strong focus\non using LLMs as an alternative, rather than complementary approach to\nsingle-cell foundation models. In this study, we therefore investigate what\nbiological insights contribute toward the performance of LLMs when applied to\nsingle-cell data, and introduce scMPT; a model which leverages synergies\nbetween scGPT, and single-cell representations from LLMs that capture these\ninsights. scMPT demonstrates stronger, more consistent performance than either\nof its component models, which frequently have large performance gaps between\neach other across datasets. We also experiment with alternate fusion methods,\ndemonstrating the potential of combining specialized reasoning models with\nscGPT to improve performance. This study ultimately showcases the potential for\nLLMs to complement single-cell foundation models and drive improvements in\nsingle-cell analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single-cell foundation models such as scGPT represent a significant\nadvancement in single-cell omics, with an ability to achieve state-of-the-art\nperformance on various downstream biological tasks. However, these models are\ninherently limited in that a vast amount of information in biology exists as\ntext, which they are unable to leverage. There have therefore been several\nrecent works that propose the use of LLMs as an alternative to single-cell\nfoundation models, achieving competitive results. However, there is little\nunderstanding of what factors drive this performance, along with a strong focus\non using LLMs as an alternative, rather than complementary approach to\nsingle-cell foundation models. In this study, we therefore investigate what\nbiological insights contribute toward the performance of LLMs when applied to\nsingle-cell data, and introduce scMPT; a model which leverages synergies\nbetween scGPT, and single-cell representations from LLMs that capture these\ninsights. scMPT demonstrates stronger, more consistent performance than either\nof its component models, which frequently have large performance gaps between\neach other across datasets. We also experiment with alternate fusion methods,\ndemonstrating the potential of combining specialized reasoning models with\nscGPT to improve performance. This study ultimately showcases the potential for\nLLMs to complement single-cell foundation models and drive improvements in\nsingle-cell analysis."
                },
                "authors": [
                    {
                        "name": "Steven Palayew"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Gary Bader"
                    }
                ],
                "author_detail": {
                    "name": "Gary Bader"
                },
                "author": "Gary Bader",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10034v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10034v1",
                "updated": "2025-07-14T08:13:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    13,
                    33,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T08:13:33Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    13,
                    33,
                    0,
                    195,
                    0
                ],
                "title": "LifelongPR: Lifelong knowledge fusion for point cloud place recognition\n  based on replay and prompt learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LifelongPR: Lifelong knowledge fusion for point cloud place recognition\n  based on replay and prompt learning"
                },
                "summary": "Point cloud place recognition (PCPR) plays a crucial role in photogrammetry\nand robotics applications such as autonomous driving, intelligent\ntransportation, and augmented reality. In real-world large-scale deployments of\na positioning system, PCPR models must continuously acquire, update, and\naccumulate knowledge to adapt to diverse and dynamic environments, i.e., the\nability known as continual learning (CL). However, existing PCPR models often\nsuffer from catastrophic forgetting, leading to significant performance\ndegradation in previously learned scenes when adapting to new environments or\nsensor types. This results in poor model scalability, increased maintenance\ncosts, and system deployment difficulties, undermining the practicality of\nPCPR. To address these issues, we propose LifelongPR, a novel continual\nlearning framework for PCPR, which effectively extracts and fuses knowledge\nfrom sequential point cloud data. First, to alleviate the knowledge loss, we\npropose a replay sample selection method that dynamically allocates sample\nsizes according to each dataset's information quantity and selects spatially\ndiverse samples for maximal representativeness. Second, to handle domain\nshifts, we design a prompt learning-based CL framework with a lightweight\nprompt module and a two-stage training strategy, enabling domain-specific\nfeature adaptation while minimizing forgetting. Comprehensive experiments on\nlarge-scale public and self-collected datasets are conducted to validate the\neffectiveness of the proposed method. Compared with state-of-the-art (SOTA)\nmethods, our method achieves 6.50% improvement in mIR@1, 7.96% improvement in\nmR@1, and an 8.95% reduction in F. The code and pre-trained models are publicly\navailable at https://github.com/zouxianghong/LifelongPR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point cloud place recognition (PCPR) plays a crucial role in photogrammetry\nand robotics applications such as autonomous driving, intelligent\ntransportation, and augmented reality. In real-world large-scale deployments of\na positioning system, PCPR models must continuously acquire, update, and\naccumulate knowledge to adapt to diverse and dynamic environments, i.e., the\nability known as continual learning (CL). However, existing PCPR models often\nsuffer from catastrophic forgetting, leading to significant performance\ndegradation in previously learned scenes when adapting to new environments or\nsensor types. This results in poor model scalability, increased maintenance\ncosts, and system deployment difficulties, undermining the practicality of\nPCPR. To address these issues, we propose LifelongPR, a novel continual\nlearning framework for PCPR, which effectively extracts and fuses knowledge\nfrom sequential point cloud data. First, to alleviate the knowledge loss, we\npropose a replay sample selection method that dynamically allocates sample\nsizes according to each dataset's information quantity and selects spatially\ndiverse samples for maximal representativeness. Second, to handle domain\nshifts, we design a prompt learning-based CL framework with a lightweight\nprompt module and a two-stage training strategy, enabling domain-specific\nfeature adaptation while minimizing forgetting. Comprehensive experiments on\nlarge-scale public and self-collected datasets are conducted to validate the\neffectiveness of the proposed method. Compared with state-of-the-art (SOTA)\nmethods, our method achieves 6.50% improvement in mIR@1, 7.96% improvement in\nmR@1, and an 8.95% reduction in F. The code and pre-trained models are publicly\navailable at https://github.com/zouxianghong/LifelongPR."
                },
                "authors": [
                    {
                        "name": "Xianghong Zou"
                    },
                    {
                        "name": "Jianping Li"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Zhen Cao"
                    },
                    {
                        "name": "Zhen Dong"
                    },
                    {
                        "name": "Qiegen Liu"
                    },
                    {
                        "name": "Bisheng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Bisheng Yang"
                },
                "author": "Bisheng Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10034v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10034v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10024v1",
                "updated": "2025-07-14T08:06:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    6,
                    12,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T08:06:12Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    6,
                    12,
                    0,
                    195,
                    0
                ],
                "title": "Qualitative Study for LLM-assisted Design Study Process: Strategies,\n  Challenges, and Roles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qualitative Study for LLM-assisted Design Study Process: Strategies,\n  Challenges, and Roles"
                },
                "summary": "Design studies aim to create visualization solutions for real-world problems\nof different application domains. Recently, the emergence of large language\nmodels (LLMs) has introduced new opportunities to enhance the design study\nprocess, providing capabilities such as creative problem-solving, data\nhandling, and insightful analysis. However, despite their growing popularity,\nthere remains a lack of systematic understanding of how LLMs can effectively\nassist researchers in visualization-specific design studies. In this paper, we\nconducted a multi-stage qualitative study to fill this gap, involving 30 design\nstudy researchers from diverse backgrounds and expertise levels. Through\nin-depth interviews and carefully-designed questionnaires, we investigated\nstrategies for utilizing LLMs, the challenges encountered, and the practices\nused to overcome them. We further compiled and summarized the roles that LLMs\ncan play across different stages of the design study process. Our findings\nhighlight practical implications to inform visualization practitioners, and\nprovide a framework for leveraging LLMs to enhance the design study process in\nvisualization research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design studies aim to create visualization solutions for real-world problems\nof different application domains. Recently, the emergence of large language\nmodels (LLMs) has introduced new opportunities to enhance the design study\nprocess, providing capabilities such as creative problem-solving, data\nhandling, and insightful analysis. However, despite their growing popularity,\nthere remains a lack of systematic understanding of how LLMs can effectively\nassist researchers in visualization-specific design studies. In this paper, we\nconducted a multi-stage qualitative study to fill this gap, involving 30 design\nstudy researchers from diverse backgrounds and expertise levels. Through\nin-depth interviews and carefully-designed questionnaires, we investigated\nstrategies for utilizing LLMs, the challenges encountered, and the practices\nused to overcome them. We further compiled and summarized the roles that LLMs\ncan play across different stages of the design study process. Our findings\nhighlight practical implications to inform visualization practitioners, and\nprovide a framework for leveraging LLMs to enhance the design study process in\nvisualization research."
                },
                "authors": [
                    {
                        "name": "Shaolun Ruan"
                    },
                    {
                        "name": "Rui Sheng"
                    },
                    {
                        "name": "Xiaolin Wen"
                    },
                    {
                        "name": "Jiachen Wang"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Yong Wang"
                    },
                    {
                        "name": "Tim Dwyer"
                    },
                    {
                        "name": "Jiannan Li"
                    }
                ],
                "author_detail": {
                    "name": "Jiannan Li"
                },
                "author": "Jiannan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08824v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08824v4",
                "updated": "2025-07-14T07:54:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    54,
                    49,
                    0,
                    195,
                    0
                ],
                "published": "2024-09-13T13:37:33Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    13,
                    37,
                    33,
                    4,
                    257,
                    0
                ],
                "title": "Pathfinder for Low-altitude Aircraft with Binary Neural Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pathfinder for Low-altitude Aircraft with Binary Neural Network"
                },
                "summary": "A prior global topological map (e.g., the OpenStreetMap, OSM) can boost the\nperformance of autonomous mapping by a ground mobile robot. However, the prior\nmap is usually incomplete due to lacking labeling in partial paths. To solve\nthis problem, this paper proposes an OSM maker using airborne sensors carried\nby low-altitude aircraft, where the core of the OSM maker is a novel efficient\npathfinder approach based on LiDAR and camera data, i.e., a binary dual-stream\nroad segmentation model. Specifically, a multi-scale feature extraction based\non the UNet architecture is implemented for images and point clouds. To reduce\nthe effect caused by the sparsity of point cloud, an attention-guided gated\nblock is designed to integrate image and point-cloud features. To optimize the\nmodel for edge deployment that significantly reduces storage footprint and\ncomputational demands, we propose a binarization streamline to each model\ncomponent, including a variant of vision transformer (ViT) architecture as the\nencoder of the image branch, and new focal and perception losses to optimize\nthe model training. The experimental results on two datasets demonstrate that\nour pathfinder method achieves SOTA accuracy with high efficiency in finding\npaths from the low-level airborne sensors, and we can create complete OSM prior\nmaps based on the segmented road skeletons. Code and data are available at:\n\\href{https://github.com/IMRL/Pathfinder}{https://github.com/IMRL/Pathfinder}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A prior global topological map (e.g., the OpenStreetMap, OSM) can boost the\nperformance of autonomous mapping by a ground mobile robot. However, the prior\nmap is usually incomplete due to lacking labeling in partial paths. To solve\nthis problem, this paper proposes an OSM maker using airborne sensors carried\nby low-altitude aircraft, where the core of the OSM maker is a novel efficient\npathfinder approach based on LiDAR and camera data, i.e., a binary dual-stream\nroad segmentation model. Specifically, a multi-scale feature extraction based\non the UNet architecture is implemented for images and point clouds. To reduce\nthe effect caused by the sparsity of point cloud, an attention-guided gated\nblock is designed to integrate image and point-cloud features. To optimize the\nmodel for edge deployment that significantly reduces storage footprint and\ncomputational demands, we propose a binarization streamline to each model\ncomponent, including a variant of vision transformer (ViT) architecture as the\nencoder of the image branch, and new focal and perception losses to optimize\nthe model training. The experimental results on two datasets demonstrate that\nour pathfinder method achieves SOTA accuracy with high efficiency in finding\npaths from the low-level airborne sensors, and we can create complete OSM prior\nmaps based on the segmented road skeletons. Code and data are available at:\n\\href{https://github.com/IMRL/Pathfinder}{https://github.com/IMRL/Pathfinder}."
                },
                "authors": [
                    {
                        "name": "Kaijie Yin"
                    },
                    {
                        "name": "Tian Gao"
                    },
                    {
                        "name": "Hui Kong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Kong"
                },
                "author": "Hui Kong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08824v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08824v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10016v1",
                "updated": "2025-07-14T07:51:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    51,
                    56,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T07:51:56Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    51,
                    56,
                    0,
                    195,
                    0
                ],
                "title": "The Man Behind the Sound: Demystifying Audio Private Attribute Profiling\n  via Multimodal Large Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Man Behind the Sound: Demystifying Audio Private Attribute Profiling\n  via Multimodal Large Language Model Agents"
                },
                "summary": "Our research uncovers a novel privacy risk associated with multimodal large\nlanguage models (MLLMs): the ability to infer sensitive personal attributes\nfrom audio data -- a technique we term audio private attribute profiling. This\ncapability poses a significant threat, as audio can be covertly captured\nwithout direct interaction or visibility. Moreover, compared to images and\ntext, audio carries unique characteristics, such as tone and pitch, which can\nbe exploited for more detailed profiling. However, two key challenges exist in\nunderstanding MLLM-employed private attribute profiling from audio: (1) the\nlack of audio benchmark datasets with sensitive attribute annotations and (2)\nthe limited ability of current MLLMs to infer such attributes directly from\naudio. To address these challenges, we introduce AP^2, an audio benchmark\ndataset that consists of two subsets collected and composed from real-world\ndata, and both are annotated with sensitive attribute labels. Additionally, we\npropose Gifts, a hybrid multi-agent framework that leverages the complementary\nstrengths of audio-language models (ALMs) and large language models (LLMs) to\nenhance inference capabilities. Gifts employs an LLM to guide the ALM in\ninferring sensitive attributes, then forensically analyzes and consolidates the\nALM's inferences, overcoming severe hallucinations of existing ALMs in\ngenerating long-context responses. Our evaluations demonstrate that Gifts\nsignificantly outperforms baseline approaches in inferring sensitive\nattributes. Finally, we investigate model-level and data-level defense\nstrategies to mitigate the risks of audio private attribute profiling. Our work\nvalidates the feasibility of audio-based privacy attacks using MLLMs,\nhighlighting the need for robust defenses, and provides a dataset and framework\nto facilitate future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our research uncovers a novel privacy risk associated with multimodal large\nlanguage models (MLLMs): the ability to infer sensitive personal attributes\nfrom audio data -- a technique we term audio private attribute profiling. This\ncapability poses a significant threat, as audio can be covertly captured\nwithout direct interaction or visibility. Moreover, compared to images and\ntext, audio carries unique characteristics, such as tone and pitch, which can\nbe exploited for more detailed profiling. However, two key challenges exist in\nunderstanding MLLM-employed private attribute profiling from audio: (1) the\nlack of audio benchmark datasets with sensitive attribute annotations and (2)\nthe limited ability of current MLLMs to infer such attributes directly from\naudio. To address these challenges, we introduce AP^2, an audio benchmark\ndataset that consists of two subsets collected and composed from real-world\ndata, and both are annotated with sensitive attribute labels. Additionally, we\npropose Gifts, a hybrid multi-agent framework that leverages the complementary\nstrengths of audio-language models (ALMs) and large language models (LLMs) to\nenhance inference capabilities. Gifts employs an LLM to guide the ALM in\ninferring sensitive attributes, then forensically analyzes and consolidates the\nALM's inferences, overcoming severe hallucinations of existing ALMs in\ngenerating long-context responses. Our evaluations demonstrate that Gifts\nsignificantly outperforms baseline approaches in inferring sensitive\nattributes. Finally, we investigate model-level and data-level defense\nstrategies to mitigate the risks of audio private attribute profiling. Our work\nvalidates the feasibility of audio-based privacy attacks using MLLMs,\nhighlighting the need for robust defenses, and provides a dataset and framework\nto facilitate future research."
                },
                "authors": [
                    {
                        "name": "Lixu Wang"
                    },
                    {
                        "name": "Kaixiang Yao"
                    },
                    {
                        "name": "Xinfeng Li"
                    },
                    {
                        "name": "Dong Yang"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Wei Dong"
                    }
                ],
                "author_detail": {
                    "name": "Wei Dong"
                },
                "author": "Wei Dong",
                "arxiv_comment": "22 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06507v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06507v2",
                "updated": "2025-07-14T07:46:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    46,
                    11,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-09T03:13:08Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    3,
                    13,
                    8,
                    2,
                    190,
                    0
                ],
                "title": "GR-LLMs: Recent Advances in Generative Recommendation Based on Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GR-LLMs: Recent Advances in Generative Recommendation Based on Large\n  Language Models"
                },
                "summary": "In the past year, Generative Recommendations (GRs) have undergone substantial\nadvancements, especially in leveraging the powerful sequence modeling and\nreasoning capabilities of Large Language Models (LLMs) to enhance overall\nrecommendation performance. LLM-based GRs are forming a new paradigm that is\ndistinctly different from discriminative recommendations, showing strong\npotential to replace traditional recommendation systems heavily dependent on\ncomplex hand-crafted features. In this paper, we provide a comprehensive survey\naimed at facilitating further research of LLM-based GRs. Initially, we outline\nthe general preliminaries and application cases of LLM-based GRs. Subsequently,\nwe introduce the main considerations when LLM-based GRs are applied in real\nindustrial scenarios. Finally, we explore promising directions for LLM-based\nGRs. We hope that this survey contributes to the ongoing advancement of the GR\ndomain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the past year, Generative Recommendations (GRs) have undergone substantial\nadvancements, especially in leveraging the powerful sequence modeling and\nreasoning capabilities of Large Language Models (LLMs) to enhance overall\nrecommendation performance. LLM-based GRs are forming a new paradigm that is\ndistinctly different from discriminative recommendations, showing strong\npotential to replace traditional recommendation systems heavily dependent on\ncomplex hand-crafted features. In this paper, we provide a comprehensive survey\naimed at facilitating further research of LLM-based GRs. Initially, we outline\nthe general preliminaries and application cases of LLM-based GRs. Subsequently,\nwe introduce the main considerations when LLM-based GRs are applied in real\nindustrial scenarios. Finally, we explore promising directions for LLM-based\nGRs. We hope that this survey contributes to the ongoing advancement of the GR\ndomain."
                },
                "authors": [
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Haitao Lin"
                    },
                    {
                        "name": "Jiawei xue"
                    },
                    {
                        "name": "Ziji Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ziji Zhang"
                },
                "author": "Ziji Zhang",
                "arxiv_comment": "8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06507v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06507v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10007v1",
                "updated": "2025-07-14T07:41:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    41,
                    35,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T07:41:35Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    41,
                    35,
                    0,
                    195,
                    0
                ],
                "title": "Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning"
                },
                "summary": "Chain of Thought (CoT) reasoning has demonstrated remarkable deep reasoning\ncapabilities in both large language models (LLMs) and multimodal large language\nmodels (MLLMs). However, its reliability is often undermined by the\naccumulation of errors in intermediate steps. This paper introduces an novel\napproach to calibrate the CoT reasoning accuracy by leveraging the model's\nintrinsic veracity encoding. We discover that specific attention head\nactivations reliably reflect the truthfulness of reasoning steps in CoT. Based\non this insight, we train a confidence predictor to evaluate the correctness of\neach reasoning step using these truthfulness-sensitive activations, dynamically\nselecting the most plausible reasoning path via beam search. Experimental\nresults demonstrate that our method significantly outperforms the\nstate-of-the-art baselines (e.g., Few-Shot CoT, Self-Consistency, and\nSelf-Evaluation Guided Beam Search) across the mathematical, symbolic, and\ncommonsense reasoning tasks, exhibiting superior accuracy and reliability in\nboth unimodal and multimodal settings. We further validate the approach on\nlarge reasoning models, confirming its applicability to specialized reasoning\nmodels. Additionally, we explore the role of the model's self-correction\nability in CoT reasoning. This work provides a novel reliability improvement\npath for CoT reasoning with broad application potential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) reasoning has demonstrated remarkable deep reasoning\ncapabilities in both large language models (LLMs) and multimodal large language\nmodels (MLLMs). However, its reliability is often undermined by the\naccumulation of errors in intermediate steps. This paper introduces an novel\napproach to calibrate the CoT reasoning accuracy by leveraging the model's\nintrinsic veracity encoding. We discover that specific attention head\nactivations reliably reflect the truthfulness of reasoning steps in CoT. Based\non this insight, we train a confidence predictor to evaluate the correctness of\neach reasoning step using these truthfulness-sensitive activations, dynamically\nselecting the most plausible reasoning path via beam search. Experimental\nresults demonstrate that our method significantly outperforms the\nstate-of-the-art baselines (e.g., Few-Shot CoT, Self-Consistency, and\nSelf-Evaluation Guided Beam Search) across the mathematical, symbolic, and\ncommonsense reasoning tasks, exhibiting superior accuracy and reliability in\nboth unimodal and multimodal settings. We further validate the approach on\nlarge reasoning models, confirming its applicability to specialized reasoning\nmodels. Additionally, we explore the role of the model's self-correction\nability in CoT reasoning. This work provides a novel reliability improvement\npath for CoT reasoning with broad application potential."
                },
                "authors": [
                    {
                        "name": "Zijun Chen"
                    },
                    {
                        "name": "Wenbo Hu"
                    },
                    {
                        "name": "Richang Hong"
                    }
                ],
                "author_detail": {
                    "name": "Richang Hong"
                },
                "author": "Richang Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09995v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09995v2",
                "updated": "2025-07-15T03:12:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    3,
                    12,
                    18,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-14T07:29:49Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    29,
                    49,
                    0,
                    195,
                    0
                ],
                "title": "Graph-based Multi-Modal Interaction Lightweight Network for Brain Tumor\n  Segmentation (GMLN-BTS) in Edge Iterative MRI Lesion Localization System\n  (EdgeIMLocSys)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based Multi-Modal Interaction Lightweight Network for Brain Tumor\n  Segmentation (GMLN-BTS) in Edge Iterative MRI Lesion Localization System\n  (EdgeIMLocSys)"
                },
                "summary": "Brain tumor segmentation plays a critical role in clinical diagnosis and\ntreatment planning, yet the variability in imaging quality across different MRI\nscanners presents significant challenges to model generalization. To address\nthis, we propose the Edge Iterative MRI Lesion Localization System\n(EdgeIMLocSys), which integrates Continuous Learning from Human Feedback to\nadaptively fine-tune segmentation models based on clinician feedback, thereby\nenhancing robustness to scanner-specific imaging characteristics. Central to\nthis system is the Graph-based Multi-Modal Interaction Lightweight Network for\nBrain Tumor Segmentation (GMLN-BTS), which employs a Modality-Aware Adaptive\nEncoder (M2AE) to extract multi-scale semantic features efficiently, and a\nGraph-based Multi-Modal Collaborative Interaction Module (G2MCIM) to model\ncomplementary cross-modal relationships via graph structures. Additionally, we\nintroduce a novel Voxel Refinement UpSampling Module (VRUM) that\nsynergistically combines linear interpolation and multi-scale transposed\nconvolutions to suppress artifacts while preserving high-frequency details,\nimproving segmentation boundary accuracy. Our proposed GMLN-BTS model achieves\na Dice score of 85.1% on the BraTS2017 dataset with only 4.58 million\nparameters, representing a 98% reduction compared to mainstream 3D Transformer\nmodels, and significantly outperforms existing lightweight approaches. This\nwork demonstrates a synergistic breakthrough in achieving high-accuracy,\nresource-efficient brain tumor segmentation suitable for deployment in\nresource-constrained clinical environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain tumor segmentation plays a critical role in clinical diagnosis and\ntreatment planning, yet the variability in imaging quality across different MRI\nscanners presents significant challenges to model generalization. To address\nthis, we propose the Edge Iterative MRI Lesion Localization System\n(EdgeIMLocSys), which integrates Continuous Learning from Human Feedback to\nadaptively fine-tune segmentation models based on clinician feedback, thereby\nenhancing robustness to scanner-specific imaging characteristics. Central to\nthis system is the Graph-based Multi-Modal Interaction Lightweight Network for\nBrain Tumor Segmentation (GMLN-BTS), which employs a Modality-Aware Adaptive\nEncoder (M2AE) to extract multi-scale semantic features efficiently, and a\nGraph-based Multi-Modal Collaborative Interaction Module (G2MCIM) to model\ncomplementary cross-modal relationships via graph structures. Additionally, we\nintroduce a novel Voxel Refinement UpSampling Module (VRUM) that\nsynergistically combines linear interpolation and multi-scale transposed\nconvolutions to suppress artifacts while preserving high-frequency details,\nimproving segmentation boundary accuracy. Our proposed GMLN-BTS model achieves\na Dice score of 85.1% on the BraTS2017 dataset with only 4.58 million\nparameters, representing a 98% reduction compared to mainstream 3D Transformer\nmodels, and significantly outperforms existing lightweight approaches. This\nwork demonstrates a synergistic breakthrough in achieving high-accuracy,\nresource-efficient brain tumor segmentation suitable for deployment in\nresource-constrained clinical environments."
                },
                "authors": [
                    {
                        "name": "Guohao Huo"
                    },
                    {
                        "name": "Ruiting Dai"
                    },
                    {
                        "name": "Hao Tang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Tang"
                },
                "author": "Hao Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09995v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09995v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09990v1",
                "updated": "2025-07-14T07:17:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    17,
                    24,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T07:17:24Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    17,
                    24,
                    0,
                    195,
                    0
                ],
                "title": "Differentially Private Federated Low Rank Adaptation Beyond Fixed-Matrix",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially Private Federated Low Rank Adaptation Beyond Fixed-Matrix"
                },
                "summary": "Large language models (LLMs) typically require fine-tuning for\ndomain-specific tasks, and LoRA offers a computationally efficient approach by\ntraining low-rank adapters. LoRA is also communication-efficient for federated\nLLMs when multiple users collaboratively fine-tune a global LLM model without\nsharing their proprietary raw data. However, even the transmission of local\nadapters between a server and clients risks serious privacy leakage. Applying\ndifferential privacy (DP) to federated LoRA encounters a dilemma: adding noise\nto both adapters amplifies synthetic noise on the model, while fixing one\nadapter impairs the learnability of fine-tuning. In this paper, we propose\nFedASK (Differentially Private Federated Low Rank Adaptation with Double\nSketching) , a novel federated LoRA framework to enable effective updating of\nboth low-rank adapters with robust differential privacy. Inspired by randomized\nSVD, our key idea is a two-stage sketching pipeline. This pipeline first\naggregates carefully sketched, privacy-preserving local updates, and then\nreconstructs the global matrices on the server to facilitate effective updating\nof both adapters. We theoretically prove FedASK's differential privacy\nguarantee and its exact aggregation property. Comprehensive experiments\ndemonstrate that FedASK consistently outperforms baseline methods across a\nvariety of privacy settings and data distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) typically require fine-tuning for\ndomain-specific tasks, and LoRA offers a computationally efficient approach by\ntraining low-rank adapters. LoRA is also communication-efficient for federated\nLLMs when multiple users collaboratively fine-tune a global LLM model without\nsharing their proprietary raw data. However, even the transmission of local\nadapters between a server and clients risks serious privacy leakage. Applying\ndifferential privacy (DP) to federated LoRA encounters a dilemma: adding noise\nto both adapters amplifies synthetic noise on the model, while fixing one\nadapter impairs the learnability of fine-tuning. In this paper, we propose\nFedASK (Differentially Private Federated Low Rank Adaptation with Double\nSketching) , a novel federated LoRA framework to enable effective updating of\nboth low-rank adapters with robust differential privacy. Inspired by randomized\nSVD, our key idea is a two-stage sketching pipeline. This pipeline first\naggregates carefully sketched, privacy-preserving local updates, and then\nreconstructs the global matrices on the server to facilitate effective updating\nof both adapters. We theoretically prove FedASK's differential privacy\nguarantee and its exact aggregation property. Comprehensive experiments\ndemonstrate that FedASK consistently outperforms baseline methods across a\nvariety of privacy settings and data distributions."
                },
                "authors": [
                    {
                        "name": "Ming Wen"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Yuedong Xu"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Dingding Han"
                    }
                ],
                "author_detail": {
                    "name": "Dingding Han"
                },
                "author": "Dingding Han",
                "arxiv_comment": "23 pages, NeurIPS 2025 under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09987v1",
                "updated": "2025-07-14T07:12:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    12,
                    16,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T07:12:16Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    12,
                    16,
                    0,
                    195,
                    0
                ],
                "title": "VoxelRF: Voxelized Radiance Field for Fast Wireless Channel Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VoxelRF: Voxelized Radiance Field for Fast Wireless Channel Modeling"
                },
                "summary": "Wireless channel modeling in complex environments is crucial for wireless\ncommunication system design and deployment. Traditional channel modeling\napproaches face challenges in balancing accuracy, efficiency, and scalability,\nwhile recent neural approaches such as neural radiance field (NeRF) suffer from\nlong training and slow inference. To tackle these challenges, we propose\nvoxelized radiance field (VoxelRF), a novel neural representation for wireless\nchannel modeling that enables fast and accurate synthesis of spatial spectra.\nVoxelRF replaces the costly multilayer perception (MLP) used in NeRF-based\nmethods with trilinear interpolation of voxel grid-based representation, and\ntwo shallow MLPs to model both propagation and transmitter-dependent effects.\nTo further accelerate training and improve generalization, we introduce\nprogressive learning, empty space skipping, and an additional background\nentropy loss function. Experimental results demonstrate that VoxelRF achieves\ncompetitive accuracy with significantly reduced computation and limited\ntraining data, making it more practical for real-time and resource-constrained\nwireless applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless channel modeling in complex environments is crucial for wireless\ncommunication system design and deployment. Traditional channel modeling\napproaches face challenges in balancing accuracy, efficiency, and scalability,\nwhile recent neural approaches such as neural radiance field (NeRF) suffer from\nlong training and slow inference. To tackle these challenges, we propose\nvoxelized radiance field (VoxelRF), a novel neural representation for wireless\nchannel modeling that enables fast and accurate synthesis of spatial spectra.\nVoxelRF replaces the costly multilayer perception (MLP) used in NeRF-based\nmethods with trilinear interpolation of voxel grid-based representation, and\ntwo shallow MLPs to model both propagation and transmitter-dependent effects.\nTo further accelerate training and improve generalization, we introduce\nprogressive learning, empty space skipping, and an additional background\nentropy loss function. Experimental results demonstrate that VoxelRF achieves\ncompetitive accuracy with significantly reduced computation and limited\ntraining data, making it more practical for real-time and resource-constrained\nwireless applications."
                },
                "authors": [
                    {
                        "name": "Zihang Zeng"
                    },
                    {
                        "name": "Shu Sun"
                    },
                    {
                        "name": "Meixia Tao"
                    },
                    {
                        "name": "Yin Xu"
                    },
                    {
                        "name": "Xianghao Yu"
                    }
                ],
                "author_detail": {
                    "name": "Xianghao Yu"
                },
                "author": "Xianghao Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07498v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07498v2",
                "updated": "2025-07-14T07:10:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    10,
                    51,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-10T07:34:05Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    7,
                    34,
                    5,
                    3,
                    191,
                    0
                ],
                "title": "Teaching LLM to Reason: Reinforcement Learning from Algorithmic Problems\n  without Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching LLM to Reason: Reinforcement Learning from Algorithmic Problems\n  without Code"
                },
                "summary": "Enhancing reasoning capabilities remains a central focus in the LLM reasearch\ncommunity. A promising direction involves requiring models to simulate code\nexecution step-by-step to derive outputs for given inputs. However, as code is\noften designed for large-scale systems, direct application leads to\nover-reliance on complex data structures and algorithms, even for simple cases,\nresulting in overfitting to algorithmic patterns rather than core reasoning\nstructures. To address this, we propose TeaR, which aims at teaching LLMs to\nreason better. TeaR leverages careful data curation and reinforcement learning\nto guide models in discovering optimal reasoning paths through code-related\ntasks, thereby improving general reasoning abilities. We conduct extensive\nexperiments using two base models and three long-CoT distillation models, with\nmodel sizes ranging from 1.5 billion to 32 billion parameters, and across 17\nbenchmarks spanning Math, Knowledge, Code, and Logical Reasoning. The results\nconsistently show significant performance improvements. Notably, TeaR achieves\na 35.9% improvement on Qwen2.5-7B and 5.9% on R1-Distilled-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing reasoning capabilities remains a central focus in the LLM reasearch\ncommunity. A promising direction involves requiring models to simulate code\nexecution step-by-step to derive outputs for given inputs. However, as code is\noften designed for large-scale systems, direct application leads to\nover-reliance on complex data structures and algorithms, even for simple cases,\nresulting in overfitting to algorithmic patterns rather than core reasoning\nstructures. To address this, we propose TeaR, which aims at teaching LLMs to\nreason better. TeaR leverages careful data curation and reinforcement learning\nto guide models in discovering optimal reasoning paths through code-related\ntasks, thereby improving general reasoning abilities. We conduct extensive\nexperiments using two base models and three long-CoT distillation models, with\nmodel sizes ranging from 1.5 billion to 32 billion parameters, and across 17\nbenchmarks spanning Math, Knowledge, Code, and Logical Reasoning. The results\nconsistently show significant performance improvements. Notably, TeaR achieves\na 35.9% improvement on Qwen2.5-7B and 5.9% on R1-Distilled-7B."
                },
                "authors": [
                    {
                        "name": "Keqin Bao"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Xiaoyuan Li"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Fuli Feng"
                    },
                    {
                        "name": "Xiangnan He"
                    },
                    {
                        "name": "Dayiheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Dayiheng Liu"
                },
                "author": "Dayiheng Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07498v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07498v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v3",
                "updated": "2025-07-14T07:05:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    5,
                    28,
                    0,
                    195,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-Gonz√°lez"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Mart√≠n"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08382v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08382v2",
                "updated": "2025-07-14T06:58:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    6,
                    58,
                    33,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-11T07:54:16Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    7,
                    54,
                    16,
                    4,
                    192,
                    0
                ],
                "title": "Two-cluster test",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-cluster test"
                },
                "summary": "Cluster analysis is a fundamental research issue in statistics and machine\nlearning. In many modern clustering methods, we need to determine whether two\nsubsets of samples come from the same cluster. Since these subsets are usually\ngenerated by certain clustering procedures, the deployment of classic\ntwo-sample tests in this context would yield extremely smaller p-values,\nleading to inflated Type-I error rate. To overcome this bias, we formally\nintroduce the two-cluster test issue and argue that it is a totally different\nsignificance testing issue from conventional two-sample test. Meanwhile, we\npresent a new method based on the boundary points between two subsets to derive\nan analytical p-value for the purpose of significance quantification.\nExperiments on both synthetic and real data sets show that the proposed test is\nable to significantly reduce the Type-I error rate, in comparison with several\nclassic two-sample testing methods. More importantly, the practical usage of\nsuch two-cluster test is further verified through its applications in\ntree-based interpretable clustering and significance-based hierarchical\nclustering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cluster analysis is a fundamental research issue in statistics and machine\nlearning. In many modern clustering methods, we need to determine whether two\nsubsets of samples come from the same cluster. Since these subsets are usually\ngenerated by certain clustering procedures, the deployment of classic\ntwo-sample tests in this context would yield extremely smaller p-values,\nleading to inflated Type-I error rate. To overcome this bias, we formally\nintroduce the two-cluster test issue and argue that it is a totally different\nsignificance testing issue from conventional two-sample test. Meanwhile, we\npresent a new method based on the boundary points between two subsets to derive\nan analytical p-value for the purpose of significance quantification.\nExperiments on both synthetic and real data sets show that the proposed test is\nable to significantly reduce the Type-I error rate, in comparison with several\nclassic two-sample testing methods. More importantly, the practical usage of\nsuch two-cluster test is further verified through its applications in\ntree-based interpretable clustering and significance-based hierarchical\nclustering."
                },
                "authors": [
                    {
                        "name": "Xinying Liu"
                    },
                    {
                        "name": "Lianyu Hu"
                    },
                    {
                        "name": "Mudi Jiang"
                    },
                    {
                        "name": "Simeng Zhang"
                    },
                    {
                        "name": "Jun Lou"
                    },
                    {
                        "name": "Zengyou He"
                    }
                ],
                "author_detail": {
                    "name": "Zengyou He"
                },
                "author": "Zengyou He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08382v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08382v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09965v1",
                "updated": "2025-07-14T06:32:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    6,
                    32,
                    23,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T06:32:23Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    6,
                    32,
                    23,
                    0,
                    195,
                    0
                ],
                "title": "AnalogTester: A Large Language Model-Based Framework for Automatic\n  Testbench Generation in Analog Circuit Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnalogTester: A Large Language Model-Based Framework for Automatic\n  Testbench Generation in Analog Circuit Design"
                },
                "summary": "Recent advancements have demonstrated the significant potential of large\nlanguage models (LLMs) in analog circuit design. Nevertheless, testbench\nconstruction for analog circuits remains manual, creating a critical bottleneck\nin achieving fully automated design processes. Particularly when replicating\ncircuit designs from academic papers, manual Testbench construction demands\ntime-intensive implementation and frequent adjustments, which fails to address\nthe dynamic diversity and flexibility requirements for automation. AnalogTester\ntackles automated analog design challenges through an LLM-powered pipeline: a)\ndomain-knowledge integration, b) paper information extraction, c) simulation\nscheme synthesis, and d) testbench code generation with Tsinghua Electronic\nDesign (TED). AnalogTester has demonstrated automated Testbench generation\ncapabilities for three fundamental analog circuit types: operational amplifiers\n(op-amps), bandgap references (BGRs), and low-dropout regulators (LDOs), while\nmaintaining a scalable framework for adaptation to broader circuit topologies.\nFurthermore, AnalogTester can generate circuit knowledge data and TED code\ncorpus, establishing fundamental training datasets for LLM specialization in\nanalog circuit design automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements have demonstrated the significant potential of large\nlanguage models (LLMs) in analog circuit design. Nevertheless, testbench\nconstruction for analog circuits remains manual, creating a critical bottleneck\nin achieving fully automated design processes. Particularly when replicating\ncircuit designs from academic papers, manual Testbench construction demands\ntime-intensive implementation and frequent adjustments, which fails to address\nthe dynamic diversity and flexibility requirements for automation. AnalogTester\ntackles automated analog design challenges through an LLM-powered pipeline: a)\ndomain-knowledge integration, b) paper information extraction, c) simulation\nscheme synthesis, and d) testbench code generation with Tsinghua Electronic\nDesign (TED). AnalogTester has demonstrated automated Testbench generation\ncapabilities for three fundamental analog circuit types: operational amplifiers\n(op-amps), bandgap references (BGRs), and low-dropout regulators (LDOs), while\nmaintaining a scalable framework for adaptation to broader circuit topologies.\nFurthermore, AnalogTester can generate circuit knowledge data and TED code\ncorpus, establishing fundamental training datasets for LLM specialization in\nanalog circuit design automation."
                },
                "authors": [
                    {
                        "name": "Weiyu Chen"
                    },
                    {
                        "name": "Chengjie Liu"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Jinyang Lyu"
                    },
                    {
                        "name": "Mingqian Yang"
                    },
                    {
                        "name": "Yuan Du"
                    },
                    {
                        "name": "Li Du"
                    },
                    {
                        "name": "Jun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Yang"
                },
                "author": "Jun Yang",
                "arxiv_comment": "accepted by ISEDA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09955v1",
                "updated": "2025-07-14T06:10:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    6,
                    10,
                    30,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T06:10:30Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    6,
                    10,
                    30,
                    0,
                    195,
                    0
                ],
                "title": "DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models"
                },
                "summary": "DeepSeek, a Chinese Artificial Intelligence (AI) startup, has released their\nV3 and R1 series models, which attracted global attention due to their low\ncost, high performance, and open-source advantages. This paper begins by\nreviewing the evolution of large AI models focusing on paradigm shifts, the\nmainstream Large Language Model (LLM) paradigm, and the DeepSeek paradigm.\nSubsequently, the paper highlights novel algorithms introduced by DeepSeek,\nincluding Multi-head Latent Attention (MLA), Mixture-of-Experts (MoE),\nMulti-Token Prediction (MTP), and Group Relative Policy Optimization (GRPO).\nThe paper then explores DeepSeek engineering breakthroughs in LLM scaling,\ntraining, inference, and system-level optimization architecture. Moreover, the\nimpact of DeepSeek models on the competitive AI landscape is analyzed,\ncomparing them to mainstream LLMs across various fields. Finally, the paper\nreflects on the insights gained from DeepSeek innovations and discusses future\ntrends in the technical and engineering development of large AI models,\nparticularly in data, training, and reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek, a Chinese Artificial Intelligence (AI) startup, has released their\nV3 and R1 series models, which attracted global attention due to their low\ncost, high performance, and open-source advantages. This paper begins by\nreviewing the evolution of large AI models focusing on paradigm shifts, the\nmainstream Large Language Model (LLM) paradigm, and the DeepSeek paradigm.\nSubsequently, the paper highlights novel algorithms introduced by DeepSeek,\nincluding Multi-head Latent Attention (MLA), Mixture-of-Experts (MoE),\nMulti-Token Prediction (MTP), and Group Relative Policy Optimization (GRPO).\nThe paper then explores DeepSeek engineering breakthroughs in LLM scaling,\ntraining, inference, and system-level optimization architecture. Moreover, the\nimpact of DeepSeek models on the competitive AI landscape is analyzed,\ncomparing them to mainstream LLMs across various fields. Finally, the paper\nreflects on the insights gained from DeepSeek innovations and discusses future\ntrends in the technical and engineering development of large AI models,\nparticularly in data, training, and reasoning."
                },
                "authors": [
                    {
                        "name": "Luolin Xiong"
                    },
                    {
                        "name": "Haofen Wang"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Lu Sheng"
                    },
                    {
                        "name": "Yun Xiong"
                    },
                    {
                        "name": "Jingping Liu"
                    },
                    {
                        "name": "Yanghua Xiao"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Qing-Long Han"
                    },
                    {
                        "name": "Yang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Tang"
                },
                "author": "Yang Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18421v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18421v2",
                "updated": "2025-07-14T06:09:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    6,
                    9,
                    12,
                    0,
                    195,
                    0
                ],
                "published": "2025-06-23T09:02:04Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    9,
                    2,
                    4,
                    0,
                    174,
                    0
                ],
                "title": "TReB: A Comprehensive Benchmark for Evaluating Table Reasoning\n  Capabilities of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TReB: A Comprehensive Benchmark for Evaluating Table Reasoning\n  Capabilities of Large Language Models"
                },
                "summary": "The majority of data in businesses and industries is stored in tables,\ndatabases, and data warehouses. Reasoning with table-structured data poses\nsignificant challenges for large language models (LLMs) due to its hidden\nsemantics, inherent complexity, and structured nature. One of these challenges\nis lacking an effective evaluation benchmark fairly reflecting the performances\nof LLMs on broad table reasoning abilities. In this paper, we fill in this gap,\npresenting a comprehensive table reasoning evolution benchmark, TReB, which\nmeasures both shallow table understanding abilities and deep table reasoning\nabilities, a total of 26 sub-tasks. We construct a high quality dataset through\nan iterative data processing procedure. We create an evaluation framework to\nrobustly measure table reasoning capabilities with three distinct inference\nmodes, TCoT, PoT and ICoT. Further, we benchmark over 20 state-of-the-art LLMs\nusing this frame work and prove its effectiveness. Experimental results reveal\nthat existing LLMs still have significant room for improvement in addressing\nthe complex and real world Table related tasks. Both the dataset and evaluation\nframework are publicly available, with the dataset hosted on\nhuggingface.co/datasets/JT-LM/JIUTIAN-TReB and the framework on\ngithub.com/JT-LM/jiutian-treb.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The majority of data in businesses and industries is stored in tables,\ndatabases, and data warehouses. Reasoning with table-structured data poses\nsignificant challenges for large language models (LLMs) due to its hidden\nsemantics, inherent complexity, and structured nature. One of these challenges\nis lacking an effective evaluation benchmark fairly reflecting the performances\nof LLMs on broad table reasoning abilities. In this paper, we fill in this gap,\npresenting a comprehensive table reasoning evolution benchmark, TReB, which\nmeasures both shallow table understanding abilities and deep table reasoning\nabilities, a total of 26 sub-tasks. We construct a high quality dataset through\nan iterative data processing procedure. We create an evaluation framework to\nrobustly measure table reasoning capabilities with three distinct inference\nmodes, TCoT, PoT and ICoT. Further, we benchmark over 20 state-of-the-art LLMs\nusing this frame work and prove its effectiveness. Experimental results reveal\nthat existing LLMs still have significant room for improvement in addressing\nthe complex and real world Table related tasks. Both the dataset and evaluation\nframework are publicly available, with the dataset hosted on\nhuggingface.co/datasets/JT-LM/JIUTIAN-TReB and the framework on\ngithub.com/JT-LM/jiutian-treb."
                },
                "authors": [
                    {
                        "name": "Ce Li"
                    },
                    {
                        "name": "Xiaofan Liu"
                    },
                    {
                        "name": "Zhiyan Song"
                    },
                    {
                        "name": "Ce Chi"
                    },
                    {
                        "name": "Chen Zhao"
                    },
                    {
                        "name": "Jingjing Yang"
                    },
                    {
                        "name": "Zhendong Wang"
                    },
                    {
                        "name": "Kexin Yang"
                    },
                    {
                        "name": "Boshen Shi"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Chao Deng"
                    },
                    {
                        "name": "Junlan Feng"
                    }
                ],
                "author_detail": {
                    "name": "Junlan Feng"
                },
                "author": "Junlan Feng",
                "arxiv_comment": "Benmark report v1.1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18421v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18421v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09950v1",
                "updated": "2025-07-14T05:59:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    5,
                    59,
                    50,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T05:59:50Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    5,
                    59,
                    50,
                    0,
                    195,
                    0
                ],
                "title": "Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion\n  Product Attributes? A Zero-Shot Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion\n  Product Attributes? A Zero-Shot Analysis"
                },
                "summary": "The fashion retail business is centered around the capacity to comprehend\nproducts. Product attribution helps in comprehending products depending on the\nbusiness process. Quality attribution improves the customer experience as they\nnavigate through millions of products offered by a retail website. It leads to\nwell-organized product catalogs. In the end, product attribution directly\nimpacts the 'discovery experience' of the customer. Although large language\nmodels (LLMs) have shown remarkable capabilities in understanding multimodal\ndata, their performance on fine-grained fashion attribute recognition remains\nunder-explored. This paper presents a zero-shot evaluation of state-of-the-art\nLLMs that balance performance with speed and cost efficiency, mainly\nGPT-4o-mini and Gemini 2.0 Flash. We have used the dataset\nDeepFashion-MultiModal (https://github.com/yumingj/DeepFashion-MultiModal) to\nevaluate these models in the attribution tasks of fashion products. Our study\nevaluates these models across 18 categories of fashion attributes, offering\ninsight into where these models excel. We only use images as the sole input for\nproduct information to create a constrained environment. Our analysis shows\nthat Gemini 2.0 Flash demonstrates the strongest overall performance with a\nmacro F1 score of 56.79% across all attributes, while GPT-4o-mini scored a\nmacro F1 score of 43.28%. Through detailed error analysis, our findings provide\npractical insights for deploying these LLMs in production e-commerce product\nattribution-related tasks and highlight the need for domain-specific\nfine-tuning approaches. This work also lays the groundwork for future research\nin fashion AI and multimodal attribute extraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fashion retail business is centered around the capacity to comprehend\nproducts. Product attribution helps in comprehending products depending on the\nbusiness process. Quality attribution improves the customer experience as they\nnavigate through millions of products offered by a retail website. It leads to\nwell-organized product catalogs. In the end, product attribution directly\nimpacts the 'discovery experience' of the customer. Although large language\nmodels (LLMs) have shown remarkable capabilities in understanding multimodal\ndata, their performance on fine-grained fashion attribute recognition remains\nunder-explored. This paper presents a zero-shot evaluation of state-of-the-art\nLLMs that balance performance with speed and cost efficiency, mainly\nGPT-4o-mini and Gemini 2.0 Flash. We have used the dataset\nDeepFashion-MultiModal (https://github.com/yumingj/DeepFashion-MultiModal) to\nevaluate these models in the attribution tasks of fashion products. Our study\nevaluates these models across 18 categories of fashion attributes, offering\ninsight into where these models excel. We only use images as the sole input for\nproduct information to create a constrained environment. Our analysis shows\nthat Gemini 2.0 Flash demonstrates the strongest overall performance with a\nmacro F1 score of 56.79% across all attributes, while GPT-4o-mini scored a\nmacro F1 score of 43.28%. Through detailed error analysis, our findings provide\npractical insights for deploying these LLMs in production e-commerce product\nattribution-related tasks and highlight the need for domain-specific\nfine-tuning approaches. This work also lays the groundwork for future research\nin fashion AI and multimodal attribute extraction."
                },
                "authors": [
                    {
                        "name": "Shubham Shukla"
                    },
                    {
                        "name": "Kunal Sonalkar"
                    }
                ],
                "author_detail": {
                    "name": "Kunal Sonalkar"
                },
                "author": "Kunal Sonalkar",
                "arxiv_comment": "11 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04607v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04607v2",
                "updated": "2025-07-14T05:54:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    5,
                    54,
                    45,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-07T01:54:34Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    1,
                    54,
                    34,
                    0,
                    188,
                    0
                ],
                "title": "PRIME: Large Language Model Personalization with Cognitive Memory and\n  Thought Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRIME: Large Language Model Personalization with Cognitive Memory and\n  Thought Processes"
                },
                "summary": "Large language model (LLM) personalization aims to align model outputs with\nindividuals' unique preferences and opinions. While recent efforts have\nimplemented various personalization methods, a unified theoretical framework\nthat can systematically understand the drivers of effective personalization is\nstill lacking. In this work, we integrate the well-established cognitive\ndual-memory model into LLM personalization, by mirroring episodic memory to\nhistorical user engagements and semantic memory to long-term, evolving user\nbeliefs. Specifically, we systematically investigate memory instantiations and\nintroduce a unified framework, PRIME, using episodic and semantic memory\nmechanisms. We further augment PRIME with a novel personalized thinking\ncapability inspired by the slow thinking strategy. Moreover, recognizing the\nabsence of suitable benchmarks, we introduce a dataset using Change My View\n(CMV) from Reddit, specifically designed to evaluate long-context\npersonalization. Extensive experiments validate PRIME's effectiveness across\nboth long- and short-context scenarios. Further analysis confirms that PRIME\neffectively captures dynamic personalization beyond mere popularity biases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) personalization aims to align model outputs with\nindividuals' unique preferences and opinions. While recent efforts have\nimplemented various personalization methods, a unified theoretical framework\nthat can systematically understand the drivers of effective personalization is\nstill lacking. In this work, we integrate the well-established cognitive\ndual-memory model into LLM personalization, by mirroring episodic memory to\nhistorical user engagements and semantic memory to long-term, evolving user\nbeliefs. Specifically, we systematically investigate memory instantiations and\nintroduce a unified framework, PRIME, using episodic and semantic memory\nmechanisms. We further augment PRIME with a novel personalized thinking\ncapability inspired by the slow thinking strategy. Moreover, recognizing the\nabsence of suitable benchmarks, we introduce a dataset using Change My View\n(CMV) from Reddit, specifically designed to evaluate long-context\npersonalization. Extensive experiments validate PRIME's effectiveness across\nboth long- and short-context scenarios. Further analysis confirms that PRIME\neffectively captures dynamic personalization beyond mere popularity biases."
                },
                "authors": [
                    {
                        "name": "Xinliang Frederick Zhang"
                    },
                    {
                        "name": "Nick Beauchamp"
                    },
                    {
                        "name": "Lu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Wang"
                },
                "author": "Lu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04607v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04607v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09948v1",
                "updated": "2025-07-14T05:48:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    5,
                    48,
                    9,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T05:48:09Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    5,
                    48,
                    9,
                    0,
                    195,
                    0
                ],
                "title": "Iceberg: Enhancing HLS Modeling with Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iceberg: Enhancing HLS Modeling with Synthetic Data"
                },
                "summary": "Deep learning-based prediction models for High-Level Synthesis (HLS) of\nhardware designs often struggle to generalize. In this paper, we study how to\nclose the generalizability gap of these models through pretraining on synthetic\ndata and introduce Iceberg, a synthetic data augmentation approach that expands\nboth large language model (LLM)-generated programs and weak labels of unseen\ndesign configurations. Our weak label generation method is integrated with an\nin-context model architecture, enabling meta-learning from actual and proximate\nlabels. Iceberg improves the geometric mean modeling accuracy by $86.4\\%$ when\nadapt to six real-world applications with few-shot examples and achieves a\n$2.47\\times$ and a $1.12\\times$ better offline DSE performance when adapting to\ntwo different test datasets. Our open-sourced code is here:\n\\href{https://github.com/UCLA-VAST/iceberg}{https://github.com/UCLA-VAST/iceberg}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning-based prediction models for High-Level Synthesis (HLS) of\nhardware designs often struggle to generalize. In this paper, we study how to\nclose the generalizability gap of these models through pretraining on synthetic\ndata and introduce Iceberg, a synthetic data augmentation approach that expands\nboth large language model (LLM)-generated programs and weak labels of unseen\ndesign configurations. Our weak label generation method is integrated with an\nin-context model architecture, enabling meta-learning from actual and proximate\nlabels. Iceberg improves the geometric mean modeling accuracy by $86.4\\%$ when\nadapt to six real-world applications with few-shot examples and achieves a\n$2.47\\times$ and a $1.12\\times$ better offline DSE performance when adapting to\ntwo different test datasets. Our open-sourced code is here:\n\\href{https://github.com/UCLA-VAST/iceberg}{https://github.com/UCLA-VAST/iceberg}"
                },
                "authors": [
                    {
                        "name": "Zijian Ding"
                    },
                    {
                        "name": "Tung Nguyen"
                    },
                    {
                        "name": "Weikai Li"
                    },
                    {
                        "name": "Aditya Grover"
                    },
                    {
                        "name": "Yizhou Sun"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "arxiv_comment": "9 pages. accepted to ICLAD'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09942v1",
                "updated": "2025-07-14T05:32:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    5,
                    32,
                    32,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T05:32:32Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    5,
                    32,
                    32,
                    0,
                    195,
                    0
                ],
                "title": "Green-LLM: Optimal Workload Allocation for Environmentally-Aware\n  Distributed Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Green-LLM: Optimal Workload Allocation for Environmentally-Aware\n  Distributed Inference"
                },
                "summary": "This letter investigates the optimal allocation of large language model (LLM)\ninference workloads across heterogeneous edge data centers (DCs) over time.\nEach DC features on-site renewable generation and faces dynamic electricity\nprices and spatiotemporal variability in renewable availability. The central\nquestion is: how can inference workloads be optimally distributed to the DCs to\nminimize energy consumption, carbon emissions, and water usage while enhancing\nuser experience? This letter proposes a novel optimization model for LLM\nservice providers to reduce operational costs and environmental impacts.\nNumerical results validate the efficacy of the proposed approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This letter investigates the optimal allocation of large language model (LLM)\ninference workloads across heterogeneous edge data centers (DCs) over time.\nEach DC features on-site renewable generation and faces dynamic electricity\nprices and spatiotemporal variability in renewable availability. The central\nquestion is: how can inference workloads be optimally distributed to the DCs to\nminimize energy consumption, carbon emissions, and water usage while enhancing\nuser experience? This letter proposes a novel optimization model for LLM\nservice providers to reduce operational costs and environmental impacts.\nNumerical results validate the efficacy of the proposed approach."
                },
                "authors": [
                    {
                        "name": "Jiaming Cheng"
                    },
                    {
                        "name": "Duong Tung Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Duong Tung Nguyen"
                },
                "author": "Duong Tung Nguyen",
                "arxiv_comment": "5 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12185v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12185v3",
                "updated": "2025-07-14T05:28:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    5,
                    28,
                    8,
                    0,
                    195,
                    0
                ],
                "published": "2025-05-18T01:02:33Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    1,
                    2,
                    33,
                    6,
                    138,
                    0
                ],
                "title": "EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency\n  Perspective"
                },
                "summary": "Assessing the programming capabilities of Large Language Models (LLMs) is\ncrucial for their effective use in software engineering. Current evaluations,\nhowever, predominantly measure the accuracy of generated code on static\nbenchmarks, neglecting the critical aspect of model robustness during\nprogramming tasks. While adversarial attacks offer insights on model\nrobustness, their effectiveness is limited and evaluation could be constrained.\nCurrent adversarial attack methods for robustness evaluation yield inconsistent\nresults, struggling to provide a unified evaluation across different LLMs. We\nintroduce EVALOOP, a novel assessment framework that evaluate the robustness\nfrom a self-consistency perspective, i.e., leveraging the natural duality\ninherent in popular software engineering tasks, e.g., code generation and code\nsummarization. EVALOOP initiates a self-contained feedback loop: an LLM\ngenerates output (e.g., code) from an input (e.g., natural language\nspecification), and then use the generated output as the input to produce a new\noutput (e.g., summarizes that code into a new specification). EVALOOP repeats\nthe process to assess the effectiveness of EVALOOP in each loop. This cyclical\nstrategy intrinsically evaluates robustness without rely on any external attack\nsetups, providing a unified metric to evaluate LLMs' robustness in programming.\nWe evaluate 16 prominent LLMs (e.g., GPT-4.1, O4-mini) on EVALOOP and found\nthat EVALOOP typically induces a 5.01%-19.31% absolute drop in pass@1\nperformance within ten loops. Intriguingly, robustness does not always align\nwith initial performance (i.e., one-time query); for instance, GPT-3.5-Turbo,\ndespite superior initial code generation compared to DeepSeek-V2, demonstrated\nlower robustness over repeated evaluation loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the programming capabilities of Large Language Models (LLMs) is\ncrucial for their effective use in software engineering. Current evaluations,\nhowever, predominantly measure the accuracy of generated code on static\nbenchmarks, neglecting the critical aspect of model robustness during\nprogramming tasks. While adversarial attacks offer insights on model\nrobustness, their effectiveness is limited and evaluation could be constrained.\nCurrent adversarial attack methods for robustness evaluation yield inconsistent\nresults, struggling to provide a unified evaluation across different LLMs. We\nintroduce EVALOOP, a novel assessment framework that evaluate the robustness\nfrom a self-consistency perspective, i.e., leveraging the natural duality\ninherent in popular software engineering tasks, e.g., code generation and code\nsummarization. EVALOOP initiates a self-contained feedback loop: an LLM\ngenerates output (e.g., code) from an input (e.g., natural language\nspecification), and then use the generated output as the input to produce a new\noutput (e.g., summarizes that code into a new specification). EVALOOP repeats\nthe process to assess the effectiveness of EVALOOP in each loop. This cyclical\nstrategy intrinsically evaluates robustness without rely on any external attack\nsetups, providing a unified metric to evaluate LLMs' robustness in programming.\nWe evaluate 16 prominent LLMs (e.g., GPT-4.1, O4-mini) on EVALOOP and found\nthat EVALOOP typically induces a 5.01%-19.31% absolute drop in pass@1\nperformance within ten loops. Intriguingly, robustness does not always align\nwith initial performance (i.e., one-time query); for instance, GPT-3.5-Turbo,\ndespite superior initial code generation compared to DeepSeek-V2, demonstrated\nlower robustness over repeated evaluation loop."
                },
                "authors": [
                    {
                        "name": "Sen Fang"
                    },
                    {
                        "name": "Weiyuan Ding"
                    },
                    {
                        "name": "Bowen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Xu"
                },
                "author": "Bowen Xu",
                "arxiv_comment": "20 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12185v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12185v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09937v1",
                "updated": "2025-07-14T05:23:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    5,
                    23,
                    27,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T05:23:27Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    5,
                    23,
                    27,
                    0,
                    195,
                    0
                ],
                "title": "Memorization Sinks: Isolating Memorization during LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memorization Sinks: Isolating Memorization during LLM Training"
                },
                "summary": "Large language models are susceptible to memorizing repeated sequences,\nposing privacy and copyright concerns. A popular mitigation strategy is to\nremove memorized information from specific neurons post-hoc. However, such\napproaches have shown limited success so far. In a controlled setting, we show\nthat the memorization of natural sequences (those that resemble linguistically\nplausible text) become mechanistically entangled with general language\nabilities, thereby becoming challenging to remove post-hoc. In this work, we\nput forward a new paradigm of MemSinks that promotes isolation of memorization\nby design. We leverage a sequence identifier that activates a unique set of\nmemorization neurons for each sequence across repetitions. By analyzing the\ndynamics of learning and forgetting, we argue that MemSinks facilitates\nisolation of memorized content, making it easier to remove without compromising\ngeneral language capabilities. We implement MemSinks at the billion-parameter\nand billion-token scale, and observe both effective isolation and strong\ngeneralization. To our knowledge, this is the first proof-of-concept on real\ndata demonstrating that simultaneous generalization and isolation is\nachievable. We open-source our code at http://github.com/grghosal/MemSinks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are susceptible to memorizing repeated sequences,\nposing privacy and copyright concerns. A popular mitigation strategy is to\nremove memorized information from specific neurons post-hoc. However, such\napproaches have shown limited success so far. In a controlled setting, we show\nthat the memorization of natural sequences (those that resemble linguistically\nplausible text) become mechanistically entangled with general language\nabilities, thereby becoming challenging to remove post-hoc. In this work, we\nput forward a new paradigm of MemSinks that promotes isolation of memorization\nby design. We leverage a sequence identifier that activates a unique set of\nmemorization neurons for each sequence across repetitions. By analyzing the\ndynamics of learning and forgetting, we argue that MemSinks facilitates\nisolation of memorized content, making it easier to remove without compromising\ngeneral language capabilities. We implement MemSinks at the billion-parameter\nand billion-token scale, and observe both effective isolation and strong\ngeneralization. To our knowledge, this is the first proof-of-concept on real\ndata demonstrating that simultaneous generalization and isolation is\nachievable. We open-source our code at http://github.com/grghosal/MemSinks."
                },
                "authors": [
                    {
                        "name": "Gaurav R. Ghosal"
                    },
                    {
                        "name": "Pratyush Maini"
                    },
                    {
                        "name": "Aditi Raghunathan"
                    }
                ],
                "author_detail": {
                    "name": "Aditi Raghunathan"
                },
                "author": "Aditi Raghunathan",
                "arxiv_comment": "Accepted at the 2025 International Conference of Machine Learning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13640v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13640v2",
                "updated": "2025-07-14T05:22:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    5,
                    22,
                    54,
                    0,
                    195,
                    0
                ],
                "published": "2025-02-19T11:33:22Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    33,
                    22,
                    2,
                    50,
                    0
                ],
                "title": "Qorgau: Evaluating LLM Safety in Kazakh-Russian Bilingual Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qorgau: Evaluating LLM Safety in Kazakh-Russian Bilingual Contexts"
                },
                "summary": "Large language models (LLMs) are known to have the potential to generate\nharmful content, posing risks to users. While significant progress has been\nmade in developing taxonomies for LLM risks and safety evaluation prompts, most\nstudies have focused on monolingual contexts, primarily in English. However,\nlanguage- and region-specific risks in bilingual contexts are often overlooked,\nand core findings can diverge from those in monolingual settings. In this\npaper, we introduce Qorgau, a novel dataset specifically designed for safety\nevaluation in Kazakh and Russian, reflecting the unique bilingual context in\nKazakhstan, where both Kazakh (a low-resource language) and Russian (a\nhigh-resource language) are spoken. Experiments with both multilingual and\nlanguage-specific LLMs reveal notable differences in safety performance,\nemphasizing the need for tailored, region-specific datasets to ensure the\nresponsible and safe deployment of LLMs in countries like Kazakhstan. Warning:\nthis paper contains example data that may be offensive, harmful, or biased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are known to have the potential to generate\nharmful content, posing risks to users. While significant progress has been\nmade in developing taxonomies for LLM risks and safety evaluation prompts, most\nstudies have focused on monolingual contexts, primarily in English. However,\nlanguage- and region-specific risks in bilingual contexts are often overlooked,\nand core findings can diverge from those in monolingual settings. In this\npaper, we introduce Qorgau, a novel dataset specifically designed for safety\nevaluation in Kazakh and Russian, reflecting the unique bilingual context in\nKazakhstan, where both Kazakh (a low-resource language) and Russian (a\nhigh-resource language) are spoken. Experiments with both multilingual and\nlanguage-specific LLMs reveal notable differences in safety performance,\nemphasizing the need for tailored, region-specific datasets to ensure the\nresponsible and safe deployment of LLMs in countries like Kazakhstan. Warning:\nthis paper contains example data that may be offensive, harmful, or biased."
                },
                "authors": [
                    {
                        "name": "Maiya Goloburda"
                    },
                    {
                        "name": "Nurkhan Laiyk"
                    },
                    {
                        "name": "Diana Turmakhan"
                    },
                    {
                        "name": "Yuxia Wang"
                    },
                    {
                        "name": "Mukhammed Togmanov"
                    },
                    {
                        "name": "Jonibek Mansurov"
                    },
                    {
                        "name": "Askhat Sametov"
                    },
                    {
                        "name": "Nurdaulet Mukhituly"
                    },
                    {
                        "name": "Minghan Wang"
                    },
                    {
                        "name": "Daniil Orel"
                    },
                    {
                        "name": "Zain Muhammad Mujahid"
                    },
                    {
                        "name": "Fajri Koto"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Preslav Nakov"
                    }
                ],
                "author_detail": {
                    "name": "Preslav Nakov"
                },
                "author": "Preslav Nakov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13640v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13640v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09935v1",
                "updated": "2025-07-14T05:21:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    5,
                    21,
                    58,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T05:21:58Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    5,
                    21,
                    58,
                    0,
                    195,
                    0
                ],
                "title": "Enhancing Retrieval Augmented Generation with Hierarchical Text\n  Segmentation Chunking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Retrieval Augmented Generation with Hierarchical Text\n  Segmentation Chunking"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies\nfor retrieval, which enhance large language models (LLMs) by enabling them to\naccess external knowledge, ensuring that the retrieved information is\nup-to-date and domain-specific. However, traditional methods often fail to\ncreate chunks that capture sufficient semantic meaning, as they do not account\nfor the underlying textual structure. This paper proposes a novel framework\nthat enhances RAG by integrating hierarchical text segmentation and clustering\nto generate more meaningful and semantically coherent chunks. During inference,\nthe framework retrieves information by leveraging both segment-level and\ncluster-level vector representations, thereby increasing the likelihood of\nretrieving more precise and contextually relevant information. Evaluations on\nthe NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method\nachieved improved results compared to traditional chunking techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies\nfor retrieval, which enhance large language models (LLMs) by enabling them to\naccess external knowledge, ensuring that the retrieved information is\nup-to-date and domain-specific. However, traditional methods often fail to\ncreate chunks that capture sufficient semantic meaning, as they do not account\nfor the underlying textual structure. This paper proposes a novel framework\nthat enhances RAG by integrating hierarchical text segmentation and clustering\nto generate more meaningful and semantically coherent chunks. During inference,\nthe framework retrieves information by leveraging both segment-level and\ncluster-level vector representations, thereby increasing the likelihood of\nretrieving more precise and contextually relevant information. Evaluations on\nthe NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method\nachieved improved results compared to traditional chunking techniques."
                },
                "authors": [
                    {
                        "name": "Hai Toan Nguyen"
                    },
                    {
                        "name": "Tien Dat Nguyen"
                    },
                    {
                        "name": "Viet Ha Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Viet Ha Nguyen"
                },
                "author": "Viet Ha Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09931v1",
                "updated": "2025-07-14T05:17:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    5,
                    17,
                    41,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T05:17:41Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    5,
                    17,
                    41,
                    0,
                    195,
                    0
                ],
                "title": "Mechanistic Interpretability of LoRA-Adapted Language Models for Nuclear\n  Reactor Safety Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mechanistic Interpretability of LoRA-Adapted Language Models for Nuclear\n  Reactor Safety Applications"
                },
                "summary": "The integration of Large Language Models (LLMs) into safety-critical domains,\nsuch as nuclear engineering, necessitates a deep understanding of their\ninternal reasoning processes. This paper presents a novel methodology for\ninterpreting how an LLM encodes and utilizes domain-specific knowledge, using a\nBoiling Water Reactor system as a case study. We adapted a general-purpose LLM\n(Gemma-3-1b-it) to the nuclear domain using a parameter-efficient fine-tuning\ntechnique known as Low-Rank Adaptation. By comparing the neuron activation\npatterns of the base model to those of the fine-tuned model, we identified a\nsparse set of neurons whose behavior was significantly altered during the\nadaptation process. To probe the causal role of these specialized neurons, we\nemployed a neuron silencing technique. Our results demonstrate that while\nsilencing most of these specialized neurons individually did not produce a\nstatistically significant effect, deactivating the entire group collectively\nled to a statistically significant degradation in task performance. Qualitative\nanalysis further revealed that silencing these neurons impaired the model's\nability to generate detailed, contextually accurate technical information. This\npaper provides a concrete methodology for enhancing the transparency of an\nopaque black-box model, allowing domain expertise to be traced to verifiable\nneural circuits. This offers a pathway towards achieving nuclear-grade\nartificial intelligence (AI) assurance, addressing the verification and\nvalidation challenges mandated by nuclear regulatory frameworks (e.g., 10 CFR\n50 Appendix B), which have limited AI deployment in safety-critical nuclear\noperations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into safety-critical domains,\nsuch as nuclear engineering, necessitates a deep understanding of their\ninternal reasoning processes. This paper presents a novel methodology for\ninterpreting how an LLM encodes and utilizes domain-specific knowledge, using a\nBoiling Water Reactor system as a case study. We adapted a general-purpose LLM\n(Gemma-3-1b-it) to the nuclear domain using a parameter-efficient fine-tuning\ntechnique known as Low-Rank Adaptation. By comparing the neuron activation\npatterns of the base model to those of the fine-tuned model, we identified a\nsparse set of neurons whose behavior was significantly altered during the\nadaptation process. To probe the causal role of these specialized neurons, we\nemployed a neuron silencing technique. Our results demonstrate that while\nsilencing most of these specialized neurons individually did not produce a\nstatistically significant effect, deactivating the entire group collectively\nled to a statistically significant degradation in task performance. Qualitative\nanalysis further revealed that silencing these neurons impaired the model's\nability to generate detailed, contextually accurate technical information. This\npaper provides a concrete methodology for enhancing the transparency of an\nopaque black-box model, allowing domain expertise to be traced to verifiable\nneural circuits. This offers a pathway towards achieving nuclear-grade\nartificial intelligence (AI) assurance, addressing the verification and\nvalidation challenges mandated by nuclear regulatory frameworks (e.g., 10 CFR\n50 Appendix B), which have limited AI deployment in safety-critical nuclear\noperations."
                },
                "authors": [
                    {
                        "name": "Yoon Pyo Lee"
                    }
                ],
                "author_detail": {
                    "name": "Yoon Pyo Lee"
                },
                "author": "Yoon Pyo Lee",
                "arxiv_comment": "Submitted to Nuclear Technology. 22 pages, 2 tables, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12851v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12851v5",
                "updated": "2025-07-14T05:06:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    5,
                    6,
                    20,
                    0,
                    195,
                    0
                ],
                "published": "2025-01-22T12:59:08Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    12,
                    59,
                    8,
                    2,
                    22,
                    0
                ],
                "title": "ACEBench: Who Wins the Match Point in Tool Usage?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACEBench: Who Wins the Match Point in Tool Usage?"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant potential in\ndecision-making and reasoning, particularly when integrated with various tools\nto effectively solve complex problems. However, existing benchmarks for\nevaluating LLMs' tool usage face several limitations: (1) limited evaluation\nscenarios, often lacking assessments in real multi-turn dialogue contexts; (2)\nnarrow evaluation dimensions, with insufficient detailed assessments of how\nLLMs use tools; and (3) reliance on LLMs or real API executions for evaluation,\nwhich introduces significant overhead. To address these challenges, we\nintroduce ACEBench, a comprehensive benchmark for assessing tool usage in LLMs.\nACEBench categorizes data into three primary types based on evaluation\nmethodology: Normal, Special, and Agent. \"Normal\" evaluates tool usage in basic\nscenarios; \"Special\" evaluates tool usage in situations with ambiguous or\nincomplete instructions; \"Agent\" evaluates tool usage through multi-agent\ninteractions to simulate real-world, multi-turn dialogues. We conducted\nextensive experiments using ACEBench, analyzing various LLMs in-depth and\nproviding a more granular examination of error causes across different data\ntypes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant potential in\ndecision-making and reasoning, particularly when integrated with various tools\nto effectively solve complex problems. However, existing benchmarks for\nevaluating LLMs' tool usage face several limitations: (1) limited evaluation\nscenarios, often lacking assessments in real multi-turn dialogue contexts; (2)\nnarrow evaluation dimensions, with insufficient detailed assessments of how\nLLMs use tools; and (3) reliance on LLMs or real API executions for evaluation,\nwhich introduces significant overhead. To address these challenges, we\nintroduce ACEBench, a comprehensive benchmark for assessing tool usage in LLMs.\nACEBench categorizes data into three primary types based on evaluation\nmethodology: Normal, Special, and Agent. \"Normal\" evaluates tool usage in basic\nscenarios; \"Special\" evaluates tool usage in situations with ambiguous or\nincomplete instructions; \"Agent\" evaluates tool usage through multi-agent\ninteractions to simulate real-world, multi-turn dialogues. We conducted\nextensive experiments using ACEBench, analyzing various LLMs in-depth and\nproviding a more granular examination of error causes across different data\ntypes."
                },
                "authors": [
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Xinlong Hao"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Xu Huang"
                    },
                    {
                        "name": "Xingshan Zeng"
                    },
                    {
                        "name": "Shuai Yu"
                    },
                    {
                        "name": "Dexun Li"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Weinan Gan"
                    },
                    {
                        "name": "Yuefeng Huang"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Xinzhi Wang"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Baoqun Yin"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Wu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wu Liu"
                },
                "author": "Wu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12851v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12851v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06122v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06122v3",
                "updated": "2025-07-14T04:41:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    4,
                    41,
                    15,
                    0,
                    195,
                    0
                ],
                "published": "2025-04-08T15:15:26Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    15,
                    15,
                    26,
                    1,
                    98,
                    0
                ],
                "title": "Leanabell-Prover: Posttraining Scaling in Formal Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leanabell-Prover: Posttraining Scaling in Formal Reasoning"
                },
                "summary": "Recent advances in automated theorem proving (ATP) through LLMs have\nhighlighted the potential of formal reasoning with Lean 4 codes. However, ATP\nhas not yet be revolutionized by the recent posttraining scaling as\ndemonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the\nentire posttraining of ATP, aiming to align it with breakthroughs in reasoning\nmodels in natural languages. To begin, we continual train current ATP models\nwith a hybrid dataset, which consists of numerous statement-proof pairs, and\nadditional data aimed at incorporating cognitive behaviors that emulate human\nreasoning and hypothesis refinement. Next, we explore reinforcement learning\nwith the use of outcome reward returned by Lean 4 compiler. Through our\ndesigned continual training and reinforcement learning processes, we have\nsuccessfully improved existing formal provers, including both\nDeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance\nin the field of whole-proof generation. For example, we achieve a 59.8% pass\nrate (pass@32) on MiniF2F. This is an on-going project and we will\nprogressively update our findings, release our data and training details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in automated theorem proving (ATP) through LLMs have\nhighlighted the potential of formal reasoning with Lean 4 codes. However, ATP\nhas not yet be revolutionized by the recent posttraining scaling as\ndemonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the\nentire posttraining of ATP, aiming to align it with breakthroughs in reasoning\nmodels in natural languages. To begin, we continual train current ATP models\nwith a hybrid dataset, which consists of numerous statement-proof pairs, and\nadditional data aimed at incorporating cognitive behaviors that emulate human\nreasoning and hypothesis refinement. Next, we explore reinforcement learning\nwith the use of outcome reward returned by Lean 4 compiler. Through our\ndesigned continual training and reinforcement learning processes, we have\nsuccessfully improved existing formal provers, including both\nDeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance\nin the field of whole-proof generation. For example, we achieve a 59.8% pass\nrate (pass@32) on MiniF2F. This is an on-going project and we will\nprogressively update our findings, release our data and training details."
                },
                "authors": [
                    {
                        "name": "Jingyuan Zhang"
                    },
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Xingguang Ji"
                    },
                    {
                        "name": "Yahui Liu"
                    },
                    {
                        "name": "Yang Yue"
                    },
                    {
                        "name": "Fuzheng Zhang"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Guorui Zhou"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "arxiv_comment": "23 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06122v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06122v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07998v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07998v2",
                "updated": "2025-07-14T04:36:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    4,
                    36,
                    19,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-10T17:59:55Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    59,
                    55,
                    3,
                    191,
                    0
                ],
                "title": "PyVision: Agentic Vision with Dynamic Tooling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PyVision: Agentic Vision with Dynamic Tooling"
                },
                "summary": "LLMs are increasingly deployed as agents, systems capable of planning,\nreasoning, and dynamically calling external tools. However, in visual\nreasoning, prior approaches largely remain limited by predefined workflows and\nstatic toolsets. In this report, we present PyVision, an interactive,\nmulti-turn framework that enables MLLMs to autonomously generate, execute, and\nrefine Python-based tools tailored to the task at hand, unlocking flexible and\ninterpretable problem-solving. We develop a taxonomy of the tools created by\nPyVision and analyze their usage across a diverse set of benchmarks.\nQuantitatively, PyVision achieves consistent performance gains, boosting\nGPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini.\nThese results point to a broader shift: dynamic tooling allows models not just\nto use tools, but to invent them, advancing toward more agentic visual\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are increasingly deployed as agents, systems capable of planning,\nreasoning, and dynamically calling external tools. However, in visual\nreasoning, prior approaches largely remain limited by predefined workflows and\nstatic toolsets. In this report, we present PyVision, an interactive,\nmulti-turn framework that enables MLLMs to autonomously generate, execute, and\nrefine Python-based tools tailored to the task at hand, unlocking flexible and\ninterpretable problem-solving. We develop a taxonomy of the tools created by\nPyVision and analyze their usage across a diverse set of benchmarks.\nQuantitatively, PyVision achieves consistent performance gains, boosting\nGPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini.\nThese results point to a broader shift: dynamic tooling allows models not just\nto use tools, but to invent them, advancing toward more agentic visual\nreasoning."
                },
                "authors": [
                    {
                        "name": "Shitian Zhao"
                    },
                    {
                        "name": "Haoquan Zhang"
                    },
                    {
                        "name": "Shaoheng Lin"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Qilong Wu"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Chen Wei"
                    }
                ],
                "author_detail": {
                    "name": "Chen Wei"
                },
                "author": "Chen Wei",
                "arxiv_comment": "26 Pages, 10 Figures, Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07998v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07998v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]