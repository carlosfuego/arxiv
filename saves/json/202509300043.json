[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.08799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08799v2",
                "updated": "2025-09-26T17:59:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    54,
                    4,
                    269,
                    0
                ],
                "published": "2025-07-11T17:59:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "KV Cache Steering for Controlling Frozen LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Steering for Controlling Frozen LLMs"
                },
                "summary": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach constructs\nsteering vectors from reasoning traces, obtained either from teacher models\n(e.g., GPT-4o) or existing human annotations, that shift model behavior toward\nmore explicit, multi-step reasoning without fine-tuning or prompt\nmodifications. Experimental evaluations on diverse reasoning benchmarks\ndemonstrate that cache steering improves both the qualitative structure of\nmodel reasoning and quantitative task performance. Additional experiments show\nthat the method also scales to larger models and yields further gains on\nchallenging datasets such as GPQA and MATH. Compared to prior activation\nsteering techniques that require continuous interventions, our one-shot cache\nsteering offers substantial advantages in terms of inference latency,\nhyperparameter stability, and ease of integration with existing inference APIs.\nBeyond mere reasoning induction, we show that cache steering enables\ncontrollable transfer of reasoning styles (e.g., stepwise, causal, analogical),\nmaking it a practical tool for behavior-level guidance of language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach constructs\nsteering vectors from reasoning traces, obtained either from teacher models\n(e.g., GPT-4o) or existing human annotations, that shift model behavior toward\nmore explicit, multi-step reasoning without fine-tuning or prompt\nmodifications. Experimental evaluations on diverse reasoning benchmarks\ndemonstrate that cache steering improves both the qualitative structure of\nmodel reasoning and quantitative task performance. Additional experiments show\nthat the method also scales to larger models and yields further gains on\nchallenging datasets such as GPQA and MATH. Compared to prior activation\nsteering techniques that require continuous interventions, our one-shot cache\nsteering offers substantial advantages in terms of inference latency,\nhyperparameter stability, and ease of integration with existing inference APIs.\nBeyond mere reasoning induction, we show that cache steering enables\ncontrollable transfer of reasoning styles (e.g., stepwise, causal, analogical),\nmaking it a practical tool for behavior-level guidance of language models."
                },
                "authors": [
                    {
                        "name": "Max Belitsky"
                    },
                    {
                        "name": "Dawid J. Kopiczko"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "M. Jehanzeb Mirza"
                    },
                    {
                        "name": "James R. Glass"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22622v1",
                "updated": "2025-09-26T17:48:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    48,
                    24,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:48:24Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    48,
                    24,
                    4,
                    269,
                    0
                ],
                "title": "LongLive: Real-time Interactive Long Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongLive: Real-time Interactive Long Video Generation"
                },
                "summary": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss."
                },
                "authors": [
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Yicheng Xiao"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Xianbang Wang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Yingcong Chen"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Yukang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yukang Chen"
                },
                "author": "Yukang Chen",
                "arxiv_comment": "Code, model, and demos are available at\n  https://github.com/NVlabs/LongLive",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22548v1",
                "updated": "2025-09-26T16:29:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    29,
                    37,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:29:37Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    29,
                    37,
                    4,
                    269,
                    0
                ],
                "title": "JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory\n  for Vision-Language Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory\n  for Vision-Language Navigation"
                },
                "summary": "Vision-and-Language Navigation requires an embodied agent to navigate through\nunseen environments, guided by natural language instructions and a continuous\nvideo stream. Recent advances in VLN have been driven by the powerful semantic\nunderstanding of Multimodal Large Language Models. However, these methods\ntypically rely on explicit semantic memory, such as building textual cognitive\nmaps or storing historical visual frames. This type of method suffers from\nspatial information loss, computational redundancy, and memory bloat, which\nimpede efficient navigation. Inspired by the implicit scene representation in\nhuman navigation, analogous to the left brain's semantic understanding and the\nright brain's spatial cognition, we propose JanusVLN, a novel VLN framework\nfeaturing a dual implicit neural memory that models spatial-geometric and\nvisual-semantic memory as separate, compact, and fixed-size neural\nrepresentations. This framework first extends the MLLM to incorporate 3D prior\nknowledge from the spatial-geometric encoder, thereby enhancing the spatial\nreasoning capabilities of models based solely on RGB input. Then, the\nhistorical key-value caches from the spatial-geometric and visual-semantic\nencoders are constructed into a dual implicit memory. By retaining only the KVs\nof tokens in the initial and sliding window, redundant computation is avoided,\nenabling efficient incremental updates. Extensive experiments demonstrate that\nJanusVLN outperforms over 20 recent methods to achieve SOTA performance. For\nexample, the success rate improves by 10.5-35.5 compared to methods using\nmultiple data types as input and by 3.6-10.8 compared to methods using more RGB\ntraining data. This indicates that the proposed dual implicit neural memory, as\na novel paradigm, explores promising new directions for future VLN research.\nOurs project page: https://miv-xjtu.github.io/JanusVLN.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation requires an embodied agent to navigate through\nunseen environments, guided by natural language instructions and a continuous\nvideo stream. Recent advances in VLN have been driven by the powerful semantic\nunderstanding of Multimodal Large Language Models. However, these methods\ntypically rely on explicit semantic memory, such as building textual cognitive\nmaps or storing historical visual frames. This type of method suffers from\nspatial information loss, computational redundancy, and memory bloat, which\nimpede efficient navigation. Inspired by the implicit scene representation in\nhuman navigation, analogous to the left brain's semantic understanding and the\nright brain's spatial cognition, we propose JanusVLN, a novel VLN framework\nfeaturing a dual implicit neural memory that models spatial-geometric and\nvisual-semantic memory as separate, compact, and fixed-size neural\nrepresentations. This framework first extends the MLLM to incorporate 3D prior\nknowledge from the spatial-geometric encoder, thereby enhancing the spatial\nreasoning capabilities of models based solely on RGB input. Then, the\nhistorical key-value caches from the spatial-geometric and visual-semantic\nencoders are constructed into a dual implicit memory. By retaining only the KVs\nof tokens in the initial and sliding window, redundant computation is avoided,\nenabling efficient incremental updates. Extensive experiments demonstrate that\nJanusVLN outperforms over 20 recent methods to achieve SOTA performance. For\nexample, the success rate improves by 10.5-35.5 compared to methods using\nmultiple data types as input and by 3.6-10.8 compared to methods using more RGB\ntraining data. This indicates that the proposed dual implicit neural memory, as\na novel paradigm, explores promising new directions for future VLN research.\nOurs project page: https://miv-xjtu.github.io/JanusVLN.github.io/."
                },
                "authors": [
                    {
                        "name": "Shuang Zeng"
                    },
                    {
                        "name": "Dekang Qi"
                    },
                    {
                        "name": "Xinyuan Chang"
                    },
                    {
                        "name": "Feng Xiong"
                    },
                    {
                        "name": "Shichao Xie"
                    },
                    {
                        "name": "Xiaolong Wu"
                    },
                    {
                        "name": "Shiyi Liang"
                    },
                    {
                        "name": "Mu Xu"
                    },
                    {
                        "name": "Xing Wei"
                    }
                ],
                "author_detail": {
                    "name": "Xing Wei"
                },
                "author": "Xing Wei",
                "arxiv_comment": "Project page: https://miv-xjtu.github.io/JanusVLN.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22516v1",
                "updated": "2025-09-26T16:00:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    0,
                    36,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:00:36Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    0,
                    36,
                    4,
                    269,
                    0
                ],
                "title": "TrueGradeAI: Retrieval-Augmented and Bias-Resistant AI for Transparent\n  and Explainable Digital Assessments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrueGradeAI: Retrieval-Augmented and Bias-Resistant AI for Transparent\n  and Explainable Digital Assessments"
                },
                "summary": "This paper introduces TrueGradeAI, an AI-driven digital examination framework\ndesigned to overcome the shortcomings of traditional paper-based assessments,\nincluding excessive paper usage, logistical complexity, grading delays, and\nevaluator bias. The system preserves natural handwriting by capturing stylus\ninput on secure tablets and applying transformer-based optical character\nrecognition for transcription. Evaluation is conducted through a\nretrieval-augmented pipeline that integrates faculty solutions, cache layers,\nand external references, enabling a large language model to assign scores with\nexplicit, evidence-linked reasoning. Unlike prior tablet-based exam systems\nthat primarily digitize responses, TrueGradeAI advances the field by\nincorporating explainable automation, bias mitigation, and auditable grading\ntrails. By uniting handwriting preservation with scalable and transparent\nevaluation, the framework reduces environmental costs, accelerates feedback\ncycles, and progressively builds a reusable knowledge base, while actively\nworking to mitigate grading bias and ensure fairness in assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces TrueGradeAI, an AI-driven digital examination framework\ndesigned to overcome the shortcomings of traditional paper-based assessments,\nincluding excessive paper usage, logistical complexity, grading delays, and\nevaluator bias. The system preserves natural handwriting by capturing stylus\ninput on secure tablets and applying transformer-based optical character\nrecognition for transcription. Evaluation is conducted through a\nretrieval-augmented pipeline that integrates faculty solutions, cache layers,\nand external references, enabling a large language model to assign scores with\nexplicit, evidence-linked reasoning. Unlike prior tablet-based exam systems\nthat primarily digitize responses, TrueGradeAI advances the field by\nincorporating explainable automation, bias mitigation, and auditable grading\ntrails. By uniting handwriting preservation with scalable and transparent\nevaluation, the framework reduces environmental costs, accelerates feedback\ncycles, and progressively builds a reusable knowledge base, while actively\nworking to mitigate grading bias and ensure fairness in assessment."
                },
                "authors": [
                    {
                        "name": "Rakesh Thakur"
                    },
                    {
                        "name": "Shivaansh Kaushik"
                    },
                    {
                        "name": "Gauri Chopra"
                    },
                    {
                        "name": "Harsh Rohilla"
                    }
                ],
                "author_detail": {
                    "name": "Harsh Rohilla"
                },
                "author": "Harsh Rohilla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22512v1",
                "updated": "2025-09-26T15:54:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    54,
                    50,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:54:50Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    54,
                    50,
                    4,
                    269,
                    0
                ],
                "title": "AxLLM: accelerator architecture for large language models with\n  computation reuse capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AxLLM: accelerator architecture for large language models with\n  computation reuse capability"
                },
                "summary": "Large language models demand massive computational power and memory\nresources, posing significant challenges for efficient deployment. While\nquantization has been widely explored to reduce model size and computation,\nthis paper demonstrates an additional benefit: quantization increases parameter\nlocality, creating opportunities for computation reuse. Building on this\ninsight, we propose AxLLM, a hardware accelerator architecture designed for\nquantized models. Axllm introduces a novel redundancy elimination technique\nthat caches and reuses multiplication results for repeated weight values,\nsubstantially reducing redundant operations. The architecture features dual\nmultiply and reuse pipelines, efficiently supporting both base models and LoRA\nfine-tuned models without altering parameters, retraining, or requiring offline\npreprocessing. Experimental results show that AxLLM achieves up to 90%\nreduction in computations, delivering 28% lower energy consumption and a 1.7x\nspeedup over baseline execution. These results highlight Axllm as a scalable\nand efficient solution for accelerating LLMs on specialized hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models demand massive computational power and memory\nresources, posing significant challenges for efficient deployment. While\nquantization has been widely explored to reduce model size and computation,\nthis paper demonstrates an additional benefit: quantization increases parameter\nlocality, creating opportunities for computation reuse. Building on this\ninsight, we propose AxLLM, a hardware accelerator architecture designed for\nquantized models. Axllm introduces a novel redundancy elimination technique\nthat caches and reuses multiplication results for repeated weight values,\nsubstantially reducing redundant operations. The architecture features dual\nmultiply and reuse pipelines, efficiently supporting both base models and LoRA\nfine-tuned models without altering parameters, retraining, or requiring offline\npreprocessing. Experimental results show that AxLLM achieves up to 90%\nreduction in computations, delivering 28% lower energy consumption and a 1.7x\nspeedup over baseline execution. These results highlight Axllm as a scalable\nand efficient solution for accelerating LLMs on specialized hardware."
                },
                "authors": [
                    {
                        "name": "Soroush Ahadi"
                    },
                    {
                        "name": "Mehdi Modarressi"
                    },
                    {
                        "name": "Masoud Daneshtalab"
                    }
                ],
                "author_detail": {
                    "name": "Masoud Daneshtalab"
                },
                "author": "Masoud Daneshtalab",
                "arxiv_comment": "7 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "n/a",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22488v1",
                "updated": "2025-09-26T15:35:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    35,
                    5,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:35:05Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    35,
                    5,
                    4,
                    269,
                    0
                ],
                "title": "Organ dose optimization for a point-of-care forearm X-ray\n  photon-counting CT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Organ dose optimization for a point-of-care forearm X-ray\n  photon-counting CT"
                },
                "summary": "Background: Spectral shaping is a computed tomography (CT) dose optimization\ntechnique that adjusts source voltage and filtration to reduce patient\nradiation exposure without compromising image quality. Traditionally, radiation\ndose has been assessed using the computed tomography dose index (CTDI).\nHowever, emerging dosimetric approaches aim to enable patient-specific\nevaluations by estimating organ absorbed doses, providing a more accurate\nrepresentation of the biological impact. This study investigates spectral\nshaping for an extremity photon-counting detector (PCD) CT, through organ\nabsorbed dose estimation and image quality evaluation. Method: Monte Carlo\nsimulations were conducted to evaluate various combinations of source voltage\nand filtration. Tube voltage ranged from 80 to 140 kV, combined with three\ndistinct filtration material and thicknesses. Simulations included three\nstages: a standardized phantom for CTDI assessment, an adult forearm phantom\nfor organ dose measurement, and an image quality phantom for evaluation of an\nadvanced image quality metric: the detectability index. Results: In a wrist\nPCD-CT imaging protocol, operating the source at 80 kV can reduce the radiation\ndose by up to 50%. This reduction is achieved while maintaining the same\ndetectability index value as the standard 120 kV protocol. However, the optimal\nfiltration depends on the organ targeted for dose reduction, as bone and skin\nbenefit from opposing filtration approaches. While CTDI provides a useful\ninitial estimate, it may lead to suboptimal optimization compared to\norgan-specific dose evaluation. Conclusions: Patient-specific dosimetry based\non organ absorbed dose estimation offers a more accurate framework for\noptimizing CT protocols through spectral shaping than conventional CTDI-based\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Spectral shaping is a computed tomography (CT) dose optimization\ntechnique that adjusts source voltage and filtration to reduce patient\nradiation exposure without compromising image quality. Traditionally, radiation\ndose has been assessed using the computed tomography dose index (CTDI).\nHowever, emerging dosimetric approaches aim to enable patient-specific\nevaluations by estimating organ absorbed doses, providing a more accurate\nrepresentation of the biological impact. This study investigates spectral\nshaping for an extremity photon-counting detector (PCD) CT, through organ\nabsorbed dose estimation and image quality evaluation. Method: Monte Carlo\nsimulations were conducted to evaluate various combinations of source voltage\nand filtration. Tube voltage ranged from 80 to 140 kV, combined with three\ndistinct filtration material and thicknesses. Simulations included three\nstages: a standardized phantom for CTDI assessment, an adult forearm phantom\nfor organ dose measurement, and an image quality phantom for evaluation of an\nadvanced image quality metric: the detectability index. Results: In a wrist\nPCD-CT imaging protocol, operating the source at 80 kV can reduce the radiation\ndose by up to 50%. This reduction is achieved while maintaining the same\ndetectability index value as the standard 120 kV protocol. However, the optimal\nfiltration depends on the organ targeted for dose reduction, as bone and skin\nbenefit from opposing filtration approaches. While CTDI provides a useful\ninitial estimate, it may lead to suboptimal optimization compared to\norgan-specific dose evaluation. Conclusions: Patient-specific dosimetry based\non organ absorbed dose estimation offers a more accurate framework for\noptimizing CT protocols through spectral shaping than conventional CTDI-based\napproaches."
                },
                "authors": [
                    {
                        "name": "Pierre-Antoine Rodesch"
                    },
                    {
                        "name": "Anaïs Viry"
                    },
                    {
                        "name": "Mouad Khorsi"
                    },
                    {
                        "name": "Fabio Becce"
                    },
                    {
                        "name": "Jérôme Damet"
                    },
                    {
                        "name": "Lucía Gallego Manzano"
                    }
                ],
                "author_detail": {
                    "name": "Lucía Gallego Manzano"
                },
                "author": "Lucía Gallego Manzano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16950v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16950v3",
                "updated": "2025-09-26T14:35:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    35,
                    4,
                    4,
                    269,
                    0
                ],
                "published": "2025-05-22T17:33:49Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    33,
                    49,
                    3,
                    142,
                    0
                ],
                "title": "Bottlenecked Transformers: Periodic KV Cache Consolidation for\n  Generalised Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bottlenecked Transformers: Periodic KV Cache Consolidation for\n  Generalised Reasoning"
                },
                "summary": "Transformer LLMs have been shown to exhibit strong reasoning ability that\nscales with inference-time compute, most prominently through token-space\n\"thinking\" chains of thought. A growing line of work pushes extra computation\ninto the model's latent space, which we term Auxiliary Latent-Space Computation\n(ALSC). Existing ALSC methods largely fall into three buckets: (i)\ntoken-mediated latent rollouts, (ii) residual/activation steering, and (iii)\nmemory (KV) compression. An underexplored alternative is memory\nconsolidation/reconsolidation, two processes in the brain that are responsible\nfor stabilising newly formed memory traces, and, upon recall, transiently\nrendering established traces plastic such they can integrate new contextual\ninformation before restabilising. In Transformer LLMs, this can be seen as\nanalogous to performing in-place rewrites of new KV segments, and rewrites of\nrecalled past segments. In this work, we give a theoretical justification as to\nwhy memory (re)consolidation via KV cache rewrites is beneficial for improved\nreasoning. We do this through the lens of Information Bottleneck (IB) theory,\nwhich posits that model generalisation emerges from an optimal balance between\ninput information compression and retention of predictive information in latent\nrepresentations. We then introduce the Bottlenecked Transformer, which augments\na backbone LLM with a Cache Processor, an auxiliary Transformer that performs\nperiodic, non-causal, in-place KV rewrites at newline-delimited reasoning step\nboundaries. The Processor consolidates recently written KV entries and\nreconsolidates a small, top-k attention-selected set of prior entries. We\nevaluate our Bottlenecked Transformer architecture on math reasoning\nbenchmarks. Our model sees consistent performance gains over vanilla\nTransformers and pause-token augmented baselines, with gains of up to +6.6pp\nfor selected tasks/backbones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer LLMs have been shown to exhibit strong reasoning ability that\nscales with inference-time compute, most prominently through token-space\n\"thinking\" chains of thought. A growing line of work pushes extra computation\ninto the model's latent space, which we term Auxiliary Latent-Space Computation\n(ALSC). Existing ALSC methods largely fall into three buckets: (i)\ntoken-mediated latent rollouts, (ii) residual/activation steering, and (iii)\nmemory (KV) compression. An underexplored alternative is memory\nconsolidation/reconsolidation, two processes in the brain that are responsible\nfor stabilising newly formed memory traces, and, upon recall, transiently\nrendering established traces plastic such they can integrate new contextual\ninformation before restabilising. In Transformer LLMs, this can be seen as\nanalogous to performing in-place rewrites of new KV segments, and rewrites of\nrecalled past segments. In this work, we give a theoretical justification as to\nwhy memory (re)consolidation via KV cache rewrites is beneficial for improved\nreasoning. We do this through the lens of Information Bottleneck (IB) theory,\nwhich posits that model generalisation emerges from an optimal balance between\ninput information compression and retention of predictive information in latent\nrepresentations. We then introduce the Bottlenecked Transformer, which augments\na backbone LLM with a Cache Processor, an auxiliary Transformer that performs\nperiodic, non-causal, in-place KV rewrites at newline-delimited reasoning step\nboundaries. The Processor consolidates recently written KV entries and\nreconsolidates a small, top-k attention-selected set of prior entries. We\nevaluate our Bottlenecked Transformer architecture on math reasoning\nbenchmarks. Our model sees consistent performance gains over vanilla\nTransformers and pause-token augmented baselines, with gains of up to +6.6pp\nfor selected tasks/backbones."
                },
                "authors": [
                    {
                        "name": "Adnan Oomerjee"
                    },
                    {
                        "name": "Zafeirios Fountas"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16950v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16950v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22323v1",
                "updated": "2025-09-26T13:20:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    20,
                    52,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T13:20:52Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    20,
                    52,
                    4,
                    269,
                    0
                ],
                "title": "RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion\n  Transformer"
                },
                "summary": "Diffusion Transformers (DiTs) excel at visual generation yet remain hampered\nby slow sampling. Existing training-free accelerators - step reduction, feature\ncaching, and sparse attention - enhance inference speed but typically rely on a\nuniform heuristic or a manually designed adaptive strategy for all images,\nleaving quality on the table. Alternatively, dynamic neural networks offer\nper-image adaptive acceleration, but their high fine-tuning costs limit broader\napplicability. To address these limitations, we introduce RAPID3: Tri-Level\nReinforced Acceleration Policies for Diffusion Transformers, a framework that\ndelivers image-wise acceleration with zero updates to the base generator.\nSpecifically, three lightweight policy heads - Step-Skip, Cache-Reuse, and\nSparse-Attention - observe the current denoising state and independently decide\ntheir corresponding speed-up at each timestep. All policy parameters are\ntrained online via Group Relative Policy Optimization (GRPO) while the\ngenerator remains frozen. Meanwhile, an adversarially learned discriminator\naugments the reward signal, discouraging reward hacking by boosting returns\nonly when generated samples stay close to the original model's distribution.\nAcross state-of-the-art DiT backbones, including Stable Diffusion 3 and FLUX,\nRAPID3 achieves nearly 3x faster sampling with competitive generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel at visual generation yet remain hampered\nby slow sampling. Existing training-free accelerators - step reduction, feature\ncaching, and sparse attention - enhance inference speed but typically rely on a\nuniform heuristic or a manually designed adaptive strategy for all images,\nleaving quality on the table. Alternatively, dynamic neural networks offer\nper-image adaptive acceleration, but their high fine-tuning costs limit broader\napplicability. To address these limitations, we introduce RAPID3: Tri-Level\nReinforced Acceleration Policies for Diffusion Transformers, a framework that\ndelivers image-wise acceleration with zero updates to the base generator.\nSpecifically, three lightweight policy heads - Step-Skip, Cache-Reuse, and\nSparse-Attention - observe the current denoising state and independently decide\ntheir corresponding speed-up at each timestep. All policy parameters are\ntrained online via Group Relative Policy Optimization (GRPO) while the\ngenerator remains frozen. Meanwhile, an adversarially learned discriminator\naugments the reward signal, discouraging reward hacking by boosting returns\nonly when generated samples stay close to the original model's distribution.\nAcross state-of-the-art DiT backbones, including Stable Diffusion 3 and FLUX,\nRAPID3 achieves nearly 3x faster sampling with competitive generation quality."
                },
                "authors": [
                    {
                        "name": "Wangbo Zhao"
                    },
                    {
                        "name": "Yizeng Han"
                    },
                    {
                        "name": "Zhiwei Tang"
                    },
                    {
                        "name": "Jiasheng Tang"
                    },
                    {
                        "name": "Pengfei Zhou"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v8",
                "updated": "2025-09-26T10:00:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    10,
                    0,
                    54,
                    4,
                    269,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13681v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13681v2",
                "updated": "2025-09-26T07:14:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    7,
                    14,
                    44,
                    4,
                    269,
                    0
                ],
                "published": "2025-07-18T06:12:08Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    12,
                    8,
                    4,
                    199,
                    0
                ],
                "title": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues"
                },
                "summary": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. As a result,\nthese models cannot accurately identify and prioritize the most relevant\ncontext, leading to degraded response quality. In this paper, we present\nLoopServe, an adaptive dual-phase inference acceleration framework for large\nlanguage models in multi-turn dialogues. LoopServe introduces two main\ninnovations. First, it performs online sparsification during the prefilling\nphase by dynamically selecting the most important parts of the attention matrix\nfor each new input. Second, it uses progressive key value compression during\ndecoding by adaptively maintaining a relevant and efficient cache based on the\nmost recently generated output tokens. We also propose a new benchmark with\neleven multi-turn datasets that reflect realistic query positions and\nconversational dependencies. Extensive experiments demonstrate that LoopServe\nconsistently achieves superior effectiveness compared to existing baselines and\nsignificantly accelerates LLM inference across a wide range of long-context\ndialogue tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. As a result,\nthese models cannot accurately identify and prioritize the most relevant\ncontext, leading to degraded response quality. In this paper, we present\nLoopServe, an adaptive dual-phase inference acceleration framework for large\nlanguage models in multi-turn dialogues. LoopServe introduces two main\ninnovations. First, it performs online sparsification during the prefilling\nphase by dynamically selecting the most important parts of the attention matrix\nfor each new input. Second, it uses progressive key value compression during\ndecoding by adaptively maintaining a relevant and efficient cache based on the\nmost recently generated output tokens. We also propose a new benchmark with\neleven multi-turn datasets that reflect realistic query positions and\nconversational dependencies. Extensive experiments demonstrate that LoopServe\nconsistently achieves superior effectiveness compared to existing baselines and\nsignificantly accelerates LLM inference across a wide range of long-context\ndialogue tasks."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Darian Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Qingfa Xiao"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13681v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13681v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21917v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21917v1",
                "updated": "2025-09-26T05:57:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    5,
                    57,
                    4,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T05:57:04Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    5,
                    57,
                    4,
                    4,
                    269,
                    0
                ],
                "title": "Taming Flow-based I2V Models for Creative Video Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming Flow-based I2V Models for Creative Video Editing"
                },
                "summary": "Although image editing techniques have advanced significantly, video editing,\nwhich aims to manipulate videos according to user intent, remains an emerging\nchallenge. Most existing image-conditioned video editing methods either require\ninversion with model-specific design or need extensive optimization, limiting\ntheir capability of leveraging up-to-date image-to-video (I2V) models to\ntransfer the editing capability of image editing models to the video domain. To\nthis end, we propose IF-V2V, an Inversion-Free method that can adapt\noff-the-shelf flow-matching-based I2V models for video editing without\nsignificant computational overhead. To circumvent inversion, we devise Vector\nField Rectification with Sample Deviation to incorporate information from the\nsource video into the denoising process by introducing a deviation term into\nthe denoising vector field. To further ensure consistency with the source video\nin a model-agnostic way, we introduce Structure-and-Motion-Preserving\nInitialization to generate motion-aware temporally correlated noise with\nstructural information embedded. We also present a Deviation Caching mechanism\nto minimize the additional computational cost for denoising vector\nrectification without significantly impacting editing quality. Evaluations\ndemonstrate that our method achieves superior editing quality and consistency\nover existing approaches, offering a lightweight plug-and-play solution to\nrealize visual creativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although image editing techniques have advanced significantly, video editing,\nwhich aims to manipulate videos according to user intent, remains an emerging\nchallenge. Most existing image-conditioned video editing methods either require\ninversion with model-specific design or need extensive optimization, limiting\ntheir capability of leveraging up-to-date image-to-video (I2V) models to\ntransfer the editing capability of image editing models to the video domain. To\nthis end, we propose IF-V2V, an Inversion-Free method that can adapt\noff-the-shelf flow-matching-based I2V models for video editing without\nsignificant computational overhead. To circumvent inversion, we devise Vector\nField Rectification with Sample Deviation to incorporate information from the\nsource video into the denoising process by introducing a deviation term into\nthe denoising vector field. To further ensure consistency with the source video\nin a model-agnostic way, we introduce Structure-and-Motion-Preserving\nInitialization to generate motion-aware temporally correlated noise with\nstructural information embedded. We also present a Deviation Caching mechanism\nto minimize the additional computational cost for denoising vector\nrectification without significantly impacting editing quality. Evaluations\ndemonstrate that our method achieves superior editing quality and consistency\nover existing approaches, offering a lightweight plug-and-play solution to\nrealize visual creativity."
                },
                "authors": [
                    {
                        "name": "Xianghao Kong"
                    },
                    {
                        "name": "Hansheng Chen"
                    },
                    {
                        "name": "Yuwei Guo"
                    },
                    {
                        "name": "Lvmin Zhang"
                    },
                    {
                        "name": "Gordon Wetzstein"
                    },
                    {
                        "name": "Maneesh Agrawala"
                    },
                    {
                        "name": "Anyi Rao"
                    }
                ],
                "author_detail": {
                    "name": "Anyi Rao"
                },
                "author": "Anyi Rao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21917v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21917v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21857v1",
                "updated": "2025-09-26T04:32:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    4,
                    32,
                    56,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T04:32:56Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    4,
                    32,
                    56,
                    4,
                    269,
                    0
                ],
                "title": "2.34 kV \\b{eta}-Ga2O3 Vertical Trench RESURF Schottky Barrier Diode with\n  sub-micron fin width",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2.34 kV \\b{eta}-Ga2O3 Vertical Trench RESURF Schottky Barrier Diode with\n  sub-micron fin width"
                },
                "summary": "In this letter, we present a kilovolt-class \\b{eta}-Ga2O3 vertical trench\nSchottky barrier diode with a field plate incorporating narrow fin width (Wfin)\nstructures of sub-micron dimensions. We used a nanolaminate dielectric\ncomprising a stack of multiple thin TiO2 and Al2O3 layers as RESURF dielectric\nand for field plate edge termination. Both Wfin of 200 nm and 500 nm\ndemonstrate excellent on-state performance with specific on-resistance (Ron,sp)\nof 9.8-12 mohmcm2, and 10^10 rectification ratio. A self-aligned photoresist\nplanarization and etch-back process was employed to expose the top of the fins\nfor Schottky contact formation, eliminating critical lithographic alignment\nchallenges in sub-micron scale processing. We achieved a breakdown of 2.34 kV\nwith very low leakage currents before catastrophic breakdown. The measured\nbreakdown voltage is limited by dielectric breakdown at the trench bottom\ncorner as verified by metal-oxide-semiconductor (MOS) test structure. TCAD\nsimulation shows a reduced electric field at the surface of the\nmetal-semiconductor junction due to the RESURF effect, resulting in very low\nreverse leakage before breakdown. The parallel plane electric field in the\n\\b{eta} -Ga2O3 is extracted to be 3.8 MV/cm from TCAD simulations using\naccurately extracted drift layer doping profile from high voltage CV\nmeasurements. A power figure of merit of 0.867 GW/cm2(0.56 GW/cm2 with current\nspreading) was calculated. Enhanced RESURF by integration of high-k dielectrics\nwith self-aligned photoresist planarization, offers a promising pathway towards\nhigh figure of merit, low leakage high-performance vertical devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this letter, we present a kilovolt-class \\b{eta}-Ga2O3 vertical trench\nSchottky barrier diode with a field plate incorporating narrow fin width (Wfin)\nstructures of sub-micron dimensions. We used a nanolaminate dielectric\ncomprising a stack of multiple thin TiO2 and Al2O3 layers as RESURF dielectric\nand for field plate edge termination. Both Wfin of 200 nm and 500 nm\ndemonstrate excellent on-state performance with specific on-resistance (Ron,sp)\nof 9.8-12 mohmcm2, and 10^10 rectification ratio. A self-aligned photoresist\nplanarization and etch-back process was employed to expose the top of the fins\nfor Schottky contact formation, eliminating critical lithographic alignment\nchallenges in sub-micron scale processing. We achieved a breakdown of 2.34 kV\nwith very low leakage currents before catastrophic breakdown. The measured\nbreakdown voltage is limited by dielectric breakdown at the trench bottom\ncorner as verified by metal-oxide-semiconductor (MOS) test structure. TCAD\nsimulation shows a reduced electric field at the surface of the\nmetal-semiconductor junction due to the RESURF effect, resulting in very low\nreverse leakage before breakdown. The parallel plane electric field in the\n\\b{eta} -Ga2O3 is extracted to be 3.8 MV/cm from TCAD simulations using\naccurately extracted drift layer doping profile from high voltage CV\nmeasurements. A power figure of merit of 0.867 GW/cm2(0.56 GW/cm2 with current\nspreading) was calculated. Enhanced RESURF by integration of high-k dielectrics\nwith self-aligned photoresist planarization, offers a promising pathway towards\nhigh figure of merit, low leakage high-performance vertical devices."
                },
                "authors": [
                    {
                        "name": "Chinmoy Nath Saha"
                    },
                    {
                        "name": "Saurav Roy"
                    },
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Carl Peterson"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21842v1",
                "updated": "2025-09-26T04:03:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    4,
                    3,
                    52,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T04:03:52Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    4,
                    3,
                    52,
                    4,
                    269,
                    0
                ],
                "title": "DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for\n  Autonomous Travel Planning Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for\n  Autonomous Travel Planning Agents"
                },
                "summary": "Travel planning (TP) agent has recently worked as an emerging building block\nto interact with external tools and resources for travel itinerary generation,\nensuring enjoyable user experience. Despite its benefits, existing studies rely\non hand craft prompt and fixed agent workflow, hindering more flexible and\nautonomous TP agent. This paper proposes DeepTravel, an end to end agentic\nreinforcement learning framework for building autonomous travel planning agent,\ncapable of autonomously planning, executing tools, and reflecting on tool\nresponses to explore, verify, and refine intermediate actions in multi step\nreasoning. To achieve this, we first construct a robust sandbox environment by\ncaching transportation, accommodation and POI data, facilitating TP agent\ntraining without being constrained by real world APIs limitations (e.g.,\ninconsistent outputs). Moreover, we develop a hierarchical reward modeling\nsystem, where a trajectory level verifier first checks spatiotemporal\nfeasibility and filters unsatisfied travel itinerary, and then the turn level\nverifier further validate itinerary detail consistency with tool responses,\nenabling efficient and precise reward service. Finally, we propose the reply\naugmented reinforcement learning method that enables TP agent to periodically\nreplay from a failures experience buffer, emerging notable agentic capacity. We\ndeploy trained TP agent on DiDi Enterprise Solutions App and conduct\ncomprehensive online and offline evaluations, demonstrating that DeepTravel\nenables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing\nfrontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Travel planning (TP) agent has recently worked as an emerging building block\nto interact with external tools and resources for travel itinerary generation,\nensuring enjoyable user experience. Despite its benefits, existing studies rely\non hand craft prompt and fixed agent workflow, hindering more flexible and\nautonomous TP agent. This paper proposes DeepTravel, an end to end agentic\nreinforcement learning framework for building autonomous travel planning agent,\ncapable of autonomously planning, executing tools, and reflecting on tool\nresponses to explore, verify, and refine intermediate actions in multi step\nreasoning. To achieve this, we first construct a robust sandbox environment by\ncaching transportation, accommodation and POI data, facilitating TP agent\ntraining without being constrained by real world APIs limitations (e.g.,\ninconsistent outputs). Moreover, we develop a hierarchical reward modeling\nsystem, where a trajectory level verifier first checks spatiotemporal\nfeasibility and filters unsatisfied travel itinerary, and then the turn level\nverifier further validate itinerary detail consistency with tool responses,\nenabling efficient and precise reward service. Finally, we propose the reply\naugmented reinforcement learning method that enables TP agent to periodically\nreplay from a failures experience buffer, emerging notable agentic capacity. We\ndeploy trained TP agent on DiDi Enterprise Solutions App and conduct\ncomprehensive online and offline evaluations, demonstrating that DeepTravel\nenables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing\nfrontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks."
                },
                "authors": [
                    {
                        "name": "Yansong Ning"
                    },
                    {
                        "name": "Rui Liu"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Jun Fang"
                    },
                    {
                        "name": "Kan Zheng"
                    },
                    {
                        "name": "Naiqiang Tan"
                    },
                    {
                        "name": "Hao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Liu"
                },
                "author": "Hao Liu",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01199v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01199v3",
                "updated": "2025-09-26T03:24:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    3,
                    24,
                    20,
                    4,
                    269,
                    0
                ],
                "published": "2025-03-03T05:52:02Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    5,
                    52,
                    2,
                    0,
                    62,
                    0
                ],
                "title": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via\n  System and Algorithm Codesign",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via\n  System and Algorithm Codesign"
                },
                "summary": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude."
                },
                "authors": [
                    {
                        "name": "Kaimin Liao"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Luchao Wang"
                    },
                    {
                        "name": "Yaohua Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yaohua Tang"
                },
                "author": "Yaohua Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01199v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01199v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19375v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19375v3",
                "updated": "2025-09-26T03:17:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    3,
                    17,
                    15,
                    4,
                    269,
                    0
                ],
                "published": "2024-09-28T15:03:28Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    3,
                    28,
                    5,
                    272,
                    0
                ],
                "title": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models"
                },
                "summary": "Vision-language foundation models (VLMs), such as CLIP, exhibit remarkable\nperformance across a wide range of tasks. However, deploying these models can\nbe unreliable when significant distribution gaps exist between training and\ntest data, while fine-tuning for diverse scenarios is often costly. Cache-based\ntest-time adapters offer an efficient alternative by storing representative\ntest samples to guide subsequent classifications. Yet, these methods typically\nemploy naive cache management with limited capacity, leading to severe\ncatastrophic forgetting when samples are inevitably dropped during updates. In\nthis paper, we propose DOTA (DistributiOnal Test-time Adaptation), a simple yet\neffective method addressing this limitation. Crucially, instead of merely\nmemorizing individual test samples, DOTA continuously estimates the underlying\ndistribution of the test data stream. Test-time posterior probabilities are\nthen computed using these dynamically estimated distributions via Bayes'\ntheorem for adaptation. This distribution-centric approach enables the model to\ncontinually learn and adapt to the deployment environment. Extensive\nexperiments validate that DOTA significantly mitigates forgetting and achieves\nstate-of-the-art performance compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language foundation models (VLMs), such as CLIP, exhibit remarkable\nperformance across a wide range of tasks. However, deploying these models can\nbe unreliable when significant distribution gaps exist between training and\ntest data, while fine-tuning for diverse scenarios is often costly. Cache-based\ntest-time adapters offer an efficient alternative by storing representative\ntest samples to guide subsequent classifications. Yet, these methods typically\nemploy naive cache management with limited capacity, leading to severe\ncatastrophic forgetting when samples are inevitably dropped during updates. In\nthis paper, we propose DOTA (DistributiOnal Test-time Adaptation), a simple yet\neffective method addressing this limitation. Crucially, instead of merely\nmemorizing individual test samples, DOTA continuously estimates the underlying\ndistribution of the test data stream. Test-time posterior probabilities are\nthen computed using these dynamically estimated distributions via Bayes'\ntheorem for adaptation. This distribution-centric approach enables the model to\ncontinually learn and adapt to the deployment environment. Extensive\nexperiments validate that DOTA significantly mitigates forgetting and achieves\nstate-of-the-art performance compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Zongbo Han"
                    },
                    {
                        "name": "Jialong Yang"
                    },
                    {
                        "name": "Guangyu Wang"
                    },
                    {
                        "name": "Junfan Li"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Changqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zhang"
                },
                "author": "Changqing Zhang",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19375v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19375v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21623v1",
                "updated": "2025-09-25T21:42:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    21,
                    42,
                    27,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T21:42:27Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    21,
                    42,
                    27,
                    3,
                    268,
                    0
                ],
                "title": "OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's\n  Rule",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's\n  Rule"
                },
                "summary": "The expanding long-context capabilities of large language models are\nconstrained by a significant memory bottleneck: the key-value (KV) cache\nrequired for autoregressive generation. This bottleneck is substantial; for\ninstance, a Llama-3.1-8B model processing a 32K-token prompt at a batch size of\n4 requires approximately 16GB for its KV cache, a size exceeding the model's\nweights. While KV-cache compression via low-rank projection is a promising\ndirection, existing methods rely on a static, offline-learned subspace that\nperforms poorly under data distribution shifts. To overcome these limitations,\nwe introduce OjaKV, a novel framework that integrates a strategic hybrid\nstorage policy with online subspace adaptation. First, OjaKV recognizes that\nnot all tokens are equally important for compression; it preserves the crucial\nfirst and most recent tokens in full-rank, maintaining high-fidelity anchors\nfor attention. Second, for the vast majority of intermediate tokens, it applies\nlow-rank compression by incrementally adapting the projection basis using Oja's\nalgorithm for online principal component analysis. This adaptation involves a\ncomprehensive update during prompt prefilling and lightweight periodic updates\nduring decoding, ensuring the subspace remains aligned with the evolving\ncontext. Crucially, our framework is fully compatible with modern attention\nmodules like FlashAttention. Experiments demonstrate that OjaKV maintains or\neven improves zero-shot accuracy at high compression ratios. In particular,\nOjaKV achieves its strongest gains on very long-context benchmarks that require\ncomplex reasoning, highlighting the importance of online subspace adaptation in\ndynamically tracking context shifts. These results establish our hybrid\nframework as a practical, plug-and-play solution for memory-efficient\nlong-context inference without requiring model fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expanding long-context capabilities of large language models are\nconstrained by a significant memory bottleneck: the key-value (KV) cache\nrequired for autoregressive generation. This bottleneck is substantial; for\ninstance, a Llama-3.1-8B model processing a 32K-token prompt at a batch size of\n4 requires approximately 16GB for its KV cache, a size exceeding the model's\nweights. While KV-cache compression via low-rank projection is a promising\ndirection, existing methods rely on a static, offline-learned subspace that\nperforms poorly under data distribution shifts. To overcome these limitations,\nwe introduce OjaKV, a novel framework that integrates a strategic hybrid\nstorage policy with online subspace adaptation. First, OjaKV recognizes that\nnot all tokens are equally important for compression; it preserves the crucial\nfirst and most recent tokens in full-rank, maintaining high-fidelity anchors\nfor attention. Second, for the vast majority of intermediate tokens, it applies\nlow-rank compression by incrementally adapting the projection basis using Oja's\nalgorithm for online principal component analysis. This adaptation involves a\ncomprehensive update during prompt prefilling and lightweight periodic updates\nduring decoding, ensuring the subspace remains aligned with the evolving\ncontext. Crucially, our framework is fully compatible with modern attention\nmodules like FlashAttention. Experiments demonstrate that OjaKV maintains or\neven improves zero-shot accuracy at high compression ratios. In particular,\nOjaKV achieves its strongest gains on very long-context benchmarks that require\ncomplex reasoning, highlighting the importance of online subspace adaptation in\ndynamically tracking context shifts. These results establish our hybrid\nframework as a practical, plug-and-play solution for memory-efficient\nlong-context inference without requiring model fine-tuning."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "David H. Yang"
                    },
                    {
                        "name": "Mohammad Mohammadi Amiri"
                    },
                    {
                        "name": "Keerthiram Murugesan"
                    },
                    {
                        "name": "Tejaswini Pedapati"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Pin-Yu Chen"
                },
                "author": "Pin-Yu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21463v1",
                "updated": "2025-09-25T19:29:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    19,
                    29,
                    25,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T19:29:25Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    19,
                    29,
                    25,
                    3,
                    268,
                    0
                ],
                "title": "Enhanced Generative Machine Listener",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Generative Machine Listener"
                },
                "summary": "We present GMLv2, a reference-based model designed for the prediction of\nsubjective audio quality as measured by MUSHRA scores. GMLv2 introduces a Beta\ndistribution-based loss to model the listener ratings and incorporates\nadditional neural audio coding (NAC) subjective datasets to extend its\ngeneralization and applicability. Extensive evaluations on diverse testset\ndemonstrate that proposed GMLv2 consistently outperforms widely used metrics,\nsuch as PEAQ and ViSQOL, both in terms of correlation with subjective scores\nand in reliably predicting these scores across diverse content types and codec\nconfigurations. Consequently, GMLv2 offers a scalable and automated framework\nfor perceptual audio quality evaluation, poised to accelerate research and\ndevelopment in modern audio coding technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present GMLv2, a reference-based model designed for the prediction of\nsubjective audio quality as measured by MUSHRA scores. GMLv2 introduces a Beta\ndistribution-based loss to model the listener ratings and incorporates\nadditional neural audio coding (NAC) subjective datasets to extend its\ngeneralization and applicability. Extensive evaluations on diverse testset\ndemonstrate that proposed GMLv2 consistently outperforms widely used metrics,\nsuch as PEAQ and ViSQOL, both in terms of correlation with subjective scores\nand in reliably predicting these scores across diverse content types and codec\nconfigurations. Consequently, GMLv2 offers a scalable and automated framework\nfor perceptual audio quality evaluation, poised to accelerate research and\ndevelopment in modern audio coding technologies."
                },
                "authors": [
                    {
                        "name": "Vishnu Raj"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Shiv Gehlot"
                    },
                    {
                        "name": "Lars Villemoes"
                    },
                    {
                        "name": "Arijit Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Arijit Biswas"
                },
                "author": "Arijit Biswas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10568v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10568v2",
                "updated": "2025-09-25T13:55:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    55,
                    44,
                    3,
                    268,
                    0
                ],
                "published": "2025-03-13T17:19:51Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Randomized Parallel Decoding"
                },
                "summary": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel decoupled decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot inference tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.83 with only\n32 sampling steps, achieving over a 30 times speedup in inference and a 75\npercent reduction in memory consumption compared to representative recent\nautoregressive models at a similar scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel decoupled decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot inference tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.83 with only\n32 sampling steps, achieving over a 30 times speedup in inference and a 75\npercent reduction in memory consumption compared to representative recent\nautoregressive models at a similar scale."
                },
                "authors": [
                    {
                        "name": "Haopeng Li"
                    },
                    {
                        "name": "Jinyue Yang"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10568v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10568v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21153v1",
                "updated": "2025-09-25T13:39:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    39,
                    16,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:39:16Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    39,
                    16,
                    3,
                    268,
                    0
                ],
                "title": "WAVECLIP: Wavelet Tokenization for Adaptive-Resolution CLIP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WAVECLIP: Wavelet Tokenization for Adaptive-Resolution CLIP"
                },
                "summary": "We introduce WAVECLIP, a single unified model for adaptive resolution\ninference in CLIP, enabled by wavelet-based tokenization. WAVECLIP replaces\nstandard patch embeddings with a multi-level wavelet decomposition, enabling\nthe model to process images coarse to fine while naturally supporting multiple\nresolutions within the same model. At inference time, the model begins with low\nresolution tokens and refines only when needed, using key-value caching and\ncausal cross-level attention to reuse computation, effectively introducing to\nthe model only new information when needed. We evaluate WAVECLIP in zero-shot\nclassification, demonstrating that a simple confidence-based gating mechanism\nenables adaptive early exits. This allows users to dynamically choose a\ncompute-accuracy trade-off using a single deployed model. Our approach requires\nonly lightweight distillation from a frozen CLIP teacher and achieves\ncompetitive accuracy with significant computational savings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce WAVECLIP, a single unified model for adaptive resolution\ninference in CLIP, enabled by wavelet-based tokenization. WAVECLIP replaces\nstandard patch embeddings with a multi-level wavelet decomposition, enabling\nthe model to process images coarse to fine while naturally supporting multiple\nresolutions within the same model. At inference time, the model begins with low\nresolution tokens and refines only when needed, using key-value caching and\ncausal cross-level attention to reuse computation, effectively introducing to\nthe model only new information when needed. We evaluate WAVECLIP in zero-shot\nclassification, demonstrating that a simple confidence-based gating mechanism\nenables adaptive early exits. This allows users to dynamically choose a\ncompute-accuracy trade-off using a single deployed model. Our approach requires\nonly lightweight distillation from a frozen CLIP teacher and achieves\ncompetitive accuracy with significant computational savings."
                },
                "authors": [
                    {
                        "name": "Moshe Kimhi"
                    },
                    {
                        "name": "Erez Koifman"
                    },
                    {
                        "name": "Ehud Rivlin"
                    },
                    {
                        "name": "Eli Schwartz"
                    },
                    {
                        "name": "Chaim Baskin"
                    }
                ],
                "author_detail": {
                    "name": "Chaim Baskin"
                },
                "author": "Chaim Baskin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22156v2",
                "updated": "2025-09-25T13:15:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    15,
                    45,
                    3,
                    268,
                    0
                ],
                "published": "2025-05-28T09:20:18Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    9,
                    20,
                    18,
                    2,
                    148,
                    0
                ],
                "title": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing"
                },
                "summary": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method."
                },
                "authors": [
                    {
                        "name": "Shuaiyi Li"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Chenlong Deng"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "arxiv_comment": "18 pages,5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17138v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17138v4",
                "updated": "2025-09-27T07:41:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    7,
                    41,
                    38,
                    5,
                    270,
                    0
                ],
                "published": "2025-05-22T06:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    12,
                    42,
                    3,
                    142,
                    0
                ],
                "title": "Runtime Adaptive Pruning for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Runtime Adaptive Pruning for LLM Inference"
                },
                "summary": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly."
                },
                "authors": [
                    {
                        "name": "Huanrong Liu"
                    },
                    {
                        "name": "Chunlin Tian"
                    },
                    {
                        "name": "Xuyang Wei"
                    },
                    {
                        "name": "Qingbiao Li"
                    },
                    {
                        "name": "Li Li"
                    }
                ],
                "author_detail": {
                    "name": "Li Li"
                },
                "author": "Li Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17138v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17138v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17396v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17396v2",
                "updated": "2025-09-25T10:24:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    10,
                    24,
                    14,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-22T06:56:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    56,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering"
                },
                "summary": "Modern large language models (LLMs) extend context lengths to up to millions\nof tokens, enabling AI assistants to generate coherent and personalized\nresponses grounded in long conversational histories. This ability, however,\nhinges on Key-Value (KV) caching, whose memory grows linearly with dialogue\nlength and quickly becomes the bottleneck in resource-constrained environments.\nAn active line of research for reducing memory bottleneck is KV cache\ncompression, which seeks to limit cache size while preserving accuracy. Yet\nexisting methods face two major limitations: (i) evicting the KV cache after\nfull-context prefill causes unbounded peak memory, and (ii) query-dependent\neviction narrows the cache to a single query, leading to failure cases in\nmulti-turn conversations. We introduce EpiCache, a training-free KV cache\nmanagement framework for long conversational question answering (LongConvQA)\nunder fixed memory budgets. EpiCache bounds cache growth through block-wise\nprefill and preserves topic-relevant context via episodic KV compression, which\nclusters conversation history into coherent episodes and applies\nepisode-specific KV cache eviction. We further design an adaptive layer-wise\nbudget allocation strategy that measures each layer's sensitivity to eviction\nand distributes the memory budget across layers accordingly. Across three\nLongConvQA benchmarks, EpiCache improves accuracy by up to 40% over recent\nbaselines, sustains near-full KV accuracy under 4-6x compression, and reduces\nlatency and memory by up to 2.4x and 3.5x, thereby enabling efficient\nmulti-turn interaction under strict resource constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) extend context lengths to up to millions\nof tokens, enabling AI assistants to generate coherent and personalized\nresponses grounded in long conversational histories. This ability, however,\nhinges on Key-Value (KV) caching, whose memory grows linearly with dialogue\nlength and quickly becomes the bottleneck in resource-constrained environments.\nAn active line of research for reducing memory bottleneck is KV cache\ncompression, which seeks to limit cache size while preserving accuracy. Yet\nexisting methods face two major limitations: (i) evicting the KV cache after\nfull-context prefill causes unbounded peak memory, and (ii) query-dependent\neviction narrows the cache to a single query, leading to failure cases in\nmulti-turn conversations. We introduce EpiCache, a training-free KV cache\nmanagement framework for long conversational question answering (LongConvQA)\nunder fixed memory budgets. EpiCache bounds cache growth through block-wise\nprefill and preserves topic-relevant context via episodic KV compression, which\nclusters conversation history into coherent episodes and applies\nepisode-specific KV cache eviction. We further design an adaptive layer-wise\nbudget allocation strategy that measures each layer's sensitivity to eviction\nand distributes the memory budget across layers accordingly. Across three\nLongConvQA benchmarks, EpiCache improves accuracy by up to 40% over recent\nbaselines, sustains near-full KV accuracy under 4-6x compression, and reduces\nlatency and memory by up to 2.4x and 3.5x, thereby enabling efficient\nmulti-turn interaction under strict resource constraints."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Han-Byul Kim"
                    },
                    {
                        "name": "Richa Dixit"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17396v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17396v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20979v1",
                "updated": "2025-09-25T10:23:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    10,
                    23,
                    50,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T10:23:50Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    10,
                    23,
                    50,
                    3,
                    268,
                    0
                ],
                "title": "Toward Robust and Efficient ML-Based GPU Caching for Modern Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Robust and Efficient ML-Based GPU Caching for Modern Inference"
                },
                "summary": "In modern GPU inference, cache efficiency remains a major bottleneck. In\nrecommendation models, embedding hit rates largely determine throughput, while\nin large language models, KV-cache misses substantially increase\ntime-to-first-token (TTFT). Heuristic policies such as \\textsc{LRU} often\nstruggle under structured access patterns. Learning-based approaches are\npromising, but in practice face two major limitations: they degrade sharply\nwhen predictions are inaccurate, or they gain little even with accurate\npredictions due to conservative designs. Some also incur high overhead, further\nlimiting practicality.\n  We present \\textsc{LCR}, a practical framework for learning-based GPU caching\nthat delivers performance gains while ensuring robustness and efficiency. Its\ncore algorithm, \\textsc{LARU}, enhances \\textsc{LRU} with machine-learned\npredictions and dynamically adapts to prediction accuracy through online error\nestimation. When predictions are accurate, \\textsc{LARU} achieves near-optimal\nperformance. With inaccurate predictions, it degrades gracefully to\nnear-\\textsc{LRU} performance. With \\textsc{LCR}, we bridge the gap between\nempirical progress and theoretical advances in learning-based caching.\n  Experiments show that \\textsc{LCR} delivers consistent gains under realistic\nconditions. In DLRM and LLM scenarios, it improves throughput by up to 24.2\\%\nand reduces P99 TTFT by up to 28.3\\%, outperforming widely used inference\nsystems. Even under poor predictions, its performance remains stable,\ndemonstrating practical robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern GPU inference, cache efficiency remains a major bottleneck. In\nrecommendation models, embedding hit rates largely determine throughput, while\nin large language models, KV-cache misses substantially increase\ntime-to-first-token (TTFT). Heuristic policies such as \\textsc{LRU} often\nstruggle under structured access patterns. Learning-based approaches are\npromising, but in practice face two major limitations: they degrade sharply\nwhen predictions are inaccurate, or they gain little even with accurate\npredictions due to conservative designs. Some also incur high overhead, further\nlimiting practicality.\n  We present \\textsc{LCR}, a practical framework for learning-based GPU caching\nthat delivers performance gains while ensuring robustness and efficiency. Its\ncore algorithm, \\textsc{LARU}, enhances \\textsc{LRU} with machine-learned\npredictions and dynamically adapts to prediction accuracy through online error\nestimation. When predictions are accurate, \\textsc{LARU} achieves near-optimal\nperformance. With inaccurate predictions, it degrades gracefully to\nnear-\\textsc{LRU} performance. With \\textsc{LCR}, we bridge the gap between\nempirical progress and theoretical advances in learning-based caching.\n  Experiments show that \\textsc{LCR} delivers consistent gains under realistic\nconditions. In DLRM and LLM scenarios, it improves throughput by up to 24.2\\%\nand reduces P99 TTFT by up to 28.3\\%, outperforming widely used inference\nsystems. Even under poor predictions, its performance remains stable,\ndemonstrating practical robustness."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Yirong Zhang"
                    },
                    {
                        "name": "Jiahong Yu"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Jianping Zou"
                    },
                    {
                        "name": "Gang Xiong"
                    },
                    {
                        "name": "Kingsum Chow"
                    },
                    {
                        "name": "Shuibing He"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16242v5",
                "updated": "2025-09-25T09:49:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    9,
                    49,
                    59,
                    3,
                    268,
                    0
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "title": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency"
                },
                "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16242v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16242v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17892v2",
                "updated": "2025-09-25T03:30:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    3,
                    30,
                    6,
                    3,
                    268,
                    0
                ],
                "published": "2025-08-25T10:59:02Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "title": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$ and trims the memory footprint to\na few tenths of that required for the full context, but also delivers\nperformance comparable to or superior to the full-context setup in long-context\nscenarios. Without additional post training or operator development, ILRe can\nprocess a single $1M$ tokens request in less than half a minute (speedup\n$\\approx 180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with\nmodel Llama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$ and trims the memory footprint to\na few tenths of that required for the full context, but also delivers\nperformance comparable to or superior to the full-context setup in long-context\nscenarios. Without additional post training or operator development, ILRe can\nprocess a single $1M$ tokens request in less than half a minute (speedup\n$\\approx 180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with\nmodel Llama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "Mandi Liu"
                    },
                    {
                        "name": "Jiangzhou Ji"
                    },
                    {
                        "name": "Huaijun Li"
                    },
                    {
                        "name": "Haobo Yang"
                    },
                    {
                        "name": "Yaohan He"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15919v2",
                "updated": "2025-09-25T03:00:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    3,
                    0,
                    22,
                    3,
                    268,
                    0
                ],
                "published": "2025-08-21T18:40:20Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    18,
                    40,
                    20,
                    3,
                    233,
                    0
                ],
                "title": "HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO\n  Serving and Fast Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO\n  Serving and Fast Scaling"
                },
                "summary": "Modern large language model (LLM) serving systems face challenges from highly\nvariable requests with diverse lengths, priorities, and stage-specific\nservice-level objectives (SLOs). Meeting these requires real-time scheduling,\nrapid and cost-effective scaling, and support for both collocated and\ndisaggregated Prefill/Decode (P/D) architectures. We present HyperFlexis, a\nunified LLM serving system that integrates algorithmic and system-level\ninnovations to jointly optimize scheduling and scaling under multiple SLOs. It\nfeatures a multi-SLO-aware scheduler that leverages budget estimation and\nrequest prioritization to ensure proactive SLO compliance for both new and\nongoing requests. The system supports prefill- and decode-stage multi-SLO\nscheduling for P/D-disaggregated architectures and KV cache transfers. It also\nenables cost-effective scaling decisions, prefill-decode instance linking\nduring scaling, and rapid P/D role transitions. To accelerate scaling and\nreduce cold-start latency, a device-to-device (D2D) weight transfer mechanism\nis proposed that lowers weight loading overhead by up to 19.39$\\times$. These\noptimizations allow the system to achieve up to 4.44$\\times$ higher SLO\nattainment, 65.82% lower request latency, and cost parity with state-of-the-art\nbaselines. The code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language model (LLM) serving systems face challenges from highly\nvariable requests with diverse lengths, priorities, and stage-specific\nservice-level objectives (SLOs). Meeting these requires real-time scheduling,\nrapid and cost-effective scaling, and support for both collocated and\ndisaggregated Prefill/Decode (P/D) architectures. We present HyperFlexis, a\nunified LLM serving system that integrates algorithmic and system-level\ninnovations to jointly optimize scheduling and scaling under multiple SLOs. It\nfeatures a multi-SLO-aware scheduler that leverages budget estimation and\nrequest prioritization to ensure proactive SLO compliance for both new and\nongoing requests. The system supports prefill- and decode-stage multi-SLO\nscheduling for P/D-disaggregated architectures and KV cache transfers. It also\nenables cost-effective scaling decisions, prefill-decode instance linking\nduring scaling, and rapid P/D role transitions. To accelerate scaling and\nreduce cold-start latency, a device-to-device (D2D) weight transfer mechanism\nis proposed that lowers weight loading overhead by up to 19.39$\\times$. These\noptimizations allow the system to achieve up to 4.44$\\times$ higher SLO\nattainment, 65.82% lower request latency, and cost parity with state-of-the-art\nbaselines. The code will be released soon."
                },
                "authors": [
                    {
                        "name": "Zahra Yousefijamarani"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Morgan Lindsay Heisler"
                    },
                    {
                        "name": "Taha Shabani"
                    },
                    {
                        "name": "Niloofar Gholipour"
                    },
                    {
                        "name": "Parham Yassini"
                    },
                    {
                        "name": "Hong Chang"
                    },
                    {
                        "name": "Kan Chen"
                    },
                    {
                        "name": "Qiantao Zhang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Jiannan Wang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20617v1",
                "updated": "2025-09-24T23:47:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    23,
                    47,
                    55,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T23:47:55Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    23,
                    47,
                    55,
                    2,
                    267,
                    0
                ],
                "title": "DELM: a Python toolkit for Data Extraction with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DELM: a Python toolkit for Data Extraction with Language Models"
                },
                "summary": "Large Language Models (LLMs) have become powerful tools for annotating\nunstructured data. However, most existing workflows rely on ad hoc scripts,\nmaking reproducibility, robustness, and systematic evaluation difficult. To\naddress these challenges, we introduce DELM (Data Extraction with Language\nModels), an open-source Python toolkit designed for rapid experimental\niteration of LLM-based data extraction pipelines and for quantifying the\ntrade-offs between them. DELM minimizes boilerplate code and offers a modular\nframework with structured outputs, built-in validation, flexible data-loading\nand scoring strategies, and efficient batch processing. It also includes robust\nsupport for working with LLM APIs, featuring retry logic, result caching,\ndetailed cost tracking, and comprehensive configuration management. We showcase\nDELM's capabilities through two case studies: one featuring a novel prompt\noptimization algorithm, and another illustrating how DELM quantifies trade-offs\nbetween cost and coverage when selecting keywords to decide which paragraphs to\npass to an LLM. DELM is available at\n\\href{https://github.com/Center-for-Applied-AI/delm}{\\texttt{github.com/Center-for-Applied-AI/delm}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become powerful tools for annotating\nunstructured data. However, most existing workflows rely on ad hoc scripts,\nmaking reproducibility, robustness, and systematic evaluation difficult. To\naddress these challenges, we introduce DELM (Data Extraction with Language\nModels), an open-source Python toolkit designed for rapid experimental\niteration of LLM-based data extraction pipelines and for quantifying the\ntrade-offs between them. DELM minimizes boilerplate code and offers a modular\nframework with structured outputs, built-in validation, flexible data-loading\nand scoring strategies, and efficient batch processing. It also includes robust\nsupport for working with LLM APIs, featuring retry logic, result caching,\ndetailed cost tracking, and comprehensive configuration management. We showcase\nDELM's capabilities through two case studies: one featuring a novel prompt\noptimization algorithm, and another illustrating how DELM quantifies trade-offs\nbetween cost and coverage when selecting keywords to decide which paragraphs to\npass to an LLM. DELM is available at\n\\href{https://github.com/Center-for-Applied-AI/delm}{\\texttt{github.com/Center-for-Applied-AI/delm}}."
                },
                "authors": [
                    {
                        "name": "Eric Fithian"
                    },
                    {
                        "name": "Kirill Skobelev"
                    }
                ],
                "author_detail": {
                    "name": "Kirill Skobelev"
                },
                "author": "Kirill Skobelev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03090v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03090v2",
                "updated": "2025-09-24T16:56:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    56,
                    17,
                    2,
                    267,
                    0
                ],
                "published": "2024-10-04T02:32:36Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    2,
                    32,
                    36,
                    4,
                    278,
                    0
                ],
                "title": "UNComp: Can Matrix Entropy Uncover Sparsity? -- A Compressor Design from\n  an Uncertainty-Aware Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNComp: Can Matrix Entropy Uncover Sparsity? -- A Compressor Design from\n  an Uncertainty-Aware Perspective"
                },
                "summary": "Deploying large language models (LLMs) for long-context inference remains\nchallenging due to their substantial memory and computational demands. While\ntechniques such as Key-Value (KV) cache compression are designed to reduce\nmemory usage, they often neglect the structured sparsity inherent in the\nrelationship between hidden states and their corresponding KV cache. In this\nwork, we explore the role of uncertainty as a potential indicator of sparsity\nwithin LLMs. We propose UNComp, an uncertainty-aware framework that leverages\ntruncated matrix entropy to identify areas of low information content, thereby\nrevealing sparsity patterns that can be used for adaptive compression. Unlike\ntraditional methods that apply uniform compression, UNComp dynamically adjusts\nits approach to compression, guided by uncertainty measures that reflect the\nimportance of various model components. Our analysis shows that sparsity\npatterns, when derived from uncertainty estimates, can be exploited to reveal\nspecial long-range dependencies, such as retrieval heads and retrieval layers.\nThis perspective not only enhances our understanding of how compression can be\noptimized but also provides new insights into the inherent sparsity of LLMs\nduring long-context inference. By focusing on uncertainty to analyze the\nsparsity pattern in detail, UNComp reduces the KV cache size to 4.74% of the\noriginal, achieves a 6% prefill speedup, and improves throughput by 6.4x - not\nonly delivering strong lossless compression performance, but also validating\nthe effectiveness of the underlying theoretical tool. We release the code at\nhttps://github.com/menik1126/UNComp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) for long-context inference remains\nchallenging due to their substantial memory and computational demands. While\ntechniques such as Key-Value (KV) cache compression are designed to reduce\nmemory usage, they often neglect the structured sparsity inherent in the\nrelationship between hidden states and their corresponding KV cache. In this\nwork, we explore the role of uncertainty as a potential indicator of sparsity\nwithin LLMs. We propose UNComp, an uncertainty-aware framework that leverages\ntruncated matrix entropy to identify areas of low information content, thereby\nrevealing sparsity patterns that can be used for adaptive compression. Unlike\ntraditional methods that apply uniform compression, UNComp dynamically adjusts\nits approach to compression, guided by uncertainty measures that reflect the\nimportance of various model components. Our analysis shows that sparsity\npatterns, when derived from uncertainty estimates, can be exploited to reveal\nspecial long-range dependencies, such as retrieval heads and retrieval layers.\nThis perspective not only enhances our understanding of how compression can be\noptimized but also provides new insights into the inherent sparsity of LLMs\nduring long-context inference. By focusing on uncertainty to analyze the\nsparsity pattern in detail, UNComp reduces the KV cache size to 4.74% of the\noriginal, achieves a 6% prefill speedup, and improves throughput by 6.4x - not\nonly delivering strong lossless compression performance, but also validating\nthe effectiveness of the underlying theoretical tool. We release the code at\nhttps://github.com/menik1126/UNComp."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "Accepted at EMNLP 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03090v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19729v1",
                "updated": "2025-09-24T03:15:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    3,
                    15,
                    37,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T03:15:37Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    3,
                    15,
                    37,
                    2,
                    267,
                    0
                ],
                "title": "Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient\n  LLM Inference"
                },
                "summary": "Efficiently processing the dynamics of requests, especially the context\nlength variance, is important in Large Language Model (LLM) serving scenarios.\nHowever, there is an intrinsic trade-off: while leveraging parallelism\nstrategies, such as Tensor Parallelism (TP), can coordinate multiple GPUs to\naccommodate larger context lengths, it inevitably results in degraded overall\nthroughput. In this paper, we propose Cross-Instance Parallelism Transformation\n(Gyges), which adaptively adjusts the parallelism strategies of running\ninstances to align with the dynamics of incoming requests. We design (1) a\npage-friendly, header-centric layout to accelerate KV cache transformations;\n(2) dedicated weight padding to accelerate model weight transformations; and\n(3) a transformation-aware scheduler to cooperatively schedule requests and\nparallelism transformations, optimizing the overall performance. Evaluations\nusing real-world traces show that Gyges improves throughput by 1.75x-6.57x\ncompared to state-of-the-art solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently processing the dynamics of requests, especially the context\nlength variance, is important in Large Language Model (LLM) serving scenarios.\nHowever, there is an intrinsic trade-off: while leveraging parallelism\nstrategies, such as Tensor Parallelism (TP), can coordinate multiple GPUs to\naccommodate larger context lengths, it inevitably results in degraded overall\nthroughput. In this paper, we propose Cross-Instance Parallelism Transformation\n(Gyges), which adaptively adjusts the parallelism strategies of running\ninstances to align with the dynamics of incoming requests. We design (1) a\npage-friendly, header-centric layout to accelerate KV cache transformations;\n(2) dedicated weight padding to accelerate model weight transformations; and\n(3) a transformation-aware scheduler to cooperatively schedule requests and\nparallelism transformations, optimizing the overall performance. Evaluations\nusing real-world traces show that Gyges improves throughput by 1.75x-6.57x\ncompared to state-of-the-art solutions."
                },
                "authors": [
                    {
                        "name": "Haoyu Chen"
                    },
                    {
                        "name": "Xue Li"
                    },
                    {
                        "name": "Kun Qian"
                    },
                    {
                        "name": "Yu Guan"
                    },
                    {
                        "name": "Jin Zhao"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "12 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13523v2",
                "updated": "2025-09-24T01:32:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    1,
                    32,
                    55,
                    2,
                    267,
                    0
                ],
                "published": "2025-08-19T05:27:53Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    27,
                    53,
                    1,
                    231,
                    0
                ],
                "title": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures"
                },
                "summary": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on three exascale machines\n-- OLCF Frontier, ALCF Aurora, and NNSA El Capitan -- as well as on the CSCS\nAlps supercomputer, for the three potentials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on three exascale machines\n-- OLCF Frontier, ALCF Aurora, and NNSA El Capitan -- as well as on the CSCS\nAlps supercomputer, for the three potentials."
                },
                "authors": [
                    {
                        "name": "Anders Johansson"
                    },
                    {
                        "name": "Evan Weinberg"
                    },
                    {
                        "name": "Christian R. Trott"
                    },
                    {
                        "name": "Megan J. McCarthy"
                    },
                    {
                        "name": "Stan G. Moore"
                    }
                ],
                "author_detail": {
                    "name": "Stan G. Moore"
                },
                "author": "Stan G. Moore",
                "arxiv_doi": "10.1145/3731599.3767498",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731599.3767498",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.13523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; C.2.4; C.4; D.1.3; D.3.4; E.1; I.6; I.6.8; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19599v1",
                "updated": "2025-09-23T21:46:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    21,
                    46,
                    38,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T21:46:38Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    21,
                    46,
                    38,
                    1,
                    266,
                    0
                ],
                "title": "Knowledge Base-Aware Orchestration: A Dynamic, Privacy-Preserving Method\n  for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Base-Aware Orchestration: A Dynamic, Privacy-Preserving Method\n  for Multi-Agent Systems"
                },
                "summary": "Multi-agent systems (MAS) are increasingly tasked with solving complex,\nknowledge-intensive problems where effective agent orchestration is critical.\nConventional orchestration methods rely on static agent descriptions, which\noften become outdated or incomplete. This limitation leads to inefficient task\nrouting, particularly in dynamic environments where agent capabilities\ncontinuously evolve. We introduce Knowledge Base-Aware (KBA) Orchestration, a\nnovel approach that augments static descriptions with dynamic,\nprivacy-preserving relevance signals derived from each agent's internal\nknowledge base (KB). In the proposed framework, when static descriptions are\ninsufficient for a clear routing decision, the orchestrator prompts the\nsubagents in parallel. Each agent then assesses the task's relevance against\nits private KB, returning a lightweight ACK signal without exposing the\nunderlying data. These collected signals populate a shared semantic cache,\nproviding dynamic indicators of agent suitability for future queries. By\ncombining this novel mechanism with static descriptions, our method achieves\nmore accurate and adaptive task routing preserving agent autonomy and data\nconfidentiality. Benchmarks show that our KBA Orchestration significantly\noutperforms static description-driven methods in routing precision and overall\nsystem efficiency, making it suitable for large-scale systems that require\nhigher accuracy than standard description-driven routing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems (MAS) are increasingly tasked with solving complex,\nknowledge-intensive problems where effective agent orchestration is critical.\nConventional orchestration methods rely on static agent descriptions, which\noften become outdated or incomplete. This limitation leads to inefficient task\nrouting, particularly in dynamic environments where agent capabilities\ncontinuously evolve. We introduce Knowledge Base-Aware (KBA) Orchestration, a\nnovel approach that augments static descriptions with dynamic,\nprivacy-preserving relevance signals derived from each agent's internal\nknowledge base (KB). In the proposed framework, when static descriptions are\ninsufficient for a clear routing decision, the orchestrator prompts the\nsubagents in parallel. Each agent then assesses the task's relevance against\nits private KB, returning a lightweight ACK signal without exposing the\nunderlying data. These collected signals populate a shared semantic cache,\nproviding dynamic indicators of agent suitability for future queries. By\ncombining this novel mechanism with static descriptions, our method achieves\nmore accurate and adaptive task routing preserving agent autonomy and data\nconfidentiality. Benchmarks show that our KBA Orchestration significantly\noutperforms static description-driven methods in routing precision and overall\nsystem efficiency, making it suitable for large-scale systems that require\nhigher accuracy than standard description-driven routing."
                },
                "authors": [
                    {
                        "name": "Danilo Trombino"
                    },
                    {
                        "name": "Vincenzo Pecorella"
                    },
                    {
                        "name": "Alessandro de Giulii"
                    },
                    {
                        "name": "Davide Tresoldi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Tresoldi"
                },
                "author": "Davide Tresoldi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v4",
                "updated": "2025-09-23T21:08:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    21,
                    8,
                    3,
                    1,
                    266,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "CVPR 2025. Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03227v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03227v3",
                "updated": "2025-09-23T20:25:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    20,
                    25,
                    15,
                    1,
                    266,
                    0
                ],
                "published": "2024-04-04T06:24:11Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    6,
                    24,
                    11,
                    3,
                    95,
                    0
                ],
                "title": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks"
                },
                "summary": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning."
                },
                "authors": [
                    {
                        "name": "Xingran Chen"
                    },
                    {
                        "name": "Navid NaderiAlizadeh"
                    },
                    {
                        "name": "Alejandro Ribeiro"
                    },
                    {
                        "name": "Shirin Saeedi Bidokhti"
                    }
                ],
                "author_detail": {
                    "name": "Shirin Saeedi Bidokhti"
                },
                "author": "Shirin Saeedi Bidokhti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03227v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03227v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19459v1",
                "updated": "2025-09-23T18:14:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    18,
                    14,
                    21,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T18:14:21Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    18,
                    14,
                    21,
                    1,
                    266,
                    0
                ],
                "title": "Automated Insertion of Flushes and Fences for Persistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Insertion of Flushes and Fences for Persistency"
                },
                "summary": "CXL shared memory and persistent memory allow the contents of memory to\npersist beyond crashes. Stores to persistent or CXL memory are typically not\nimmediately made persistent; developers must manually flush the corresponding\ncache lines to force the data to be written to the underlying storage.\nCorrectly using flush and fence operations is known to be challenging. While\nstate-of-the-art tools can find missing flush instructions, they often require\nbug-revealing test cases. No existing tools can ensure the absence of missing\nflush bugs.\n  In this paper, we present PMRobust, a compiler that automatically inserts\nflush and fence operations to ensure that code using persistent memory is free\nfrom missing flush and fence bugs. PMRobust employs a novel static analysis\nwith optimizations that target newly allocated objects. We have evaluated\nPMRobust on persistent memory libraries and several persistent memory data\nstructures and measured a geometric mean overhead of 0.26% relative to the\noriginal benchmarks with hand-placed flush and fence operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL shared memory and persistent memory allow the contents of memory to\npersist beyond crashes. Stores to persistent or CXL memory are typically not\nimmediately made persistent; developers must manually flush the corresponding\ncache lines to force the data to be written to the underlying storage.\nCorrectly using flush and fence operations is known to be challenging. While\nstate-of-the-art tools can find missing flush instructions, they often require\nbug-revealing test cases. No existing tools can ensure the absence of missing\nflush bugs.\n  In this paper, we present PMRobust, a compiler that automatically inserts\nflush and fence operations to ensure that code using persistent memory is free\nfrom missing flush and fence bugs. PMRobust employs a novel static analysis\nwith optimizations that target newly allocated objects. We have evaluated\nPMRobust on persistent memory libraries and several persistent memory data\nstructures and measured a geometric mean overhead of 0.26% relative to the\noriginal benchmarks with hand-placed flush and fence operations."
                },
                "authors": [
                    {
                        "name": "Yutong Guo"
                    },
                    {
                        "name": "Weiyu Luo"
                    },
                    {
                        "name": "Brian Demsky"
                    }
                ],
                "author_detail": {
                    "name": "Brian Demsky"
                },
                "author": "Brian Demsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19260v1",
                "updated": "2025-09-23T17:18:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    18,
                    59,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T17:18:59Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    18,
                    59,
                    1,
                    266,
                    0
                ],
                "title": "Reconstruction of a potential parameter in time-fractional diffusion\n  problems via a Kohn--Vogelius type functional: Theoretical aspects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstruction of a potential parameter in time-fractional diffusion\n  problems via a Kohn--Vogelius type functional: Theoretical aspects"
                },
                "summary": "Of concern is the problem of reconstructing a space-dependent potential from\nboundary observations in the Caputo time-fractional diffusion equation,\nutilizing a stable and robust recovery method. We develop an algorithm to\nminimize the Kohn-Vogelius (KV) cost function, which measures the difference\nbetween the solutions of two excitations. The inverse potential problem is\nrecast into an optimization problem, where the objective is to minimize a\nKohn-Vogelius-type functional within a set of admissible potentials. We\nestablish the well-posedness of this optimization problem by proving the\nexistence and uniqueness of a minimizer and demonstrating its stability with\nrespect to perturbations in the boundary data. Furthermore, we analyze the\nFr\\'echet differentiability of the KV functional and prove the Lipschitz\ncontinuity of its gradient. These theoretical results enable the development of\na convergent conjugate gradient algorithm for numerical reconstruction. The\neffectiveness and robustness of the proposed method are confirmed through\nseveral numerical examples in both one and two dimensions, including cases with\nnoisy data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Of concern is the problem of reconstructing a space-dependent potential from\nboundary observations in the Caputo time-fractional diffusion equation,\nutilizing a stable and robust recovery method. We develop an algorithm to\nminimize the Kohn-Vogelius (KV) cost function, which measures the difference\nbetween the solutions of two excitations. The inverse potential problem is\nrecast into an optimization problem, where the objective is to minimize a\nKohn-Vogelius-type functional within a set of admissible potentials. We\nestablish the well-posedness of this optimization problem by proving the\nexistence and uniqueness of a minimizer and demonstrating its stability with\nrespect to perturbations in the boundary data. Furthermore, we analyze the\nFr\\'echet differentiability of the KV functional and prove the Lipschitz\ncontinuity of its gradient. These theoretical results enable the development of\na convergent conjugate gradient algorithm for numerical reconstruction. The\neffectiveness and robustness of the proposed method are confirmed through\nseveral numerical examples in both one and two dimensions, including cases with\nnoisy data."
                },
                "authors": [
                    {
                        "name": "Hamza Kahlaoui"
                    },
                    {
                        "name": "Mourad Hrizi"
                    },
                    {
                        "name": "Abdessamad Oulmelk"
                    },
                    {
                        "name": "Xiangcheng Zheng"
                    },
                    {
                        "name": "Ahmed Hendy"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Hendy"
                },
                "author": "Ahmed Hendy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19228v1",
                "updated": "2025-09-23T16:49:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    49,
                    43,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T16:49:43Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    49,
                    43,
                    1,
                    266,
                    0
                ],
                "title": "CompLLM: Compression for Long Context Q&A",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompLLM: Compression for Long Context Q&A"
                },
                "summary": "Large Language Models (LLMs) face significant computational challenges when\nprocessing long contexts due to the quadratic complexity of self-attention.\nWhile soft context compression methods, which map input text to smaller latent\nrepresentations, have shown promise, their real-world adoption is limited.\nExisting techniques typically compress the context as a single unit, which\nleads to quadratic compression complexity and an inability to reuse\ncomputations across queries with overlapping contexts. In this work, we\nintroduce CompLLM, a soft compression technique designed for practical\ndeployment. Instead of processing the context holistically, CompLLM divides it\ninto segments and compresses each one independently. This simple design choice\nyields three critical properties: efficiency, as the compression step scales\nlinearly with the context length; scalability, enabling models trained on short\nsequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and\nreusability, allowing compressed segments to be cached and reused across\ndifferent queries. Our experiments show that with a 2x compression rate, at\nhigh context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x\nand reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance\ncomparable to that obtained with the uncompressed context, and even surpasses\nit on very long sequences, demonstrating its effectiveness and practical\nutility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face significant computational challenges when\nprocessing long contexts due to the quadratic complexity of self-attention.\nWhile soft context compression methods, which map input text to smaller latent\nrepresentations, have shown promise, their real-world adoption is limited.\nExisting techniques typically compress the context as a single unit, which\nleads to quadratic compression complexity and an inability to reuse\ncomputations across queries with overlapping contexts. In this work, we\nintroduce CompLLM, a soft compression technique designed for practical\ndeployment. Instead of processing the context holistically, CompLLM divides it\ninto segments and compresses each one independently. This simple design choice\nyields three critical properties: efficiency, as the compression step scales\nlinearly with the context length; scalability, enabling models trained on short\nsequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and\nreusability, allowing compressed segments to be cached and reused across\ndifferent queries. Our experiments show that with a 2x compression rate, at\nhigh context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x\nand reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance\ncomparable to that obtained with the uncompressed context, and even surpasses\nit on very long sequences, demonstrating its effectiveness and practical\nutility."
                },
                "authors": [
                    {
                        "name": "Gabriele Berton"
                    },
                    {
                        "name": "Jayakrishnan Unnikrishnan"
                    },
                    {
                        "name": "Son Tran"
                    },
                    {
                        "name": "Mubarak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Mubarak Shah"
                },
                "author": "Mubarak Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19061v1",
                "updated": "2025-09-23T14:25:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    25,
                    13,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T14:25:13Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    25,
                    13,
                    1,
                    266,
                    0
                ],
                "title": "3D Blocking for Matrix-free Smoothers in 2D Variable-Viscosity Stokes\n  Equations with Applications to Geodynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Blocking for Matrix-free Smoothers in 2D Variable-Viscosity Stokes\n  Equations with Applications to Geodynamics"
                },
                "summary": "We present the design, implementation, and evaluation of optimized\nmatrix-free stencil kernels for multigrid smoothing in the incompressible\nStokes equations with variable viscosity, motivated by geophysical flow\nproblems. We investigate five smoother variants derived from different\noptimisation strategies: Red-Black Gauss-Seidel, Jacobi, fused Jacobi, blocked\nfused Jacobi, and a novel Jacobi smoother with RAS-type temporal blocking, a\nstrategy that applies local iterations on overlapping tiles to improve cache\nreuse. To ensure correctness, we introduce an energy-based residual norm that\nbalances velocity and pressure contributions, and validate all implementations\nusing a high-contrast sinker benchmark representative of realistic geodynamic\nnumerical models. Our performance study on NVIDIA GH200 Grace Hopper nodes of\nthe ALPS supercomputer demonstrates that all smoothers scale well within a\nsingle NUMA domain, but the RAS-Jacobi smoother consistently achieves the best\nperformance at higher core counts. It sustains over 90% weak-scaling efficiency\nup to 64 cores and delivers up to a threefold speedup compared to the C++\nJacobi baseline, owing to improved cache reuse and reduced memory traffic.\nThese results show that temporal blocking, already employed in\ndistributed-memory solvers to reduce communication, can also provide\nsubstantial benefits at the socket and NUMA level. This work highlights the\nimportance of cache-aware stencil design for harnessing modern heterogeneous\narchitectures and lays the groundwork for extending RAS-type temporal blocking\nstrategies to three-dimensional problems and GPU accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design, implementation, and evaluation of optimized\nmatrix-free stencil kernels for multigrid smoothing in the incompressible\nStokes equations with variable viscosity, motivated by geophysical flow\nproblems. We investigate five smoother variants derived from different\noptimisation strategies: Red-Black Gauss-Seidel, Jacobi, fused Jacobi, blocked\nfused Jacobi, and a novel Jacobi smoother with RAS-type temporal blocking, a\nstrategy that applies local iterations on overlapping tiles to improve cache\nreuse. To ensure correctness, we introduce an energy-based residual norm that\nbalances velocity and pressure contributions, and validate all implementations\nusing a high-contrast sinker benchmark representative of realistic geodynamic\nnumerical models. Our performance study on NVIDIA GH200 Grace Hopper nodes of\nthe ALPS supercomputer demonstrates that all smoothers scale well within a\nsingle NUMA domain, but the RAS-Jacobi smoother consistently achieves the best\nperformance at higher core counts. It sustains over 90% weak-scaling efficiency\nup to 64 cores and delivers up to a threefold speedup compared to the C++\nJacobi baseline, owing to improved cache reuse and reduced memory traffic.\nThese results show that temporal blocking, already employed in\ndistributed-memory solvers to reduce communication, can also provide\nsubstantial benefits at the socket and NUMA level. This work highlights the\nimportance of cache-aware stencil design for harnessing modern heterogeneous\narchitectures and lays the groundwork for extending RAS-type temporal blocking\nstrategies to three-dimensional problems and GPU accelerators."
                },
                "authors": [
                    {
                        "name": "Marcel Ferrari"
                    },
                    {
                        "name": "Cyrill Püntener"
                    },
                    {
                        "name": "Alexander Sotoudeh"
                    },
                    {
                        "name": "Niklas Viebig"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Viebig"
                },
                "author": "Niklas Viebig",
                "arxiv_comment": "15 pages, 5 figures, appendix has 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F08, 65N55, 65N22, 76M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.1.8; F.2.1; D.1.3; C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18909v1",
                "updated": "2025-09-23T12:32:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    32,
                    51,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T12:32:51Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    32,
                    51,
                    1,
                    266,
                    0
                ],
                "title": "Obelix: Mitigating Side-Channels Through Dynamic Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Obelix: Mitigating Side-Channels Through Dynamic Obfuscation"
                },
                "summary": "Trusted execution environments (TEEs) offer hardware-assisted means to\nprotect code and data. However, as shown in numerous results over the years,\nattackers can use side-channels to leak data access patterns and even\nsingle-step the code. While the vendors are slowly introducing hardware-based\ncountermeasures for some attacks, others will stay unaddressed. This makes a\nsoftware-level countermeasure desirable, but current available solutions only\naddress very specific attack vectors or have a narrow leakage model.\n  In this work, we take a holistic view at the vulnerabilities of TEEs and\ndesign a tool named Obelix, which is the first to protect both code and data\nagainst a wide range of TEE attacks, from cache attacks over single-stepping to\nciphertext side-channels. We analyze the practically achievable precision of\nstate-of-the-art single-stepping tools, and present an algorithm which uses\nthat knowledge to divide a program into uniform code blocks, that are\nindistinguishable for a strong attacker. By storing these blocks and the\nprogram data in oblivious RAM, the attacker cannot follow execution,\neffectively protecting both secret code and data. We describe how we automate\nour approach to make it available for developers who are unfamiliar with\nside-channels. As an obfuscation tool, Obelix comes with a considerable\nperformance overhead, but compensates this with strong security guarantees and\neasy applicability without requiring any expert knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trusted execution environments (TEEs) offer hardware-assisted means to\nprotect code and data. However, as shown in numerous results over the years,\nattackers can use side-channels to leak data access patterns and even\nsingle-step the code. While the vendors are slowly introducing hardware-based\ncountermeasures for some attacks, others will stay unaddressed. This makes a\nsoftware-level countermeasure desirable, but current available solutions only\naddress very specific attack vectors or have a narrow leakage model.\n  In this work, we take a holistic view at the vulnerabilities of TEEs and\ndesign a tool named Obelix, which is the first to protect both code and data\nagainst a wide range of TEE attacks, from cache attacks over single-stepping to\nciphertext side-channels. We analyze the practically achievable precision of\nstate-of-the-art single-stepping tools, and present an algorithm which uses\nthat knowledge to divide a program into uniform code blocks, that are\nindistinguishable for a strong attacker. By storing these blocks and the\nprogram data in oblivious RAM, the attacker cannot follow execution,\neffectively protecting both secret code and data. We describe how we automate\nour approach to make it available for developers who are unfamiliar with\nside-channels. As an obfuscation tool, Obelix comes with a considerable\nperformance overhead, but compensates this with strong security guarantees and\neasy applicability without requiring any expert knowledge."
                },
                "authors": [
                    {
                        "name": "Jan Wichelmann"
                    },
                    {
                        "name": "Anja Rabich"
                    },
                    {
                        "name": "Anna P\"atschke"
                    },
                    {
                        "name": "Thomas Eisenbarth"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Eisenbarth"
                },
                "author": "Thomas Eisenbarth",
                "arxiv_doi": "10.1109/SP54263.2024.00261",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/SP54263.2024.00261",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.18909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "2024 IEEE Symposium on Security and Privacy (SP), San Francisco,\n  CA, USA, 2024, pp. 4182-4199",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04467v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04467v3",
                "updated": "2025-09-23T08:31:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    8,
                    31,
                    26,
                    1,
                    266,
                    0
                ],
                "published": "2025-08-29T02:29:52Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    2,
                    29,
                    52,
                    4,
                    241,
                    0
                ],
                "title": "PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the same (default) settings, our\nmethod achieves improved performance and faster inference, along with a\n4.95$\\times$ reduction in data transmission bandwidth consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the same (default) settings, our\nmethod achieves improved performance and faster inference, along with a\n4.95$\\times$ reduction in data transmission bandwidth consumption."
                },
                "authors": [
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Mengsi Lyu"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Xingrun Xing"
                    },
                    {
                        "name": "Yulong Ao"
                    },
                    {
                        "name": "Yonghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yonghua Lin"
                },
                "author": "Yonghua Lin",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04467v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04467v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08378v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08378v2",
                "updated": "2025-09-23T08:24:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    8,
                    24,
                    7,
                    1,
                    266,
                    0
                ],
                "published": "2025-04-11T09:26:47Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "title": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash"
                },
                "summary": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods."
                },
                "authors": [
                    {
                        "name": "Fucheng Jia"
                    },
                    {
                        "name": "Zewen Wu"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Ju Ren"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Ting Cao"
                    }
                ],
                "author_detail": {
                    "name": "Ting Cao"
                },
                "author": "Ting Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08378v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08378v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18684v1",
                "updated": "2025-09-23T06:10:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    6,
                    10,
                    20,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T06:10:20Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    6,
                    10,
                    20,
                    1,
                    266,
                    0
                ],
                "title": "Static Estimation of Reuse Profiles for Arrays in Nested Loops",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Estimation of Reuse Profiles for Arrays in Nested Loops"
                },
                "summary": "Efficient memory access patterns play a crucial role in determining the\noverall performance of applications by exploiting temporal and spatial\nlocality, thus maximizing cache locality. The Reuse Distance Histogram (RDH) is\na widely used metric to quantify temporal locality, measuring the distance\nbetween consecutive accesses to the same memory location. Traditionally,\ncalculating RDH requires program execution and memory trace collection to\nobtain dynamic memory access behavior. This trace collection is often\ntime-consuming, resource-intensive, and unsuitable for early-stage optimization\nor large-scale applications. Static prediction, on the other hand, offers a\nsignificant speedup in estimating RDH and cache hit rates. However, these\napproaches lack accuracy, since the predictions come without running the\nprogram and knowing the complete memory access pattern, more specifically when\narrays are used inside nested loops. This paper presents a novel static\nanalysis framework for predicting the reuse profiles of array references in\nprograms with nested loop structures, without requiring any runtime\ninformation. By analyzing loop bounds, access patterns in smaller problem\nsizes, and predictive equations, our method predicts access patterns of arrays\nand estimates reuse distances and cache hit rate at compile time. This paper\nextends our previous study by incorporating more analysis and improving\nprediction by addressing previously unhandled reuse patterns. We evaluate our\ntechnique against a widely accepted traditional trace-driven profiling tool,\nParallel Reuse Distance Analysis (PARDA). The results demonstrate that our\nstatic predictor achieves comparable accuracy while offering\norders-of-magnitude improvement in the analysis speed. This work offers a\npractical alternative to dynamic reuse profiling and paves the way for\nintegration into compilers and static performance modeling tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient memory access patterns play a crucial role in determining the\noverall performance of applications by exploiting temporal and spatial\nlocality, thus maximizing cache locality. The Reuse Distance Histogram (RDH) is\na widely used metric to quantify temporal locality, measuring the distance\nbetween consecutive accesses to the same memory location. Traditionally,\ncalculating RDH requires program execution and memory trace collection to\nobtain dynamic memory access behavior. This trace collection is often\ntime-consuming, resource-intensive, and unsuitable for early-stage optimization\nor large-scale applications. Static prediction, on the other hand, offers a\nsignificant speedup in estimating RDH and cache hit rates. However, these\napproaches lack accuracy, since the predictions come without running the\nprogram and knowing the complete memory access pattern, more specifically when\narrays are used inside nested loops. This paper presents a novel static\nanalysis framework for predicting the reuse profiles of array references in\nprograms with nested loop structures, without requiring any runtime\ninformation. By analyzing loop bounds, access patterns in smaller problem\nsizes, and predictive equations, our method predicts access patterns of arrays\nand estimates reuse distances and cache hit rate at compile time. This paper\nextends our previous study by incorporating more analysis and improving\nprediction by addressing previously unhandled reuse patterns. We evaluate our\ntechnique against a widely accepted traditional trace-driven profiling tool,\nParallel Reuse Distance Analysis (PARDA). The results demonstrate that our\nstatic predictor achieves comparable accuracy while offering\norders-of-magnitude improvement in the analysis speed. This work offers a\npractical alternative to dynamic reuse profiling and paves the way for\nintegration into compilers and static performance modeling tools."
                },
                "authors": [
                    {
                        "name": "Abdur Razzak"
                    },
                    {
                        "name": "Atanu Barai"
                    },
                    {
                        "name": "Nandakishore Santhi"
                    },
                    {
                        "name": "Abdel-Hameed A. Badawy"
                    }
                ],
                "author_detail": {
                    "name": "Abdel-Hameed A. Badawy"
                },
                "author": "Abdel-Hameed A. Badawy",
                "arxiv_comment": "This paper is accepted at the MEMSYS 2025 conference, 11th\n  International Symposium on Memory Systems, Washington D.C., October 7 -\n  October 8, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18670v1",
                "updated": "2025-09-23T05:39:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    5,
                    39,
                    47,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T05:39:47Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    5,
                    39,
                    47,
                    1,
                    266,
                    0
                ],
                "title": "CALL: Context-Aware Low-Latency Retrieval in Disk-Based Vector Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALL: Context-Aware Low-Latency Retrieval in Disk-Based Vector Databases"
                },
                "summary": "Embedding models capture both semantic and syntactic structures of queries,\noften mapping different queries to similar regions in vector space. This\nresults in non-uniform cluster access patterns in modern disk-based vector\ndatabases. While existing approaches optimize individual queries, they overlook\nthe impact of cluster access patterns, failing to account for the locality\neffects of queries that access similar clusters. This oversight increases cache\nmiss penalty. To minimize the cache miss penalty, we propose CALL, a\ncontext-aware query grouping mechanism that organizes queries based on shared\ncluster access patterns. Additionally, CALL incorporates a group-aware\nprefetching method to minimize cache misses during transitions between query\ngroups and latency-aware cluster loading. Experimental results show that CALL\nreduces the 99th percentile tail latency by up to 33% while consistently\nmaintaining a higher cache hit ratio, substantially reducing search latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding models capture both semantic and syntactic structures of queries,\noften mapping different queries to similar regions in vector space. This\nresults in non-uniform cluster access patterns in modern disk-based vector\ndatabases. While existing approaches optimize individual queries, they overlook\nthe impact of cluster access patterns, failing to account for the locality\neffects of queries that access similar clusters. This oversight increases cache\nmiss penalty. To minimize the cache miss penalty, we propose CALL, a\ncontext-aware query grouping mechanism that organizes queries based on shared\ncluster access patterns. Additionally, CALL incorporates a group-aware\nprefetching method to minimize cache misses during transitions between query\ngroups and latency-aware cluster loading. Experimental results show that CALL\nreduces the 99th percentile tail latency by up to 33% while consistently\nmaintaining a higher cache hit ratio, substantially reducing search latency."
                },
                "authors": [
                    {
                        "name": "Yeonwoo Jeong"
                    },
                    {
                        "name": "Hyunji Cho"
                    },
                    {
                        "name": "Kyuri Park"
                    },
                    {
                        "name": "Youngjae Kim"
                    },
                    {
                        "name": "Sungyong Park"
                    }
                ],
                "author_detail": {
                    "name": "Sungyong Park"
                },
                "author": "Sungyong Park",
                "arxiv_comment": "11 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18592v1",
                "updated": "2025-09-23T03:23:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    3,
                    23,
                    3,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T03:23:03Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    3,
                    23,
                    3,
                    1,
                    266,
                    0
                ],
                "title": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic\n  Vision-Language Planning for Zero-Shot Transfer in Robot Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic\n  Vision-Language Planning for Zero-Shot Transfer in Robot Navigation"
                },
                "summary": "Rapid adaptation in unseen environments is essential for scalable real-world\nautonomy, yet existing approaches rely on exhaustive exploration or rigid\nnavigation policies that fail to generalize. We present VLN-Zero, a two-phase\nvision-language navigation framework that leverages vision-language models to\nefficiently construct symbolic scene graphs and enable zero-shot neurosymbolic\nnavigation. In the exploration phase, structured prompts guide VLM-based search\ntoward informative and diverse trajectories, yielding compact scene graph\nrepresentations. In the deployment phase, a neurosymbolic planner reasons over\nthe scene graph and environmental observations to generate executable plans,\nwhile a cache-enabled execution module accelerates adaptation by reusing\npreviously computed task-location trajectories. By combining rapid exploration,\nsymbolic reasoning, and cache-enabled execution, the proposed framework\novercomes the computational inefficiency and poor generalization of prior\nvision-language navigation methods, enabling robust and scalable\ndecision-making in unseen environments. VLN-Zero achieves 2x higher success\nrate compared to state-of-the-art zero-shot models, outperforms most fine-tuned\nbaselines, and reaches goal locations in half the time with 55% fewer VLM calls\non average compared to state-of-the-art models across diverse environments.\nCodebase, datasets, and videos for VLN-Zero are available at:\nhttps://vln-zero.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid adaptation in unseen environments is essential for scalable real-world\nautonomy, yet existing approaches rely on exhaustive exploration or rigid\nnavigation policies that fail to generalize. We present VLN-Zero, a two-phase\nvision-language navigation framework that leverages vision-language models to\nefficiently construct symbolic scene graphs and enable zero-shot neurosymbolic\nnavigation. In the exploration phase, structured prompts guide VLM-based search\ntoward informative and diverse trajectories, yielding compact scene graph\nrepresentations. In the deployment phase, a neurosymbolic planner reasons over\nthe scene graph and environmental observations to generate executable plans,\nwhile a cache-enabled execution module accelerates adaptation by reusing\npreviously computed task-location trajectories. By combining rapid exploration,\nsymbolic reasoning, and cache-enabled execution, the proposed framework\novercomes the computational inefficiency and poor generalization of prior\nvision-language navigation methods, enabling robust and scalable\ndecision-making in unseen environments. VLN-Zero achieves 2x higher success\nrate compared to state-of-the-art zero-shot models, outperforms most fine-tuned\nbaselines, and reaches goal locations in half the time with 55% fewer VLM calls\non average compared to state-of-the-art models across diverse environments.\nCodebase, datasets, and videos for VLN-Zero are available at:\nhttps://vln-zero.github.io/."
                },
                "authors": [
                    {
                        "name": "Neel P. Bhatt"
                    },
                    {
                        "name": "Yunhao Yang"
                    },
                    {
                        "name": "Rohan Siva"
                    },
                    {
                        "name": "Pranay Samineni"
                    },
                    {
                        "name": "Daniel Milan"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Ufuk Topcu"
                    }
                ],
                "author_detail": {
                    "name": "Ufuk Topcu"
                },
                "author": "Ufuk Topcu",
                "arxiv_comment": "Codebase, datasets, and videos for VLN-Zero are available at:\n  https://vln-zero.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00329v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00329v2",
                "updated": "2025-09-22T19:20:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    20,
                    33,
                    0,
                    265,
                    0
                ],
                "published": "2025-05-31T00:52:17Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    0,
                    52,
                    17,
                    5,
                    151,
                    0
                ],
                "title": "Foresight: Adaptive Layer Reuse for Accelerated and High-Quality\n  Text-to-Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foresight: Adaptive Layer Reuse for Accelerated and High-Quality\n  Text-to-Video Generation"
                },
                "summary": "Diffusion Transformers (DiTs) achieve state-of-the-art results in\ntext-to-image, text-to-video generation, and editing. However, their large\nmodel size and the quadratic cost of spatial-temporal attention over multiple\ndenoising steps make video generation computationally expensive. Static caching\nmitigates this by reusing features across fixed steps but fails to adapt to\ngeneration dynamics, leading to suboptimal trade-offs between speed and\nquality.\n  We propose Foresight, an adaptive layer-reuse technique that reduces\ncomputational redundancy across denoising steps while preserving baseline\nperformance. Foresight dynamically identifies and reuses DiT block outputs for\nall layers across steps, adapting to generation parameters such as resolution\nand denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and\nCogVideoX, Foresight achieves up to \\latencyimprv end-to-end speedup, while\nmaintaining video quality. The source code of Foresight is available at\n\\href{https://github.com/STAR-Laboratory/foresight}{https://github.com/STAR-Laboratory/foresight}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) achieve state-of-the-art results in\ntext-to-image, text-to-video generation, and editing. However, their large\nmodel size and the quadratic cost of spatial-temporal attention over multiple\ndenoising steps make video generation computationally expensive. Static caching\nmitigates this by reusing features across fixed steps but fails to adapt to\ngeneration dynamics, leading to suboptimal trade-offs between speed and\nquality.\n  We propose Foresight, an adaptive layer-reuse technique that reduces\ncomputational redundancy across denoising steps while preserving baseline\nperformance. Foresight dynamically identifies and reuses DiT block outputs for\nall layers across steps, adapting to generation parameters such as resolution\nand denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and\nCogVideoX, Foresight achieves up to \\latencyimprv end-to-end speedup, while\nmaintaining video quality. The source code of Foresight is available at\n\\href{https://github.com/STAR-Laboratory/foresight}{https://github.com/STAR-Laboratory/foresight}."
                },
                "authors": [
                    {
                        "name": "Muhammad Adnan"
                    },
                    {
                        "name": "Nithesh Kurella"
                    },
                    {
                        "name": "Akhil Arunkumar"
                    },
                    {
                        "name": "Prashant J. Nair"
                    }
                ],
                "author_detail": {
                    "name": "Prashant J. Nair"
                },
                "author": "Prashant J. Nair",
                "arxiv_comment": "Accepted at the 39th Conference on Neural Information Processing\n  Systems (NeurIPS), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00329v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00329v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18344v1",
                "updated": "2025-09-22T19:08:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    8,
                    57,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T19:08:57Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    8,
                    57,
                    0,
                    265,
                    0
                ],
                "title": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for\n  Offloaded LLMs via Substitute Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for\n  Offloaded LLMs via Substitute Speculative Decoding"
                },
                "summary": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit)."
                },
                "authors": [
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "Chun-Che Yang"
                    },
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18307v1",
                "updated": "2025-09-22T18:32:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    18,
                    32,
                    32,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T18:32:32Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    18,
                    32,
                    32,
                    0,
                    265,
                    0
                ],
                "title": "Comparison of Adaptive plan doses using Velocity generated synthetic CT\n  with KV CBCT and re-planning CT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparison of Adaptive plan doses using Velocity generated synthetic CT\n  with KV CBCT and re-planning CT"
                },
                "summary": "Introduction: This study uses KV CBCT based Synthetic CT (sCT) generated\nthrough Velocity workstation and compare the target and normal tissue doses\nwith Adaptive plan CT doses.\n  Methods: Thirty head and neck cancer patients undergoing Adaptive Radiation\nTherapy (ART) were included in this retrospective study. Initially, patient\nunderwent treatment with the primary plan. After subsequent indications of\nmajor changes in patients' physicality and anatomy adaptive CT scans were\nacquired as per institutional protocol. Both the primary planning CT and the\nindicative cone-beam CT (CBCT) last acquired before the commencement of the\nadaptive treatment were imported into Velocity workstation. Rigid and\ndeformable image registration techniques were used for the generation of a\nSynthetic CT (sCT). Simultaneously replanning was done on re-planning CT (rCT)\nfor adaptive plan execution. The primary plan dose was subsequently mapped and\ndeformed onto the Synthetic CT in Velocity workstation, allowing for a\ncomparative dosimetric analysis between the sCT and rCT plan doses. This\ncomparison was conducted in both Velocity and Eclipse, focusing on dose\nvariations across different organs at risk (OARs) and the planning target\nvolume (PTV). Additionally, dosimetric indices were evaluated to assess and\nvalidate the accuracy and quality of the synthetic CT-based dose mapping\nrelative to adaptive planning.\n  Results: The dosimetric comparison between sCT and rCT stated that Mean dose\nfor OARs and PTVs were found to be similar in the two planning and the level of\nconfidence by using T-statistics. Collaborative research has the potential to\neliminate the need of rCT as a standard requirement.\n  Conclusion: The sCT shows comparable CT numbers and doses to the replanning\nCT, suggesting it's potential as a replacement pending clinical correlation and\ncontour adjustments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introduction: This study uses KV CBCT based Synthetic CT (sCT) generated\nthrough Velocity workstation and compare the target and normal tissue doses\nwith Adaptive plan CT doses.\n  Methods: Thirty head and neck cancer patients undergoing Adaptive Radiation\nTherapy (ART) were included in this retrospective study. Initially, patient\nunderwent treatment with the primary plan. After subsequent indications of\nmajor changes in patients' physicality and anatomy adaptive CT scans were\nacquired as per institutional protocol. Both the primary planning CT and the\nindicative cone-beam CT (CBCT) last acquired before the commencement of the\nadaptive treatment were imported into Velocity workstation. Rigid and\ndeformable image registration techniques were used for the generation of a\nSynthetic CT (sCT). Simultaneously replanning was done on re-planning CT (rCT)\nfor adaptive plan execution. The primary plan dose was subsequently mapped and\ndeformed onto the Synthetic CT in Velocity workstation, allowing for a\ncomparative dosimetric analysis between the sCT and rCT plan doses. This\ncomparison was conducted in both Velocity and Eclipse, focusing on dose\nvariations across different organs at risk (OARs) and the planning target\nvolume (PTV). Additionally, dosimetric indices were evaluated to assess and\nvalidate the accuracy and quality of the synthetic CT-based dose mapping\nrelative to adaptive planning.\n  Results: The dosimetric comparison between sCT and rCT stated that Mean dose\nfor OARs and PTVs were found to be similar in the two planning and the level of\nconfidence by using T-statistics. Collaborative research has the potential to\neliminate the need of rCT as a standard requirement.\n  Conclusion: The sCT shows comparable CT numbers and doses to the replanning\nCT, suggesting it's potential as a replacement pending clinical correlation and\ncontour adjustments."
                },
                "authors": [
                    {
                        "name": "Sudam Masanta"
                    },
                    {
                        "name": "Gurvinder Singh"
                    },
                    {
                        "name": "Shefali Pahwa"
                    },
                    {
                        "name": "Shekhar Dwivedi"
                    },
                    {
                        "name": "Devaraju Sampathirao"
                    },
                    {
                        "name": "Ramandeep Singh"
                    }
                ],
                "author_detail": {
                    "name": "Ramandeep Singh"
                },
                "author": "Ramandeep Singh",
                "arxiv_comment": "8 pages; comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18085v1",
                "updated": "2025-09-22T17:58:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    58,
                    21,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:58:21Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    58,
                    21,
                    0,
                    265,
                    0
                ],
                "title": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding"
                },
                "summary": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$."
                },
                "authors": [
                    {
                        "name": "Sudhanshu Agrawal"
                    },
                    {
                        "name": "Risheek Garrepalli"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Christopher Lott"
                    },
                    {
                        "name": "Fatih Porikli"
                    }
                ],
                "author_detail": {
                    "name": "Fatih Porikli"
                },
                "author": "Fatih Porikli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00919v2",
                "updated": "2025-09-22T16:16:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    16,
                    25,
                    0,
                    265,
                    0
                ],
                "published": "2025-02-02T21:15:07Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    21,
                    15,
                    7,
                    6,
                    33,
                    0
                ],
                "title": "Attention Sinks: A 'Catch, Tag, Release' Mechanism for Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Sinks: A 'Catch, Tag, Release' Mechanism for Embeddings"
                },
                "summary": "Large language models (LLMs) often concentrate their attention on a few\nspecific tokens referred to as attention sinks. Common examples include the\nfirst token, a prompt-independent sink, and punctuation tokens, which are\nprompt-dependent. While the tokens causing the sinks often lack direct semantic\nmeaning, the presence of the sinks is critical for model performance,\nparticularly under model compression and KV-caching. Despite their ubiquity,\nthe function, semantic role, and origin of attention sinks -- especially those\nbeyond the first token -- remain poorly understood. In this work, we conduct a\ncomprehensive investigation demonstrating that attention sinks: catch a\nsequence of tokens, tag them using a common direction in embedding space, and\nrelease them back into the residual stream, where tokens are later retrieved\nbased on the tags they have acquired. Probing experiments reveal these tags\ncarry semantically meaningful information, such as the truth of a statement.\nThese findings extend to reasoning models, where the mechanism spans more heads\nand explains greater variance in embeddings, or recent models with query-key\nnormalization, where sinks remain just as prevalent. To encourage future\ntheoretical analysis, we introduce a minimal problem which can be solved\nthrough the 'catch, tag, release' mechanism, and where it emerges through\ntraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often concentrate their attention on a few\nspecific tokens referred to as attention sinks. Common examples include the\nfirst token, a prompt-independent sink, and punctuation tokens, which are\nprompt-dependent. While the tokens causing the sinks often lack direct semantic\nmeaning, the presence of the sinks is critical for model performance,\nparticularly under model compression and KV-caching. Despite their ubiquity,\nthe function, semantic role, and origin of attention sinks -- especially those\nbeyond the first token -- remain poorly understood. In this work, we conduct a\ncomprehensive investigation demonstrating that attention sinks: catch a\nsequence of tokens, tag them using a common direction in embedding space, and\nrelease them back into the residual stream, where tokens are later retrieved\nbased on the tags they have acquired. Probing experiments reveal these tags\ncarry semantically meaningful information, such as the truth of a statement.\nThese findings extend to reasoning models, where the mechanism spans more heads\nand explains greater variance in embeddings, or recent models with query-key\nnormalization, where sinks remain just as prevalent. To encourage future\ntheoretical analysis, we introduce a minimal problem which can be solved\nthrough the 'catch, tag, release' mechanism, and where it emerges through\ntraining."
                },
                "authors": [
                    {
                        "name": "Stephen Zhang"
                    },
                    {
                        "name": "Mustafa Khan"
                    },
                    {
                        "name": "Vardan Papyan"
                    }
                ],
                "author_detail": {
                    "name": "Vardan Papyan"
                },
                "author": "Vardan Papyan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00085v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00085v2",
                "updated": "2025-09-22T12:28:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    28,
                    41,
                    0,
                    265,
                    0
                ],
                "published": "2025-01-31T16:22:36Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    22,
                    36,
                    4,
                    31,
                    0
                ],
                "title": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding"
                },
                "summary": "This work presents a novel trie (prefix-tree)-based parallel decoding method\nthat addresses the memory inefficiency of batch-based beam search. By sharing a\nsingle KV cache across beams with common prefixes, our approach dramatically\nreduces memory usage and enables efficient decoding. We evaluated our method\nacross three attention architectures, Multi-Head Attention\n(Phi-3.5-mini-instruct), Grouped Query Attention (Llama-3.1-8B-Instruct), and\nSliding Window Attention (Mistral-Small-24B-Instruct-2501), using CNN/DailyMail\nfor abstractive summarization and HumanEval for code generation. Our\nexperiments demonstrate substantial memory savings (4--8$\\times$) and up to\n2.4$\\times$ faster decoding, without compromising generation quality. These\nresults highlight our method's suitability for memory-constrained environments\nand large-scale deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a novel trie (prefix-tree)-based parallel decoding method\nthat addresses the memory inefficiency of batch-based beam search. By sharing a\nsingle KV cache across beams with common prefixes, our approach dramatically\nreduces memory usage and enables efficient decoding. We evaluated our method\nacross three attention architectures, Multi-Head Attention\n(Phi-3.5-mini-instruct), Grouped Query Attention (Llama-3.1-8B-Instruct), and\nSliding Window Attention (Mistral-Small-24B-Instruct-2501), using CNN/DailyMail\nfor abstractive summarization and HumanEval for code generation. Our\nexperiments demonstrate substantial memory savings (4--8$\\times$) and up to\n2.4$\\times$ faster decoding, without compromising generation quality. These\nresults highlight our method's suitability for memory-constrained environments\nand large-scale deployments."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "MaoXun Huang"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "arxiv_comment": "13 pages, accepted as a main conference paper at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00085v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00085v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13251v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13251v3",
                "updated": "2025-09-22T12:03:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    3,
                    22,
                    0,
                    265,
                    0
                ],
                "published": "2025-02-18T19:22:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    22,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Neural Attention Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Attention Search"
                },
                "summary": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance."
                },
                "authors": [
                    {
                        "name": "Difan Deng"
                    },
                    {
                        "name": "Marius Lindauer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Lindauer"
                },
                "author": "Marius Lindauer",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13251v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13251v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17650v1",
                "updated": "2025-09-22T11:54:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    11,
                    54,
                    58,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T11:54:58Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    11,
                    54,
                    58,
                    0,
                    265,
                    0
                ],
                "title": "Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming\n  Visual Geometry Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming\n  Visual Geometry Transformers"
                },
                "summary": "Streaming visual transformers like StreamVGGT achieve strong 3D perception\nbut suffer from unbounded growth of key value (KV) memory, which limits\nscalability. We propose a training-free, inference-time token eviction policy\nthat bounds memory by discarding redundant tokens while keeping the most\ninformative ones. Our method uses significantly less memory with little to no\ndrop in accuracy: on 7-Scenes with long sequences it reduces peak memory from\n18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under\nstrict memory budgets, eviction enables denser frame sampling, which improves\nreconstruction accuracy compared to the baseline. Experiments across video\ndepth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and\ncamera pose estimation (Sintel, TUM-dynamics) show that our approach closely\nmatches StreamVGGT at a fraction of the memory and makes long-horizon streaming\ninference more practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming visual transformers like StreamVGGT achieve strong 3D perception\nbut suffer from unbounded growth of key value (KV) memory, which limits\nscalability. We propose a training-free, inference-time token eviction policy\nthat bounds memory by discarding redundant tokens while keeping the most\ninformative ones. Our method uses significantly less memory with little to no\ndrop in accuracy: on 7-Scenes with long sequences it reduces peak memory from\n18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under\nstrict memory budgets, eviction enables denser frame sampling, which improves\nreconstruction accuracy compared to the baseline. Experiments across video\ndepth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and\ncamera pose estimation (Sintel, TUM-dynamics) show that our approach closely\nmatches StreamVGGT at a fraction of the memory and makes long-horizon streaming\ninference more practical."
                },
                "authors": [
                    {
                        "name": "Soroush Mahdi"
                    },
                    {
                        "name": "Fardin Ayar"
                    },
                    {
                        "name": "Ehsan Javanmardi"
                    },
                    {
                        "name": "Manabu Tsukada"
                    },
                    {
                        "name": "Mahdi Javanmardi"
                    }
                ],
                "author_detail": {
                    "name": "Mahdi Javanmardi"
                },
                "author": "Mahdi Javanmardi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17388v1",
                "updated": "2025-09-22T06:52:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    52,
                    35,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T06:52:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    52,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory"
                },
                "summary": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed."
                },
                "authors": [
                    {
                        "name": "Manel Lurbe"
                    },
                    {
                        "name": "Miguel Avargues"
                    },
                    {
                        "name": "Salvador Petit"
                    },
                    {
                        "name": "Maria E. Gomez"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Guanhao Wang"
                    },
                    {
                        "name": "Julio Sahuquillo"
                    }
                ],
                "author_detail": {
                    "name": "Julio Sahuquillo"
                },
                "author": "Julio Sahuquillo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17360v1",
                "updated": "2025-09-22T05:24:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    5,
                    24,
                    22,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T05:24:22Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    5,
                    24,
                    22,
                    0,
                    265,
                    0
                ],
                "title": "Asteria: Semantic-Aware Cross-Region Caching for Agentic LLM Tool Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asteria: Semantic-Aware Cross-Region Caching for Agentic LLM Tool Access"
                },
                "summary": "Large Language Model (LLM) agents tackle data-intensive tasks such as deep\nresearch and code generation. However, their effectiveness depends on frequent\ninteractions with knowledge sources across remote clouds or regions. Such\ninteractions can create non-trivial latency and cost bottlenecks. Existing\ncaching solutions focus on exact-match queries, limiting their effectiveness\nfor semantic knowledge reuse.\n  To address this challenge, we introduce Asteria, a novel cross-region\nknowledge caching architecture for LLM agents. At its core are two\nabstractions: Semantic Element (SE) and Semantic Retrieval Index (Sine). A\nsemantic element captures the semantic embedding representation of an LLM query\ntogether with performance-aware metadata such as latency, cost, and staticity.\nSine then provides two-stage retrieval: a vector similar index with semantic\nembedding for fast candidate selection and a lightweight LLM-powered semantic\njudger for precise validation. Atop these primitives, Asteria builds a new\ncache interface that includes a new semantic-aware cache hit definition, a\ncost-efficient eviction policy, and proactive prefetching. To reduce overhead,\nAsteria co-locates the small LLM judger with the main LLM using adaptive\nscheduling and resource sharing. Our evaluation demonstrates that Asteria\ndelivers substantial performance improvements without compromising correctness.\nOn representative search workloads, Asteria achieves up to a 3.6$\\times$\nincrease in throughput by maintaining cache hit rates of over 85%, while\npreserving accuracy virtually identical to non-cached baselines. Asteria also\nimproves throughput for complex coding tasks by 20%, showcasing its versatility\nacross diverse agentic workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents tackle data-intensive tasks such as deep\nresearch and code generation. However, their effectiveness depends on frequent\ninteractions with knowledge sources across remote clouds or regions. Such\ninteractions can create non-trivial latency and cost bottlenecks. Existing\ncaching solutions focus on exact-match queries, limiting their effectiveness\nfor semantic knowledge reuse.\n  To address this challenge, we introduce Asteria, a novel cross-region\nknowledge caching architecture for LLM agents. At its core are two\nabstractions: Semantic Element (SE) and Semantic Retrieval Index (Sine). A\nsemantic element captures the semantic embedding representation of an LLM query\ntogether with performance-aware metadata such as latency, cost, and staticity.\nSine then provides two-stage retrieval: a vector similar index with semantic\nembedding for fast candidate selection and a lightweight LLM-powered semantic\njudger for precise validation. Atop these primitives, Asteria builds a new\ncache interface that includes a new semantic-aware cache hit definition, a\ncost-efficient eviction policy, and proactive prefetching. To reduce overhead,\nAsteria co-locates the small LLM judger with the main LLM using adaptive\nscheduling and resource sharing. Our evaluation demonstrates that Asteria\ndelivers substantial performance improvements without compromising correctness.\nOn representative search workloads, Asteria achieves up to a 3.6$\\times$\nincrease in throughput by maintaining cache hit rates of over 85%, while\npreserving accuracy virtually identical to non-cached baselines. Asteria also\nimproves throughput for complex coding tasks by 20%, showcasing its versatility\nacross diverse agentic workloads."
                },
                "authors": [
                    {
                        "name": "Chaoyi Ruan"
                    },
                    {
                        "name": "Chao Bi"
                    },
                    {
                        "name": "Kaiwen Zheng"
                    },
                    {
                        "name": "Ziji Shi"
                    },
                    {
                        "name": "Xinyi Wan"
                    },
                    {
                        "name": "Jialin Li"
                    }
                ],
                "author_detail": {
                    "name": "Jialin Li"
                },
                "author": "Jialin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17257v1",
                "updated": "2025-09-21T22:14:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    22,
                    14,
                    56,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-21T22:14:56Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    22,
                    14,
                    56,
                    6,
                    264,
                    0
                ],
                "title": "On efficient block Krylov-solvers for $\\mathcal H^2$-matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On efficient block Krylov-solvers for $\\mathcal H^2$-matrices"
                },
                "summary": "Hierarchical matrices provide a highly memory-efficient way of storing dense\nlinear operators arising, for example, from boundary element methods,\nparticularly when stored in the H^2 format. In such data-sparse\nrepresentations, iterative solvers are preferred over direct ones due to the\ncost-efficient matrix-vector multiplications they enable. Solving multiple\nsystems of linear equations with the same hierarchical matrix naturally leads\nto block methods, which in turn make heavy use of BLAS level-3 functions such\nas GEMM. We present an efficient implementation of H^2-matrix-vector and\nH^2-matrix-matrix multiplication that fully exploits the potential of modern\nhardware in terms of memory and cache utilization. The latter is employed to\naccelerate block Krylov subspace methods, which we present later as the main\nresults of this paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical matrices provide a highly memory-efficient way of storing dense\nlinear operators arising, for example, from boundary element methods,\nparticularly when stored in the H^2 format. In such data-sparse\nrepresentations, iterative solvers are preferred over direct ones due to the\ncost-efficient matrix-vector multiplications they enable. Solving multiple\nsystems of linear equations with the same hierarchical matrix naturally leads\nto block methods, which in turn make heavy use of BLAS level-3 functions such\nas GEMM. We present an efficient implementation of H^2-matrix-vector and\nH^2-matrix-matrix multiplication that fully exploits the potential of modern\nhardware in terms of memory and cache utilization. The latter is employed to\naccelerate block Krylov subspace methods, which we present later as the main\nresults of this paper."
                },
                "authors": [
                    {
                        "name": "Sven Christophersen"
                    }
                ],
                "author_detail": {
                    "name": "Sven Christophersen"
                },
                "author": "Sven Christophersen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F55, 65F08, 65F10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17238v1",
                "updated": "2025-09-21T21:05:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    21,
                    5,
                    29,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-21T21:05:29Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    21,
                    5,
                    29,
                    6,
                    264,
                    0
                ],
                "title": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with\n  RoE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with\n  RoE"
                },
                "summary": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction.To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction.To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters."
                },
                "authors": [
                    {
                        "name": "Soheil Zibakhsh"
                    },
                    {
                        "name": "Mohammad Samragh"
                    },
                    {
                        "name": "Kumari Nishu"
                    },
                    {
                        "name": "Lauren Hannah"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07639v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07639v2",
                "updated": "2025-09-21T11:48:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    11,
                    48,
                    15,
                    6,
                    264,
                    0
                ],
                "published": "2025-06-09T11:04:13Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    11,
                    4,
                    13,
                    0,
                    160,
                    0
                ],
                "title": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse"
                },
                "summary": "Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action\n(VLA) models by improving performance and interpretability through intermediate\nreasoning steps. However, its sequential autoregressive token generation\nintroduces significant inference latency, limiting real-time deployment. We\npropose Fast ECoT, an inference-time acceleration method that exploits the\nstructured and repetitive nature of ECoT to (1) cache and reuse high-level\nreasoning across timesteps and (2) parallelise the generation of modular\nreasoning steps. Additionally, we introduce an asynchronous scheduler that\ndecouples reasoning from action decoding, further boosting responsiveness. Fast\nECoT requires no model changes or additional training and integrates easily\ninto existing VLA pipelines. Experiments in both simulation (LIBERO) and\nreal-world robot tasks show up to a 7.5% reduction in latency with comparable\nor improved task success rate and reasoning faithfulness, bringing ECoT\npolicies closer to practical real-time deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action\n(VLA) models by improving performance and interpretability through intermediate\nreasoning steps. However, its sequential autoregressive token generation\nintroduces significant inference latency, limiting real-time deployment. We\npropose Fast ECoT, an inference-time acceleration method that exploits the\nstructured and repetitive nature of ECoT to (1) cache and reuse high-level\nreasoning across timesteps and (2) parallelise the generation of modular\nreasoning steps. Additionally, we introduce an asynchronous scheduler that\ndecouples reasoning from action decoding, further boosting responsiveness. Fast\nECoT requires no model changes or additional training and integrates easily\ninto existing VLA pipelines. Experiments in both simulation (LIBERO) and\nreal-world robot tasks show up to a 7.5% reduction in latency with comparable\nor improved task success rate and reasoning faithfulness, bringing ECoT\npolicies closer to practical real-time deployment."
                },
                "authors": [
                    {
                        "name": "Zhekai Duan"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Shikai Geng"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Joschka Boedecker"
                    },
                    {
                        "name": "Chris Xiaoxuan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Chris Xiaoxuan Lu"
                },
                "author": "Chris Xiaoxuan Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07639v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07639v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10367v2",
                "updated": "2025-09-21T07:03:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    7,
                    3,
                    46,
                    6,
                    264,
                    0
                ],
                "published": "2025-07-14T15:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline"
                },
                "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junbin Kang"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Shaohong Guo"
                    },
                    {
                        "name": "Ziyan Qiu"
                    },
                    {
                        "name": "Mingzhen You"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Anqi Yu"
                    },
                    {
                        "name": "Tianhong Ding"
                    },
                    {
                        "name": "Xinwei Hu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by NSDI'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11815v2",
                "updated": "2025-09-21T03:35:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    3,
                    35,
                    36,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-15T11:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    53,
                    56,
                    0,
                    258,
                    0
                ],
                "title": "SpecVLM: Fast Speculative Decoding in Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecVLM: Fast Speculative Decoding in Vision-Language Models"
                },
                "summary": "Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM."
                },
                "authors": [
                    {
                        "name": "Haiduo Huang"
                    },
                    {
                        "name": "Fuwei Yang"
                    },
                    {
                        "name": "Zhenhua Liu"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Pengju Ren"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16857v1",
                "updated": "2025-09-21T00:59:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    0,
                    59,
                    45,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-21T00:59:45Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    0,
                    59,
                    45,
                    6,
                    264,
                    0
                ],
                "title": "ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix\n  Caching"
                },
                "summary": "Distributed prefix caching accelerates long-context LLM serving by reusing KV\ncache entries for common context prefixes. However, KV cache fetches can become\na bottleneck when network bandwidth is limited. Compression mitigates the\nbandwidth issue, but can degrade overall performance when decompression\ninterferes with model computation.\n  We present ShadowServe, the first SmartNIC-accelerated, interference-free\nprefix caching system for LLM serving. ShadowServe separates a control plane on\nthe host and a data plane fully offloaded to the SmartNIC, which eliminates\ninterference to both host GPU and CPU. To overcome the SmartNIC's limited\ncompute and memory resources, we design a chunked pipeline that parallelizes\ndata plane operations across the SmartNIC's compute resources, and a\nminimal-copy memory management scheme that reduces memory pressure on the\nSmartNIC. Compared to state-of-the-art solutions, ShadowServe achieves up to\n2.2x lower loaded time-per-output-token (TPOT), and reduces time-to-first-token\n(TTFT) by up to 1.38x in low-bandwidth scenarios (<= 20 Gbps), translating to\nup to 1.35x higher throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed prefix caching accelerates long-context LLM serving by reusing KV\ncache entries for common context prefixes. However, KV cache fetches can become\na bottleneck when network bandwidth is limited. Compression mitigates the\nbandwidth issue, but can degrade overall performance when decompression\ninterferes with model computation.\n  We present ShadowServe, the first SmartNIC-accelerated, interference-free\nprefix caching system for LLM serving. ShadowServe separates a control plane on\nthe host and a data plane fully offloaded to the SmartNIC, which eliminates\ninterference to both host GPU and CPU. To overcome the SmartNIC's limited\ncompute and memory resources, we design a chunked pipeline that parallelizes\ndata plane operations across the SmartNIC's compute resources, and a\nminimal-copy memory management scheme that reduces memory pressure on the\nSmartNIC. Compared to state-of-the-art solutions, ShadowServe achieves up to\n2.2x lower loaded time-per-output-token (TPOT), and reduces time-to-first-token\n(TTFT) by up to 1.38x in low-bandwidth scenarios (<= 20 Gbps), translating to\nup to 1.35x higher throughput."
                },
                "authors": [
                    {
                        "name": "Xingyu Xiang"
                    },
                    {
                        "name": "Raj Joshi"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Chenxingyu Zhao"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Eddie Kohler"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01960v2",
                "updated": "2025-09-20T13:54:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    13,
                    54,
                    37,
                    5,
                    263,
                    0
                ],
                "published": "2025-02-04T03:13:09Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    3,
                    13,
                    9,
                    1,
                    35,
                    0
                ],
                "title": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving"
                },
                "summary": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local disks when\nreceiving multimodal data, and calculates and loads the KV cache in parallel\nduring inference. To mitigate accuracy degradation, we have incorporated the\nintegrated reuse and recompute mechanism within the system. The experimental\nresults demonstrate that MPIC can achieve up to 54\\% reduction in response time\nand 2$\\times$ improvement in throughput compared to existing context caching\nsystems, while maintaining negligible or no accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local disks when\nreceiving multimodal data, and calculates and loads the KV cache in parallel\nduring inference. To mitigate accuracy degradation, we have incorporated the\nintegrated reuse and recompute mechanism within the system. The experimental\nresults demonstrate that MPIC can achieve up to 54\\% reduction in response time\nand 2$\\times$ improvement in throughput compared to existing context caching\nsystems, while maintaining negligible or no accuracy loss."
                },
                "authors": [
                    {
                        "name": "Shiju Zhao"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Rongxiao Huang"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "17 pages, 13 figures, the second version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16686v1",
                "updated": "2025-09-20T13:27:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    13,
                    27,
                    13,
                    5,
                    263,
                    0
                ],
                "published": "2025-09-20T13:27:13Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    13,
                    27,
                    13,
                    5,
                    263,
                    0
                ],
                "title": "EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and\n  Efficient LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and\n  Efficient LLMs"
                },
                "summary": "Reducing the key-value (KV) cache size is a crucial step toward enabling\nefficient inference in large language models (LLMs), especially under latency\nand memory constraints. While Multi-Head Attention (MHA) offers strong\nrepresentational power, it incurs significant memory overhead. Recent work on\nMulti-head Latent Attention (MLA) mitigates this by compressing KV\nrepresentations into a shared latent space, achieving a better trade-off\nbetween performance and cache efficiency. While MLA already achieves\nsignificant KV cache reduction, the scope for further compression remains\nlimited without performance loss. In this paper, we propose\n\\textbf{Embedding-Gated Multi-head Latent Attention (EG-MLA)}, a novel\nextension of MLA that further reduces KV cache size while enhancing\nrepresentational expressiveness. EG-MLA introduces a token-specific embedding\ngating mechanism applied in the latent space, enabling fine-grained modulation\nof compressed KV vectors with minimal additional computation. Compared to MHA,\nEG-MLA achieves over 91.6\\% reduction in KV cache size with negligible\nperformance degradation. Relative to MLA, EG-MLA consistently improves task\naccuracy across diverse reasoning benchmarks while achieving up to 59.9\\%\nadditional memory savings. Our theoretical analysis highlights how embedding\ngating induces implicit high-order interactions, and empirical evaluations\ndemonstrate robust generalization across model scales and compression regimes.\nNotably, we successfully scale EG-MLA to over 1 billion parameters,\ndemonstrating its practical viability for large-scale LLM deployment. These\nresults establish EG-MLA as a memory- and compute-efficient attention mechanism\nthat enables scalable, high-performance inference in modern LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache size is a crucial step toward enabling\nefficient inference in large language models (LLMs), especially under latency\nand memory constraints. While Multi-Head Attention (MHA) offers strong\nrepresentational power, it incurs significant memory overhead. Recent work on\nMulti-head Latent Attention (MLA) mitigates this by compressing KV\nrepresentations into a shared latent space, achieving a better trade-off\nbetween performance and cache efficiency. While MLA already achieves\nsignificant KV cache reduction, the scope for further compression remains\nlimited without performance loss. In this paper, we propose\n\\textbf{Embedding-Gated Multi-head Latent Attention (EG-MLA)}, a novel\nextension of MLA that further reduces KV cache size while enhancing\nrepresentational expressiveness. EG-MLA introduces a token-specific embedding\ngating mechanism applied in the latent space, enabling fine-grained modulation\nof compressed KV vectors with minimal additional computation. Compared to MHA,\nEG-MLA achieves over 91.6\\% reduction in KV cache size with negligible\nperformance degradation. Relative to MLA, EG-MLA consistently improves task\naccuracy across diverse reasoning benchmarks while achieving up to 59.9\\%\nadditional memory savings. Our theoretical analysis highlights how embedding\ngating induces implicit high-order interactions, and empirical evaluations\ndemonstrate robust generalization across model scales and compression regimes.\nNotably, we successfully scale EG-MLA to over 1 billion parameters,\ndemonstrating its practical viability for large-scale LLM deployment. These\nresults establish EG-MLA as a memory- and compute-efficient attention mechanism\nthat enables scalable, high-performance inference in modern LLMs."
                },
                "authors": [
                    {
                        "name": "Zhengge Cai"
                    },
                    {
                        "name": "Haowen Hou"
                    }
                ],
                "author_detail": {
                    "name": "Haowen Hou"
                },
                "author": "Haowen Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16630v1",
                "updated": "2025-09-20T11:09:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    11,
                    9,
                    1,
                    5,
                    263,
                    0
                ],
                "published": "2025-09-20T11:09:01Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    11,
                    9,
                    1,
                    5,
                    263,
                    0
                ],
                "title": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and\n  Expressive Freestyle Portrait Animation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and\n  Expressive Freestyle Portrait Animation"
                },
                "summary": "We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework\nfor freestyle portrait animation driven by facial landmarks. The main\nchallenges in this task are preserving the identity of the reference portrait,\naccurately transferring target expressions, and maintaining long-term temporal\nconsistency while ensuring generation efficiency. To address identity\npreservation and accurate expression retargeting, we enhance Stable Diffusion\nwith two key components: a expression-aware landmarks as explicit motion\nsignals, which improve motion alignment, support exaggerated expressions, and\nreduce identity leakage; and a fine-grained facial loss that leverages both\nexpression and facial masks to better capture subtle expressions and faithfully\npreserve the reference appearance. With these components, our model supports\ncontrollable and expressive animation across diverse portrait types, including\nreal faces, cartoons, sculptures, and animals. However, diffusion-based\nframeworks typically struggle to efficiently generate long-term stable\nanimation results, which remains a core challenge in this task. To address\nthis, we propose a progressive generation strategy for stable long-term\nanimation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless\nacceleration. These two strategies ensure that our method produces high-quality\nresults efficiently, making it user-friendly and accessible. Finally, we\nintroduce EmojiBench++, a more comprehensive benchmark comprising diverse\nportraits, driving videos, and landmark sequences. Extensive evaluations on\nEmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior\nperformance in both animation quality and controllability. The code, training\ndataset and benchmark will be found in https://follow-your-emoji.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework\nfor freestyle portrait animation driven by facial landmarks. The main\nchallenges in this task are preserving the identity of the reference portrait,\naccurately transferring target expressions, and maintaining long-term temporal\nconsistency while ensuring generation efficiency. To address identity\npreservation and accurate expression retargeting, we enhance Stable Diffusion\nwith two key components: a expression-aware landmarks as explicit motion\nsignals, which improve motion alignment, support exaggerated expressions, and\nreduce identity leakage; and a fine-grained facial loss that leverages both\nexpression and facial masks to better capture subtle expressions and faithfully\npreserve the reference appearance. With these components, our model supports\ncontrollable and expressive animation across diverse portrait types, including\nreal faces, cartoons, sculptures, and animals. However, diffusion-based\nframeworks typically struggle to efficiently generate long-term stable\nanimation results, which remains a core challenge in this task. To address\nthis, we propose a progressive generation strategy for stable long-term\nanimation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless\nacceleration. These two strategies ensure that our method produces high-quality\nresults efficiently, making it user-friendly and accessible. Finally, we\nintroduce EmojiBench++, a more comprehensive benchmark comprising diverse\nportraits, driving videos, and landmark sequences. Extensive evaluations on\nEmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior\nperformance in both animation quality and controllability. The code, training\ndataset and benchmark will be found in https://follow-your-emoji.github.io/."
                },
                "authors": [
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Hongyu Liu"
                    },
                    {
                        "name": "Hongfa Wang"
                    },
                    {
                        "name": "Heng Pan"
                    },
                    {
                        "name": "Yingqing He"
                    },
                    {
                        "name": "Junkun Yuan"
                    },
                    {
                        "name": "Ailing Zeng"
                    },
                    {
                        "name": "Chengfei Cai"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Zhifeng Li"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Qifeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qifeng Chen"
                },
                "author": "Qifeng Chen",
                "arxiv_comment": "accepted by IJCV2025. project\n  page:https://follow-your-emoji.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21354v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21354v1",
                "updated": "2025-09-20T02:04:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    2,
                    4,
                    24,
                    5,
                    263,
                    0
                ],
                "published": "2025-09-20T02:04:24Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    2,
                    4,
                    24,
                    5,
                    263,
                    0
                ],
                "title": "KV-Efficient VLA: A Method of Speed up Vision Language Model with\n  RNN-Gated Chunked KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Efficient VLA: A Method of Speed up Vision Language Model with\n  RNN-Gated Chunked KV Cache"
                },
                "summary": "Vision-Language-Action (VLA) models promise unified robotic perception and\ncontrol, yet their scalability is constrained by the quadratic cost of\nattention and the unbounded growth of key-value (KV) memory during long-horizon\ninference. While recent methods improve generalization through scaling backbone\narchitectures, they often neglect the inference inefficiencies critical to\nreal-time deployment. In this work, we present KV-Efficient VLA, a\nmodel-agnostic memory compression framework that addresses these limitations by\nintroducing a lightweight, training-friendly mechanism to selectively retain\nhigh-utility context. Our method partitions the KV cache into fixed size chunks\nand employs a recurrent gating module to summarize and filter historical\ncontext according to learned utility scores. This design preserves recent\nfine-grained detail while aggressively pruning stale, low-relevance memory, all\nwhile maintaining causality. Theoretically, KV-Efficient VLA yields up to 1.21x\ninference speedup and 36% KV memory reduction, with minimal impact on task\nsuccess. Our method integrates seamlessly into existing autoregressive and\nhybrid VLA stacks, enabling scalable inference without modifying training\npipelines or downstream control logic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models promise unified robotic perception and\ncontrol, yet their scalability is constrained by the quadratic cost of\nattention and the unbounded growth of key-value (KV) memory during long-horizon\ninference. While recent methods improve generalization through scaling backbone\narchitectures, they often neglect the inference inefficiencies critical to\nreal-time deployment. In this work, we present KV-Efficient VLA, a\nmodel-agnostic memory compression framework that addresses these limitations by\nintroducing a lightweight, training-friendly mechanism to selectively retain\nhigh-utility context. Our method partitions the KV cache into fixed size chunks\nand employs a recurrent gating module to summarize and filter historical\ncontext according to learned utility scores. This design preserves recent\nfine-grained detail while aggressively pruning stale, low-relevance memory, all\nwhile maintaining causality. Theoretically, KV-Efficient VLA yields up to 1.21x\ninference speedup and 36% KV memory reduction, with minimal impact on task\nsuccess. Our method integrates seamlessly into existing autoregressive and\nhybrid VLA stacks, enabling scalable inference without modifying training\npipelines or downstream control logic."
                },
                "authors": [
                    {
                        "name": "Wanshun Xu"
                    },
                    {
                        "name": "Long Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Long Zhuang"
                },
                "author": "Long Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21354v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16495v1",
                "updated": "2025-09-20T01:56:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    1,
                    56,
                    25,
                    5,
                    263,
                    0
                ],
                "published": "2025-09-20T01:56:25Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    1,
                    56,
                    25,
                    5,
                    263,
                    0
                ],
                "title": "Shift Parallelism: Low-Latency, High-Throughput LLM Inference for\n  Dynamic Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shift Parallelism: Low-Latency, High-Throughput LLM Inference for\n  Dynamic Workloads"
                },
                "summary": "Efficient parallelism is necessary for achieving low-latency, high-throughput\ninference with large language models (LLMs). Tensor parallelism (TP) is the\nstate-of-the-art method for reducing LLM response latency, however GPU\ncommunications reduces combined token throughput. On the other hand, data\nparallelism (DP) obtains a higher throughput yet is slow in response latency.\nBest of both worlds does not exist, and it is not possible to combine TP and DP\nbecause of the KV cache variance across the parallelisms.\n  We notice Sequence Parallelism (SP - Ulysses in training) has similar\nproperties as DP but with KV cache invariance. We adapt SP to inference, and\ncombine it with TP to get the best of both worlds. Our solution: Shift\nParallelism.\n  Shift Parallelism dynamically switches across TP and SP, and minimizes\nlatency in low traffic without losing throughput in high traffic. The efficient\nGPU communications of Shift Parallelism yields up to i) 1.51x faster response\nin interactive workloads and ii) 50% higher throughput in batch workloads,\ncompared to a TP-only solution.\n  We evaluate Shift Parallelism with real-world production traces with dynamic\ntraffic patterns as well as synthetic benchmarking patterns across models,\ncontext sizes, and arrival rates. All results affirm the same: Shift\nParallelism has a better the latency vs. throughput tradeoff than TP or DP, and\nhence obtains low latency without degrading throughput in dynamic workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient parallelism is necessary for achieving low-latency, high-throughput\ninference with large language models (LLMs). Tensor parallelism (TP) is the\nstate-of-the-art method for reducing LLM response latency, however GPU\ncommunications reduces combined token throughput. On the other hand, data\nparallelism (DP) obtains a higher throughput yet is slow in response latency.\nBest of both worlds does not exist, and it is not possible to combine TP and DP\nbecause of the KV cache variance across the parallelisms.\n  We notice Sequence Parallelism (SP - Ulysses in training) has similar\nproperties as DP but with KV cache invariance. We adapt SP to inference, and\ncombine it with TP to get the best of both worlds. Our solution: Shift\nParallelism.\n  Shift Parallelism dynamically switches across TP and SP, and minimizes\nlatency in low traffic without losing throughput in high traffic. The efficient\nGPU communications of Shift Parallelism yields up to i) 1.51x faster response\nin interactive workloads and ii) 50% higher throughput in batch workloads,\ncompared to a TP-only solution.\n  We evaluate Shift Parallelism with real-world production traces with dynamic\ntraffic patterns as well as synthetic benchmarking patterns across models,\ncontext sizes, and arrival rates. All results affirm the same: Shift\nParallelism has a better the latency vs. throughput tradeoff than TP or DP, and\nhence obtains low latency without degrading throughput in dynamic workloads."
                },
                "authors": [
                    {
                        "name": "Mert Hidayetoglu"
                    },
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Michael Wyatt"
                    },
                    {
                        "name": "Jeff Rasley"
                    },
                    {
                        "name": "Yuxiong He"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    }
                ],
                "author_detail": {
                    "name": "Samyam Rajbhandari"
                },
                "author": "Samyam Rajbhandari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16471v1",
                "updated": "2025-09-19T23:46:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    23,
                    46,
                    8,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T23:46:08Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    23,
                    46,
                    8,
                    4,
                    262,
                    0
                ],
                "title": "From Coated to Uncoated: Scanning Electron Microscopy Corrections to\n  Estimate True Surface Pore Size in Nanoporous Membranes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Coated to Uncoated: Scanning Electron Microscopy Corrections to\n  Estimate True Surface Pore Size in Nanoporous Membranes"
                },
                "summary": "Scanning electron microscopy (SEM) is the premier method for characterizing\nthe nanoscale surface pores in ultrafiltration (UF) membranes and the support\nlayers of reverse osmosis (RO) membranes. Based on SEM, the conventional\nunderstanding is that membranes typically have low surface porosities of <10%.\nWe hypothesized that high acceleration voltage during SEM imaging and sputter\nmetal coatings required for SEM have led to systematic underestimations of\nporosity and pore size. We showed that imaging a commercial UF membrane at 1,\n5, and 10 kV reduced measured porosity from 10.3% (1 kV) to 6.3% (10 kV), while\nincreasing Pt coating thickness from 1.5 to 5 nm lowered porosity by 54% for\nthe UF membrane (12.9% to 5.8%) and 46% for an RO support (13.1% to 7.0%). To\naccount for coating thickness, we developed a digital correction method that\nsimulates pore dilation, enabling the pore structure to be estimated for\nuncoated membranes. Dilation yielded uncoated porosity values of 23% for the UF\nmembrane and 20% for the RO support, about 3-fold greater than values observed\nwith a 4 nm coating. Mean pore diameters were 2-fold greater for the UF\nmembrane and 1.5-fold greater for the RO support. Critically, dilation-derived\npore-size distributions agreed with low-flux dextran-retention data fitted with\nthe Bungay-Brenner model. Our results suggest that surface porosities and pore\nsizes of nanoporous membranes are much larger than previously understood, with\nmajor implications for structure/transport relationships. For future nanoscale\npore analysis of membranes (and other nanoporous materials), we recommend low\nacceleration voltage (1 kV), minimal coatings (1-2 nm), and digital dilation to\naccount for coating artifacts",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scanning electron microscopy (SEM) is the premier method for characterizing\nthe nanoscale surface pores in ultrafiltration (UF) membranes and the support\nlayers of reverse osmosis (RO) membranes. Based on SEM, the conventional\nunderstanding is that membranes typically have low surface porosities of <10%.\nWe hypothesized that high acceleration voltage during SEM imaging and sputter\nmetal coatings required for SEM have led to systematic underestimations of\nporosity and pore size. We showed that imaging a commercial UF membrane at 1,\n5, and 10 kV reduced measured porosity from 10.3% (1 kV) to 6.3% (10 kV), while\nincreasing Pt coating thickness from 1.5 to 5 nm lowered porosity by 54% for\nthe UF membrane (12.9% to 5.8%) and 46% for an RO support (13.1% to 7.0%). To\naccount for coating thickness, we developed a digital correction method that\nsimulates pore dilation, enabling the pore structure to be estimated for\nuncoated membranes. Dilation yielded uncoated porosity values of 23% for the UF\nmembrane and 20% for the RO support, about 3-fold greater than values observed\nwith a 4 nm coating. Mean pore diameters were 2-fold greater for the UF\nmembrane and 1.5-fold greater for the RO support. Critically, dilation-derived\npore-size distributions agreed with low-flux dextran-retention data fitted with\nthe Bungay-Brenner model. Our results suggest that surface porosities and pore\nsizes of nanoporous membranes are much larger than previously understood, with\nmajor implications for structure/transport relationships. For future nanoscale\npore analysis of membranes (and other nanoporous materials), we recommend low\nacceleration voltage (1 kV), minimal coatings (1-2 nm), and digital dilation to\naccount for coating artifacts"
                },
                "authors": [
                    {
                        "name": "Sima Zeinali Danalou"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Niher R. Sarker"
                    },
                    {
                        "name": "Hooman Chamani"
                    },
                    {
                        "name": "Jane Y. Howe"
                    },
                    {
                        "name": "Patrick C. Lee"
                    },
                    {
                        "name": "Jay R. Werber"
                    }
                ],
                "author_detail": {
                    "name": "Jay R. Werber"
                },
                "author": "Jay R. Werber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16407v1",
                "updated": "2025-09-19T20:31:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    20,
                    31,
                    38,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T20:31:38Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    20,
                    31,
                    38,
                    4,
                    262,
                    0
                ],
                "title": "WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables"
                },
                "summary": "GPU hash tables are increasingly used to accelerate data processing, but\ntheir limited functionality restricts adoption in large-scale data processing\napplications. Current limitations include incomplete concurrency support and\nmissing compound operations such as upserts.\n  This paper presents WarpSpeed, a library of high-performance concurrent GPU\nhash tables with a unified benchmarking framework for performance analysis.\nWarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and\nprovides a rich API designed for modern GPU applications. Our evaluation uses\ndiverse benchmarks to assess both correctness and scalability, and we\ndemonstrate real-world impact by integrating these hash tables into three\ndownstream applications.\n  We propose several optimization techniques to reduce concurrency overhead,\nincluding fingerprint-based metadata to minimize cache line probes and\nspecialized Nvidia GPU instructions for lock-free queries. Our findings provide\nnew insights into concurrent GPU hash table design and offer practical guidance\nfor developing efficient, scalable data structures on modern GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hash tables are increasingly used to accelerate data processing, but\ntheir limited functionality restricts adoption in large-scale data processing\napplications. Current limitations include incomplete concurrency support and\nmissing compound operations such as upserts.\n  This paper presents WarpSpeed, a library of high-performance concurrent GPU\nhash tables with a unified benchmarking framework for performance analysis.\nWarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and\nprovides a rich API designed for modern GPU applications. Our evaluation uses\ndiverse benchmarks to assess both correctness and scalability, and we\ndemonstrate real-world impact by integrating these hash tables into three\ndownstream applications.\n  We propose several optimization techniques to reduce concurrency overhead,\nincluding fingerprint-based metadata to minimize cache line probes and\nspecialized Nvidia GPU instructions for lock-free queries. Our findings provide\nnew insights into concurrent GPU hash table design and offer practical guidance\nfor developing efficient, scalable data structures on modern GPUs."
                },
                "authors": [
                    {
                        "name": "Hunter McCoy"
                    },
                    {
                        "name": "Prashant Pandey"
                    }
                ],
                "author_detail": {
                    "name": "Prashant Pandey"
                },
                "author": "Prashant Pandey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20587v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20587v2",
                "updated": "2025-09-19T17:18:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    18,
                    26,
                    4,
                    262,
                    0
                ],
                "published": "2025-02-27T23:09:20Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    23,
                    9,
                    20,
                    3,
                    58,
                    0
                ],
                "title": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Reasoning"
                },
                "summary": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general reasoning benchmarks, and show that\nCoT increases overall reasoning performance by up to 7.7% under the same\nbudget, and specifically boosts the performance of apprentice VLMs by up to\n36.6%. Our code is available at https://github.com/UIUC-MONET/Cache-of-Thoughts",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general reasoning benchmarks, and show that\nCoT increases overall reasoning performance by up to 7.7% under the same\nbudget, and specifically boosts the performance of apprentice VLMs by up to\n36.6%. Our code is available at https://github.com/UIUC-MONET/Cache-of-Thoughts"
                },
                "authors": [
                    {
                        "name": "Mingyuan Wu"
                    },
                    {
                        "name": "Jize Jiang"
                    },
                    {
                        "name": "Haozhen Zheng"
                    },
                    {
                        "name": "Meitang Li"
                    },
                    {
                        "name": "Zhaoheng Li"
                    },
                    {
                        "name": "Beitong Tian"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yongjoo Park"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Chengxiang Zhai"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    }
                ],
                "author_detail": {
                    "name": "Klara Nahrstedt"
                },
                "author": "Klara Nahrstedt",
                "arxiv_comment": "EMNLP 2025 Main Conference. Mingyuan, Jize, and Haozhen contributed\n  equally, while Minjia, Chengxiang, and Klara advised equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20587v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20587v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05165v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05165v2",
                "updated": "2025-09-19T15:19:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    19,
                    26,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-05T14:58:24Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "title": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens"
                },
                "summary": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment."
                },
                "authors": [
                    {
                        "name": "Dmitry Akulov"
                    },
                    {
                        "name": "Mohamed Sana"
                    },
                    {
                        "name": "Antonio De Domenico"
                    },
                    {
                        "name": "Tareq Si Salem"
                    },
                    {
                        "name": "Nicola Piovesan"
                    },
                    {
                        "name": "Fadhel Ayed"
                    }
                ],
                "author_detail": {
                    "name": "Fadhel Ayed"
                },
                "author": "Fadhel Ayed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05165v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05165v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09536v2",
                "updated": "2025-09-19T14:14:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    14,
                    32,
                    4,
                    262,
                    0
                ],
                "published": "2025-06-11T09:08:59Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    8,
                    59,
                    2,
                    162,
                    0
                ],
                "title": "Commissioning, characterization and first high dose rate irradiations at\n  a compact X-ray tube for microbeam and minibeam radiation therapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commissioning, characterization and first high dose rate irradiations at\n  a compact X-ray tube for microbeam and minibeam radiation therapy"
                },
                "summary": "Minibeam and microbeam radiation therapy promise improved treatment outcomes\nthrough reduced normal tissue toxicity at better tumor control rates. The lack\nof suitable compact radiation sources limits the clinical application of\nminibeams to superficial tumors and renders it impossible for microbeams. We\ndeveloped the first prototype of a compact line-focus X-ray tube (LFXT) with\ntechnology potentially suitable for clinical translation of minibeams and\nmicrobeams. We give an overview of the commissioning process preceding first\noperation, present optical and radiological focal spot characterization\nmethods, and dosimetric measurements. Additionally, we report on first\npreclinical in vitro cell and in vivo mouse brain irradiations conducted with\nthe LFXT prototype. The LFXT was high voltage conditioned up to 300 kV.The\nfocal spot characterization resulted in a strongly eccentric electron\ndistribution with a width of 72.3 $\\mu$m. Dosimetry showed sharp microbeam dose\nprofiles with steep lateral penumbras and a peak-to-valley dose ratio above 10\nthroughout a 70 mm thick PMMA phantom. An open-field dose rate of 4.3 Gy/s was\nmeasured at an acceleration voltage of 150 kV and a beam current of 17.4 mA at\n150 mm distance from the focal spot. In vitro and in vivo experiments\ndemonstrated the feasibility of the LFXT for minibeam and microbeam\napplications with field sizes of 1.5-2 cm. The mice displayed no observable\nside effects after whole-brain 260 $\\mu$m-minibeam irradiation. We successfully\nconstructed and commissioned the first proof-of-concept LFXT prototype.\nDosimetric characterizations of the achieved microbeam field showed the\nsuperiority of the LFXT compared to conventional X-ray tubes in terms of beam\nquality. In future developments, the remaining limitations of the prototype\nwill be addressed for improved minibeam and first ever microbeam radiation\ntherapy in a clinical setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minibeam and microbeam radiation therapy promise improved treatment outcomes\nthrough reduced normal tissue toxicity at better tumor control rates. The lack\nof suitable compact radiation sources limits the clinical application of\nminibeams to superficial tumors and renders it impossible for microbeams. We\ndeveloped the first prototype of a compact line-focus X-ray tube (LFXT) with\ntechnology potentially suitable for clinical translation of minibeams and\nmicrobeams. We give an overview of the commissioning process preceding first\noperation, present optical and radiological focal spot characterization\nmethods, and dosimetric measurements. Additionally, we report on first\npreclinical in vitro cell and in vivo mouse brain irradiations conducted with\nthe LFXT prototype. The LFXT was high voltage conditioned up to 300 kV.The\nfocal spot characterization resulted in a strongly eccentric electron\ndistribution with a width of 72.3 $\\mu$m. Dosimetry showed sharp microbeam dose\nprofiles with steep lateral penumbras and a peak-to-valley dose ratio above 10\nthroughout a 70 mm thick PMMA phantom. An open-field dose rate of 4.3 Gy/s was\nmeasured at an acceleration voltage of 150 kV and a beam current of 17.4 mA at\n150 mm distance from the focal spot. In vitro and in vivo experiments\ndemonstrated the feasibility of the LFXT for minibeam and microbeam\napplications with field sizes of 1.5-2 cm. The mice displayed no observable\nside effects after whole-brain 260 $\\mu$m-minibeam irradiation. We successfully\nconstructed and commissioned the first proof-of-concept LFXT prototype.\nDosimetric characterizations of the achieved microbeam field showed the\nsuperiority of the LFXT compared to conventional X-ray tubes in terms of beam\nquality. In future developments, the remaining limitations of the prototype\nwill be addressed for improved minibeam and first ever microbeam radiation\ntherapy in a clinical setting."
                },
                "authors": [
                    {
                        "name": "Christian Petrich"
                    },
                    {
                        "name": "Johanna Winter"
                    },
                    {
                        "name": "Anton Dimroth"
                    },
                    {
                        "name": "Thomas Beiser"
                    },
                    {
                        "name": "Monika Dehn"
                    },
                    {
                        "name": "Jessica Stolz"
                    },
                    {
                        "name": "Jacopo Frignani"
                    },
                    {
                        "name": "Stephanie E. Combs"
                    },
                    {
                        "name": "Franz Schilling"
                    },
                    {
                        "name": "Ghaleb Natour"
                    },
                    {
                        "name": "Kurt Aulenbacher"
                    },
                    {
                        "name": "Thomas E. Schmid"
                    },
                    {
                        "name": "Jan J. Wilkens"
                    },
                    {
                        "name": "Stefan Bartzsch"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Bartzsch"
                },
                "author": "Stefan Bartzsch",
                "arxiv_comment": "CP, JW, and AD share first authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15763v1",
                "updated": "2025-09-19T08:47:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    8,
                    47,
                    37,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T08:47:37Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    8,
                    47,
                    37,
                    4,
                    262,
                    0
                ],
                "title": "UniGist: Towards General and Hardware-aligned Sequence-level Long\n  Context Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniGist: Towards General and Hardware-aligned Sequence-level Long\n  Context Compression"
                },
                "summary": "Large language models are increasingly capable of handling long-context\ninputs, but the memory overhead of key-value (KV) cache remains a major\nbottleneck for general-purpose deployment. While various compression strategies\nhave been explored, sequence-level compression, which drops the full KV caches\nfor certain tokens, is particularly challenging as it can lead to the loss of\nimportant contextual information. To address this, we introduce UniGist, a\nsequence-level long-context compression framework that efficiently preserves\ncontext information by replacing raw tokens with special compression tokens\n(gists) in a fine-grained manner. We adopt a chunk-free training strategy and\ndesign an efficient kernel with a gist shift trick, enabling optimized GPU\ntraining. Our scheme also supports flexible inference by allowing the actual\nremoval of compressed tokens, resulting in real-time memory savings.\nExperiments across multiple long-context tasks demonstrate that UniGist\nsignificantly improves compression quality, with especially strong performance\nin detail-recalling tasks and long-range dependency modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are increasingly capable of handling long-context\ninputs, but the memory overhead of key-value (KV) cache remains a major\nbottleneck for general-purpose deployment. While various compression strategies\nhave been explored, sequence-level compression, which drops the full KV caches\nfor certain tokens, is particularly challenging as it can lead to the loss of\nimportant contextual information. To address this, we introduce UniGist, a\nsequence-level long-context compression framework that efficiently preserves\ncontext information by replacing raw tokens with special compression tokens\n(gists) in a fine-grained manner. We adopt a chunk-free training strategy and\ndesign an efficient kernel with a gist shift trick, enabling optimized GPU\ntraining. Our scheme also supports flexible inference by allowing the actual\nremoval of compressed tokens, resulting in real-time memory savings.\nExperiments across multiple long-context tasks demonstrate that UniGist\nsignificantly improves compression quality, with especially strong performance\nin detail-recalling tasks and long-range dependency modeling."
                },
                "authors": [
                    {
                        "name": "Chenlong Deng"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Kelong Mao"
                    },
                    {
                        "name": "Shuaiyi Li"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04462v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04462v2",
                "updated": "2025-09-19T06:20:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    6,
                    20,
                    14,
                    4,
                    262,
                    0
                ],
                "published": "2025-08-06T14:02:10Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    2,
                    10,
                    2,
                    218,
                    0
                ],
                "title": "CARD: A Cache-Assisted Parallel Speculative Decoding Framework via\n  Query-and-Correct Paradigm for Accelerating LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARD: A Cache-Assisted Parallel Speculative Decoding Framework via\n  Query-and-Correct Paradigm for Accelerating LLM Inference"
                },
                "summary": "Speculative decoding (SD), where a draft model provides multiple candidate\ntokens for the target model to verify in parallel, has demonstrated significant\npotential for accelerating LLM inference. Yet, existing SD approaches adhere to\na strict draft-then-verify paradigm, enforcing a sequential process that\nhampers performance and constrains the draft model's capacity. Moreover,\nrejecting a token in the candidate sequence invalidates all subsequent tokens,\nleading to wasted computation during drafting. To overcome these limitations,\nwe propose a cache-assisted parallel speculative decoding framework called\nCARD, which employs a novel query-and-correct paradigm. Our approach decouples\ndrafting from verification: the draft model populates a shared cache with\ncandidate tokens, while the target model concurrently refines the draft's\ntrajectory. This enables inference at near-draft-speed, effectively leveraging\nthe draft model's efficiency without additional fine-tuning. Experimental\nresults show that CARD significantly outperforms existing state-of-the-art\nmethods, achieving up to a 4.83x acceleration over vanilla autoregressive\ndecoding, with no fine-tuning required for either models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD), where a draft model provides multiple candidate\ntokens for the target model to verify in parallel, has demonstrated significant\npotential for accelerating LLM inference. Yet, existing SD approaches adhere to\na strict draft-then-verify paradigm, enforcing a sequential process that\nhampers performance and constrains the draft model's capacity. Moreover,\nrejecting a token in the candidate sequence invalidates all subsequent tokens,\nleading to wasted computation during drafting. To overcome these limitations,\nwe propose a cache-assisted parallel speculative decoding framework called\nCARD, which employs a novel query-and-correct paradigm. Our approach decouples\ndrafting from verification: the draft model populates a shared cache with\ncandidate tokens, while the target model concurrently refines the draft's\ntrajectory. This enables inference at near-draft-speed, effectively leveraging\nthe draft model's efficiency without additional fine-tuning. Experimental\nresults show that CARD significantly outperforms existing state-of-the-art\nmethods, achieving up to a 4.83x acceleration over vanilla autoregressive\ndecoding, with no fine-tuning required for either models."
                },
                "authors": [
                    {
                        "name": "Enyu Zhou"
                    },
                    {
                        "name": "Kai Sheng"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Xin He"
                    }
                ],
                "author_detail": {
                    "name": "Xin He"
                },
                "author": "Xin He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04462v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04462v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15529v1",
                "updated": "2025-09-19T02:27:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    2,
                    27,
                    1,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T02:27:01Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    2,
                    27,
                    1,
                    4,
                    262,
                    0
                ],
                "title": "Optimization techniques for SQL+ML queries: A performance analysis of\n  real-time feature computation in OpenMLDB",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization techniques for SQL+ML queries: A performance analysis of\n  real-time feature computation in OpenMLDB"
                },
                "summary": "In this study, we optimize SQL+ML queries on top of OpenMLDB, an open-source\ndatabase that seamlessly integrates offline and online feature computations.\nThe work used feature-rich synthetic dataset experiments in Docker, which acted\nlike production environments that processed 100 to 500 records per batch and 6\nto 12 requests per batch in parallel. Efforts have been concentrated in the\nareas of better query plans, cached execution plans, parallel processing, and\nresource management. The experimental results show that OpenMLDB can support\napproximately 12,500 QPS with less than 1 ms latency, outperforming SparkSQL\nand ClickHouse by a factor of 23 and PostgreSQL and MySQL by 3.57 times. This\nstudy assessed the impact of optimization and showed that query plan\noptimization accounted for 35% of the performance gains, caching for 25%, and\nparallel processing for 20%. These results illustrate OpenMLDB's capability for\ntime-sensitive ML use cases, such as fraud detection, personalized\nrecommendation, and time series forecasting. The system's modular optimization\nframework, which combines batch and stream processing without interference,\ncontributes to its significant performance gain over traditional database\nsystems, particularly in applications that require real-time feature\ncomputation and serving. This study contributes to the understanding and design\nof high-performance SQL+ML systems and highlights the need for specialized SQL\noptimization for ML workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we optimize SQL+ML queries on top of OpenMLDB, an open-source\ndatabase that seamlessly integrates offline and online feature computations.\nThe work used feature-rich synthetic dataset experiments in Docker, which acted\nlike production environments that processed 100 to 500 records per batch and 6\nto 12 requests per batch in parallel. Efforts have been concentrated in the\nareas of better query plans, cached execution plans, parallel processing, and\nresource management. The experimental results show that OpenMLDB can support\napproximately 12,500 QPS with less than 1 ms latency, outperforming SparkSQL\nand ClickHouse by a factor of 23 and PostgreSQL and MySQL by 3.57 times. This\nstudy assessed the impact of optimization and showed that query plan\noptimization accounted for 35% of the performance gains, caching for 25%, and\nparallel processing for 20%. These results illustrate OpenMLDB's capability for\ntime-sensitive ML use cases, such as fraud detection, personalized\nrecommendation, and time series forecasting. The system's modular optimization\nframework, which combines batch and stream processing without interference,\ncontributes to its significant performance gain over traditional database\nsystems, particularly in applications that require real-time feature\ncomputation and serving. This study contributes to the understanding and design\nof high-performance SQL+ML systems and highlights the need for specialized SQL\noptimization for ML workloads."
                },
                "authors": [
                    {
                        "name": "Mashkhal A. Sidiq"
                    },
                    {
                        "name": "Aras A. Salih"
                    },
                    {
                        "name": "Samrand M. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Samrand M. Hassan"
                },
                "author": "Samrand M. Hassan",
                "arxiv_doi": "10.5121/ijdms.2025.17501",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5121/ijdms.2025.17501",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.15529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 4 figures, 1 Table",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15515v1",
                "updated": "2025-09-19T01:39:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    1,
                    39,
                    8,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T01:39:08Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    1,
                    39,
                    8,
                    4,
                    262,
                    0
                ],
                "title": "LLM Cache Bandit Revisited: Addressing Query Heterogeneity for\n  Cost-Effective LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Cache Bandit Revisited: Addressing Query Heterogeneity for\n  Cost-Effective LLM Inference"
                },
                "summary": "This paper revisits the LLM cache bandit problem, with a special focus on\naddressing the query heterogeneity for cost-effective LLM inference. Previous\nworks often assume uniform query sizes. Heterogeneous query sizes introduce a\ncombinatorial structure for cache selection, making the cache replacement\nprocess more computationally and statistically challenging. We treat optimal\ncache selection as a knapsack problem and employ an accumulation-based strategy\nto effectively balance computational overhead and cache updates. In theoretical\nanalysis, we prove that the regret of our algorithm achieves an $O(\\sqrt{MNT})$\nbound, improving the coefficient of $\\sqrt{MN}$ compared to the $O(MN\\sqrt{T})$\nresult in Berkeley, where $N$ is the total number of queries and $M$ is the\ncache size. Additionally, we also provide a problem-dependent bound, which was\nabsent in previous works. The experiment rely on real-world data show that our\nalgorithm reduces the total cost by approximately 12\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper revisits the LLM cache bandit problem, with a special focus on\naddressing the query heterogeneity for cost-effective LLM inference. Previous\nworks often assume uniform query sizes. Heterogeneous query sizes introduce a\ncombinatorial structure for cache selection, making the cache replacement\nprocess more computationally and statistically challenging. We treat optimal\ncache selection as a knapsack problem and employ an accumulation-based strategy\nto effectively balance computational overhead and cache updates. In theoretical\nanalysis, we prove that the regret of our algorithm achieves an $O(\\sqrt{MNT})$\nbound, improving the coefficient of $\\sqrt{MN}$ compared to the $O(MN\\sqrt{T})$\nresult in Berkeley, where $N$ is the total number of queries and $M$ is the\ncache size. Additionally, we also provide a problem-dependent bound, which was\nabsent in previous works. The experiment rely on real-world data show that our\nalgorithm reduces the total cost by approximately 12\\%."
                },
                "authors": [
                    {
                        "name": "Hantao Yang"
                    },
                    {
                        "name": "Hong Xie"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01002v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01002v3",
                "updated": "2025-09-18T23:34:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    23,
                    34,
                    50,
                    3,
                    261,
                    0
                ],
                "published": "2025-05-02T04:57:06Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "title": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber"
                },
                "summary": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses."
                },
                "authors": [
                    {
                        "name": "NEXT Collaboration"
                    },
                    {
                        "name": "C. Adams"
                    },
                    {
                        "name": "H. Almazán"
                    },
                    {
                        "name": "V. Álvarez"
                    },
                    {
                        "name": "K. Bailey"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "B. J. P. Jones"
                    },
                    {
                        "name": "S. Johnston"
                    },
                    {
                        "name": "K. Mistry"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "D. R. Nygren"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "L. Rogers"
                    },
                    {
                        "name": "J. Waldschmidt"
                    },
                    {
                        "name": "B. Aparicio"
                    },
                    {
                        "name": "A. I. Aranburu"
                    },
                    {
                        "name": "L. Arazi"
                    },
                    {
                        "name": "I. J. Arnquist"
                    },
                    {
                        "name": "F. Auria-Luna"
                    },
                    {
                        "name": "S. Ayet"
                    },
                    {
                        "name": "C. D. R. Azevedo"
                    },
                    {
                        "name": "F. Ballester"
                    },
                    {
                        "name": "M. del Barrio-Torregrosa"
                    },
                    {
                        "name": "A. Bayo"
                    },
                    {
                        "name": "J. M. Benlloch-Rodríguez"
                    },
                    {
                        "name": "F. I. G. M. Borges"
                    },
                    {
                        "name": "A. Brodolin"
                    },
                    {
                        "name": "S. Cárcel"
                    },
                    {
                        "name": "A. Castillo"
                    },
                    {
                        "name": "L. Cid"
                    },
                    {
                        "name": "C. A. N. Conde"
                    },
                    {
                        "name": "T. Contreras"
                    },
                    {
                        "name": "F. P. Cossío"
                    },
                    {
                        "name": "R. Coupe"
                    },
                    {
                        "name": "E. Dey"
                    },
                    {
                        "name": "G. Díaz"
                    },
                    {
                        "name": "C. Echevarria"
                    },
                    {
                        "name": "M. Elorza"
                    },
                    {
                        "name": "J. Escada"
                    },
                    {
                        "name": "R. Esteve"
                    },
                    {
                        "name": "R. Felkai"
                    },
                    {
                        "name": "L. M. P. Fernandes"
                    },
                    {
                        "name": "P. Ferrario"
                    },
                    {
                        "name": "A. L. Ferreira"
                    },
                    {
                        "name": "F. W. Foss"
                    },
                    {
                        "name": "Z. Freixa"
                    },
                    {
                        "name": "J. García-Barrena"
                    },
                    {
                        "name": "J. J. Gómez-Cadenas"
                    },
                    {
                        "name": "J. W. R. Grocott"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "J. Hauptman"
                    },
                    {
                        "name": "C. A. O. Henriques"
                    },
                    {
                        "name": "J. A. Hernando Morata"
                    },
                    {
                        "name": "P. Herrero-Gómez"
                    },
                    {
                        "name": "V. Herrero"
                    },
                    {
                        "name": "C. Hervés Carrete"
                    },
                    {
                        "name": "Y. Ifergan"
                    },
                    {
                        "name": "F. Kellerer"
                    },
                    {
                        "name": "L. Larizgoitia"
                    },
                    {
                        "name": "A. Larumbe"
                    },
                    {
                        "name": "P. Lebrun"
                    },
                    {
                        "name": "F. Lopez"
                    },
                    {
                        "name": "N. López-March"
                    },
                    {
                        "name": "R. Madigan"
                    },
                    {
                        "name": "R. D. P. Mano"
                    },
                    {
                        "name": "A. P. Marques"
                    },
                    {
                        "name": "J. Martín-Albo"
                    },
                    {
                        "name": "G. Martínez-Lema"
                    },
                    {
                        "name": "M. Martínez-Vara"
                    },
                    {
                        "name": "R. L. Miller"
                    },
                    {
                        "name": "J. Molina-Canteras"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "C. M. B. Monteiro"
                    },
                    {
                        "name": "F. J. Mora"
                    },
                    {
                        "name": "P. Novella"
                    },
                    {
                        "name": "A. Nuñez"
                    },
                    {
                        "name": "E. Oblak"
                    },
                    {
                        "name": "J. Palacio"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "A. Para"
                    },
                    {
                        "name": "A. Pazos"
                    },
                    {
                        "name": "J. Pelegrin"
                    },
                    {
                        "name": "M. Pérez Maneiro"
                    },
                    {
                        "name": "M. Querol"
                    },
                    {
                        "name": "J. Renner"
                    },
                    {
                        "name": "I. Rivilla"
                    },
                    {
                        "name": "C. Rogero"
                    },
                    {
                        "name": "B. Romeo"
                    },
                    {
                        "name": "C. Romo-Luque"
                    },
                    {
                        "name": "V. San Nacienciano"
                    },
                    {
                        "name": "F. P. Santos"
                    },
                    {
                        "name": "J. M. F. dos Santos"
                    },
                    {
                        "name": "M. Seemann"
                    },
                    {
                        "name": "I. Shomroni"
                    },
                    {
                        "name": "P. A. O. C. Silva"
                    },
                    {
                        "name": "A. Simón"
                    },
                    {
                        "name": "S. R. Soleti"
                    },
                    {
                        "name": "M. Sorel"
                    },
                    {
                        "name": "J. Soto-Oton"
                    },
                    {
                        "name": "J. M. R. Teixeira"
                    },
                    {
                        "name": "S. Teruel-Pardo"
                    },
                    {
                        "name": "J. F. Toledo"
                    },
                    {
                        "name": "C. Tonnelé"
                    },
                    {
                        "name": "S. Torelli"
                    },
                    {
                        "name": "J. Torrent"
                    },
                    {
                        "name": "A. Trettin"
                    },
                    {
                        "name": "A. Usón"
                    },
                    {
                        "name": "P. R. G. Valle"
                    },
                    {
                        "name": "J. F. C. A. Veloso"
                    },
                    {
                        "name": "J. Waiton"
                    },
                    {
                        "name": "A. Yubero-Navarro"
                    }
                ],
                "author_detail": {
                    "name": "A. Yubero-Navarro"
                },
                "author": "A. Yubero-Navarro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01002v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01002v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16278v1",
                "updated": "2025-09-18T17:38:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    38,
                    48,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:38:48Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    38,
                    48,
                    3,
                    261,
                    0
                ],
                "title": "Language Modeling with Learned Meta-Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Modeling with Learned Meta-Tokens"
                },
                "summary": "While modern Transformer-based language models (LMs) have achieved major\nsuccess in multi-task generalization, they often struggle to capture long-range\ndependencies within their context window. This work introduces a novel approach\nusing meta-tokens, special tokens injected during pre-training, along with a\ndedicated meta-attention mechanism to guide LMs to use these tokens. We\npre-train a language model with a modified GPT-2 architecture equipped with\nmeta-attention in addition to causal multi-head attention, and study the impact\nof these tokens on a suite of synthetic tasks. We find that data-efficient\nlanguage model pre-training on fewer than 100B tokens utilizing meta-tokens and\nour meta-attention mechanism achieves strong performance on these tasks after\nfine-tuning. We suggest that these gains arise due to the meta-tokens\nsharpening the positional encoding. This enables them to operate as trainable,\ncontent-based landmarks, implicitly compressing preceding context and \"caching\"\nit in the meta-token. At inference-time, the meta-token points to relevant\ncontext, facilitating length generalization up to 2$\\times$ its context window,\neven after extension with YaRN. We provide further evidence of these behaviors\nby visualizing model internals to study the residual stream, and assessing the\ncompression quality by information-theoretic analysis on the rate-distortion\ntradeoff. Our findings suggest that pre-training LMs with meta-tokens offers a\nsimple, data-efficient method to enhance long-context language modeling\nperformance, while introducing new insights into the nature of their behavior\ntowards length generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While modern Transformer-based language models (LMs) have achieved major\nsuccess in multi-task generalization, they often struggle to capture long-range\ndependencies within their context window. This work introduces a novel approach\nusing meta-tokens, special tokens injected during pre-training, along with a\ndedicated meta-attention mechanism to guide LMs to use these tokens. We\npre-train a language model with a modified GPT-2 architecture equipped with\nmeta-attention in addition to causal multi-head attention, and study the impact\nof these tokens on a suite of synthetic tasks. We find that data-efficient\nlanguage model pre-training on fewer than 100B tokens utilizing meta-tokens and\nour meta-attention mechanism achieves strong performance on these tasks after\nfine-tuning. We suggest that these gains arise due to the meta-tokens\nsharpening the positional encoding. This enables them to operate as trainable,\ncontent-based landmarks, implicitly compressing preceding context and \"caching\"\nit in the meta-token. At inference-time, the meta-token points to relevant\ncontext, facilitating length generalization up to 2$\\times$ its context window,\neven after extension with YaRN. We provide further evidence of these behaviors\nby visualizing model internals to study the residual stream, and assessing the\ncompression quality by information-theoretic analysis on the rate-distortion\ntradeoff. Our findings suggest that pre-training LMs with meta-tokens offers a\nsimple, data-efficient method to enhance long-context language modeling\nperformance, while introducing new insights into the nature of their behavior\ntowards length generalization."
                },
                "authors": [
                    {
                        "name": "Alok N. Shah"
                    },
                    {
                        "name": "Khush Gupta"
                    },
                    {
                        "name": "Keshav Ramji"
                    },
                    {
                        "name": "Pratik Chaudhari"
                    }
                ],
                "author_detail": {
                    "name": "Pratik Chaudhari"
                },
                "author": "Pratik Chaudhari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15038v1",
                "updated": "2025-09-18T15:04:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    4,
                    6,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:04:06Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    4,
                    6,
                    3,
                    261,
                    0
                ],
                "title": "Value-Guided KV Compression for LLMs via Approximated CUR Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-Guided KV Compression for LLMs via Approximated CUR Decomposition"
                },
                "summary": "Key-value (KV) cache compression has emerged as a critical technique for\nreducing the memory and latency overhead of autoregressive language models\nduring inference. Prior approaches predominantly rely on query-key attention\nscores to rank and evict cached tokens, assuming that attention intensity\ncorrelates with semantic importance. However, this heuristic overlooks the\ncontribution of value vectors, which directly influence the attention output.\nIn this paper, we propose CurDKV, a novel, value-centric KV compression method\nthat selects keys and values based on leverage scores computed from CUR matrix\ndecomposition. Our approach approximates the dominant subspace of the attention\noutput $softmax(QK^T)V$, ensuring that the retained tokens best preserve the\nmodel's predictive behavior. Theoretically, we show that attention score\napproximation does not guarantee output preservation, and demonstrate that\nCUR-based selection minimizes end-to-end attention reconstruction loss.\nEmpirically, CurDKV achieves up to 9.6% higher accuracy than state-of-the-art\nmethods like SnapKV and ChunkKV under aggressive compression budgets on LLaMA\nand Mistral, while maintaining compatibility with FlashAttention and Grouped\nQuery Attention. In addition to improved accuracy, CurDKV reduces generation\nlatency by up to 40% at high compression, offering a practical speed-accuracy\ntradeoff.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) cache compression has emerged as a critical technique for\nreducing the memory and latency overhead of autoregressive language models\nduring inference. Prior approaches predominantly rely on query-key attention\nscores to rank and evict cached tokens, assuming that attention intensity\ncorrelates with semantic importance. However, this heuristic overlooks the\ncontribution of value vectors, which directly influence the attention output.\nIn this paper, we propose CurDKV, a novel, value-centric KV compression method\nthat selects keys and values based on leverage scores computed from CUR matrix\ndecomposition. Our approach approximates the dominant subspace of the attention\noutput $softmax(QK^T)V$, ensuring that the retained tokens best preserve the\nmodel's predictive behavior. Theoretically, we show that attention score\napproximation does not guarantee output preservation, and demonstrate that\nCUR-based selection minimizes end-to-end attention reconstruction loss.\nEmpirically, CurDKV achieves up to 9.6% higher accuracy than state-of-the-art\nmethods like SnapKV and ChunkKV under aggressive compression budgets on LLaMA\nand Mistral, while maintaining compatibility with FlashAttention and Grouped\nQuery Attention. In addition to improved accuracy, CurDKV reduces generation\nlatency by up to 40% at high compression, offering a practical speed-accuracy\ntradeoff."
                },
                "authors": [
                    {
                        "name": "Ayan Sengupta"
                    },
                    {
                        "name": "Siddhant Chaudhary"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15024v1",
                "updated": "2025-09-18T14:51:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    51,
                    13,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T14:51:13Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    51,
                    13,
                    3,
                    261,
                    0
                ],
                "title": "Attention Beyond Neighborhoods: Reviving Transformer for Graph\n  Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Beyond Neighborhoods: Reviving Transformer for Graph\n  Clustering"
                },
                "summary": "Attention mechanisms have become a cornerstone in modern neural networks,\ndriving breakthroughs across diverse domains. However, their application to\ngraph structured data, where capturing topological connections is essential,\nremains underexplored and underperforming compared to Graph Neural Networks\n(GNNs), particularly in the graph clustering task. GNN tends to overemphasize\nneighborhood aggregation, leading to a homogenization of node representations.\nConversely, Transformer tends to over globalize, highlighting distant nodes at\nthe expense of meaningful local patterns. This dichotomy raises a key question:\nIs attention inherently redundant for unsupervised graph learning? To address\nthis, we conduct a comprehensive empirical analysis, uncovering the\ncomplementary weaknesses of GNN and Transformer in graph clustering. Motivated\nby these insights, we propose the Attentive Graph Clustering Network (AGCN) a\nnovel architecture that reinterprets the notion that graph is attention. AGCN\ndirectly embeds the attention mechanism into the graph structure, enabling\neffective global information extraction while maintaining sensitivity to local\ntopological cues. Our framework incorporates theoretical analysis to contrast\nAGCN behavior with GNN and Transformer and introduces two innovations: (1) a KV\ncache mechanism to improve computational efficiency, and (2) a pairwise margin\ncontrastive loss to boost the discriminative capacity of the attention space.\nExtensive experimental results demonstrate that AGCN outperforms\nstate-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms have become a cornerstone in modern neural networks,\ndriving breakthroughs across diverse domains. However, their application to\ngraph structured data, where capturing topological connections is essential,\nremains underexplored and underperforming compared to Graph Neural Networks\n(GNNs), particularly in the graph clustering task. GNN tends to overemphasize\nneighborhood aggregation, leading to a homogenization of node representations.\nConversely, Transformer tends to over globalize, highlighting distant nodes at\nthe expense of meaningful local patterns. This dichotomy raises a key question:\nIs attention inherently redundant for unsupervised graph learning? To address\nthis, we conduct a comprehensive empirical analysis, uncovering the\ncomplementary weaknesses of GNN and Transformer in graph clustering. Motivated\nby these insights, we propose the Attentive Graph Clustering Network (AGCN) a\nnovel architecture that reinterprets the notion that graph is attention. AGCN\ndirectly embeds the attention mechanism into the graph structure, enabling\neffective global information extraction while maintaining sensitivity to local\ntopological cues. Our framework incorporates theoretical analysis to contrast\nAGCN behavior with GNN and Transformer and introduces two innovations: (1) a KV\ncache mechanism to improve computational efficiency, and (2) a pairwise margin\ncontrastive loss to boost the discriminative capacity of the attention space.\nExtensive experimental results demonstrate that AGCN outperforms\nstate-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Xuanting Xie"
                    },
                    {
                        "name": "Bingheng Li"
                    },
                    {
                        "name": "Erlin Pan"
                    },
                    {
                        "name": "Rui Hou"
                    },
                    {
                        "name": "Wenyu Chen"
                    },
                    {
                        "name": "Zhao Kang"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Kang"
                },
                "author": "Zhao Kang",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13789v2",
                "updated": "2025-09-18T04:57:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    4,
                    57,
                    32,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-17T07:58:36Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    58,
                    36,
                    2,
                    260,
                    0
                ],
                "title": "BWCache: Accelerating Video Diffusion Transformers through Block-Wise\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BWCache: Accelerating Video Diffusion Transformers through Block-Wise\n  Caching"
                },
                "summary": "Recent advancements in Diffusion Transformers (DiTs) have established them as\nthe state-of-the-art method for video generation. However, their inherently\nsequential denoising process results in inevitable latency, limiting real-world\napplicability. Existing acceleration methods either compromise visual quality\ndue to architectural modifications or fail to reuse intermediate features at\nproper granularity. Our analysis reveals that DiT blocks are the primary\ncontributors to inference latency. Across diffusion timesteps, the feature\nvariations of DiT blocks exhibit a U-shaped pattern with high similarity during\nintermediate timesteps, which suggests substantial computational redundancy. In\nthis paper, we propose Block-Wise Caching (BWCache), a training-free method to\naccelerate DiT-based video generation. BWCache dynamically caches and reuses\nfeatures from DiT blocks across diffusion timesteps. Furthermore, we introduce\na similarity indicator that triggers feature reuse only when the differences\nbetween block features at adjacent timesteps fall below a threshold, thereby\nminimizing redundant computations while maintaining visual fidelity. Extensive\nexperiments on several video diffusion models demonstrate that BWCache achieves\nup to 2.24$\\times$ speedup with comparable visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Diffusion Transformers (DiTs) have established them as\nthe state-of-the-art method for video generation. However, their inherently\nsequential denoising process results in inevitable latency, limiting real-world\napplicability. Existing acceleration methods either compromise visual quality\ndue to architectural modifications or fail to reuse intermediate features at\nproper granularity. Our analysis reveals that DiT blocks are the primary\ncontributors to inference latency. Across diffusion timesteps, the feature\nvariations of DiT blocks exhibit a U-shaped pattern with high similarity during\nintermediate timesteps, which suggests substantial computational redundancy. In\nthis paper, we propose Block-Wise Caching (BWCache), a training-free method to\naccelerate DiT-based video generation. BWCache dynamically caches and reuses\nfeatures from DiT blocks across diffusion timesteps. Furthermore, we introduce\na similarity indicator that triggers feature reuse only when the differences\nbetween block features at adjacent timesteps fall below a threshold, thereby\nminimizing redundant computations while maintaining visual fidelity. Extensive\nexperiments on several video diffusion models demonstrate that BWCache achieves\nup to 2.24$\\times$ speedup with comparable visual quality."
                },
                "authors": [
                    {
                        "name": "Hanshuai Cui"
                    },
                    {
                        "name": "Zhiqing Tang"
                    },
                    {
                        "name": "Zhifei Xu"
                    },
                    {
                        "name": "Zhi Yao"
                    },
                    {
                        "name": "Wenyi Zeng"
                    },
                    {
                        "name": "Weijia Jia"
                    }
                ],
                "author_detail": {
                    "name": "Weijia Jia"
                },
                "author": "Weijia Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14403v1",
                "updated": "2025-09-17T20:08:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    20,
                    8,
                    53,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T20:08:53Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    20,
                    8,
                    53,
                    2,
                    260,
                    0
                ],
                "title": "Kilovolt-Class $β-Ga_2O_3$ Field-Plated Schottky Barrier Diodes with\n  MOCVD-Grown Intentionally $10^{15}$ $cm^{-3}$ Doped Drift Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kilovolt-Class $β-Ga_2O_3$ Field-Plated Schottky Barrier Diodes with\n  MOCVD-Grown Intentionally $10^{15}$ $cm^{-3}$ Doped Drift Layers"
                },
                "summary": "We report on the growth optimization of intentionally low-doped ($10^{15}$\n$cm^{-3}$) high-quality $\\beta-Ga_2O_3$ drift layers up to 10 $\\mu m$ thick via\nMOCVD and the fabrication of kilovolt-class field plated Schottky barrier\ndiodes on these thick drift layers. Homoepitaxial growth was performed on (010)\n$10^{15}$ $cm^{-3}$ substrates using TMGa as the Ga precursor. Growth\nparameters were systematically optimized to determine the best conditions for\nhigh quality thick growths with the given reactor geometry. Chamber pressure\nwas found to improve the growth rate, mobility, and roughness of the samples.\nGrowth rates of up to 7.2 $\\mu m$/hr., thicknesses of up to 10 $\\mu m$, Hall\nmobilities of up to 176 $cm^2$/Vs, RMS roughness down to 5.45 nm, UID\nconcentrations as low as $2 \\times$ $10^{15}$ $cm^{-3}$, and controllable\nintentional doping down to $3 \\times$ $10^{15}$ $cm^{-3}$ were achieved. Field\nplated Schottky barrier diodes (FP-SBDs) were fabricated on a $6.5 \\times$\n$10^{15}$ $cm^{-3}$ intentionally doped 10 $\\mu m$ thick film to determine the\nelectrical performance of the MOCVD-grown material. The FP-SBD was found to\nhave current density $>$100 A/$cm^2$ at 3 V forward bias with a specific\ndifferential on resistance ($R_{on,sp}$) of 16.22 m$\\Omega$.$cm^2$ and a turn\non voltage of 1 V. The diodes were found to have high quality anode\nmetal/semiconductor interfaces with an ideality factor of 1.04, close to unity.\nDiodes had a maximum breakdown voltage of 1.50 kV, leading to a punch-through\nmaximum field of 2.04 MV/cm under the anode metal, which is a state-of-the-art\nresult for SBDs on MOCVD-grown (010) drift layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on the growth optimization of intentionally low-doped ($10^{15}$\n$cm^{-3}$) high-quality $\\beta-Ga_2O_3$ drift layers up to 10 $\\mu m$ thick via\nMOCVD and the fabrication of kilovolt-class field plated Schottky barrier\ndiodes on these thick drift layers. Homoepitaxial growth was performed on (010)\n$10^{15}$ $cm^{-3}$ substrates using TMGa as the Ga precursor. Growth\nparameters were systematically optimized to determine the best conditions for\nhigh quality thick growths with the given reactor geometry. Chamber pressure\nwas found to improve the growth rate, mobility, and roughness of the samples.\nGrowth rates of up to 7.2 $\\mu m$/hr., thicknesses of up to 10 $\\mu m$, Hall\nmobilities of up to 176 $cm^2$/Vs, RMS roughness down to 5.45 nm, UID\nconcentrations as low as $2 \\times$ $10^{15}$ $cm^{-3}$, and controllable\nintentional doping down to $3 \\times$ $10^{15}$ $cm^{-3}$ were achieved. Field\nplated Schottky barrier diodes (FP-SBDs) were fabricated on a $6.5 \\times$\n$10^{15}$ $cm^{-3}$ intentionally doped 10 $\\mu m$ thick film to determine the\nelectrical performance of the MOCVD-grown material. The FP-SBD was found to\nhave current density $>$100 A/$cm^2$ at 3 V forward bias with a specific\ndifferential on resistance ($R_{on,sp}$) of 16.22 m$\\Omega$.$cm^2$ and a turn\non voltage of 1 V. The diodes were found to have high quality anode\nmetal/semiconductor interfaces with an ideality factor of 1.04, close to unity.\nDiodes had a maximum breakdown voltage of 1.50 kV, leading to a punch-through\nmaximum field of 2.04 MV/cm under the anode metal, which is a state-of-the-art\nresult for SBDs on MOCVD-grown (010) drift layers."
                },
                "authors": [
                    {
                        "name": "Carl Peterson"
                    },
                    {
                        "name": "Chinmoy Nath Saha"
                    },
                    {
                        "name": "Rachel Kahler"
                    },
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Akhila Mattapalli"
                    },
                    {
                        "name": "Saurav Roy"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14347v1",
                "updated": "2025-09-17T18:26:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    18,
                    26,
                    29,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T18:26:29Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    18,
                    26,
                    29,
                    2,
                    260,
                    0
                ],
                "title": "On the Illusion of Success: An Empirical Study of Build Reruns and\n  Silent Failures in Industrial CI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Illusion of Success: An Empirical Study of Build Reruns and\n  Silent Failures in Industrial CI"
                },
                "summary": "Reliability of build outcomes is a cornerstone of effective Continuous\nIntegration (CI). Yet in practice, developers often struggle with\nnon-deterministic issues in the code or CI infrastructure, which undermine\ntrust in build results. When faced with such unexpected outcomes, developers\noften repeatedly rerun jobs hoping for true success, but this practice is known\nto increase CI costs and reduce productivity. While recent studies have focused\non intermittent job failures, no prior work has investigated silent failures,\nwhere build jobs are marked as successful but fail to complete all or part of\ntheir tasks. Such silent failures often go unnoticed, creating an illusion of\nsuccess with detrimental consequences such as bugs escaping into production.\nThis paper presents the first empirical study of silent failures through the\npractice of rerunning successful jobs. An analysis of 142,387 jobs across 81\nindustrial projects shows that 11% of successful jobs are rerun, with 35% of\nthese reruns occurring after more than 24 hours. Using mixed-effects models on\n32 independent variables (AUC of 85%), we identified key factors associated\nwith reruns of successful jobs, notably testing and static analysis tasks,\nscripting languages like Shell, and developers prior rerun tendencies. A\nfurther analysis of 92 public issues revealed 11 categories of silent failures\naligning with these factors, the most frequent being artifact operation errors,\ncaching errors, and ignored exit codes. Overall, our findings provide valuable\ninsights into the circumstances and causes of silent failures to raise\nawareness among teams, and present solutions to improve CI reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliability of build outcomes is a cornerstone of effective Continuous\nIntegration (CI). Yet in practice, developers often struggle with\nnon-deterministic issues in the code or CI infrastructure, which undermine\ntrust in build results. When faced with such unexpected outcomes, developers\noften repeatedly rerun jobs hoping for true success, but this practice is known\nto increase CI costs and reduce productivity. While recent studies have focused\non intermittent job failures, no prior work has investigated silent failures,\nwhere build jobs are marked as successful but fail to complete all or part of\ntheir tasks. Such silent failures often go unnoticed, creating an illusion of\nsuccess with detrimental consequences such as bugs escaping into production.\nThis paper presents the first empirical study of silent failures through the\npractice of rerunning successful jobs. An analysis of 142,387 jobs across 81\nindustrial projects shows that 11% of successful jobs are rerun, with 35% of\nthese reruns occurring after more than 24 hours. Using mixed-effects models on\n32 independent variables (AUC of 85%), we identified key factors associated\nwith reruns of successful jobs, notably testing and static analysis tasks,\nscripting languages like Shell, and developers prior rerun tendencies. A\nfurther analysis of 92 public issues revealed 11 categories of silent failures\naligning with these factors, the most frequent being artifact operation errors,\ncaching errors, and ignored exit codes. Overall, our findings provide valuable\ninsights into the circumstances and causes of silent failures to raise\nawareness among teams, and present solutions to improve CI reliability."
                },
                "authors": [
                    {
                        "name": "Henri Aïdasso"
                    },
                    {
                        "name": "Francis Bordeleau"
                    },
                    {
                        "name": "Ali Tizghadam"
                    }
                ],
                "author_detail": {
                    "name": "Ali Tizghadam"
                },
                "author": "Ali Tizghadam",
                "arxiv_comment": "17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14093v1",
                "updated": "2025-09-17T15:33:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    33,
                    44,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T15:33:44Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    33,
                    44,
                    2,
                    260,
                    0
                ],
                "title": "Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A\n  Self-Optimizing Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A\n  Self-Optimizing Framework"
                },
                "summary": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by\nprompting intermediate steps, improving accuracy and robustness in arithmetic,\nlogic, and commonsense tasks. However, this benefit comes with high\ncomputational costs: longer outputs increase latency, memory usage, and\nKV-cache demands. These issues are especially critical in software engineering\ntasks where concise and deterministic outputs are required. To investigate\nthese trade-offs, we conduct an empirical study based on code generation\nbenchmarks. The results reveal that longer CoT does not always help. Excessive\nreasoning often causes truncation, accuracy drops, and latency up to five times\nhigher, with failed outputs consistently longer than successful ones. These\nfindings challenge the assumption that longer reasoning is inherently better\nand highlight the need for adaptive CoT control. Motivated by this, we propose\nSEER (Self-Enhancing Efficient Reasoning), an adaptive framework that\ncompresses CoT while preserving accuracy. SEER combines Best-of-N sampling with\ntask-aware adaptive filtering, dynamically adjusting thresholds based on\npre-inference outputs to reduce verbosity and computational overhead. We then\nevaluate SEER on three software engineering tasks and one math task. On\naverage, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,\nand eliminates most infinite loops. These results demonstrate SEER as a\npractical method to make CoT-enhanced LLMs more efficient and robust, even\nunder resource constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by\nprompting intermediate steps, improving accuracy and robustness in arithmetic,\nlogic, and commonsense tasks. However, this benefit comes with high\ncomputational costs: longer outputs increase latency, memory usage, and\nKV-cache demands. These issues are especially critical in software engineering\ntasks where concise and deterministic outputs are required. To investigate\nthese trade-offs, we conduct an empirical study based on code generation\nbenchmarks. The results reveal that longer CoT does not always help. Excessive\nreasoning often causes truncation, accuracy drops, and latency up to five times\nhigher, with failed outputs consistently longer than successful ones. These\nfindings challenge the assumption that longer reasoning is inherently better\nand highlight the need for adaptive CoT control. Motivated by this, we propose\nSEER (Self-Enhancing Efficient Reasoning), an adaptive framework that\ncompresses CoT while preserving accuracy. SEER combines Best-of-N sampling with\ntask-aware adaptive filtering, dynamically adjusting thresholds based on\npre-inference outputs to reduce verbosity and computational overhead. We then\nevaluate SEER on three software engineering tasks and one math task. On\naverage, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,\nand eliminates most infinite loops. These results demonstrate SEER as a\npractical method to make CoT-enhanced LLMs more efficient and robust, even\nunder resource constraints."
                },
                "authors": [
                    {
                        "name": "Kerui Huang"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Tongtong Xu"
                    },
                    {
                        "name": "Lingfeng Bao"
                    },
                    {
                        "name": "Xin Xia"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xia"
                },
                "author": "Xin Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14041v1",
                "updated": "2025-09-17T14:42:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    42,
                    38,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T14:42:38Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    42,
                    38,
                    2,
                    260,
                    0
                ],
                "title": "A TRRIP Down Memory Lane: Temperature-Based Re-Reference Interval\n  Prediction For Instruction Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A TRRIP Down Memory Lane: Temperature-Based Re-Reference Interval\n  Prediction For Instruction Caching"
                },
                "summary": "Modern mobile CPU software pose challenges for conventional instruction cache\nreplacement policies due to their complex runtime behavior causing high reuse\ndistance between executions of the same instruction. Mobile code commonly\nsuffers from large amounts of stalls in the CPU frontend and thus starvation of\nthe rest of the CPU resources. Complexity of these applications and their code\nfootprint are projected to grow at a rate faster than available on-chip memory\ndue to power and area constraints, making conventional hardware-centric methods\nfor managing instruction caches to be inadequate. We present a novel\nsoftware-hardware co-design approach called TRRIP (Temperature-based\nRe-Reference Interval Prediction) that enables the compiler to analyze,\nclassify, and transform code based on \"temperature\" (hot/cold), and to provide\nthe hardware with a summary of code temperature information through a\nwell-defined OS interface based on using code page attributes. TRRIP's\nlightweight hardware extension employs code temperature attributes to optimize\nthe instruction cache replacement policy resulting in the eviction rate\nreduction of hot code. TRRIP is designed to be practical and adoptable in real\nmobile systems that have strict feature requirements on both the software and\nhardware components. TRRIP can reduce the L2 MPKI for instructions by 26.5%\nresulting in geomean speedup of 3.9%, on top of RRIP cache replacement running\nmobile code already optimized using PGO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern mobile CPU software pose challenges for conventional instruction cache\nreplacement policies due to their complex runtime behavior causing high reuse\ndistance between executions of the same instruction. Mobile code commonly\nsuffers from large amounts of stalls in the CPU frontend and thus starvation of\nthe rest of the CPU resources. Complexity of these applications and their code\nfootprint are projected to grow at a rate faster than available on-chip memory\ndue to power and area constraints, making conventional hardware-centric methods\nfor managing instruction caches to be inadequate. We present a novel\nsoftware-hardware co-design approach called TRRIP (Temperature-based\nRe-Reference Interval Prediction) that enables the compiler to analyze,\nclassify, and transform code based on \"temperature\" (hot/cold), and to provide\nthe hardware with a summary of code temperature information through a\nwell-defined OS interface based on using code page attributes. TRRIP's\nlightweight hardware extension employs code temperature attributes to optimize\nthe instruction cache replacement policy resulting in the eviction rate\nreduction of hot code. TRRIP is designed to be practical and adoptable in real\nmobile systems that have strict feature requirements on both the software and\nhardware components. TRRIP can reduce the L2 MPKI for instructions by 26.5%\nresulting in geomean speedup of 3.9%, on top of RRIP cache replacement running\nmobile code already optimized using PGO."
                },
                "authors": [
                    {
                        "name": "Henry Kao"
                    },
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Prabhdeep Singh Soni"
                    },
                    {
                        "name": "Ali Sedaghati"
                    },
                    {
                        "name": "Fang Su"
                    },
                    {
                        "name": "Bryan Chan"
                    },
                    {
                        "name": "Maziar Goudarzi"
                    },
                    {
                        "name": "Reza Azimi"
                    }
                ],
                "author_detail": {
                    "name": "Reza Azimi"
                },
                "author": "Reza Azimi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18172v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18172v1",
                "updated": "2025-09-17T13:51:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    51,
                    27,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T13:51:27Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    51,
                    27,
                    2,
                    260,
                    0
                ],
                "title": "SBVR: Summation of BitVector Representation for Efficient LLM\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SBVR: Summation of BitVector Representation for Efficient LLM\n  Quantization"
                },
                "summary": "With the advent of large language models (LLMs), numerous Post-Training\nQuantization (PTQ) strategies have been proposed to alleviate deployment\nbarriers created by their enormous parameter counts. Quantization achieves\ncompression by limiting the number of representable points in the data.\nTherefore, the key to achieving efficient quantization is selecting the optimal\ncombination of representation points, or codes, for the given data. Existing\nPTQ solutions adopt two major approaches to this problem: Round-To-Nearest\n(RTN)-based methods and codebook-based methods. RTN-based methods map LLM\nweights onto uniformly distributed integer grids, failing to account for the\nGaussian-like weight distribution of LLM weights. Codebook-based methods\nmitigate this issue by constructing distribution-aware codebooks; however, they\nsuffer from random and strided memory access patterns, resulting in degraded\ninference speed that is exacerbated by the limited size of GPU L1 cache. To\novercome these limitations, we propose a novel LLM quantization method, SBVR\n(Summation of BitVector Representation), that enables Gaussian-like code\nrepresentation in a hardware-friendly manner for fast inference. SBVR maps\nweight values to non-uniform representation points whose distribution follows\nthe actual distribution of LLM weights, enabling more accurate compression.\nAdditionally, we design a custom CUDA kernel that allows matrix-vector\nmultiplication directly in the SBVR format without decompression, thereby\nenabling high-performance execution of SBVR-compressed models. Our evaluations\nof SBVR on various models demonstrate state-of-the-art perplexity and accuracy\nbenchmark performance while delivering a 2.21x- 3.04x end-to-end\ntoken-generation speedup over naive FP16 models in the 4-bit quantization\nregime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of large language models (LLMs), numerous Post-Training\nQuantization (PTQ) strategies have been proposed to alleviate deployment\nbarriers created by their enormous parameter counts. Quantization achieves\ncompression by limiting the number of representable points in the data.\nTherefore, the key to achieving efficient quantization is selecting the optimal\ncombination of representation points, or codes, for the given data. Existing\nPTQ solutions adopt two major approaches to this problem: Round-To-Nearest\n(RTN)-based methods and codebook-based methods. RTN-based methods map LLM\nweights onto uniformly distributed integer grids, failing to account for the\nGaussian-like weight distribution of LLM weights. Codebook-based methods\nmitigate this issue by constructing distribution-aware codebooks; however, they\nsuffer from random and strided memory access patterns, resulting in degraded\ninference speed that is exacerbated by the limited size of GPU L1 cache. To\novercome these limitations, we propose a novel LLM quantization method, SBVR\n(Summation of BitVector Representation), that enables Gaussian-like code\nrepresentation in a hardware-friendly manner for fast inference. SBVR maps\nweight values to non-uniform representation points whose distribution follows\nthe actual distribution of LLM weights, enabling more accurate compression.\nAdditionally, we design a custom CUDA kernel that allows matrix-vector\nmultiplication directly in the SBVR format without decompression, thereby\nenabling high-performance execution of SBVR-compressed models. Our evaluations\nof SBVR on various models demonstrate state-of-the-art perplexity and accuracy\nbenchmark performance while delivering a 2.21x- 3.04x end-to-end\ntoken-generation speedup over naive FP16 models in the 4-bit quantization\nregime."
                },
                "authors": [
                    {
                        "name": "Wonjun Bang"
                    },
                    {
                        "name": "Jongseok Park"
                    },
                    {
                        "name": "Hongseung Yu"
                    },
                    {
                        "name": "Kyungmin Bin"
                    },
                    {
                        "name": "Kyunghan Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyunghan Lee"
                },
                "author": "Kyunghan Lee",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18172v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13848v1",
                "updated": "2025-09-17T09:24:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    24,
                    40,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T09:24:40Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    24,
                    40,
                    2,
                    260,
                    0
                ],
                "title": "SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation"
                },
                "summary": "Feature caching has recently emerged as a promising method for diffusion\nmodel acceleration. It effectively alleviates the inefficiency problem caused\nby high computational requirements by caching similar features in the inference\nprocess of the diffusion model. In this paper, we analyze existing feature\ncaching methods from the perspective of information utilization, and point out\nthat relying solely on historical information will lead to constrained accuracy\nand speed performance. And we propose a novel paradigm that introduces future\ninformation via self-speculation based on the information similarity at the\nsame time step across different iteration times. Based on this paradigm, we\npresent \\textit{SpecDiff}, a training-free multi-level feature caching strategy\nincluding a cached feature selection algorithm and a multi-level feature\nclassification algorithm. (1) Feature selection algorithm based on\nself-speculative information. \\textit{SpecDiff} determines a dynamic importance\nscore for each token based on self-speculative information and historical\ninformation, and performs cached feature selection through the importance\nscore. (2) Multi-level feature classification algorithm based on feature\nimportance scores. \\textit{SpecDiff} classifies tokens by leveraging the\ndifferences in feature importance scores and introduces a multi-level feature\ncalculation strategy. Extensive experiments show that \\textit{SpecDiff}\nachieves average 2.80 \\times, 2.74 \\times , and 3.17\\times speedup with\nnegligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow\non NVIDIA A800-80GB GPU. By merging speculative and historical information,\n\\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing\nthe Pareto frontier of speedup and accuracy in the efficient diffusion model\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature caching has recently emerged as a promising method for diffusion\nmodel acceleration. It effectively alleviates the inefficiency problem caused\nby high computational requirements by caching similar features in the inference\nprocess of the diffusion model. In this paper, we analyze existing feature\ncaching methods from the perspective of information utilization, and point out\nthat relying solely on historical information will lead to constrained accuracy\nand speed performance. And we propose a novel paradigm that introduces future\ninformation via self-speculation based on the information similarity at the\nsame time step across different iteration times. Based on this paradigm, we\npresent \\textit{SpecDiff}, a training-free multi-level feature caching strategy\nincluding a cached feature selection algorithm and a multi-level feature\nclassification algorithm. (1) Feature selection algorithm based on\nself-speculative information. \\textit{SpecDiff} determines a dynamic importance\nscore for each token based on self-speculative information and historical\ninformation, and performs cached feature selection through the importance\nscore. (2) Multi-level feature classification algorithm based on feature\nimportance scores. \\textit{SpecDiff} classifies tokens by leveraging the\ndifferences in feature importance scores and introduces a multi-level feature\ncalculation strategy. Extensive experiments show that \\textit{SpecDiff}\nachieves average 2.80 \\times, 2.74 \\times , and 3.17\\times speedup with\nnegligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow\non NVIDIA A800-80GB GPU. By merging speculative and historical information,\n\\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing\nthe Pareto frontier of speedup and accuracy in the efficient diffusion model\ninference."
                },
                "authors": [
                    {
                        "name": "Jiayi Pan"
                    },
                    {
                        "name": "Jiaming Xu"
                    },
                    {
                        "name": "Yongkang Zhou"
                    },
                    {
                        "name": "Guohao Dai"
                    }
                ],
                "author_detail": {
                    "name": "Guohao Dai"
                },
                "author": "Guohao Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13604v1",
                "updated": "2025-09-17T00:28:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    0,
                    28,
                    49,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T00:28:49Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    0,
                    28,
                    49,
                    2,
                    260,
                    0
                ],
                "title": "A Framework for Multi-source Prefetching Through Adaptive Weight",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Multi-source Prefetching Through Adaptive Weight"
                },
                "summary": "The World Wide Web has come to be a great part of our daily life, yet user\nobserved latency is still a problem that needs a proper means of handling. Even\nthough earlier attempts focused on caching as the chief solution to tackling\nthis issue, its success was extremely limited. Prefetching has come to be the\nprimary technique in supplementing caching towards soothing the latency problem\nassociated with the contemporary Internet. However, existing approaches in\nprefetching are extremely limited in their ability to employ application level\nweb document relationship which is often visible only to the content developer.\nThis is because most approaches are access history based schemes that make\nfuture users' access prediction only based on past user access. Attempts to\nincorporate prefetching schemes that utilize semantic information with those\nthat use users past access history are extremely limited in their\nextensibility. In this work we present a novel framework that enables\nintegration of schemes from both worlds of prefetching without the need for a\nmajor modification to the algorithms. When there is a need/possibility to\ncapture new application level context, a new algorithm could be developed to do\nso and then it can be integrated into the framework. Since each participating\nscheme is merely viewed as an algorithm that produces a list of candidate\nobjects that are likely to be accessed in the near future, the framework can\nentertain any one of the existing prefetching schemes. With its adaptive weight\nmanagement technique the framework adjusts the effect of each algorithm in the\noverall prediction to parallel with its observed performance so far. We have\nfound this formwork to be less aggressive than its contemporary counterparts\nwhich is extremely important for resource constrained mobile devices that have\ncome to be the major means of access by users of the current web.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The World Wide Web has come to be a great part of our daily life, yet user\nobserved latency is still a problem that needs a proper means of handling. Even\nthough earlier attempts focused on caching as the chief solution to tackling\nthis issue, its success was extremely limited. Prefetching has come to be the\nprimary technique in supplementing caching towards soothing the latency problem\nassociated with the contemporary Internet. However, existing approaches in\nprefetching are extremely limited in their ability to employ application level\nweb document relationship which is often visible only to the content developer.\nThis is because most approaches are access history based schemes that make\nfuture users' access prediction only based on past user access. Attempts to\nincorporate prefetching schemes that utilize semantic information with those\nthat use users past access history are extremely limited in their\nextensibility. In this work we present a novel framework that enables\nintegration of schemes from both worlds of prefetching without the need for a\nmajor modification to the algorithms. When there is a need/possibility to\ncapture new application level context, a new algorithm could be developed to do\nso and then it can be integrated into the framework. Since each participating\nscheme is merely viewed as an algorithm that produces a list of candidate\nobjects that are likely to be accessed in the near future, the framework can\nentertain any one of the existing prefetching schemes. With its adaptive weight\nmanagement technique the framework adjusts the effect of each algorithm in the\noverall prediction to parallel with its observed performance so far. We have\nfound this formwork to be less aggressive than its contemporary counterparts\nwhich is extremely important for resource constrained mobile devices that have\ncome to be the major means of access by users of the current web."
                },
                "authors": [
                    {
                        "name": "Yoseph Berhanu Alebachew"
                    },
                    {
                        "name": "Mulugeta Libsie"
                    }
                ],
                "author_detail": {
                    "name": "Mulugeta Libsie"
                },
                "author": "Mulugeta Libsie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21492v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21492v3",
                "updated": "2025-09-16T23:56:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    23,
                    56,
                    55,
                    1,
                    259,
                    0
                ],
                "published": "2025-07-29T04:21:11Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    4,
                    21,
                    11,
                    1,
                    210,
                    0
                ],
                "title": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist"
                },
                "summary": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts."
                },
                "authors": [
                    {
                        "name": "Yicong Luo"
                    },
                    {
                        "name": "Senhe Hao"
                    },
                    {
                        "name": "Brian Wheatman"
                    },
                    {
                        "name": "Prashant Pandey"
                    },
                    {
                        "name": "Helen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Helen Xu"
                },
                "author": "Helen Xu",
                "arxiv_doi": "10.1145/3754598.3754655",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754655",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.21492v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21492v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Original paper was accepted into ICPP 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08256v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08256v2",
                "updated": "2025-09-16T23:15:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    23,
                    15,
                    44,
                    1,
                    259,
                    0
                ],
                "published": "2025-05-28T05:22:44Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    5,
                    22,
                    44,
                    2,
                    148,
                    0
                ],
                "title": "FIER: Fine-Grained and Efficient KV Cache Retrieval for Long-context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FIER: Fine-Grained and Efficient KV Cache Retrieval for Long-context LLM\n  Inference"
                },
                "summary": "The Key-Value (KV) cache reading latency increases significantly with context\nlengths, hindering the efficiency of long-context LLM inference. To address\nthis, previous works propose retaining a small fraction of KV cache based on\ntoken importance. For example, KV eviction uses static heuristics to retain\ntokens, while KV retrieval dynamically selects query-relevant tokens for more\nadaptive cache management. However, we observe that important tokens are often\nsparsely distributed across the long context. This sparsity makes existing\npage-level KV retrieval inaccurate, as each page may include irrelevant tokens\nand miss critical ones. In this work, we propose Fier, a\n\\underline{Fi}ne-Grained and \\underline{E}fficient KV cache\n\\underline{R}etrieval method. Fier uses 1-bit quantized keys to estimate the\nimportance of each token, resulting in efficient and precise retrieval.\nExperiments show that Fier matches full KV performance using only 11\\% of the\ncache budget across various long-context tasks, reducing decoding latency by\n1.2$\\times$ to 1.5$\\times$.Code is available at\nhttps://github.com/SimWangArizona/FIER",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache reading latency increases significantly with context\nlengths, hindering the efficiency of long-context LLM inference. To address\nthis, previous works propose retaining a small fraction of KV cache based on\ntoken importance. For example, KV eviction uses static heuristics to retain\ntokens, while KV retrieval dynamically selects query-relevant tokens for more\nadaptive cache management. However, we observe that important tokens are often\nsparsely distributed across the long context. This sparsity makes existing\npage-level KV retrieval inaccurate, as each page may include irrelevant tokens\nand miss critical ones. In this work, we propose Fier, a\n\\underline{Fi}ne-Grained and \\underline{E}fficient KV cache\n\\underline{R}etrieval method. Fier uses 1-bit quantized keys to estimate the\nimportance of each token, resulting in efficient and precise retrieval.\nExperiments show that Fier matches full KV performance using only 11\\% of the\ncache budget across various long-context tasks, reducing decoding latency by\n1.2$\\times$ to 1.5$\\times$.Code is available at\nhttps://github.com/SimWangArizona/FIER"
                },
                "authors": [
                    {
                        "name": "Dongwei Wang"
                    },
                    {
                        "name": "Zijie Liu"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Yuxin Ren"
                    },
                    {
                        "name": "Jianing Deng"
                    },
                    {
                        "name": "Jingtong Hu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Huanrui Yang"
                    }
                ],
                "author_detail": {
                    "name": "Huanrui Yang"
                },
                "author": "Huanrui Yang",
                "arxiv_comment": "EMNLP2025 Camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08256v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08256v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v3",
                "updated": "2025-09-16T10:33:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    33,
                    29,
                    1,
                    259,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_doi": "10.1145/3744916.3764523",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3744916.3764523",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.08523v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ICSE '26 (The 48th IEEE/ACM International Conference on\n  Software Engineering)",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12900v1",
                "updated": "2025-09-16T09:54:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    54,
                    58,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T09:54:58Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    54,
                    58,
                    1,
                    259,
                    0
                ],
                "title": "Topology and Fragility of European High-Voltage Networks: A\n  Cross-Country Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topology and Fragility of European High-Voltage Networks: A\n  Cross-Country Comparative Analysis"
                },
                "summary": "Reliable electricity supply depends on the seamless operation of high-voltage\ngrid infrastructure spanning both transmission and sub-transmission levels.\nBeneath this apparent uniformity lies a striking structural diversity, which\nleaves a clear imprint on system vulnerability. In this paper, we present\nharmonized topological models of the high-voltage grids of 15 European\ncountries, integrating all elements at voltage levels above 110 kV. Topological\nanalysis of these networks reveals a simple yet robust pattern: node degree\ndistributions consistently follow an exponential decay, but the rate of decay\nvaries significantly across countries. Through a detailed and systematic\nevaluation of network tolerance to node and edge removals, we show that the\ndecay rate delineates the boundary between systems that are more resilient to\nfailures and those that are prone to large-scale disruptions. Furthermore, we\ndemonstrate that this numerical boundary is highly sensitive to which layers of\nthe infrastructure are included in the models. To our knowledge, this study\nprovides the first quantitative cross-country comparison of 15 European\nhigh-voltage networks, linking topological properties with vulnerability\ncharacteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable electricity supply depends on the seamless operation of high-voltage\ngrid infrastructure spanning both transmission and sub-transmission levels.\nBeneath this apparent uniformity lies a striking structural diversity, which\nleaves a clear imprint on system vulnerability. In this paper, we present\nharmonized topological models of the high-voltage grids of 15 European\ncountries, integrating all elements at voltage levels above 110 kV. Topological\nanalysis of these networks reveals a simple yet robust pattern: node degree\ndistributions consistently follow an exponential decay, but the rate of decay\nvaries significantly across countries. Through a detailed and systematic\nevaluation of network tolerance to node and edge removals, we show that the\ndecay rate delineates the boundary between systems that are more resilient to\nfailures and those that are prone to large-scale disruptions. Furthermore, we\ndemonstrate that this numerical boundary is highly sensitive to which layers of\nthe infrastructure are included in the models. To our knowledge, this study\nprovides the first quantitative cross-country comparison of 15 European\nhigh-voltage networks, linking topological properties with vulnerability\ncharacteristics."
                },
                "authors": [
                    {
                        "name": "Bálint Hartmann"
                    },
                    {
                        "name": "Michelle T. Cirunay"
                    }
                ],
                "author_detail": {
                    "name": "Michelle T. Cirunay"
                },
                "author": "Michelle T. Cirunay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12867v1",
                "updated": "2025-09-16T09:22:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    22,
                    21,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T09:22:21Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    22,
                    21,
                    1,
                    259,
                    0
                ],
                "title": "Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use"
                },
                "summary": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding and reasoning, yet they remain limited when tackling\nreal-world tasks that require up-to-date knowledge, precise operations, or\nspecialized tool use. To address this, we propose Tool-R1, a reinforcement\nlearning framework that enables LLMs to perform general, compositional, and\nmulti-step tool use by generating executable Python code. Tool-R1 supports\nintegration of user-defined tools and standard libraries, with variable sharing\nacross steps to construct coherent workflows. An outcome-based reward function,\ncombining LLM-based answer judgment and code execution success, guides policy\noptimization. To improve training efficiency, we maintain a dynamic sample\nqueue to cache and reuse high-quality trajectories, reducing the overhead of\ncostly online sampling. Experiments on the GAIA benchmark show that Tool-R1\nsubstantially improves both accuracy and robustness, achieving about 10\\% gain\nover strong baselines, with larger improvements on complex multi-step tasks.\nThese results highlight the potential of Tool-R1 for enabling reliable and\nefficient tool-augmented reasoning in real-world applications. Our code will be\navailable at https://github.com/YBYBZhang/Tool-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding and reasoning, yet they remain limited when tackling\nreal-world tasks that require up-to-date knowledge, precise operations, or\nspecialized tool use. To address this, we propose Tool-R1, a reinforcement\nlearning framework that enables LLMs to perform general, compositional, and\nmulti-step tool use by generating executable Python code. Tool-R1 supports\nintegration of user-defined tools and standard libraries, with variable sharing\nacross steps to construct coherent workflows. An outcome-based reward function,\ncombining LLM-based answer judgment and code execution success, guides policy\noptimization. To improve training efficiency, we maintain a dynamic sample\nqueue to cache and reuse high-quality trajectories, reducing the overhead of\ncostly online sampling. Experiments on the GAIA benchmark show that Tool-R1\nsubstantially improves both accuracy and robustness, achieving about 10\\% gain\nover strong baselines, with larger improvements on complex multi-step tasks.\nThese results highlight the potential of Tool-R1 for enabling reliable and\nefficient tool-augmented reasoning in real-world applications. Our code will be\navailable at https://github.com/YBYBZhang/Tool-R1."
                },
                "authors": [
                    {
                        "name": "Yabo Zhang"
                    },
                    {
                        "name": "Yihan Zeng"
                    },
                    {
                        "name": "Qingyun Li"
                    },
                    {
                        "name": "Zhen Hu"
                    },
                    {
                        "name": "Kavin Han"
                    },
                    {
                        "name": "Wangmeng Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Wangmeng Zuo"
                },
                "author": "Wangmeng Zuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19341v1",
                "updated": "2025-09-16T09:14:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    14,
                    15,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T09:14:15Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    14,
                    15,
                    1,
                    259,
                    0
                ],
                "title": "Fine-Grained AI Model Caching and Downloading With Coordinated\n  Multipoint Broadcasting in Multi-Cell Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained AI Model Caching and Downloading With Coordinated\n  Multipoint Broadcasting in Multi-Cell Edge Networks"
                },
                "summary": "6G networks are envisioned to support on-demand AI model downloading to\naccommodate diverse inference requirements of end users. By proactively caching\nmodels at edge nodes, users can retrieve the requested models with low latency\nfor on-device AI inference. However, the substantial size of contemporary AI\nmodels poses significant challenges for edge caching under limited storage\ncapacity, as well as for the concurrent delivery of heterogeneous models over\nwireless channels. To address these challenges, we propose a fine-grained AI\nmodel caching and downloading system that exploits parameter reusability,\nstemming from the common practice of fine-tuning task-specific models from a\nshared pre-trained model with frozen parameters. This system selectively caches\nmodel parameter blocks (PBs) at edge nodes, eliminating redundant storage of\nreusable parameters across different cached models. Additionally, it\nincorporates coordinated multipoint (CoMP) broadcasting to simultaneously\ndeliver reusable PBs to multiple users, thereby enhancing downlink spectrum\nutilization. Under this arrangement, we formulate a model downloading delay\nminimization problem to jointly optimize PB caching, migration (among edge\nnodes), and broadcasting beamforming. To tackle this intractable problem, we\ndevelop a distributed multi-agent learning framework that enables edge nodes to\nexplicitly learn mutual influence among their actions, thereby facilitating\ncooperation. Furthermore, a data augmentation approach is proposed to\nadaptively generate synthetic training samples through a predictive model,\nboosting sample efficiency and accelerating policy learning. Both theoretical\nanalysis and simulation experiments validate the superior convergence\nperformance of the proposed learning framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G networks are envisioned to support on-demand AI model downloading to\naccommodate diverse inference requirements of end users. By proactively caching\nmodels at edge nodes, users can retrieve the requested models with low latency\nfor on-device AI inference. However, the substantial size of contemporary AI\nmodels poses significant challenges for edge caching under limited storage\ncapacity, as well as for the concurrent delivery of heterogeneous models over\nwireless channels. To address these challenges, we propose a fine-grained AI\nmodel caching and downloading system that exploits parameter reusability,\nstemming from the common practice of fine-tuning task-specific models from a\nshared pre-trained model with frozen parameters. This system selectively caches\nmodel parameter blocks (PBs) at edge nodes, eliminating redundant storage of\nreusable parameters across different cached models. Additionally, it\nincorporates coordinated multipoint (CoMP) broadcasting to simultaneously\ndeliver reusable PBs to multiple users, thereby enhancing downlink spectrum\nutilization. Under this arrangement, we formulate a model downloading delay\nminimization problem to jointly optimize PB caching, migration (among edge\nnodes), and broadcasting beamforming. To tackle this intractable problem, we\ndevelop a distributed multi-agent learning framework that enables edge nodes to\nexplicitly learn mutual influence among their actions, thereby facilitating\ncooperation. Furthermore, a data augmentation approach is proposed to\nadaptively generate synthetic training samples through a predictive model,\nboosting sample efficiency and accelerating policy learning. Both theoretical\nanalysis and simulation experiments validate the superior convergence\nperformance of the proposed learning framework."
                },
                "authors": [
                    {
                        "name": "Yang Fu"
                    },
                    {
                        "name": "Peng Qin"
                    },
                    {
                        "name": "Yueyue Zhang"
                    },
                    {
                        "name": "Yifei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yifei Wang"
                },
                "author": "Yifei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12817v1",
                "updated": "2025-09-16T08:36:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    8,
                    36,
                    5,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T08:36:05Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    8,
                    36,
                    5,
                    1,
                    259,
                    0
                ],
                "title": "SAGA: Selective Adaptive Gating for Efficient and Expressive Linear\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAGA: Selective Adaptive Gating for Efficient and Expressive Linear\n  Attention"
                },
                "summary": "While Transformer architecture excel at modeling long-range dependencies\ncontributing to its widespread adoption in vision tasks the quadratic\ncomplexity of softmax-based attention mechanisms imposes a major bottleneck,\nparticularly when processing high-resolution images. Linear attention presents\na promising alternative by reformulating the attention computation from $(QK)V$\nto $Q(KV)$, thereby reducing the complexity from $\\mathcal{O}(N^2)$ to\n$\\mathcal{O}(N)$ while preserving the global receptive field. However, most\nexisting methods compress historical key-value (KV) information uniformly,\nwhich can lead to feature redundancy and the loss of directional alignment with\nthe query (Q). This uniform compression results in low-rank $KV$ feature maps,\ncontributing to a performance gap compared to softmax attention. To mitigate\nthis limitation, we propose \\textbf{S}elective \\textbf{A}daptive\n\\textbf{GA}ting for Efficient and Expressive Linear Attention (SAGA) , which\nintroduces input-adaptive learnable gates to selectively modulate information\naggregation into the $KV$ feature map. These gates enhance semantic diversity\nand alleviate the low-rank constraint inherent in conventional linear\nattention. Additionally, we propose an efficient Hadamard-product decomposition\nmethod for gate computation, which introduces no additional memory overhead.\nExperiments demonstrate that SAGA achieves a 1.76$\\times$ improvement in\nthroughput and a 2.69$\\times$ reduction in peak GPU memory compared to PVT-T at\na resolution of $1280 \\times 1280$. Moreover, it improves top-1 accuracy by up\nto 4.4\\% on the ImageNet dataset, demonstrating both computational efficiency\nand model effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer architecture excel at modeling long-range dependencies\ncontributing to its widespread adoption in vision tasks the quadratic\ncomplexity of softmax-based attention mechanisms imposes a major bottleneck,\nparticularly when processing high-resolution images. Linear attention presents\na promising alternative by reformulating the attention computation from $(QK)V$\nto $Q(KV)$, thereby reducing the complexity from $\\mathcal{O}(N^2)$ to\n$\\mathcal{O}(N)$ while preserving the global receptive field. However, most\nexisting methods compress historical key-value (KV) information uniformly,\nwhich can lead to feature redundancy and the loss of directional alignment with\nthe query (Q). This uniform compression results in low-rank $KV$ feature maps,\ncontributing to a performance gap compared to softmax attention. To mitigate\nthis limitation, we propose \\textbf{S}elective \\textbf{A}daptive\n\\textbf{GA}ting for Efficient and Expressive Linear Attention (SAGA) , which\nintroduces input-adaptive learnable gates to selectively modulate information\naggregation into the $KV$ feature map. These gates enhance semantic diversity\nand alleviate the low-rank constraint inherent in conventional linear\nattention. Additionally, we propose an efficient Hadamard-product decomposition\nmethod for gate computation, which introduces no additional memory overhead.\nExperiments demonstrate that SAGA achieves a 1.76$\\times$ improvement in\nthroughput and a 2.69$\\times$ reduction in peak GPU memory compared to PVT-T at\na resolution of $1280 \\times 1280$. Moreover, it improves top-1 accuracy by up\nto 4.4\\% on the ImageNet dataset, demonstrating both computational efficiency\nand model effectiveness."
                },
                "authors": [
                    {
                        "name": "Yuan Cao"
                    },
                    {
                        "name": "Dong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dong Wang"
                },
                "author": "Dong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11156v2",
                "updated": "2025-09-16T07:49:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    7,
                    49,
                    41,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-14T08:22:37Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    8,
                    22,
                    37,
                    6,
                    257,
                    0
                ],
                "title": "Adaptive K-PackCache: Cost-Centric Data Caching in Cloud",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive K-PackCache: Cost-Centric Data Caching in Cloud"
                },
                "summary": "Recent advances in data analytics have enabled the accurate prediction of\nuser access patterns, giving rise to the idea of packed caching delivering\nmultiple co accessed data items together as a bundle. This improves caching\nefficiency, as accessing one item often implies the need for others. Prior work\nhas explored only 2 item pairwise packing. In this paper, we extend the concept\nto general K packing, allowing variable size bundles for improved flexibility\nand performance. We formulate the K PackCache problem from a content delivery\nnetwork CDN operator perspective, aiming to minimize total cost comprising two\ncomponents: transfer cost modeled as a base cost plus a linearly increasing\nterm with the number of items packed, and memory rental cost for caching, which\ndepends on how long and how much is stored. Overpacking increases cost due to\nlow utility, underpacking leads to missed sharing opportunities. We propose an\nonline algorithm, Adaptive K PackCache AKPC, which dynamically forms, merges,\nand splits data cliques based on user access patterns and content correlation.\nOur approach supports batch requests, enables approximate clique merging, and\noffers a formal competitive guarantee. Through extensive evaluation on the\nNetflix and Spotify datasets, AKPC reduces total cost by up to 63 and 55\npercentage over online baselines, respectively, and achieves performance within\n15 and 13 percentage of the optimal. This demonstrates its scalability and\neffectiveness for real world caching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in data analytics have enabled the accurate prediction of\nuser access patterns, giving rise to the idea of packed caching delivering\nmultiple co accessed data items together as a bundle. This improves caching\nefficiency, as accessing one item often implies the need for others. Prior work\nhas explored only 2 item pairwise packing. In this paper, we extend the concept\nto general K packing, allowing variable size bundles for improved flexibility\nand performance. We formulate the K PackCache problem from a content delivery\nnetwork CDN operator perspective, aiming to minimize total cost comprising two\ncomponents: transfer cost modeled as a base cost plus a linearly increasing\nterm with the number of items packed, and memory rental cost for caching, which\ndepends on how long and how much is stored. Overpacking increases cost due to\nlow utility, underpacking leads to missed sharing opportunities. We propose an\nonline algorithm, Adaptive K PackCache AKPC, which dynamically forms, merges,\nand splits data cliques based on user access patterns and content correlation.\nOur approach supports batch requests, enables approximate clique merging, and\noffers a formal competitive guarantee. Through extensive evaluation on the\nNetflix and Spotify datasets, AKPC reduces total cost by up to 63 and 55\npercentage over online baselines, respectively, and achieves performance within\n15 and 13 percentage of the optimal. This demonstrates its scalability and\neffectiveness for real world caching systems."
                },
                "authors": [
                    {
                        "name": "Suvarthi Sarkar"
                    },
                    {
                        "name": "Aadarshraj Sah"
                    },
                    {
                        "name": "Poddutoori Sweeya Reddy"
                    },
                    {
                        "name": "Aryabartta Sahu"
                    }
                ],
                "author_detail": {
                    "name": "Aryabartta Sahu"
                },
                "author": "Aryabartta Sahu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13231v2",
                "updated": "2025-09-15T14:40:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    40,
                    16,
                    0,
                    258,
                    0
                ],
                "published": "2025-08-17T19:07:08Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    19,
                    7,
                    8,
                    6,
                    229,
                    0
                ],
                "title": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System"
                },
                "summary": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference."
                },
                "authors": [
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Kaoutar El Maghraoui"
                    },
                    {
                        "name": "Naigang Wang"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "IEEE Computer Architecture Letter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11628v1",
                "updated": "2025-09-15T06:46:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    6,
                    46,
                    22,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T06:46:22Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    6,
                    46,
                    22,
                    0,
                    258,
                    0
                ],
                "title": "SpeCa: Accelerating Diffusion Transformers with Speculative Feature\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeCa: Accelerating Diffusion Transformers with Speculative Feature\n  Caching"
                },
                "summary": "Diffusion models have revolutionized high-fidelity image and video synthesis,\nyet their computational demands remain prohibitive for real-time applications.\nThese models face two fundamental challenges: strict temporal dependencies\npreventing parallelization, and computationally intensive forward passes\nrequired at each denoising step. Drawing inspiration from speculative decoding\nin large language models, we present SpeCa, a novel 'Forecast-then-verify'\nacceleration framework that effectively addresses both limitations. SpeCa's\ncore innovation lies in introducing Speculative Sampling to diffusion models,\npredicting intermediate features for subsequent timesteps based on fully\ncomputed reference timesteps. Our approach implements a parameter-free\nverification mechanism that efficiently evaluates prediction reliability,\nenabling real-time decisions to accept or reject each prediction while\nincurring negligible computational overhead. Furthermore, SpeCa introduces\nsample-adaptive computation allocation that dynamically modulates resources\nbased on generation complexity, allocating reduced computation for simpler\nsamples while preserving intensive processing for complex instances.\nExperiments demonstrate 6.34x acceleration on FLUX with minimal quality\ndegradation (5.5% drop), 7.3x speedup on DiT while preserving generation\nfidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The\nverification mechanism incurs minimal overhead (1.67%-3.5% of full inference\ncosts), establishing a new paradigm for efficient diffusion model inference\nwhile maintaining generation quality even at aggressive acceleration ratios.\nOur codes have been released in Github:\n\\textbf{https://github.com/Shenyi-Z/Cache4Diffusion}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have revolutionized high-fidelity image and video synthesis,\nyet their computational demands remain prohibitive for real-time applications.\nThese models face two fundamental challenges: strict temporal dependencies\npreventing parallelization, and computationally intensive forward passes\nrequired at each denoising step. Drawing inspiration from speculative decoding\nin large language models, we present SpeCa, a novel 'Forecast-then-verify'\nacceleration framework that effectively addresses both limitations. SpeCa's\ncore innovation lies in introducing Speculative Sampling to diffusion models,\npredicting intermediate features for subsequent timesteps based on fully\ncomputed reference timesteps. Our approach implements a parameter-free\nverification mechanism that efficiently evaluates prediction reliability,\nenabling real-time decisions to accept or reject each prediction while\nincurring negligible computational overhead. Furthermore, SpeCa introduces\nsample-adaptive computation allocation that dynamically modulates resources\nbased on generation complexity, allocating reduced computation for simpler\nsamples while preserving intensive processing for complex instances.\nExperiments demonstrate 6.34x acceleration on FLUX with minimal quality\ndegradation (5.5% drop), 7.3x speedup on DiT while preserving generation\nfidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The\nverification mechanism incurs minimal overhead (1.67%-3.5% of full inference\ncosts), establishing a new paradigm for efficient diffusion model inference\nwhile maintaining generation quality even at aggressive acceleration ratios.\nOur codes have been released in Github:\n\\textbf{https://github.com/Shenyi-Z/Cache4Diffusion}"
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Fei Ren"
                    },
                    {
                        "name": "Shaobo Wang"
                    },
                    {
                        "name": "Kaixin Li"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_doi": "10.1145/3746027.3755331",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746027.3755331",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.11628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 9 figures, ACM Multimedia 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14089v2",
                "updated": "2025-09-15T01:15:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    1,
                    15,
                    50,
                    0,
                    258,
                    0
                ],
                "published": "2025-04-18T22:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models"
                },
                "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average."
                },
                "authors": [
                    {
                        "name": "Kang He"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06261v2",
                "updated": "2025-09-15T00:51:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    0,
                    51,
                    47,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-08T00:57:50Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    0,
                    57,
                    50,
                    0,
                    251,
                    0
                ],
                "title": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\n  Heterogeneous Precision LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\n  Heterogeneous Precision LLM Serving"
                },
                "summary": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems."
                },
                "authors": [
                    {
                        "name": "Kyungmin Bin"
                    },
                    {
                        "name": "Seungbeom Choi"
                    },
                    {
                        "name": "Jimyoung Son"
                    },
                    {
                        "name": "Jieun Choi"
                    },
                    {
                        "name": "Daseul Bae"
                    },
                    {
                        "name": "Daehyeon Baek"
                    },
                    {
                        "name": "Kihyo Moon"
                    },
                    {
                        "name": "Minsung Jang"
                    },
                    {
                        "name": "Hyojung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hyojung Lee"
                },
                "author": "Hyojung Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11239v1",
                "updated": "2025-09-14T12:29:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    14,
                    12,
                    29,
                    49,
                    6,
                    257,
                    0
                ],
                "published": "2025-09-14T12:29:49Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    12,
                    29,
                    49,
                    6,
                    257,
                    0
                ],
                "title": "Multi-Layer Perceptron-Based Relay Node Selection for Next-Generation\n  Intelligent Delay-Tolerant Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Layer Perceptron-Based Relay Node Selection for Next-Generation\n  Intelligent Delay-Tolerant Networks"
                },
                "summary": "Delay Tolerant Networks (DTNs) are critical for emergency communication in\nhighly dynamic and challenging scenarios characterized by intermittent\nconnectivity, frequent disruptions, and unpredictable node mobility. While some\nprotocols are widely adopted for simplicity and low overhead, their static\nreplication strategy lacks the ability to adaptively distinguish high-quality\nrelay nodes, often leading to inefficient and suboptimal message dissemination.\nTo address this challenge, we propose a novel intelligent routing enhancement\nthat integrates machine learning-based node evaluation into the Spray and Wait\nframework. Several dynamic, core features are extracted from simulation logs\nand are used to train multiple classifiers - Multi-Layer Perceptron (MLP),\nSupport Vector Machine (SVM), and Random Forest (RF) - to predict whether a\nnode is suitable as a relay under dynamic conditions. The trained models are\ndeployed via a lightweight Flask-based RESTful API, enabling real-time,\nadaptive predictions. We implement the enhanced router MLPBasedSprayRouter,\nwhich selectively forwards messages based on the predicted relay quality. A\ncaching mechanism is incorporated to reduce computational overhead and ensure\nstable, low-latency inference. Extensive experiments under realistic emergency\nmobility scenarios demonstrate that the proposed framework significantly\nimproves delivery ratio while reducing average latency compared to the baseline\nprotocols. Among all evaluated classifiers, MLP achieved the most robust\nperformance, consistently outperforming both SVM and RF in terms of accuracy,\nadaptability, and inference speed. These results confirm the novelty and\npracticality of integrating machine learning into DTN routing, paving the way\nfor resilient and intelligent communication systems in smart cities, disaster\nrecovery, and other dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delay Tolerant Networks (DTNs) are critical for emergency communication in\nhighly dynamic and challenging scenarios characterized by intermittent\nconnectivity, frequent disruptions, and unpredictable node mobility. While some\nprotocols are widely adopted for simplicity and low overhead, their static\nreplication strategy lacks the ability to adaptively distinguish high-quality\nrelay nodes, often leading to inefficient and suboptimal message dissemination.\nTo address this challenge, we propose a novel intelligent routing enhancement\nthat integrates machine learning-based node evaluation into the Spray and Wait\nframework. Several dynamic, core features are extracted from simulation logs\nand are used to train multiple classifiers - Multi-Layer Perceptron (MLP),\nSupport Vector Machine (SVM), and Random Forest (RF) - to predict whether a\nnode is suitable as a relay under dynamic conditions. The trained models are\ndeployed via a lightweight Flask-based RESTful API, enabling real-time,\nadaptive predictions. We implement the enhanced router MLPBasedSprayRouter,\nwhich selectively forwards messages based on the predicted relay quality. A\ncaching mechanism is incorporated to reduce computational overhead and ensure\nstable, low-latency inference. Extensive experiments under realistic emergency\nmobility scenarios demonstrate that the proposed framework significantly\nimproves delivery ratio while reducing average latency compared to the baseline\nprotocols. Among all evaluated classifiers, MLP achieved the most robust\nperformance, consistently outperforming both SVM and RF in terms of accuracy,\nadaptability, and inference speed. These results confirm the novelty and\npracticality of integrating machine learning into DTN routing, paving the way\nfor resilient and intelligent communication systems in smart cities, disaster\nrecovery, and other dynamic environments."
                },
                "authors": [
                    {
                        "name": "Zhekun Huang"
                    },
                    {
                        "name": "Milena Radenkovic"
                    }
                ],
                "author_detail": {
                    "name": "Milena Radenkovic"
                },
                "author": "Milena Radenkovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11181v1",
                "updated": "2025-09-14T09:26:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    14,
                    9,
                    26,
                    44,
                    6,
                    257,
                    0
                ],
                "published": "2025-09-14T09:26:44Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    9,
                    26,
                    44,
                    6,
                    257,
                    0
                ],
                "title": "Dislocation response to electric fields in strontium titanate: A\n  mesoscale indentation study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dislocation response to electric fields in strontium titanate: A\n  mesoscale indentation study"
                },
                "summary": "Dislocations in perovskite oxides have drawn increasing research interest due\nto their potential of tuning functional properties of electroceramics. Open\nquestions remain regarding the behavior of dislocations concerning their\nstability under strong externally applied electric fields. In this study, we\ninvestigate the dielectric breakdown strength of nominally undoped SrTiO3\ncrystals after the introduction of high-density dislocations. The\ndislocation-rich samples are prepared using the Brinell scratching method, and\nthey consistently exhibit lower dielectric breakdown strength as well as a\nlarger scatter in the breakdown probability. We also study the impact of\nelectric field on the introduction and movement of dislocations in SrTiO3\ncrystals using Brinell indentation coupled with an electric field of 2 kV/mm.\nNo changes on the dislocation plastic zone size, depth, and dislocation\ndistribution are observed under this electric field. Based on the charge state\nof the dislocations in SrTiO3 as well as the electrical and thermal\nconductivity modified by dislocations, we discuss the forces induced by the\nelectric field to act on the dislocations to underline the possible mechanisms\nfor such dislocation behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dislocations in perovskite oxides have drawn increasing research interest due\nto their potential of tuning functional properties of electroceramics. Open\nquestions remain regarding the behavior of dislocations concerning their\nstability under strong externally applied electric fields. In this study, we\ninvestigate the dielectric breakdown strength of nominally undoped SrTiO3\ncrystals after the introduction of high-density dislocations. The\ndislocation-rich samples are prepared using the Brinell scratching method, and\nthey consistently exhibit lower dielectric breakdown strength as well as a\nlarger scatter in the breakdown probability. We also study the impact of\nelectric field on the introduction and movement of dislocations in SrTiO3\ncrystals using Brinell indentation coupled with an electric field of 2 kV/mm.\nNo changes on the dislocation plastic zone size, depth, and dislocation\ndistribution are observed under this electric field. Based on the charge state\nof the dislocations in SrTiO3 as well as the electrical and thermal\nconductivity modified by dislocations, we discuss the forces induced by the\nelectric field to act on the dislocations to underline the possible mechanisms\nfor such dislocation behavior."
                },
                "authors": [
                    {
                        "name": "Alexander Frisch"
                    },
                    {
                        "name": "Daniel Isaia"
                    },
                    {
                        "name": "Oliver Preuß"
                    },
                    {
                        "name": "Xufei Fang"
                    }
                ],
                "author_detail": {
                    "name": "Xufei Fang"
                },
                "author": "Xufei Fang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11155v1",
                "updated": "2025-09-14T08:20:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    14,
                    8,
                    20,
                    48,
                    6,
                    257,
                    0
                ],
                "published": "2025-09-14T08:20:48Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    8,
                    20,
                    48,
                    6,
                    257,
                    0
                ],
                "title": "AQUA: Attention via QUery mAgnitudes for Memory and Compute Efficient\n  Inference in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AQUA: Attention via QUery mAgnitudes for Memory and Compute Efficient\n  Inference in LLMs"
                },
                "summary": "The quadratic complexity of the attention mechanism remains a fundamental\nbarrier to scaling Large Language Models (LLMs) to longer contexts, creating a\ncritical bottleneck in both computation and memory. To address this, we\nintroduce AQUA (Attention via QUery mAgnitudes) a novel and versatile\napproximation strategy that significantly reduces the cost of attention with a\ngraceful performance trade-off. Our method operates in two phases: an efficient\noffline step where we compute a universal, language agnostic projection matrix\nvia SVD on a calibration dataset, and an online inference step where we project\nquery and key vectors and dynamically select a sparse subset of dimensions\nbased on the query's magnitude. We provide a formal theoretical analysis of\nAQUA, establishing the break-even point at which it becomes more\ncomputationally efficient than standard attention. Our empirical evaluations on\nstate-of-the-art models like Llama-3.1-8B demonstrate that a 25% reduction in\nthe attention dot-product computation can be achieved with a statistically\ninsignificant impact on performance across a wide range of benchmarks. We\nfurther showcase the versatility of AQUA by demonstrating its ability to\nsynergistically accelerate existing token eviction methods like H2O and to\ndirectly reduce KV-cache memory size. By offering a controllable knob to\nbalance efficiency and accuracy, AQUA provides a practical and powerful tool\nfor making large-scale LLM inference more accessible and sustainable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic complexity of the attention mechanism remains a fundamental\nbarrier to scaling Large Language Models (LLMs) to longer contexts, creating a\ncritical bottleneck in both computation and memory. To address this, we\nintroduce AQUA (Attention via QUery mAgnitudes) a novel and versatile\napproximation strategy that significantly reduces the cost of attention with a\ngraceful performance trade-off. Our method operates in two phases: an efficient\noffline step where we compute a universal, language agnostic projection matrix\nvia SVD on a calibration dataset, and an online inference step where we project\nquery and key vectors and dynamically select a sparse subset of dimensions\nbased on the query's magnitude. We provide a formal theoretical analysis of\nAQUA, establishing the break-even point at which it becomes more\ncomputationally efficient than standard attention. Our empirical evaluations on\nstate-of-the-art models like Llama-3.1-8B demonstrate that a 25% reduction in\nthe attention dot-product computation can be achieved with a statistically\ninsignificant impact on performance across a wide range of benchmarks. We\nfurther showcase the versatility of AQUA by demonstrating its ability to\nsynergistically accelerate existing token eviction methods like H2O and to\ndirectly reduce KV-cache memory size. By offering a controllable knob to\nbalance efficiency and accuracy, AQUA provides a practical and powerful tool\nfor making large-scale LLM inference more accessible and sustainable."
                },
                "authors": [
                    {
                        "name": "Santhosh G S"
                    },
                    {
                        "name": "Saurav Prakash"
                    },
                    {
                        "name": "Balaraman Ravindran"
                    }
                ],
                "author_detail": {
                    "name": "Balaraman Ravindran"
                },
                "author": "Balaraman Ravindran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.22647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22647v1",
                "updated": "2025-09-26T17:59:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    55,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:59:55Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    55,
                    4,
                    269,
                    0
                ],
                "title": "CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement\n  Learning"
                },
                "summary": "Image captioning is a fundamental task that bridges the visual and linguistic\ndomains, playing a critical role in pre-training Large Vision-Language Models\n(LVLMs). Current state-of-the-art captioning models are typically trained with\nSupervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable\ndata annotated by humans or proprietary models. This approach often leads to\nmodels that memorize specific ground-truth answers, limiting their generality\nand ability to generate diverse, creative descriptions. To overcome the\nlimitation of SFT, we propose applying the Reinforcement Learning with\nVerifiable Rewards (RLVR) paradigm to the open-ended task of image captioning.\nA primary challenge, however, is designing an objective reward function for the\ninherently subjective nature of what constitutes a \"good\" caption. We introduce\nCaptioning Reinforcement Learning (CapRL), a novel training framework that\nredefines caption quality through its utility: a high-quality caption should\nenable a non-visual language model to accurately answer questions about the\ncorresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM\ngenerates a caption, and the objective reward is derived from the accuracy of a\nseparate, vision-free LLM answering Multiple-Choice Questions based solely on\nthat caption. As the first study to apply RLVR to the subjective image\ncaptioning task, we demonstrate that CapRL significantly enhances multiple\nsettings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B\nresults in substantial gains across 12 benchmarks. Moreover, within the Prism\nFramework for caption quality evaluation, CapRL achieves performance comparable\nto Qwen2.5-VL-72B, while exceeding the baseline by an average margin of 8.4%.\nCode is available here: https://github.com/InternLM/CapRL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image captioning is a fundamental task that bridges the visual and linguistic\ndomains, playing a critical role in pre-training Large Vision-Language Models\n(LVLMs). Current state-of-the-art captioning models are typically trained with\nSupervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable\ndata annotated by humans or proprietary models. This approach often leads to\nmodels that memorize specific ground-truth answers, limiting their generality\nand ability to generate diverse, creative descriptions. To overcome the\nlimitation of SFT, we propose applying the Reinforcement Learning with\nVerifiable Rewards (RLVR) paradigm to the open-ended task of image captioning.\nA primary challenge, however, is designing an objective reward function for the\ninherently subjective nature of what constitutes a \"good\" caption. We introduce\nCaptioning Reinforcement Learning (CapRL), a novel training framework that\nredefines caption quality through its utility: a high-quality caption should\nenable a non-visual language model to accurately answer questions about the\ncorresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM\ngenerates a caption, and the objective reward is derived from the accuracy of a\nseparate, vision-free LLM answering Multiple-Choice Questions based solely on\nthat caption. As the first study to apply RLVR to the subjective image\ncaptioning task, we demonstrate that CapRL significantly enhances multiple\nsettings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B\nresults in substantial gains across 12 benchmarks. Moreover, within the Prism\nFramework for caption quality evaluation, CapRL achieves performance comparable\nto Qwen2.5-VL-72B, while exceeding the baseline by an average margin of 8.4%.\nCode is available here: https://github.com/InternLM/CapRL."
                },
                "authors": [
                    {
                        "name": "Long Xing"
                    },
                    {
                        "name": "Xiaoyi Dong"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Yuhang Cao"
                    },
                    {
                        "name": "Jianze Liang"
                    },
                    {
                        "name": "Qidong Huang"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Feng Wu"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "arxiv_comment": "Code is available at https://github.com/InternLM/CapRL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08799v2",
                "updated": "2025-09-26T17:59:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    54,
                    4,
                    269,
                    0
                ],
                "published": "2025-07-11T17:59:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "KV Cache Steering for Controlling Frozen LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Steering for Controlling Frozen LLMs"
                },
                "summary": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach constructs\nsteering vectors from reasoning traces, obtained either from teacher models\n(e.g., GPT-4o) or existing human annotations, that shift model behavior toward\nmore explicit, multi-step reasoning without fine-tuning or prompt\nmodifications. Experimental evaluations on diverse reasoning benchmarks\ndemonstrate that cache steering improves both the qualitative structure of\nmodel reasoning and quantitative task performance. Additional experiments show\nthat the method also scales to larger models and yields further gains on\nchallenging datasets such as GPQA and MATH. Compared to prior activation\nsteering techniques that require continuous interventions, our one-shot cache\nsteering offers substantial advantages in terms of inference latency,\nhyperparameter stability, and ease of integration with existing inference APIs.\nBeyond mere reasoning induction, we show that cache steering enables\ncontrollable transfer of reasoning styles (e.g., stepwise, causal, analogical),\nmaking it a practical tool for behavior-level guidance of language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach constructs\nsteering vectors from reasoning traces, obtained either from teacher models\n(e.g., GPT-4o) or existing human annotations, that shift model behavior toward\nmore explicit, multi-step reasoning without fine-tuning or prompt\nmodifications. Experimental evaluations on diverse reasoning benchmarks\ndemonstrate that cache steering improves both the qualitative structure of\nmodel reasoning and quantitative task performance. Additional experiments show\nthat the method also scales to larger models and yields further gains on\nchallenging datasets such as GPQA and MATH. Compared to prior activation\nsteering techniques that require continuous interventions, our one-shot cache\nsteering offers substantial advantages in terms of inference latency,\nhyperparameter stability, and ease of integration with existing inference APIs.\nBeyond mere reasoning induction, we show that cache steering enables\ncontrollable transfer of reasoning styles (e.g., stepwise, causal, analogical),\nmaking it a practical tool for behavior-level guidance of language models."
                },
                "authors": [
                    {
                        "name": "Max Belitsky"
                    },
                    {
                        "name": "Dawid J. Kopiczko"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "M. Jehanzeb Mirza"
                    },
                    {
                        "name": "James R. Glass"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22646v1",
                "updated": "2025-09-26T17:59:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    54,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:59:54Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    54,
                    4,
                    269,
                    0
                ],
                "title": "Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal\n  LLMs"
                },
                "summary": "Can humans identify AI-generated (fake) videos and provide grounded reasons?\nWhile video generation models have advanced rapidly, a critical dimension --\nwhether humans can detect deepfake traces within a generated video, i.e.,\nspatiotemporal grounded visual artifacts that reveal a video as machine\ngenerated -- has been largely overlooked. We introduce DeeptraceReward, the\nfirst fine-grained, spatially- and temporally- aware benchmark that annotates\nhuman-perceived fake traces for video generation reward. The dataset comprises\n4.3K detailed annotations across 3.3K high-quality generated videos. Each\nannotation provides a natural-language explanation, pinpoints a bounding-box\nregion containing the perceived trace, and marks precise onset and offset\ntimestamps. We consolidate these annotations into 9 major categories of\ndeepfake traces that lead humans to identify a video as AI-generated, and train\nmultimodal language models (LMs) as reward models to mimic human judgments and\nlocalizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by\n34.7% on average across fake clue identification, grounding, and explanation.\nInterestingly, we observe a consistent difficulty gradient: binary fake v.s.\nreal classification is substantially easier than fine-grained deepfake trace\ndetection; within the latter, performance degrades from natural language\nexplanations (easiest), to spatial grounding, to temporal labeling (hardest).\nBy foregrounding human-perceived deepfake traces, DeeptraceReward provides a\nrigorous testbed and training signal for socially aware and trustworthy video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can humans identify AI-generated (fake) videos and provide grounded reasons?\nWhile video generation models have advanced rapidly, a critical dimension --\nwhether humans can detect deepfake traces within a generated video, i.e.,\nspatiotemporal grounded visual artifacts that reveal a video as machine\ngenerated -- has been largely overlooked. We introduce DeeptraceReward, the\nfirst fine-grained, spatially- and temporally- aware benchmark that annotates\nhuman-perceived fake traces for video generation reward. The dataset comprises\n4.3K detailed annotations across 3.3K high-quality generated videos. Each\nannotation provides a natural-language explanation, pinpoints a bounding-box\nregion containing the perceived trace, and marks precise onset and offset\ntimestamps. We consolidate these annotations into 9 major categories of\ndeepfake traces that lead humans to identify a video as AI-generated, and train\nmultimodal language models (LMs) as reward models to mimic human judgments and\nlocalizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by\n34.7% on average across fake clue identification, grounding, and explanation.\nInterestingly, we observe a consistent difficulty gradient: binary fake v.s.\nreal classification is substantially easier than fine-grained deepfake trace\ndetection; within the latter, performance degrades from natural language\nexplanations (easiest), to spatial grounding, to temporal labeling (hardest).\nBy foregrounding human-perceived deepfake traces, DeeptraceReward provides a\nrigorous testbed and training signal for socially aware and trustworthy video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Xingyu Fu"
                    },
                    {
                        "name": "Siyi Liu"
                    },
                    {
                        "name": "Yinuo Xu"
                    },
                    {
                        "name": "Pan Lu"
                    },
                    {
                        "name": "Guangqiuse Hu"
                    },
                    {
                        "name": "Tianbo Yang"
                    },
                    {
                        "name": "Taran Anantasagar"
                    },
                    {
                        "name": "Christopher Shen"
                    },
                    {
                        "name": "Yikai Mao"
                    },
                    {
                        "name": "Yuanzhe Liu"
                    },
                    {
                        "name": "Keyush Shah"
                    },
                    {
                        "name": "Chung Un Lee"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    }
                ],
                "author_detail": {
                    "name": "Chris Callison-Burch"
                },
                "author": "Chris Callison-Burch",
                "arxiv_comment": "Project Page: https://deeptracereward.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22644v1",
                "updated": "2025-09-26T17:59:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    51,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:59:51Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    51,
                    4,
                    269,
                    0
                ],
                "title": "WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level\n  Feedback and Step-Level Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level\n  Feedback and Step-Level Reinforcement Learning"
                },
                "summary": "Agent systems powered by large language models (LLMs) have demonstrated\nimpressive performance on repository-level code-generation tasks. However, for\ntasks such as website codebase generation, which depend heavily on visual\neffects and user-interaction feedback, current code agents rely only on simple\ncode execution for feedback and verification. This approach fails to capture\nthe actual quality of the generated code. In this paper, we propose\nWebGen-Agent, a novel website-generation agent that leverages comprehensive and\nmulti-level visual feedback to iteratively generate and refine the website\ncodebase. Detailed and expressive text descriptions and suggestions regarding\nthe screenshots and GUI-agent testing of the websites are generated by a visual\nlanguage model (VLM), together with scores that quantify their quality. The\nscreenshot and GUI-agent scores are further integrated with a backtracking and\nselect-best mechanism, enhancing the performance of the agent. Utilizing the\naccurate visual scores inherent in the WebGen-Agent workflow, we further\nintroduce \\textit{Step-GRPO with Screenshot and GUI-agent Feedback} to improve\nthe ability of LLMs to act as the reasoning engine of WebGen-Agent. By using\nthe screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we\nprovide a dense and reliable process supervision signal, which effectively\nimproves the model's website-generation ability. On the WebGen-Bench dataset,\nWebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9%\nand its appearance score from 3.0 to 3.9, outperforming the previous\nstate-of-the-art agent system. Additionally, our Step-GRPO training approach\nincreases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and\nraises the appearance score from 3.4 to 3.7.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent systems powered by large language models (LLMs) have demonstrated\nimpressive performance on repository-level code-generation tasks. However, for\ntasks such as website codebase generation, which depend heavily on visual\neffects and user-interaction feedback, current code agents rely only on simple\ncode execution for feedback and verification. This approach fails to capture\nthe actual quality of the generated code. In this paper, we propose\nWebGen-Agent, a novel website-generation agent that leverages comprehensive and\nmulti-level visual feedback to iteratively generate and refine the website\ncodebase. Detailed and expressive text descriptions and suggestions regarding\nthe screenshots and GUI-agent testing of the websites are generated by a visual\nlanguage model (VLM), together with scores that quantify their quality. The\nscreenshot and GUI-agent scores are further integrated with a backtracking and\nselect-best mechanism, enhancing the performance of the agent. Utilizing the\naccurate visual scores inherent in the WebGen-Agent workflow, we further\nintroduce \\textit{Step-GRPO with Screenshot and GUI-agent Feedback} to improve\nthe ability of LLMs to act as the reasoning engine of WebGen-Agent. By using\nthe screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we\nprovide a dense and reliable process supervision signal, which effectively\nimproves the model's website-generation ability. On the WebGen-Bench dataset,\nWebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9%\nand its appearance score from 3.0 to 3.9, outperforming the previous\nstate-of-the-art agent system. Additionally, our Step-GRPO training approach\nincreases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and\nraises the appearance score from 3.4 to 3.7."
                },
                "authors": [
                    {
                        "name": "Zimu Lu"
                    },
                    {
                        "name": "Houxing Ren"
                    },
                    {
                        "name": "Yunqiao Yang"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Zhuofan Zong"
                    },
                    {
                        "name": "Junting Pan"
                    },
                    {
                        "name": "Mingjie Zhan"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22645v1",
                "updated": "2025-09-26T17:59:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    51,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:59:51Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    51,
                    4,
                    269,
                    0
                ],
                "title": "Hierarchical Representation Matching for CLIP-based Class-Incremental\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Representation Matching for CLIP-based Class-Incremental\n  Learning"
                },
                "summary": "Class-Incremental Learning (CIL) aims to endow models with the ability to\ncontinuously adapt to evolving data streams. Recent advances in pre-trained\nvision-language models (e.g., CLIP) provide a powerful foundation for this\ntask. However, existing approaches often rely on simplistic templates, such as\n\"a photo of a [CLASS]\", which overlook the hierarchical nature of visual\nconcepts. For example, recognizing \"cat\" versus \"car\" depends on coarse-grained\ncues, while distinguishing \"cat\" from \"lion\" requires fine-grained details.\nSimilarly, the current feature mapping in CLIP relies solely on the\nrepresentation from the last layer, neglecting the hierarchical information\ncontained in earlier layers. In this work, we introduce HiErarchical\nRepresentation MAtchiNg (HERMAN) for CLIP-based CIL. Our approach leverages\nLLMs to recursively generate discriminative textual descriptors, thereby\naugmenting the semantic space with explicit hierarchical cues. These\ndescriptors are matched to different levels of the semantic hierarchy and\nadaptively routed based on task-specific requirements, enabling precise\ndiscrimination while alleviating catastrophic forgetting in incremental tasks.\nExtensive experiments on multiple benchmarks demonstrate that our method\nconsistently achieves state-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Class-Incremental Learning (CIL) aims to endow models with the ability to\ncontinuously adapt to evolving data streams. Recent advances in pre-trained\nvision-language models (e.g., CLIP) provide a powerful foundation for this\ntask. However, existing approaches often rely on simplistic templates, such as\n\"a photo of a [CLASS]\", which overlook the hierarchical nature of visual\nconcepts. For example, recognizing \"cat\" versus \"car\" depends on coarse-grained\ncues, while distinguishing \"cat\" from \"lion\" requires fine-grained details.\nSimilarly, the current feature mapping in CLIP relies solely on the\nrepresentation from the last layer, neglecting the hierarchical information\ncontained in earlier layers. In this work, we introduce HiErarchical\nRepresentation MAtchiNg (HERMAN) for CLIP-based CIL. Our approach leverages\nLLMs to recursively generate discriminative textual descriptors, thereby\naugmenting the semantic space with explicit hierarchical cues. These\ndescriptors are matched to different levels of the semantic hierarchy and\nadaptively routed based on task-specific requirements, enabling precise\ndiscrimination while alleviating catastrophic forgetting in incremental tasks.\nExtensive experiments on multiple benchmarks demonstrate that our method\nconsistently achieves state-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Zhen-Hao Wen"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Ji Feng"
                    },
                    {
                        "name": "Han-Jia Ye"
                    },
                    {
                        "name": "De-Chuan Zhan"
                    },
                    {
                        "name": "Da-Wei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Da-Wei Zhou"
                },
                "author": "Da-Wei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22641v1",
                "updated": "2025-09-26T17:59:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    5,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:59:05Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    5,
                    4,
                    269,
                    0
                ],
                "title": "Death of the Novel(ty): Beyond n-Gram Novelty as a Metric for Textual\n  Creativity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Death of the Novel(ty): Beyond n-Gram Novelty as a Metric for Textual\n  Creativity"
                },
                "summary": "N-gram novelty is widely used to evaluate language models' ability to\ngenerate text outside of their training data. More recently, it has also been\nadopted as a metric for measuring textual creativity. However, theoretical work\non creativity suggests that this approach may be inadequate, as it does not\naccount for creativity's dual nature: novelty (how original the text is) and\nappropriateness (how sensical and pragmatic it is). We investigate the\nrelationship between this notion of creativity and n-gram novelty through 7542\nexpert writer annotations (n=26) of novelty, pragmaticality, and sensicality\nvia close reading of human and AI-generated text. We find that while n-gram\nnovelty is positively associated with expert writer-judged creativity, ~91% of\ntop-quartile expressions by n-gram novelty are not judged as creative,\ncautioning against relying on n-gram novelty alone. Furthermore, unlike\nhuman-written text, higher n-gram novelty in open-source LLMs correlates with\nlower pragmaticality. In an exploratory study with frontier close-source\nmodels, we additionally confirm that they are less likely to produce creative\nexpressions than humans. Using our dataset, we test whether zero-shot,\nfew-shot, and finetuned models are able to identify creative expressions (a\npositive aspect of writing) and non-pragmatic ones (a negative aspect).\nOverall, frontier LLMs exhibit performance much higher than random but leave\nroom for improvement, especially struggling to identify non-pragmatic\nexpressions. We further find that LLM-as-a-Judge novelty scores from the\nbest-performing model were predictive of expert writer preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "N-gram novelty is widely used to evaluate language models' ability to\ngenerate text outside of their training data. More recently, it has also been\nadopted as a metric for measuring textual creativity. However, theoretical work\non creativity suggests that this approach may be inadequate, as it does not\naccount for creativity's dual nature: novelty (how original the text is) and\nappropriateness (how sensical and pragmatic it is). We investigate the\nrelationship between this notion of creativity and n-gram novelty through 7542\nexpert writer annotations (n=26) of novelty, pragmaticality, and sensicality\nvia close reading of human and AI-generated text. We find that while n-gram\nnovelty is positively associated with expert writer-judged creativity, ~91% of\ntop-quartile expressions by n-gram novelty are not judged as creative,\ncautioning against relying on n-gram novelty alone. Furthermore, unlike\nhuman-written text, higher n-gram novelty in open-source LLMs correlates with\nlower pragmaticality. In an exploratory study with frontier close-source\nmodels, we additionally confirm that they are less likely to produce creative\nexpressions than humans. Using our dataset, we test whether zero-shot,\nfew-shot, and finetuned models are able to identify creative expressions (a\npositive aspect of writing) and non-pragmatic ones (a negative aspect).\nOverall, frontier LLMs exhibit performance much higher than random but leave\nroom for improvement, especially struggling to identify non-pragmatic\nexpressions. We further find that LLM-as-a-Judge novelty scores from the\nbest-performing model were predictive of expert writer preferences."
                },
                "authors": [
                    {
                        "name": "Arkadiy Saakyan"
                    },
                    {
                        "name": "Najoung Kim"
                    },
                    {
                        "name": "Smaranda Muresan"
                    },
                    {
                        "name": "Tuhin Chakrabarty"
                    }
                ],
                "author_detail": {
                    "name": "Tuhin Chakrabarty"
                },
                "author": "Tuhin Chakrabarty",
                "arxiv_comment": "26 pages, 10 figures, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22638v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22638v1",
                "updated": "2025-09-26T17:58:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    58,
                    27,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:58:27Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    58,
                    27,
                    4,
                    269,
                    0
                ],
                "title": "Language Models Can Learn from Verbal Feedback Without Scalar Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models Can Learn from Verbal Feedback Without Scalar Rewards"
                },
                "summary": "LLMs are often trained with RL from human or AI feedback, yet such methods\ntypically compress nuanced feedback into scalar rewards, discarding much of\ntheir richness and inducing scale imbalance. We propose treating verbal\nfeedback as a conditioning signal. Inspired by language priors in text-to-image\ngeneration, which enable novel outputs from unseen prompts, we introduce the\nfeedback-conditional policy (FCP). FCP learns directly from response-feedback\npairs, approximating the feedback-conditional posterior through maximum\nlikelihood training on offline data. We further develop an online bootstrapping\nstage where the policy generates under positive conditions and receives fresh\nfeedback to refine itself. This reframes feedback-driven learning as\nconditional generation rather than reward optimization, offering a more\nexpressive way for LLMs to directly learn from verbal feedback. Our code is\navailable at https://github.com/sail-sg/feedback-conditional-policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are often trained with RL from human or AI feedback, yet such methods\ntypically compress nuanced feedback into scalar rewards, discarding much of\ntheir richness and inducing scale imbalance. We propose treating verbal\nfeedback as a conditioning signal. Inspired by language priors in text-to-image\ngeneration, which enable novel outputs from unseen prompts, we introduce the\nfeedback-conditional policy (FCP). FCP learns directly from response-feedback\npairs, approximating the feedback-conditional posterior through maximum\nlikelihood training on offline data. We further develop an online bootstrapping\nstage where the policy generates under positive conditions and receives fresh\nfeedback to refine itself. This reframes feedback-driven learning as\nconditional generation rather than reward optimization, offering a more\nexpressive way for LLMs to directly learn from verbal feedback. Our code is\navailable at https://github.com/sail-sg/feedback-conditional-policy."
                },
                "authors": [
                    {
                        "name": "Renjie Luo"
                    },
                    {
                        "name": "Zichen Liu"
                    },
                    {
                        "name": "Xiangyan Liu"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Min Lin"
                    },
                    {
                        "name": "Wenhu Chen"
                    },
                    {
                        "name": "Wei Lu"
                    },
                    {
                        "name": "Tianyu Pang"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Pang"
                },
                "author": "Tianyu Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22638v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22637v1",
                "updated": "2025-09-26T17:58:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    58,
                    10,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:58:10Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    58,
                    10,
                    4,
                    269,
                    0
                ],
                "title": "Variational Reasoning for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Reasoning for Language Models"
                },
                "summary": "We introduce a variational reasoning framework for language models that\ntreats thinking traces as latent variables and optimizes them through\nvariational inference. Starting from the evidence lower bound (ELBO), we extend\nit to a multi-trace objective for tighter bounds and propose a forward-KL\nformulation that stabilizes the training of the variational posterior. We\nfurther show that rejection sampling finetuning and binary-reward RL, including\nGRPO, can be interpreted as local forward-KL objectives, where an implicit\nweighting by model accuracy naturally arises from the derivation and reveals a\npreviously unnoticed bias toward easier questions. We empirically validate our\nmethod on the Qwen 2.5 and Qwen 3 model families across a wide range of\nreasoning tasks. Overall, our work provides a principled probabilistic\nperspective that unifies variational inference with RL-style methods and yields\nstable objectives for improving the reasoning ability of language models. Our\ncode is available at https://github.com/sail-sg/variational-reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a variational reasoning framework for language models that\ntreats thinking traces as latent variables and optimizes them through\nvariational inference. Starting from the evidence lower bound (ELBO), we extend\nit to a multi-trace objective for tighter bounds and propose a forward-KL\nformulation that stabilizes the training of the variational posterior. We\nfurther show that rejection sampling finetuning and binary-reward RL, including\nGRPO, can be interpreted as local forward-KL objectives, where an implicit\nweighting by model accuracy naturally arises from the derivation and reveals a\npreviously unnoticed bias toward easier questions. We empirically validate our\nmethod on the Qwen 2.5 and Qwen 3 model families across a wide range of\nreasoning tasks. Overall, our work provides a principled probabilistic\nperspective that unifies variational inference with RL-style methods and yields\nstable objectives for improving the reasoning ability of language models. Our\ncode is available at https://github.com/sail-sg/variational-reasoning."
                },
                "authors": [
                    {
                        "name": "Xiangxin Zhou"
                    },
                    {
                        "name": "Zichen Liu"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Min Lin"
                    },
                    {
                        "name": "Chongxuan Li"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Tianyu Pang"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Pang"
                },
                "author": "Tianyu Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22636v1",
                "updated": "2025-09-26T17:58:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    58,
                    4,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:58:04Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    58,
                    4,
                    4,
                    269,
                    0
                ],
                "title": "Scale-Wise VAR is Secretly Discrete Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scale-Wise VAR is Secretly Discrete Diffusion"
                },
                "summary": "Autoregressive (AR) transformers have emerged as a powerful paradigm for\nvisual generation, largely due to their scalability, computational efficiency\nand unified architecture with language and vision. Among them, next scale\nprediction Visual Autoregressive Generation (VAR) has recently demonstrated\nremarkable performance, even surpassing diffusion-based models. In this work,\nwe revisit VAR and uncover a theoretical insight: when equipped with a\nMarkovian attention mask, VAR is mathematically equivalent to a discrete\ndiffusion. We term this reinterpretation as Scalable Visual Refinement with\nDiscrete Diffusion (SRDD), establishing a principled bridge between AR\ntransformers and diffusion models. Leveraging this new perspective, we show how\none can directly import the advantages of diffusion such as iterative\nrefinement and reduce architectural inefficiencies into VAR, yielding faster\nconvergence, lower inference cost, and improved zero-shot reconstruction.\nAcross multiple datasets, we show that the diffusion based perspective of VAR\nleads to consistent gains in efficiency and generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) transformers have emerged as a powerful paradigm for\nvisual generation, largely due to their scalability, computational efficiency\nand unified architecture with language and vision. Among them, next scale\nprediction Visual Autoregressive Generation (VAR) has recently demonstrated\nremarkable performance, even surpassing diffusion-based models. In this work,\nwe revisit VAR and uncover a theoretical insight: when equipped with a\nMarkovian attention mask, VAR is mathematically equivalent to a discrete\ndiffusion. We term this reinterpretation as Scalable Visual Refinement with\nDiscrete Diffusion (SRDD), establishing a principled bridge between AR\ntransformers and diffusion models. Leveraging this new perspective, we show how\none can directly import the advantages of diffusion such as iterative\nrefinement and reduce architectural inefficiencies into VAR, yielding faster\nconvergence, lower inference cost, and improved zero-shot reconstruction.\nAcross multiple datasets, we show that the diffusion based perspective of VAR\nleads to consistent gains in efficiency and generation."
                },
                "authors": [
                    {
                        "name": "Amandeep Kumar"
                    },
                    {
                        "name": "Nithin Gopalakrishnan Nair"
                    },
                    {
                        "name": "Vishal M. Patel"
                    }
                ],
                "author_detail": {
                    "name": "Vishal M. Patel"
                },
                "author": "Vishal M. Patel",
                "arxiv_comment": "Technical Reports",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22633v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22633v1",
                "updated": "2025-09-26T17:57:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    57,
                    17,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:57:17Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    57,
                    17,
                    4,
                    269,
                    0
                ],
                "title": "Towards Efficient Online Exploration for Reinforcement Learning with\n  Human Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient Online Exploration for Reinforcement Learning with\n  Human Feedback"
                },
                "summary": "Reinforcement learning with human feedback (RLHF), which learns a reward\nmodel from human preference data and then optimizes a policy to favor preferred\nresponses, has emerged as a central paradigm for aligning large language models\n(LLMs) with human preferences. In this paper, we investigate exploration\nprinciples for online RLHF, where one seeks to adaptively collect new\npreference data to refine both the reward model and the policy in a\ndata-efficient manner. By examining existing optimism-based exploration\nalgorithms, we identify a drawback in their sampling protocol: they tend to\ngather comparisons that fail to reduce the most informative uncertainties in\nreward differences, and we prove lower bounds showing that such methods can\nincur linear regret over exponentially long horizons. Motivated by this\ninsight, we propose a new exploration scheme that directs preference queries\ntoward reducing uncertainty in reward differences most relevant to policy\nimprovement. Under a multi-armed bandit model of RLHF, we establish regret\nbounds of order $T^{(\\beta+1)/(\\beta+2)}$, where $\\beta>0$ is a hyperparameter\nthat balances reward maximization against mitigating distribution shift. To our\nknowledge, this is the first online RLHF algorithm with regret scaling\npolynomially in all model parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with human feedback (RLHF), which learns a reward\nmodel from human preference data and then optimizes a policy to favor preferred\nresponses, has emerged as a central paradigm for aligning large language models\n(LLMs) with human preferences. In this paper, we investigate exploration\nprinciples for online RLHF, where one seeks to adaptively collect new\npreference data to refine both the reward model and the policy in a\ndata-efficient manner. By examining existing optimism-based exploration\nalgorithms, we identify a drawback in their sampling protocol: they tend to\ngather comparisons that fail to reduce the most informative uncertainties in\nreward differences, and we prove lower bounds showing that such methods can\nincur linear regret over exponentially long horizons. Motivated by this\ninsight, we propose a new exploration scheme that directs preference queries\ntoward reducing uncertainty in reward differences most relevant to policy\nimprovement. Under a multi-armed bandit model of RLHF, we establish regret\nbounds of order $T^{(\\beta+1)/(\\beta+2)}$, where $\\beta>0$ is a hyperparameter\nthat balances reward maximization against mitigating distribution shift. To our\nknowledge, this is the first online RLHF algorithm with regret scaling\npolynomially in all model parameters."
                },
                "authors": [
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Yuling Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yuling Yan"
                },
                "author": "Yuling Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22633v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22633v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05418v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05418v2",
                "updated": "2025-09-26T17:57:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    57,
                    11,
                    4,
                    269,
                    0
                ],
                "published": "2025-07-07T19:04:36Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    19,
                    4,
                    36,
                    0,
                    188,
                    0
                ],
                "title": "Learn Globally, Speak Locally: Bridging the Gaps in Multilingual\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learn Globally, Speak Locally: Bridging the Gaps in Multilingual\n  Reasoning"
                },
                "summary": "Large Language Models (LLMs) have achieved strong performance in domains like\nmathematics, factual question answering, and code generation, yet their ability\nto reason on these tasks in different languages remains underdeveloped.\nEspecially for low-resource languages such as Swahili or Thai, LLMs can often\nmisinterpret prompts or default to reasoning in English. This implicit bias\ntoward high-resource languages undermines factual accuracy, interpretability,\nand trust. We propose M2A, a novel method that combines multi-scale\nmultilingual alignment with language-consistency rewards on machine-translated\nquestions, training models to reason directly and accurately in the target\nlanguage. Furthermore, existing multilingual benchmarks only evaluate on final\nanswers, overlooking whether reasoning occurs in the intended language. To\nclose this gap, we introduce GeoFact-X, a geography-based multilingual factual\nreasoning benchmark together with reasoning traces in five languages: English,\nHindi, Japanese, Swahili, and Thai. Our results show that M2A significantly\nenhances multilingual reasoning fidelity in both mathematical and factual\nreasoning tasks, highlighting that reasoning-aware multilingual reinforcement\nlearning is crucial for robust cross-lingual generalization.\nhttps://jd730.github.io/projects/M2A_GeoFact-X",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved strong performance in domains like\nmathematics, factual question answering, and code generation, yet their ability\nto reason on these tasks in different languages remains underdeveloped.\nEspecially for low-resource languages such as Swahili or Thai, LLMs can often\nmisinterpret prompts or default to reasoning in English. This implicit bias\ntoward high-resource languages undermines factual accuracy, interpretability,\nand trust. We propose M2A, a novel method that combines multi-scale\nmultilingual alignment with language-consistency rewards on machine-translated\nquestions, training models to reason directly and accurately in the target\nlanguage. Furthermore, existing multilingual benchmarks only evaluate on final\nanswers, overlooking whether reasoning occurs in the intended language. To\nclose this gap, we introduce GeoFact-X, a geography-based multilingual factual\nreasoning benchmark together with reasoning traces in five languages: English,\nHindi, Japanese, Swahili, and Thai. Our results show that M2A significantly\nenhances multilingual reasoning fidelity in both mathematical and factual\nreasoning tasks, highlighting that reasoning-aware multilingual reinforcement\nlearning is crucial for robust cross-lingual generalization.\nhttps://jd730.github.io/projects/M2A_GeoFact-X"
                },
                "authors": [
                    {
                        "name": "Jaedong Hwang"
                    },
                    {
                        "name": "Kumar Tanmay"
                    },
                    {
                        "name": "Seok-Jin Lee"
                    },
                    {
                        "name": "Ayush Agrawal"
                    },
                    {
                        "name": "Hamid Palangi"
                    },
                    {
                        "name": "Kumar Ayush"
                    },
                    {
                        "name": "Ila Fiete"
                    },
                    {
                        "name": "Paul Pu Liang"
                    }
                ],
                "author_detail": {
                    "name": "Paul Pu Liang"
                },
                "author": "Paul Pu Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05418v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05418v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22628v1",
                "updated": "2025-09-26T17:51:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    51,
                    46,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:51:46Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    51,
                    46,
                    4,
                    269,
                    0
                ],
                "title": "UML-CoT: Structured Reasoning and Planning with Unified Modeling\n  Language for Robotic Room Cleaning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UML-CoT: Structured Reasoning and Planning with Unified Modeling\n  Language for Robotic Room Cleaning"
                },
                "summary": "Chain-of-Thought (CoT) prompting improves reasoning in large language models\n(LLMs), but its reliance on unstructured text limits interpretability and\nexecutability in embodied tasks. Prior work has explored structured CoTs using\nscene or logic graphs, yet these remain fundamentally limited: they model only\nlow-order relations, lack constructs like inheritance or behavioral\nabstraction, and provide no standardized semantics for sequential or\nconditional planning. We propose UML-CoT, a structured reasoning and planning\nframework that leverages Unified Modeling Language (UML) to generate symbolic\nCoTs and executable action plans. UML class diagrams capture compositional\nobject semantics, while activity diagrams model procedural control flow. Our\nthree-stage training pipeline combines supervised fine-tuning with Group\nRelative Policy Optimization (GRPO), including reward learning from answer-only\ndata. We evaluate UML-CoT on MRoom-30k, a new benchmark of cluttered\nroom-cleaning scenarios. UML-CoT outperforms unstructured CoTs in\ninterpretability, planning coherence, and execution success, highlighting UML\nas a more expressive and actionable structured reasoning formalism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting improves reasoning in large language models\n(LLMs), but its reliance on unstructured text limits interpretability and\nexecutability in embodied tasks. Prior work has explored structured CoTs using\nscene or logic graphs, yet these remain fundamentally limited: they model only\nlow-order relations, lack constructs like inheritance or behavioral\nabstraction, and provide no standardized semantics for sequential or\nconditional planning. We propose UML-CoT, a structured reasoning and planning\nframework that leverages Unified Modeling Language (UML) to generate symbolic\nCoTs and executable action plans. UML class diagrams capture compositional\nobject semantics, while activity diagrams model procedural control flow. Our\nthree-stage training pipeline combines supervised fine-tuning with Group\nRelative Policy Optimization (GRPO), including reward learning from answer-only\ndata. We evaluate UML-CoT on MRoom-30k, a new benchmark of cluttered\nroom-cleaning scenarios. UML-CoT outperforms unstructured CoTs in\ninterpretability, planning coherence, and execution success, highlighting UML\nas a more expressive and actionable structured reasoning formalism."
                },
                "authors": [
                    {
                        "name": "Hongyu Chen"
                    },
                    {
                        "name": "Guangrun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangrun Wang"
                },
                "author": "Guangrun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7; I.2.8; I.4.8; I.5.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22624v1",
                "updated": "2025-09-26T17:50:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    50,
                    12,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:50:12Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    50,
                    12,
                    4,
                    269,
                    0
                ],
                "title": "SPARK: Synergistic Policy And Reward Co-Evolving Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPARK: Synergistic Policy And Reward Co-Evolving Framework"
                },
                "summary": "Recent Large Language Models (LLMs) and Large Vision-Language Models (LVLMs)\nincreasingly use Reinforcement Learning (RL) for post-pretraining, such as RL\nwith Verifiable Rewards (RLVR) for objective tasks and RL from Human Feedback\n(RLHF) for subjective tasks. However, RLHF incurs high costs and potential\nreward-policy mismatch due to reliance on human preferences, while RLVR still\nwastes supervision by discarding rollouts and correctness signals after each\nupdate. To address these challenges, we introduce the Synergistic Policy And\nReward Co-Evolving Framework (SPARK), an efficient, on-policy, and stable\nmethod that builds on RLVR. Instead of discarding rollouts and correctness\ndata, SPARK recycles this valuable information to simultaneously train the\nmodel itself as a generative reward model. This auxiliary training uses a mix\nof objectives, such as pointwise reward score, pairwise comparison, and\nevaluation conditioned on further-reflection responses, to teach the model to\nevaluate and improve its own responses. Our process eliminates the need for a\nseparate reward model and costly human preference data. SPARK creates a\npositive co-evolving feedback loop: improved reward accuracy yields better\npolicy gradients, which in turn produce higher-quality rollouts that further\nrefine the reward model. Our unified framework supports test-time scaling via\nself-reflection without external reward models and their associated costs. We\nshow that SPARK achieves significant performance gains on multiple LLM and LVLM\nmodels and multiple reasoning, reward models, and general benchmarks. For\nexample, SPARK-VL-7B achieves an average 9.7% gain on 7 reasoning benchmarks,\n12.1% on 2 reward benchmarks, and 1.5% on 8 general benchmarks over the\nbaselines, demonstrating robustness and broad generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Large Language Models (LLMs) and Large Vision-Language Models (LVLMs)\nincreasingly use Reinforcement Learning (RL) for post-pretraining, such as RL\nwith Verifiable Rewards (RLVR) for objective tasks and RL from Human Feedback\n(RLHF) for subjective tasks. However, RLHF incurs high costs and potential\nreward-policy mismatch due to reliance on human preferences, while RLVR still\nwastes supervision by discarding rollouts and correctness signals after each\nupdate. To address these challenges, we introduce the Synergistic Policy And\nReward Co-Evolving Framework (SPARK), an efficient, on-policy, and stable\nmethod that builds on RLVR. Instead of discarding rollouts and correctness\ndata, SPARK recycles this valuable information to simultaneously train the\nmodel itself as a generative reward model. This auxiliary training uses a mix\nof objectives, such as pointwise reward score, pairwise comparison, and\nevaluation conditioned on further-reflection responses, to teach the model to\nevaluate and improve its own responses. Our process eliminates the need for a\nseparate reward model and costly human preference data. SPARK creates a\npositive co-evolving feedback loop: improved reward accuracy yields better\npolicy gradients, which in turn produce higher-quality rollouts that further\nrefine the reward model. Our unified framework supports test-time scaling via\nself-reflection without external reward models and their associated costs. We\nshow that SPARK achieves significant performance gains on multiple LLM and LVLM\nmodels and multiple reasoning, reward models, and general benchmarks. For\nexample, SPARK-VL-7B achieves an average 9.7% gain on 7 reasoning benchmarks,\n12.1% on 2 reward benchmarks, and 1.5% on 8 general benchmarks over the\nbaselines, demonstrating robustness and broad generalization."
                },
                "authors": [
                    {
                        "name": "Ziyu Liu"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Shengyuan Ding"
                    },
                    {
                        "name": "Yuhang Cao"
                    },
                    {
                        "name": "Xiaoyi Dong"
                    },
                    {
                        "name": "Haodong Duan"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiaqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Wang"
                },
                "author": "Jiaqi Wang",
                "arxiv_comment": "Project:https://github.com/InternLM/Spark",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22622v1",
                "updated": "2025-09-26T17:48:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    48,
                    24,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:48:24Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    48,
                    24,
                    4,
                    269,
                    0
                ],
                "title": "LongLive: Real-time Interactive Long Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongLive: Real-time Interactive Long Video Generation"
                },
                "summary": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss."
                },
                "authors": [
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Yicheng Xiao"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Xianbang Wang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Yingcong Chen"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Yukang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yukang Chen"
                },
                "author": "Yukang Chen",
                "arxiv_comment": "Code, model, and demos are available at\n  https://github.com/NVlabs/LongLive",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22621v1",
                "updated": "2025-09-26T17:46:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    46,
                    32,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:46:32Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    46,
                    32,
                    4,
                    269,
                    0
                ],
                "title": "IA2: Alignment with ICL Activations Improves Supervised Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IA2: Alignment with ICL Activations Improves Supervised Fine-Tuning"
                },
                "summary": "Supervised Fine-Tuning (SFT) is used to specialize model behavior by training\nweights to produce intended target responses for queries. In contrast,\nIn-Context Learning (ICL) adapts models during inference with instructions or\ndemonstrations in the prompt. ICL can offer better generalizability and more\ncalibrated responses compared to SFT in data scarce settings, at the cost of\nmore inference compute. In this work, we ask the question: Can ICL's internal\ncomputations be used to improve the qualities of SFT? We first show that ICL\nand SFT produce distinct activation patterns, indicating that the two methods\nachieve adaptation through different functional mechanisms. Motivated by this\nobservation and to use ICL's rich functionality, we introduce ICL Activation\nAlignment (IA2), a self-distillation technique which aims to replicate ICL's\nactivation patterns in SFT models and incentivizes ICL-like internal reasoning.\nPerforming IA2 as a priming step before SFT significantly improves the accuracy\nand calibration of model outputs, as shown by our extensive empirical results\non 12 popular benchmarks and 2 model families. This finding is not only\npractically useful, but also offers a conceptual window into the inner\nmechanics of model adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised Fine-Tuning (SFT) is used to specialize model behavior by training\nweights to produce intended target responses for queries. In contrast,\nIn-Context Learning (ICL) adapts models during inference with instructions or\ndemonstrations in the prompt. ICL can offer better generalizability and more\ncalibrated responses compared to SFT in data scarce settings, at the cost of\nmore inference compute. In this work, we ask the question: Can ICL's internal\ncomputations be used to improve the qualities of SFT? We first show that ICL\nand SFT produce distinct activation patterns, indicating that the two methods\nachieve adaptation through different functional mechanisms. Motivated by this\nobservation and to use ICL's rich functionality, we introduce ICL Activation\nAlignment (IA2), a self-distillation technique which aims to replicate ICL's\nactivation patterns in SFT models and incentivizes ICL-like internal reasoning.\nPerforming IA2 as a priming step before SFT significantly improves the accuracy\nand calibration of model outputs, as shown by our extensive empirical results\non 12 popular benchmarks and 2 model families. This finding is not only\npractically useful, but also offers a conceptual window into the inner\nmechanics of model adaptation."
                },
                "authors": [
                    {
                        "name": "Aayush Mishra"
                    },
                    {
                        "name": "Daniel Khashabi"
                    },
                    {
                        "name": "Anqi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Anqi Liu"
                },
                "author": "Anqi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17967v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17967v3",
                "updated": "2025-09-26T17:42:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    42,
                    23,
                    4,
                    269,
                    0
                ],
                "published": "2025-05-23T14:37:00Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    14,
                    37,
                    0,
                    4,
                    143,
                    0
                ],
                "title": "FFT-based Dynamic Subspace Selection for Low-Rank Adaptive Optimization\n  of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FFT-based Dynamic Subspace Selection for Low-Rank Adaptive Optimization\n  of Large Language Models"
                },
                "summary": "Low-rank optimization has emerged as a promising direction in training large\nlanguage models (LLMs) to improve running time and reduce the memory usage of\nadaptive optimizers by constraining learning to a lower-dimensional space.\nPrior work typically projects gradients of linear layers using approaches based\non Singular Value Decomposition (SVD) or QR-decomposition. Applying these\ntechniques individually to each layer in large models is computationally\nexpensive and incurs additional memory costs due to storing the projection\nmatrices. In this work, we propose a computationally efficient and conceptually\nsimple, two-step procedure to approximate SVD/QR-based gradient projections\ninto lower-dimensional spaces by using a predefined orthogonal matrix of the\nDiscrete Cosine Transform (DCT). We dynamically select columns from the DCT\nmatrix based on their alignment with the gradient of each layer. The effective\nprojection matrices are obtained via a simple matmul with the DCT matrix in\n$O(n^3)$ time, followed by a lightweight sorting step to identify the most\nrelevant basis vectors. For large layers, DCT can be computed via Makhoul's\n$N$-point algorithm based on Fast Fourier Transform (FFT) in $O(n^2 \\log(n))$\ntime. Due to the predefined nature of the orthogonal bases, they are computed\nonce at the start of training. Our numerical experiments on both pre-training\nand fine-tuning tasks demonstrate the effectiveness of our dual strategy in\napproximating optimal low-rank projections, obtaining an approach with\nrank-independent running time that matches the performance of costly\nSVD/QR-based methods while achieving faster runtime and reduced memory usage by\nup to $25\\%$ across different model sizes. Our code is available at\n\\href{https://github.com/IST-DASLab/ISTA-DASLab-Optimizers/tree/main/ista_daslab_optimizers/fft_low_rank}{ISTA-DASLab-Optimizers}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rank optimization has emerged as a promising direction in training large\nlanguage models (LLMs) to improve running time and reduce the memory usage of\nadaptive optimizers by constraining learning to a lower-dimensional space.\nPrior work typically projects gradients of linear layers using approaches based\non Singular Value Decomposition (SVD) or QR-decomposition. Applying these\ntechniques individually to each layer in large models is computationally\nexpensive and incurs additional memory costs due to storing the projection\nmatrices. In this work, we propose a computationally efficient and conceptually\nsimple, two-step procedure to approximate SVD/QR-based gradient projections\ninto lower-dimensional spaces by using a predefined orthogonal matrix of the\nDiscrete Cosine Transform (DCT). We dynamically select columns from the DCT\nmatrix based on their alignment with the gradient of each layer. The effective\nprojection matrices are obtained via a simple matmul with the DCT matrix in\n$O(n^3)$ time, followed by a lightweight sorting step to identify the most\nrelevant basis vectors. For large layers, DCT can be computed via Makhoul's\n$N$-point algorithm based on Fast Fourier Transform (FFT) in $O(n^2 \\log(n))$\ntime. Due to the predefined nature of the orthogonal bases, they are computed\nonce at the start of training. Our numerical experiments on both pre-training\nand fine-tuning tasks demonstrate the effectiveness of our dual strategy in\napproximating optimal low-rank projections, obtaining an approach with\nrank-independent running time that matches the performance of costly\nSVD/QR-based methods while achieving faster runtime and reduced memory usage by\nup to $25\\%$ across different model sizes. Our code is available at\n\\href{https://github.com/IST-DASLab/ISTA-DASLab-Optimizers/tree/main/ista_daslab_optimizers/fft_low_rank}{ISTA-DASLab-Optimizers}."
                },
                "authors": [
                    {
                        "name": "Ionut-Vlad Modoranu"
                    },
                    {
                        "name": "Mher Safaryan"
                    },
                    {
                        "name": "Erik Schultheis"
                    },
                    {
                        "name": "Max Ryabinin"
                    },
                    {
                        "name": "Artem Chumachenko"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17967v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17967v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10823v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10823v3",
                "updated": "2025-09-26T17:40:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    40,
                    31,
                    4,
                    269,
                    0
                ],
                "published": "2025-04-15T02:54:16Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    2,
                    54,
                    16,
                    1,
                    105,
                    0
                ],
                "title": "CLASH: Evaluating Language Models on Judging High-Stakes Dilemmas from\n  Multiple Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLASH: Evaluating Language Models on Judging High-Stakes Dilemmas from\n  Multiple Perspectives"
                },
                "summary": "Navigating dilemmas involving conflicting values is challenging even for\nhumans in high-stakes domains, let alone for AI, yet prior work has been\nlimited to everyday scenarios. To close this gap, we introduce CLASH (Character\nperspective-based LLM Assessments in Situations with High-stakes), a\nmeticulously curated dataset consisting of 345 high-impact dilemmas along with\n3,795 individual perspectives of diverse values. CLASH enables the study of\ncritical yet underexplored aspects of value-based decision-making processes,\nincluding understanding of decision ambivalence and psychological discomfort as\nwell as capturing the temporal shifts of values in the perspectives of\ncharacters. By benchmarking 14 non-thinking and thinking models, we uncover\nseveral key findings. (1) Even strong proprietary models, such as GPT-5 and\nClaude-4-Sonnet, struggle with ambivalent decisions, achieving only 24.06 and\n51.01 accuracy. (2) Although LLMs reasonably predict psychological discomfort,\nthey do not adequately comprehend perspectives involving value shifts. (3)\nCognitive behaviors that are effective in the math-solving and game strategy\ndomains do not transfer to value reasoning. Instead, new failure patterns\nemerge, including early commitment and overcommitment. (4) The steerability of\nLLMs towards a given value is significantly correlated with their value\npreferences. (5) Finally, LLMs exhibit greater steerability when reasoning from\na third-party perspective, although certain values (e.g., safety) benefit\nuniquely from first-person framing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating dilemmas involving conflicting values is challenging even for\nhumans in high-stakes domains, let alone for AI, yet prior work has been\nlimited to everyday scenarios. To close this gap, we introduce CLASH (Character\nperspective-based LLM Assessments in Situations with High-stakes), a\nmeticulously curated dataset consisting of 345 high-impact dilemmas along with\n3,795 individual perspectives of diverse values. CLASH enables the study of\ncritical yet underexplored aspects of value-based decision-making processes,\nincluding understanding of decision ambivalence and psychological discomfort as\nwell as capturing the temporal shifts of values in the perspectives of\ncharacters. By benchmarking 14 non-thinking and thinking models, we uncover\nseveral key findings. (1) Even strong proprietary models, such as GPT-5 and\nClaude-4-Sonnet, struggle with ambivalent decisions, achieving only 24.06 and\n51.01 accuracy. (2) Although LLMs reasonably predict psychological discomfort,\nthey do not adequately comprehend perspectives involving value shifts. (3)\nCognitive behaviors that are effective in the math-solving and game strategy\ndomains do not transfer to value reasoning. Instead, new failure patterns\nemerge, including early commitment and overcommitment. (4) The steerability of\nLLMs towards a given value is significantly correlated with their value\npreferences. (5) Finally, LLMs exhibit greater steerability when reasoning from\na third-party perspective, although certain values (e.g., safety) benefit\nuniquely from first-person framing."
                },
                "authors": [
                    {
                        "name": "Ayoung Lee"
                    },
                    {
                        "name": "Ryan Sungmo Kwon"
                    },
                    {
                        "name": "Peter Railton"
                    },
                    {
                        "name": "Lu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Wang"
                },
                "author": "Lu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10823v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10823v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22613v1",
                "updated": "2025-09-26T17:39:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    39,
                    48,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:39:48Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    39,
                    48,
                    4,
                    269,
                    0
                ],
                "title": "Benefits and Pitfalls of Reinforcement Learning for Language Model\n  Planning: A Theoretical Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benefits and Pitfalls of Reinforcement Learning for Language Model\n  Planning: A Theoretical Perspective"
                },
                "summary": "Recent reinforcement learning (RL) methods have substantially enhanced the\nplanning capabilities of Large Language Models (LLMs), yet the theoretical\nbasis for their effectiveness remains elusive. In this work, we investigate\nRL's benefits and limitations through a tractable graph-based abstraction,\nfocusing on policy gradient (PG) and Q-learning methods. Our theoretical\nanalyses reveal that supervised fine-tuning (SFT) may introduce\nco-occurrence-based spurious solutions, whereas RL achieves correct planning\nprimarily through exploration, underscoring exploration's role in enabling\nbetter generalization. However, we also show that PG suffers from diversity\ncollapse, where output diversity decreases during training and persists even\nafter perfect accuracy is attained. By contrast, Q-learning provides two key\nadvantages: off-policy learning and diversity preservation at convergence. We\nfurther demonstrate that careful reward design is necessary to prevent reward\nhacking in Q-learning. Finally, applying our framework to the real-world\nplanning benchmark Blocksworld, we confirm that these behaviors manifest in\npractice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent reinforcement learning (RL) methods have substantially enhanced the\nplanning capabilities of Large Language Models (LLMs), yet the theoretical\nbasis for their effectiveness remains elusive. In this work, we investigate\nRL's benefits and limitations through a tractable graph-based abstraction,\nfocusing on policy gradient (PG) and Q-learning methods. Our theoretical\nanalyses reveal that supervised fine-tuning (SFT) may introduce\nco-occurrence-based spurious solutions, whereas RL achieves correct planning\nprimarily through exploration, underscoring exploration's role in enabling\nbetter generalization. However, we also show that PG suffers from diversity\ncollapse, where output diversity decreases during training and persists even\nafter perfect accuracy is attained. By contrast, Q-learning provides two key\nadvantages: off-policy learning and diversity preservation at convergence. We\nfurther demonstrate that careful reward design is necessary to prevent reward\nhacking in Q-learning. Finally, applying our framework to the real-world\nplanning benchmark Blocksworld, we confirm that these behaviors manifest in\npractice."
                },
                "authors": [
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "Yifei Shen"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Shi Feng"
                    },
                    {
                        "name": "Shang-Hua Teng"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Yaru Hao"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15735v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15735v2",
                "updated": "2025-09-26T17:38:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    38,
                    41,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-19T08:05:28Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    8,
                    5,
                    28,
                    4,
                    262,
                    0
                ],
                "title": "EigenTrack: Spectral Activation Feature Tracking for Hallucination and\n  Out-of-Distribution Detection in LLMs and VLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EigenTrack: Spectral Activation Feature Tracking for Hallucination and\n  Out-of-Distribution Detection in LLMs and VLMs"
                },
                "summary": "Large language models (LLMs) offer broad utility but remain prone to\nhallucination and out-of-distribution (OOD) errors. We propose EigenTrack, an\ninterpretable real-time detector that uses the spectral geometry of hidden\nactivations, a compact global signature of model dynamics. By streaming\ncovariance-spectrum statistics such as entropy, eigenvalue gaps, and KL\ndivergence from random baselines into a lightweight recurrent classifier,\nEigenTrack tracks temporal shifts in representation structure that signal\nhallucination and OOD drift before surface errors appear. Unlike black- and\ngrey-box methods, it needs only a single forward pass without resampling.\nUnlike existing white-box detectors, it preserves temporal context, aggregates\nglobal signals, and offers interpretable accuracy-latency trade-offs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) offer broad utility but remain prone to\nhallucination and out-of-distribution (OOD) errors. We propose EigenTrack, an\ninterpretable real-time detector that uses the spectral geometry of hidden\nactivations, a compact global signature of model dynamics. By streaming\ncovariance-spectrum statistics such as entropy, eigenvalue gaps, and KL\ndivergence from random baselines into a lightweight recurrent classifier,\nEigenTrack tracks temporal shifts in representation structure that signal\nhallucination and OOD drift before surface errors appear. Unlike black- and\ngrey-box methods, it needs only a single forward pass without resampling.\nUnlike existing white-box detectors, it preserves temporal context, aggregates\nglobal signals, and offers interpretable accuracy-latency trade-offs."
                },
                "authors": [
                    {
                        "name": "Davide Ettori"
                    },
                    {
                        "name": "Nastaran Darabi"
                    },
                    {
                        "name": "Sina Tayebati"
                    },
                    {
                        "name": "Ranganath Krishnan"
                    },
                    {
                        "name": "Mahesh Subedar"
                    },
                    {
                        "name": "Omesh Tickoo"
                    },
                    {
                        "name": "Amit Ranjan Trivedi"
                    }
                ],
                "author_detail": {
                    "name": "Amit Ranjan Trivedi"
                },
                "author": "Amit Ranjan Trivedi",
                "arxiv_comment": "5 pages, submitted to ICASSP 2026, September 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15735v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15735v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22611v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22611v1",
                "updated": "2025-09-26T17:37:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    37,
                    52,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:37:52Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    37,
                    52,
                    4,
                    269,
                    0
                ],
                "title": "Quantile Advantage Estimation for Entropy-Safe Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantile Advantage Estimation for Entropy-Safe Reasoning"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) strengthens LLM\nreasoning, but training often oscillates between {entropy collapse} and\n{entropy explosion}. We trace both hazards to the mean baseline used in\nvalue-free RL (e.g., GRPO and DAPO), which improperly penalizes\nnegative-advantage samples under reward outliers. We propose {Quantile\nAdvantage Estimation} (QAE), replacing the mean with a group-wise K-quantile\nbaseline. QAE induces a response-level, two-regime gate: on hard queries (p <=\n1 - K) it reinforces rare successes, while on easy queries (p > 1 - K) it\ntargets remaining failures. Under first-order softmax updates, we prove\n{two-sided entropy safety}, giving lower and upper bounds on one-step entropy\nchange that curb explosion and prevent collapse. Empirically, this minimal\nmodification stabilizes entropy, sparsifies credit assignment (with tuned K,\nroughly 80% of responses receive zero advantage), and yields sustained pass@1\ngains on Qwen3-8B/14B-Base across AIME 2024/2025 and AMC 2023. These results\nidentify {baseline design} -- rather than token-level heuristics -- as the\nprimary mechanism for scaling RLVR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) strengthens LLM\nreasoning, but training often oscillates between {entropy collapse} and\n{entropy explosion}. We trace both hazards to the mean baseline used in\nvalue-free RL (e.g., GRPO and DAPO), which improperly penalizes\nnegative-advantage samples under reward outliers. We propose {Quantile\nAdvantage Estimation} (QAE), replacing the mean with a group-wise K-quantile\nbaseline. QAE induces a response-level, two-regime gate: on hard queries (p <=\n1 - K) it reinforces rare successes, while on easy queries (p > 1 - K) it\ntargets remaining failures. Under first-order softmax updates, we prove\n{two-sided entropy safety}, giving lower and upper bounds on one-step entropy\nchange that curb explosion and prevent collapse. Empirically, this minimal\nmodification stabilizes entropy, sparsifies credit assignment (with tuned K,\nroughly 80% of responses receive zero advantage), and yields sustained pass@1\ngains on Qwen3-8B/14B-Base across AIME 2024/2025 and AMC 2023. These results\nidentify {baseline design} -- rather than token-level heuristics -- as the\nprimary mechanism for scaling RLVR."
                },
                "authors": [
                    {
                        "name": "Junkang Wu"
                    },
                    {
                        "name": "Kexin Huang"
                    },
                    {
                        "name": "Jiancan Wu"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22611v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05723v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05723v3",
                "updated": "2025-09-26T17:37:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    37,
                    0,
                    4,
                    269,
                    0
                ],
                "published": "2024-12-07T18:49:27Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    18,
                    49,
                    27,
                    5,
                    342,
                    0
                ],
                "title": "Training-Free Bayesianization for Low-Rank Adapters of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Bayesianization for Low-Rank Adapters of Large Language\n  Models"
                },
                "summary": "Estimating the uncertainty of responses from Large Language Models (LLMs)\nremains a critical challenge. While recent Bayesian methods have demonstrated\neffectiveness in quantifying uncertainty through low-rank weight updates, they\ntypically require complex fine-tuning or post-training procedures. In this\npaper, we propose Training-Free Bayesianization (TFB), a simple yet\ntheoretically grounded framework that efficiently transforms trained low-rank\nadapters into Bayesian ones without additional training. TFB systematically\nsearches for the maximally acceptable level of variance in the weight\nposterior, constrained within a family of low-rank isotropic Gaussian\ndistributions. Our theoretical analysis shows that under mild conditions, this\nsearch process is equivalent to KL-regularized variational optimization, a\ngeneralized form of variational inference. Through comprehensive experiments,\nwe show that TFB achieves superior uncertainty estimation and generalization\ncompared to existing methods while eliminating the need for complex\nBayesianization training procedures. Code will be available at\nhttps://github.com/Wang-ML-Lab/bayesian-peft.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating the uncertainty of responses from Large Language Models (LLMs)\nremains a critical challenge. While recent Bayesian methods have demonstrated\neffectiveness in quantifying uncertainty through low-rank weight updates, they\ntypically require complex fine-tuning or post-training procedures. In this\npaper, we propose Training-Free Bayesianization (TFB), a simple yet\ntheoretically grounded framework that efficiently transforms trained low-rank\nadapters into Bayesian ones without additional training. TFB systematically\nsearches for the maximally acceptable level of variance in the weight\nposterior, constrained within a family of low-rank isotropic Gaussian\ndistributions. Our theoretical analysis shows that under mild conditions, this\nsearch process is equivalent to KL-regularized variational optimization, a\ngeneralized form of variational inference. Through comprehensive experiments,\nwe show that TFB achieves superior uncertainty estimation and generalization\ncompared to existing methods while eliminating the need for complex\nBayesianization training procedures. Code will be available at\nhttps://github.com/Wang-ML-Lab/bayesian-peft."
                },
                "authors": [
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Yibin Wang"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Huan Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "arxiv_comment": "Accepted at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05723v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05723v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15724v2",
                "updated": "2025-09-26T17:34:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    34,
                    41,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-19T07:53:55Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    7,
                    53,
                    55,
                    4,
                    262,
                    0
                ],
                "title": "RMT-KD: Random Matrix Theoretic Causal Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RMT-KD: Random Matrix Theoretic Causal Knowledge Distillation"
                },
                "summary": "Large deep learning models such as BERT and ResNet achieve state-of-the-art\nperformance but are costly to deploy at the edge due to their size and compute\ndemands. We present RMT-KD, a compression method that leverages Random Matrix\nTheory (RMT) for knowledge distillation to iteratively reduce network size.\nInstead of pruning or heuristic rank selection, RMT-KD preserves only\ninformative directions identified via the spectral properties of hidden\nrepresentations. RMT-based causal reduction is applied layer by layer with\nself-distillation to maintain stability and accuracy. On GLUE, AG News, and\nCIFAR-10, RMT-KD achieves up to 80% parameter reduction with only 2% accuracy\nloss, delivering 2.8x faster inference and nearly halved power consumption.\nThese results establish RMT-KD as a mathematically grounded approach to network\ndistillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large deep learning models such as BERT and ResNet achieve state-of-the-art\nperformance but are costly to deploy at the edge due to their size and compute\ndemands. We present RMT-KD, a compression method that leverages Random Matrix\nTheory (RMT) for knowledge distillation to iteratively reduce network size.\nInstead of pruning or heuristic rank selection, RMT-KD preserves only\ninformative directions identified via the spectral properties of hidden\nrepresentations. RMT-based causal reduction is applied layer by layer with\nself-distillation to maintain stability and accuracy. On GLUE, AG News, and\nCIFAR-10, RMT-KD achieves up to 80% parameter reduction with only 2% accuracy\nloss, delivering 2.8x faster inference and nearly halved power consumption.\nThese results establish RMT-KD as a mathematically grounded approach to network\ndistillation."
                },
                "authors": [
                    {
                        "name": "Davide Ettori"
                    },
                    {
                        "name": "Nastaran Darabi"
                    },
                    {
                        "name": "Sureshkumar Senthilkumar"
                    },
                    {
                        "name": "Amit Ranjan Trivedi"
                    }
                ],
                "author_detail": {
                    "name": "Amit Ranjan Trivedi"
                },
                "author": "Amit Ranjan Trivedi",
                "arxiv_comment": "5 pages, submitted to ICASSP 2026, September 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16355v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16355v2",
                "updated": "2025-09-26T17:21:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    21,
                    50,
                    4,
                    269,
                    0
                ],
                "published": "2025-01-20T01:39:03Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    1,
                    39,
                    3,
                    0,
                    20,
                    0
                ],
                "title": "How Strategic Agents Respond: Comparing Analytical Models with\n  LLM-Generated Responses in Strategic Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Strategic Agents Respond: Comparing Analytical Models with\n  LLM-Generated Responses in Strategic Classification"
                },
                "summary": "When ML algorithms are deployed to automate human-related decisions, human\nagents may learn the underlying decision policies and adapt their behavior.\nStrategic Classification (SC) has emerged as a framework for studying this\ninteraction between agents and decision-makers to design more trustworthy ML\nsystems. Prior theoretical models in SC assume that agents are perfectly or\napproximately rational and respond to decision policies by optimizing their\nutility. However, the growing prevalence of LLMs raises the possibility that\nreal-world agents may instead rely on these tools for strategic advice. This\nshift prompts two questions: (i) Can LLMs generate effective and socially\nresponsible strategies in SC settings? (ii) Can existing SC theoretical models\naccurately capture agent behavior when agents follow LLM-generated advice? To\ninvestigate these questions, we examine five critical SC scenarios: hiring,\nloan applications, school admissions, personal income, and public assistance\nprograms. We simulate agents with diverse profiles who interact with three\ncommercial LLMs (GPT-4o, GPT-4.1, and GPT-5), following their suggestions on\neffort allocations on features. We compare the resulting agent behaviors with\nthe best responses in existing SC models. Our findings show that: (i) Even\nwithout access to the decision policy, LLMs can generate effective strategies\nthat improve both agents' scores and qualification; (ii) At the population\nlevel, LLM-guided effort allocation strategies yield similar or even higher\nscore improvements, qualification rates, and fairness metrics as those\npredicted by the SC theoretical model, suggesting that the theoretical model\nmay still serve as a reasonable proxy for LLM-influenced behavior; and (iii) At\nthe individual level, LLMs tend to produce more diverse and balanced effort\nallocations than theoretical models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When ML algorithms are deployed to automate human-related decisions, human\nagents may learn the underlying decision policies and adapt their behavior.\nStrategic Classification (SC) has emerged as a framework for studying this\ninteraction between agents and decision-makers to design more trustworthy ML\nsystems. Prior theoretical models in SC assume that agents are perfectly or\napproximately rational and respond to decision policies by optimizing their\nutility. However, the growing prevalence of LLMs raises the possibility that\nreal-world agents may instead rely on these tools for strategic advice. This\nshift prompts two questions: (i) Can LLMs generate effective and socially\nresponsible strategies in SC settings? (ii) Can existing SC theoretical models\naccurately capture agent behavior when agents follow LLM-generated advice? To\ninvestigate these questions, we examine five critical SC scenarios: hiring,\nloan applications, school admissions, personal income, and public assistance\nprograms. We simulate agents with diverse profiles who interact with three\ncommercial LLMs (GPT-4o, GPT-4.1, and GPT-5), following their suggestions on\neffort allocations on features. We compare the resulting agent behaviors with\nthe best responses in existing SC models. Our findings show that: (i) Even\nwithout access to the decision policy, LLMs can generate effective strategies\nthat improve both agents' scores and qualification; (ii) At the population\nlevel, LLM-guided effort allocation strategies yield similar or even higher\nscore improvements, qualification rates, and fairness metrics as those\npredicted by the SC theoretical model, suggesting that the theoretical model\nmay still serve as a reasonable proxy for LLM-influenced behavior; and (iii) At\nthe individual level, LLMs tend to produce more diverse and balanced effort\nallocations than theoretical models."
                },
                "authors": [
                    {
                        "name": "Tian Xie"
                    },
                    {
                        "name": "Pavan Rauch"
                    },
                    {
                        "name": "Xueru Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xueru Zhang"
                },
                "author": "Xueru Zhang",
                "arxiv_comment": "Add GPT 5 experiments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16355v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16355v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22601v1",
                "updated": "2025-09-26T17:20:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    20,
                    38,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:20:38Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    20,
                    38,
                    4,
                    269,
                    0
                ],
                "title": "Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive\n  Exploration for Agentic Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive\n  Exploration for Agentic Reinforcement Learning"
                },
                "summary": "Reinforcement learning (RL) is the dominant paradigm for sharpening strategic\ntool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks,\nyet it faces a fundamental challenge of exploration-exploitation trade-off.\nExisting studies stimulate exploration through the lens of policy entropy, but\nsuch mechanical entropy maximization is prone to RL training instability due to\nthe multi-turn distribution shifting. In this paper, we target the progressive\nexploration-exploitation balance under the guidance of the agent own\nexperiences without succumbing to either entropy collapsing or runaway\ndivergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL)\nrecipe for training agentic LLMs. It extends the vanilla SIL framework, where a\nreplay buffer stores self-generated promising trajectories for off-policy\nupdate, by gradually steering the policy evolution within a well-balanced range\nof entropy across stages. Specifically, our approach incorporates a curriculum\nto manage the exploration process, utilizing intrinsic rewards to foster\nskill-level exploration and facilitating action-level exploration through SIL.\nAt first, the auxiliary tool call reward plays a critical role in the\naccumulation of tool-use skills, enabling broad exposure to the unfamiliar\ndistributions of the environment feedback with an upward entropy trend. As\ntraining progresses, self-imitation gets strengthened to exploit existing\nsuccessful patterns from replayed experiences for comparative action-level\nexploration, accelerating solution iteration without unbounded entropy growth.\nTo further stabilize training, we recalibrate the advantages of experiences in\nthe replay buffer to address the potential policy drift. Reugularizations such\nas the clipping of tokens with high covariance between probability and\nadvantage are introduced to the trajectory-level entropy control to curb\nover-confidence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) is the dominant paradigm for sharpening strategic\ntool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks,\nyet it faces a fundamental challenge of exploration-exploitation trade-off.\nExisting studies stimulate exploration through the lens of policy entropy, but\nsuch mechanical entropy maximization is prone to RL training instability due to\nthe multi-turn distribution shifting. In this paper, we target the progressive\nexploration-exploitation balance under the guidance of the agent own\nexperiences without succumbing to either entropy collapsing or runaway\ndivergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL)\nrecipe for training agentic LLMs. It extends the vanilla SIL framework, where a\nreplay buffer stores self-generated promising trajectories for off-policy\nupdate, by gradually steering the policy evolution within a well-balanced range\nof entropy across stages. Specifically, our approach incorporates a curriculum\nto manage the exploration process, utilizing intrinsic rewards to foster\nskill-level exploration and facilitating action-level exploration through SIL.\nAt first, the auxiliary tool call reward plays a critical role in the\naccumulation of tool-use skills, enabling broad exposure to the unfamiliar\ndistributions of the environment feedback with an upward entropy trend. As\ntraining progresses, self-imitation gets strengthened to exploit existing\nsuccessful patterns from replayed experiences for comparative action-level\nexploration, accelerating solution iteration without unbounded entropy growth.\nTo further stabilize training, we recalibrate the advantages of experiences in\nthe replay buffer to address the potential policy drift. Reugularizations such\nas the clipping of tokens with high covariance between probability and\nadvantage are introduced to the trajectory-level entropy control to curb\nover-confidence."
                },
                "authors": [
                    {
                        "name": "Yulei Qin"
                    },
                    {
                        "name": "Xiaoyu Tan"
                    },
                    {
                        "name": "Zhengbao He"
                    },
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Haojia Lin"
                    },
                    {
                        "name": "Zongyi Li"
                    },
                    {
                        "name": "Zihan Xu"
                    },
                    {
                        "name": "Yuchen Shi"
                    },
                    {
                        "name": "Siqi Cai"
                    },
                    {
                        "name": "Renting Rui"
                    },
                    {
                        "name": "Shaofei Cai"
                    },
                    {
                        "name": "Yuzheng Cai"
                    },
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Sheng Ye"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "26 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22597v1",
                "updated": "2025-09-26T17:17:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    17,
                    14,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:17:14Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    17,
                    14,
                    4,
                    269,
                    0
                ],
                "title": "A Nonparametric Bayesian Solution of the Empirical Stochastic Inverse\n  Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Nonparametric Bayesian Solution of the Empirical Stochastic Inverse\n  Problem"
                },
                "summary": "The stochastic inverse problem is a key ingredient in making inferences,\npredictions, and decisions for complex science and engineering systems. We\nformulate and analyze a nonparametric Bayesian solution for the stochastic\ninverse problem. Key properties of the solution are proved and the convergence\nand error of a computational solution obtained by random sampling is analyzed.\nSeveral applications illustrate the results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The stochastic inverse problem is a key ingredient in making inferences,\npredictions, and decisions for complex science and engineering systems. We\nformulate and analyze a nonparametric Bayesian solution for the stochastic\ninverse problem. Key properties of the solution are proved and the convergence\nand error of a computational solution obtained by random sampling is analyzed.\nSeveral applications illustrate the results."
                },
                "authors": [
                    {
                        "name": "Haiyi Shi"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Jiarui Chi"
                    },
                    {
                        "name": "Troy Butler"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Derek Bingham"
                    },
                    {
                        "name": "Don Estep"
                    }
                ],
                "author_detail": {
                    "name": "Don Estep"
                },
                "author": "Don Estep",
                "arxiv_comment": "48 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "Primary 62G05, 65C60 Secondary 62P30, 62P35, 60D05, 60A10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22592v1",
                "updated": "2025-09-26T17:12:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    12,
                    19,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:12:19Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    12,
                    19,
                    4,
                    269,
                    0
                ],
                "title": "Transport Based Mean Flows for Generative Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transport Based Mean Flows for Generative Modeling"
                },
                "summary": "Flow-matching generative models have emerged as a powerful paradigm for\ncontinuous data generation, achieving state-of-the-art results across domains\nsuch as images, 3D shapes, and point clouds. Despite their success, these\nmodels suffer from slow inference due to the requirement of numerous sequential\nsampling steps. Recent work has sought to accelerate inference by reducing the\nnumber of sampling steps. In particular, Mean Flows offer a one-step generation\napproach that delivers substantial speedups while retaining strong generative\nperformance. Yet, in many continuous domains, Mean Flows fail to faithfully\napproximate the behavior of the original multi-step flow-matching process. In\nthis work, we address this limitation by incorporating optimal transport-based\nsampling strategies into the Mean Flow framework, enabling one-step generators\nthat better preserve the fidelity and diversity of the original multi-step flow\nprocess. Experiments on controlled low-dimensional settings and on\nhigh-dimensional tasks such as image generation, image-to-image translation,\nand point cloud generation demonstrate that our approach achieves superior\ninference accuracy in one-step generative modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow-matching generative models have emerged as a powerful paradigm for\ncontinuous data generation, achieving state-of-the-art results across domains\nsuch as images, 3D shapes, and point clouds. Despite their success, these\nmodels suffer from slow inference due to the requirement of numerous sequential\nsampling steps. Recent work has sought to accelerate inference by reducing the\nnumber of sampling steps. In particular, Mean Flows offer a one-step generation\napproach that delivers substantial speedups while retaining strong generative\nperformance. Yet, in many continuous domains, Mean Flows fail to faithfully\napproximate the behavior of the original multi-step flow-matching process. In\nthis work, we address this limitation by incorporating optimal transport-based\nsampling strategies into the Mean Flow framework, enabling one-step generators\nthat better preserve the fidelity and diversity of the original multi-step flow\nprocess. Experiments on controlled low-dimensional settings and on\nhigh-dimensional tasks such as image generation, image-to-image translation,\nand point cloud generation demonstrate that our approach achieves superior\ninference accuracy in one-step generative modeling."
                },
                "authors": [
                    {
                        "name": "Elaheh Akbari"
                    },
                    {
                        "name": "Ping He"
                    },
                    {
                        "name": "Ahmadreza Moradipari"
                    },
                    {
                        "name": "Yikun Bai"
                    },
                    {
                        "name": "Soheil Kolouri"
                    }
                ],
                "author_detail": {
                    "name": "Soheil Kolouri"
                },
                "author": "Soheil Kolouri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03814v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03814v5",
                "updated": "2025-09-26T17:11:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    11,
                    25,
                    4,
                    269,
                    0
                ],
                "published": "2025-04-04T14:41:41Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    41,
                    41,
                    4,
                    94,
                    0
                ],
                "title": "Recursive Training Loops in LLMs: How training data properties modulate\n  distribution shift in generated data?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recursive Training Loops in LLMs: How training data properties modulate\n  distribution shift in generated data?"
                },
                "summary": "Large language models (LLMs) are increasingly used in the creation of online\ncontent, creating feedback loops as subsequent generations of models will be\ntrained on this synthetic data. Such loops were shown to lead to distribution\nshifts - models misrepresenting the true underlying distributions of human data\n(also called model collapse). However, how human data properties affect such\nshifts remains poorly understood. In this paper, we provide the first empirical\nexamination of the effect of such properties on the outcome of recursive\ntraining. We first confirm that using different human datasets leads to\ndistribution shifts of different magnitudes. Through exhaustive manipulation of\ndataset properties combined with regression analyses, we then identify a set of\nproperties predicting distribution shift magnitudes. Lexical diversity is found\nto amplify these shifts, while semantic diversity and data quality mitigate\nthem. Furthermore, we find that these influences are highly modular: data\nscrapped from a given internet domain has little influence on the content\ngenerated for another domain. Finally, experiments on political bias reveal\nthat human data properties affect whether the initial bias will be amplified or\nreduced. Overall, our results portray a novel view, where different parts of\ninternet may undergo different types of distribution shift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used in the creation of online\ncontent, creating feedback loops as subsequent generations of models will be\ntrained on this synthetic data. Such loops were shown to lead to distribution\nshifts - models misrepresenting the true underlying distributions of human data\n(also called model collapse). However, how human data properties affect such\nshifts remains poorly understood. In this paper, we provide the first empirical\nexamination of the effect of such properties on the outcome of recursive\ntraining. We first confirm that using different human datasets leads to\ndistribution shifts of different magnitudes. Through exhaustive manipulation of\ndataset properties combined with regression analyses, we then identify a set of\nproperties predicting distribution shift magnitudes. Lexical diversity is found\nto amplify these shifts, while semantic diversity and data quality mitigate\nthem. Furthermore, we find that these influences are highly modular: data\nscrapped from a given internet domain has little influence on the content\ngenerated for another domain. Finally, experiments on political bias reveal\nthat human data properties affect whether the initial bias will be amplified or\nreduced. Overall, our results portray a novel view, where different parts of\ninternet may undergo different types of distribution shift."
                },
                "authors": [
                    {
                        "name": "Grgur Kovač"
                    },
                    {
                        "name": "Jérémy Perez"
                    },
                    {
                        "name": "Rémy Portelas"
                    },
                    {
                        "name": "Peter Ford Dominey"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-Yves Oudeyer"
                },
                "author": "Pierre-Yves Oudeyer",
                "arxiv_comment": "Accepted to EMNLP 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03814v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03814v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22582v1",
                "updated": "2025-09-26T17:03:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    3,
                    24,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:03:24Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    3,
                    24,
                    4,
                    269,
                    0
                ],
                "title": "Fine-Grained Detection of Context-Grounded Hallucinations Using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained Detection of Context-Grounded Hallucinations Using LLMs"
                },
                "summary": "Context-grounded hallucinations are cases where model outputs contain\ninformation not verifiable against the source text. We study the applicability\nof LLMs for localizing such hallucinations, as a more practical alternative to\nexisting complex evaluation pipelines. In the absence of established benchmarks\nfor meta-evaluation of hallucinations localization, we construct one tailored\nto LLMs, involving a challenging human annotation of over 1,000 examples. We\ncomplement the benchmark with an LLM-based evaluation protocol, verifying its\nquality in a human evaluation. Since existing representations of hallucinations\nlimit the types of errors that can be expressed, we propose a new\nrepresentation based on free-form textual descriptions, capturing the full\nrange of possible errors. We conduct a comprehensive study, evaluating four\nlarge-scale LLMs, which highlights the benchmark's difficulty, as the best\nmodel achieves an F1 score of only 0.67. Through careful analysis, we offer\ninsights into optimal prompting strategies for the task and identify the main\nfactors that make it challenging for LLMs: (1) a tendency to incorrectly flag\nmissing details as inconsistent, despite being instructed to check only facts\nin the output; and (2) difficulty with outputs containing factually correct\ninformation absent from the source - and thus not verifiable - due to alignment\nwith the model's parametric knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-grounded hallucinations are cases where model outputs contain\ninformation not verifiable against the source text. We study the applicability\nof LLMs for localizing such hallucinations, as a more practical alternative to\nexisting complex evaluation pipelines. In the absence of established benchmarks\nfor meta-evaluation of hallucinations localization, we construct one tailored\nto LLMs, involving a challenging human annotation of over 1,000 examples. We\ncomplement the benchmark with an LLM-based evaluation protocol, verifying its\nquality in a human evaluation. Since existing representations of hallucinations\nlimit the types of errors that can be expressed, we propose a new\nrepresentation based on free-form textual descriptions, capturing the full\nrange of possible errors. We conduct a comprehensive study, evaluating four\nlarge-scale LLMs, which highlights the benchmark's difficulty, as the best\nmodel achieves an F1 score of only 0.67. Through careful analysis, we offer\ninsights into optimal prompting strategies for the task and identify the main\nfactors that make it challenging for LLMs: (1) a tendency to incorrectly flag\nmissing details as inconsistent, despite being instructed to check only facts\nin the output; and (2) difficulty with outputs containing factually correct\ninformation absent from the source - and thus not verifiable - due to alignment\nwith the model's parametric knowledge."
                },
                "authors": [
                    {
                        "name": "Yehonatan Pesiakhovsky"
                    },
                    {
                        "name": "Zorik Gekhman"
                    },
                    {
                        "name": "Yosi Mass"
                    },
                    {
                        "name": "Liat Ein-Dor"
                    },
                    {
                        "name": "Roi Reichart"
                    }
                ],
                "author_detail": {
                    "name": "Roi Reichart"
                },
                "author": "Roi Reichart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22576v1",
                "updated": "2025-09-26T16:51:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    51,
                    44,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:51:44Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    51,
                    44,
                    4,
                    269,
                    0
                ],
                "title": "EPO: Entropy-regularized Policy Optimization for LLM Agents\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPO: Entropy-regularized Policy Optimization for LLM Agents\n  Reinforcement Learning"
                },
                "summary": "Training LLM agents in multi-turn environments with sparse rewards, where\ncompleting a single task requires 30+ turns of interaction within an episode,\npresents a fundamental challenge for reinforcement learning. We identify a\ncritical failure mode unique to this setting: the exploration-exploitation\ncascade failure. This cascade begins with early-stage policy premature\nconvergence, where sparse feedback causes agents to commit to flawed,\nlow-entropy strategies. Subsequently, agents enter late-stage policy collapse,\nwhere conventional entropy regularization becomes counterproductive, promoting\nchaotic exploration that destabilizes training. We propose Entropy-regularized\nPolicy Optimization (EPO), a general framework that breaks this failure cycle\nthrough three synergistic mechanisms: (1) adopting entropy regularization in\nmulti-turn settings to enhance exploration, (2) an entropy smoothing\nregularizer that bounds policy entropy within historical averages to prevent\nabrupt fluctuations, and (3) adaptive phase-based weighting that balances\nexploration and exploitation across training. Our analysis justifies that EPO\nguarantees monotonically decreasing entropy variance while maintaining\nconvergence. EPO achieves up to 152% performance improvement on ScienceWorld\nand up to 19.8% on ALFWorld. Our work demonstrates that multi-turn\nsparse-reward settings require fundamentally different entropy control than\ntraditional RL, with broad implications for LLM agent training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training LLM agents in multi-turn environments with sparse rewards, where\ncompleting a single task requires 30+ turns of interaction within an episode,\npresents a fundamental challenge for reinforcement learning. We identify a\ncritical failure mode unique to this setting: the exploration-exploitation\ncascade failure. This cascade begins with early-stage policy premature\nconvergence, where sparse feedback causes agents to commit to flawed,\nlow-entropy strategies. Subsequently, agents enter late-stage policy collapse,\nwhere conventional entropy regularization becomes counterproductive, promoting\nchaotic exploration that destabilizes training. We propose Entropy-regularized\nPolicy Optimization (EPO), a general framework that breaks this failure cycle\nthrough three synergistic mechanisms: (1) adopting entropy regularization in\nmulti-turn settings to enhance exploration, (2) an entropy smoothing\nregularizer that bounds policy entropy within historical averages to prevent\nabrupt fluctuations, and (3) adaptive phase-based weighting that balances\nexploration and exploitation across training. Our analysis justifies that EPO\nguarantees monotonically decreasing entropy variance while maintaining\nconvergence. EPO achieves up to 152% performance improvement on ScienceWorld\nand up to 19.8% on ALFWorld. Our work demonstrates that multi-turn\nsparse-reward settings require fundamentally different entropy control than\ntraditional RL, with broad implications for LLM agent training."
                },
                "authors": [
                    {
                        "name": "Xu Wujiang"
                    },
                    {
                        "name": "Wentian Zhao"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Li Yu-Jhe"
                    },
                    {
                        "name": "Jin Can"
                    },
                    {
                        "name": "Jin Mingyu"
                    },
                    {
                        "name": "Mei Kai"
                    },
                    {
                        "name": "Wan Kun"
                    },
                    {
                        "name": "Metaxas Dimitris"
                    }
                ],
                "author_detail": {
                    "name": "Metaxas Dimitris"
                },
                "author": "Metaxas Dimitris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22572v1",
                "updated": "2025-09-26T16:49:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    49,
                    10,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:49:10Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    49,
                    10,
                    4,
                    269,
                    0
                ],
                "title": "Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs\n  at Test Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs\n  at Test Time"
                },
                "summary": "Test-Time Scaling (TTS) enhances the reasoning ability of large language\nmodels (LLMs) by allocating additional computation during inference. However,\nexisting approaches primarily rely on output-level sampling while overlooking\nthe role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we\nobserve that varying the number of activated experts yields complementary\nsolution sets with stable accuracy, revealing a new and underexplored source of\ndiversity. Motivated by this observation, we propose Dynamic Experts Search\n(DES), a TTS strategy that elevates expert activation into a controllable\ndimension of the search space. DES integrates two key components: (1) Dynamic\nMoE, which enables direct control of expert counts during inference to generate\ndiverse reasoning trajectories without additional cost; and (2) Expert\nConfiguration Inheritance, which preserves consistent expert counts within a\nreasoning path while varying them across runs, thereby balancing stability and\ndiversity throughout the search. Extensive experiments across MoE\narchitectures, verifiers and reasoning benchmarks (i.e., math, code and\nknowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing\naccuracy and stability without additional cost. These results highlight DES as\na practical and scalable form of architecture-aware TTS, illustrating how\nstructural flexibility in modern LLMs can advance reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Scaling (TTS) enhances the reasoning ability of large language\nmodels (LLMs) by allocating additional computation during inference. However,\nexisting approaches primarily rely on output-level sampling while overlooking\nthe role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we\nobserve that varying the number of activated experts yields complementary\nsolution sets with stable accuracy, revealing a new and underexplored source of\ndiversity. Motivated by this observation, we propose Dynamic Experts Search\n(DES), a TTS strategy that elevates expert activation into a controllable\ndimension of the search space. DES integrates two key components: (1) Dynamic\nMoE, which enables direct control of expert counts during inference to generate\ndiverse reasoning trajectories without additional cost; and (2) Expert\nConfiguration Inheritance, which preserves consistent expert counts within a\nreasoning path while varying them across runs, thereby balancing stability and\ndiversity throughout the search. Extensive experiments across MoE\narchitectures, verifiers and reasoning benchmarks (i.e., math, code and\nknowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing\naccuracy and stability without additional cost. These results highlight DES as\na practical and scalable form of architecture-aware TTS, illustrating how\nstructural flexibility in modern LLMs can advance reasoning."
                },
                "authors": [
                    {
                        "name": "Yixuan Han"
                    },
                    {
                        "name": "Fan Ma"
                    },
                    {
                        "name": "Ruijie Quan"
                    },
                    {
                        "name": "Yi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Yang"
                },
                "author": "Yi Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22565v1",
                "updated": "2025-09-26T16:42:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    42,
                    43,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:42:43Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    42,
                    43,
                    4,
                    269,
                    0
                ],
                "title": "Retrieval-Augmented Guardrails for AI-Drafted Patient-Portal Messages:\n  Error Taxonomy Construction and Large-Scale Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Guardrails for AI-Drafted Patient-Portal Messages:\n  Error Taxonomy Construction and Large-Scale Evaluation"
                },
                "summary": "Asynchronous patient-clinician messaging via EHR portals is a growing source\nof clinician workload, prompting interest in large language models (LLMs) to\nassist with draft responses. However, LLM outputs may contain clinical\ninaccuracies, omissions, or tone mismatches, making robust evaluation\nessential. Our contributions are threefold: (1) we introduce a clinically\ngrounded error ontology comprising 5 domains and 59 granular error codes,\ndeveloped through inductive coding and expert adjudication; (2) we develop a\nretrieval-augmented evaluation pipeline (RAEC) that leverages semantically\nsimilar historical message-response pairs to improve judgment quality; and (3)\nwe provide a two-stage prompting architecture using DSPy to enable scalable,\ninterpretable, and hierarchical error detection. Our approach assesses the\nquality of drafts both in isolation and with reference to similar past\nmessage-response pairs retrieved from institutional archives. Using a two-stage\nDSPy pipeline, we compared baseline and reference-enhanced evaluations on over\n1,500 patient messages. Retrieval context improved error identification in\ndomains such as clinical completeness and workflow appropriateness. Human\nvalidation on 100 messages demonstrated superior agreement (concordance = 50%\nvs. 33%) and performance (F1 = 0.500 vs. 0.256) of context-enhanced labels vs.\nbaseline, supporting the use of our RAEC pipeline as AI guardrails for patient\nmessaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asynchronous patient-clinician messaging via EHR portals is a growing source\nof clinician workload, prompting interest in large language models (LLMs) to\nassist with draft responses. However, LLM outputs may contain clinical\ninaccuracies, omissions, or tone mismatches, making robust evaluation\nessential. Our contributions are threefold: (1) we introduce a clinically\ngrounded error ontology comprising 5 domains and 59 granular error codes,\ndeveloped through inductive coding and expert adjudication; (2) we develop a\nretrieval-augmented evaluation pipeline (RAEC) that leverages semantically\nsimilar historical message-response pairs to improve judgment quality; and (3)\nwe provide a two-stage prompting architecture using DSPy to enable scalable,\ninterpretable, and hierarchical error detection. Our approach assesses the\nquality of drafts both in isolation and with reference to similar past\nmessage-response pairs retrieved from institutional archives. Using a two-stage\nDSPy pipeline, we compared baseline and reference-enhanced evaluations on over\n1,500 patient messages. Retrieval context improved error identification in\ndomains such as clinical completeness and workflow appropriateness. Human\nvalidation on 100 messages demonstrated superior agreement (concordance = 50%\nvs. 33%) and performance (F1 = 0.500 vs. 0.256) of context-enhanced labels vs.\nbaseline, supporting the use of our RAEC pipeline as AI guardrails for patient\nmessaging."
                },
                "authors": [
                    {
                        "name": "Wenyuan Chen"
                    },
                    {
                        "name": "Fateme Nateghi Haredasht"
                    },
                    {
                        "name": "Kameron C. Black"
                    },
                    {
                        "name": "Francois Grolleau"
                    },
                    {
                        "name": "Emily Alsentzer"
                    },
                    {
                        "name": "Jonathan H. Chen"
                    },
                    {
                        "name": "Stephen P. Ma"
                    }
                ],
                "author_detail": {
                    "name": "Stephen P. Ma"
                },
                "author": "Stephen P. Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22561v1",
                "updated": "2025-09-26T16:41:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    41,
                    11,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:41:11Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    41,
                    11,
                    4,
                    269,
                    0
                ],
                "title": "Likelihood-free inference for gravitational-wave data analysis and\n  public alerts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Likelihood-free inference for gravitational-wave data analysis and\n  public alerts"
                },
                "summary": "Rapid and reliable detection and dissemination of source parameter estimation\ndata products from gravitational-wave events, especially sky localization, is\ncritical for maximizing the potential of multi-messenger astronomy. Machine\nlearning based detection and parameter estimation algorithms are emerging as\nproduction ready alternatives to traditional approaches. Here, we report\nvalidation studies of AMPLFI, a likelihood-free inference solution to\nlow-latency parameter estimation of binary black holes. We use simulated\nsignals added into data from the LIGO-Virgo-KAGRA's (LVK's) third observing run\n(O3) to compare sky localization performance with BAYESTAR, the algorithm\ncurrently in production for rapid sky localization of candidates from\nmatched-filter pipelines. We demonstrate sky localization performance, measured\nby searched area and volume, to be equivalent with BAYESTAR. We show accurate\nreconstruction of source parameters with uncertainties for use distributing\nlow-latency coarse-grained chirp mass information. In addition, we analyze\nseveral candidate events reported by the LVK in the third gravitational-wave\ntransient catalog (GWTC-3) and show consistency with the LVK's analysis.\nAltogether, we demonstrate AMPLFI's ability to produce data products for\nlow-latency public alerts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid and reliable detection and dissemination of source parameter estimation\ndata products from gravitational-wave events, especially sky localization, is\ncritical for maximizing the potential of multi-messenger astronomy. Machine\nlearning based detection and parameter estimation algorithms are emerging as\nproduction ready alternatives to traditional approaches. Here, we report\nvalidation studies of AMPLFI, a likelihood-free inference solution to\nlow-latency parameter estimation of binary black holes. We use simulated\nsignals added into data from the LIGO-Virgo-KAGRA's (LVK's) third observing run\n(O3) to compare sky localization performance with BAYESTAR, the algorithm\ncurrently in production for rapid sky localization of candidates from\nmatched-filter pipelines. We demonstrate sky localization performance, measured\nby searched area and volume, to be equivalent with BAYESTAR. We show accurate\nreconstruction of source parameters with uncertainties for use distributing\nlow-latency coarse-grained chirp mass information. In addition, we analyze\nseveral candidate events reported by the LVK in the third gravitational-wave\ntransient catalog (GWTC-3) and show consistency with the LVK's analysis.\nAltogether, we demonstrate AMPLFI's ability to produce data products for\nlow-latency public alerts."
                },
                "authors": [
                    {
                        "name": "Ethan Marx"
                    },
                    {
                        "name": "Deep Chatterjee"
                    },
                    {
                        "name": "Malina Desai"
                    },
                    {
                        "name": "Ravi Kumar"
                    },
                    {
                        "name": "William Benoit"
                    },
                    {
                        "name": "Argyro Sasli"
                    },
                    {
                        "name": "Leo Singer"
                    },
                    {
                        "name": "Michael W. Coughlin"
                    },
                    {
                        "name": "Philip Harris"
                    },
                    {
                        "name": "Erik Katsavounidis"
                    }
                ],
                "author_detail": {
                    "name": "Erik Katsavounidis"
                },
                "author": "Erik Katsavounidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22560v1",
                "updated": "2025-09-26T16:40:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    40,
                    44,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:40:44Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    40,
                    44,
                    4,
                    269,
                    0
                ],
                "title": "LLM-Augmented and Fair Machine Learning Framework for University\n  Admission Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Augmented and Fair Machine Learning Framework for University\n  Admission Prediction"
                },
                "summary": "Universities face surging applications and heightened expectations for\nfairness, making accurate admission prediction increasingly vital. This work\npresents a comprehensive framework that fuses machine learning, deep learning,\nand large language model techniques to combine structured academic and\ndemographic variables with unstructured text signals. Drawing on more than\n2,000 student records, the study benchmarks logistic regression, Naive Bayes,\nrandom forests, deep neural networks, and a stacked ensemble. Logistic\nregression offers a strong, interpretable baseline at 89.5% accuracy, while the\nstacked ensemble achieves the best performance at 91.0%, with Naive Bayes and\nrandom forests close behind. To probe text integration, GPT-4-simulated\nevaluations of personal statements are added as features, yielding modest gains\nbut demonstrating feasibility for authentic essays and recommendation letters.\nTransparency is ensured through feature-importance visualizations and fairness\naudits. The audits reveal a 9% gender gap (67% male vs. 76% female) and an 11%\ngap by parental education, underscoring the need for continued monitoring. The\nframework is interpretable, fairness-aware, and deployable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universities face surging applications and heightened expectations for\nfairness, making accurate admission prediction increasingly vital. This work\npresents a comprehensive framework that fuses machine learning, deep learning,\nand large language model techniques to combine structured academic and\ndemographic variables with unstructured text signals. Drawing on more than\n2,000 student records, the study benchmarks logistic regression, Naive Bayes,\nrandom forests, deep neural networks, and a stacked ensemble. Logistic\nregression offers a strong, interpretable baseline at 89.5% accuracy, while the\nstacked ensemble achieves the best performance at 91.0%, with Naive Bayes and\nrandom forests close behind. To probe text integration, GPT-4-simulated\nevaluations of personal statements are added as features, yielding modest gains\nbut demonstrating feasibility for authentic essays and recommendation letters.\nTransparency is ensured through feature-importance visualizations and fairness\naudits. The audits reveal a 9% gender gap (67% male vs. 76% female) and an 11%\ngap by parental education, underscoring the need for continued monitoring. The\nframework is interpretable, fairness-aware, and deployable."
                },
                "authors": [
                    {
                        "name": "Mohammad Abbadi"
                    },
                    {
                        "name": "Yassine Himeur"
                    },
                    {
                        "name": "Shadi Atalla"
                    },
                    {
                        "name": "Dahlia Mansoor"
                    },
                    {
                        "name": "Wathiq Mansoor"
                    }
                ],
                "author_detail": {
                    "name": "Wathiq Mansoor"
                },
                "author": "Wathiq Mansoor",
                "arxiv_comment": "The 9th International Symposium on Multidisciplinary Studies and\n  Innovative Technologies (ISMSIT)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22558v1",
                "updated": "2025-09-26T16:39:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    39,
                    10,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:39:10Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    39,
                    10,
                    4,
                    269,
                    0
                ],
                "title": "StepORLM: A Self-Evolving Framework With Generative Process Supervision\n  For Operations Research Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StepORLM: A Self-Evolving Framework With Generative Process Supervision\n  For Operations Research Language Models"
                },
                "summary": "Large Language Models (LLMs) have shown promising capabilities for solving\nOperations Research (OR) problems. While reinforcement learning serves as a\npowerful paradigm for LLM training on OR problems, existing works generally\nface two key limitations. First, outcome reward suffers from the credit\nassignment problem, where correct final answers can reinforce flawed reasoning.\nSecond, conventional discriminative process supervision is myopic, failing to\nevaluate the interdependent steps of OR modeling holistically. To this end, we\nintroduce StepORLM, a novel self-evolving framework with generative process\nsupervision. At its core, StepORLM features a co-evolutionary loop where a\npolicy model and a generative process reward model (GenPRM) iteratively improve\non each other. This loop is driven by a dual-feedback mechanism: definitive,\noutcome-based verification from an external solver, and nuanced, holistic\nprocess evaluation from the GenPRM. The combined signal is used to align the\npolicy via Weighted Direct Preference Optimization (W-DPO) and simultaneously\nrefine the GenPRM. Our resulting 8B-parameter StepORLM establishes a new\nstate-of-the-art across six benchmarks, significantly outperforming vastly\nlarger generalist models, agentic methods, and specialized baselines. Moreover,\nthe co-evolved GenPRM is able to act as a powerful and universally applicable\nprocess verifier, substantially boosting the inference scaling performance of\nboth our own model and other existing LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown promising capabilities for solving\nOperations Research (OR) problems. While reinforcement learning serves as a\npowerful paradigm for LLM training on OR problems, existing works generally\nface two key limitations. First, outcome reward suffers from the credit\nassignment problem, where correct final answers can reinforce flawed reasoning.\nSecond, conventional discriminative process supervision is myopic, failing to\nevaluate the interdependent steps of OR modeling holistically. To this end, we\nintroduce StepORLM, a novel self-evolving framework with generative process\nsupervision. At its core, StepORLM features a co-evolutionary loop where a\npolicy model and a generative process reward model (GenPRM) iteratively improve\non each other. This loop is driven by a dual-feedback mechanism: definitive,\noutcome-based verification from an external solver, and nuanced, holistic\nprocess evaluation from the GenPRM. The combined signal is used to align the\npolicy via Weighted Direct Preference Optimization (W-DPO) and simultaneously\nrefine the GenPRM. Our resulting 8B-parameter StepORLM establishes a new\nstate-of-the-art across six benchmarks, significantly outperforming vastly\nlarger generalist models, agentic methods, and specialized baselines. Moreover,\nthe co-evolved GenPRM is able to act as a powerful and universally applicable\nprocess verifier, substantially boosting the inference scaling performance of\nboth our own model and other existing LLMs."
                },
                "authors": [
                    {
                        "name": "Chenyu Zhou"
                    },
                    {
                        "name": "Tianyi Xu"
                    },
                    {
                        "name": "Jianghao Lin"
                    },
                    {
                        "name": "Dongdong Ge"
                    }
                ],
                "author_detail": {
                    "name": "Dongdong Ge"
                },
                "author": "Dongdong Ge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22557v1",
                "updated": "2025-09-26T16:39:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    39,
                    0,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:39:00Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    39,
                    0,
                    4,
                    269,
                    0
                ],
                "title": "Learning to Price Bundles: A GCN Approach for Mixed Bundling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Price Bundles: A GCN Approach for Mixed Bundling"
                },
                "summary": "Bundle pricing refers to designing several product combinations (i.e.,\nbundles) and determining their prices in order to maximize the expected profit.\nIt is a classic problem in revenue management and arises in many industries,\nsuch as e-commerce, tourism, and video games. However, the problem is typically\nintractable due to the exponential number of candidate bundles. In this paper,\nwe explore the usage of graph convolutional networks (GCNs) in solving the\nbundle pricing problem. Specifically, we first develop a graph representation\nof the mixed bundling model (where every possible bundle is assigned with a\nspecific price) and then train a GCN to learn the latent patterns of optimal\nbundles. Based on the trained GCN, we propose two inference strategies to\nderive high-quality feasible solutions. A local-search technique is further\nproposed to improve the solution quality. Numerical experiments validate the\neffectiveness and efficiency of our proposed GCN-based framework. Using a GCN\ntrained on instances with 5 products, our methods consistently achieve\nnear-optimal solutions (better than 97%) with only a fraction of computational\ntime for problems of small to medium size. It also achieves superior solutions\nfor larger size of problems compared with other heuristic methods such as\nbundle size pricing (BSP). The method can also provide high quality solutions\nfor instances with more than 30 products even for the challenging cases where\nproduct utilities are non-additive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bundle pricing refers to designing several product combinations (i.e.,\nbundles) and determining their prices in order to maximize the expected profit.\nIt is a classic problem in revenue management and arises in many industries,\nsuch as e-commerce, tourism, and video games. However, the problem is typically\nintractable due to the exponential number of candidate bundles. In this paper,\nwe explore the usage of graph convolutional networks (GCNs) in solving the\nbundle pricing problem. Specifically, we first develop a graph representation\nof the mixed bundling model (where every possible bundle is assigned with a\nspecific price) and then train a GCN to learn the latent patterns of optimal\nbundles. Based on the trained GCN, we propose two inference strategies to\nderive high-quality feasible solutions. A local-search technique is further\nproposed to improve the solution quality. Numerical experiments validate the\neffectiveness and efficiency of our proposed GCN-based framework. Using a GCN\ntrained on instances with 5 products, our methods consistently achieve\nnear-optimal solutions (better than 97%) with only a fraction of computational\ntime for problems of small to medium size. It also achieves superior solutions\nfor larger size of problems compared with other heuristic methods such as\nbundle size pricing (BSP). The method can also provide high quality solutions\nfor instances with more than 30 products even for the challenging cases where\nproduct utilities are non-additive."
                },
                "authors": [
                    {
                        "name": "Liangyu Ding"
                    },
                    {
                        "name": "Chenghan Wu"
                    },
                    {
                        "name": "Guokai Li"
                    },
                    {
                        "name": "Zizhuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zizhuo Wang"
                },
                "author": "Zizhuo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20900v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20900v2",
                "updated": "2025-09-26T16:37:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    37,
                    45,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-25T08:36:19Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    8,
                    36,
                    19,
                    3,
                    268,
                    0
                ],
                "title": "Learning to Summarize by Learning to Quiz: Adversarial Agentic\n  Collaboration for Long Document Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Summarize by Learning to Quiz: Adversarial Agentic\n  Collaboration for Long Document Summarization"
                },
                "summary": "Long document summarization remains a significant challenge for current large\nlanguage models (LLMs), as existing approaches commonly struggle with\ninformation loss, factual inconsistencies, and coherence issues when processing\nexcessively long documents. We propose SummQ, a novel adversarial multi-agent\nframework that addresses these limitations through collaborative intelligence\nbetween specialized agents operating in two complementary domains:\nsummarization and quizzing. Our approach employs summary generators and\nreviewers that work collaboratively to create and evaluate comprehensive\nsummaries, while quiz generators and reviewers create comprehension questions\nthat serve as continuous quality checks for the summarization process. This\nadversarial dynamic, enhanced by an examinee agent that validates whether the\ngenerated summary contains the information needed to answer the quiz questions,\nenables iterative refinement through multifaceted feedback mechanisms. We\nevaluate SummQ on three widely used long document summarization benchmarks.\nExperimental results demonstrate that our framework significantly outperforms\nexisting state-of-the-art methods across ROUGE and BERTScore metrics, as well\nas in LLM-as-a-Judge and human evaluations. Our comprehensive analyses reveal\nthe effectiveness of the multi-agent collaboration dynamics, the influence of\ndifferent agent configurations, and the impact of the quizzing mechanism. This\nwork establishes a new approach for long document summarization that uses\nadversarial agentic collaboration to improve summarization quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long document summarization remains a significant challenge for current large\nlanguage models (LLMs), as existing approaches commonly struggle with\ninformation loss, factual inconsistencies, and coherence issues when processing\nexcessively long documents. We propose SummQ, a novel adversarial multi-agent\nframework that addresses these limitations through collaborative intelligence\nbetween specialized agents operating in two complementary domains:\nsummarization and quizzing. Our approach employs summary generators and\nreviewers that work collaboratively to create and evaluate comprehensive\nsummaries, while quiz generators and reviewers create comprehension questions\nthat serve as continuous quality checks for the summarization process. This\nadversarial dynamic, enhanced by an examinee agent that validates whether the\ngenerated summary contains the information needed to answer the quiz questions,\nenables iterative refinement through multifaceted feedback mechanisms. We\nevaluate SummQ on three widely used long document summarization benchmarks.\nExperimental results demonstrate that our framework significantly outperforms\nexisting state-of-the-art methods across ROUGE and BERTScore metrics, as well\nas in LLM-as-a-Judge and human evaluations. Our comprehensive analyses reveal\nthe effectiveness of the multi-agent collaboration dynamics, the influence of\ndifferent agent configurations, and the impact of the quizzing mechanism. This\nwork establishes a new approach for long document summarization that uses\nadversarial agentic collaboration to improve summarization quality."
                },
                "authors": [
                    {
                        "name": "Weixuan Wang"
                    },
                    {
                        "name": "Minghao Wu"
                    },
                    {
                        "name": "Barry Haddow"
                    },
                    {
                        "name": "Alexandra Birch"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Birch"
                },
                "author": "Alexandra Birch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20900v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20900v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22553v1",
                "updated": "2025-09-26T16:35:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    35,
                    42,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:35:42Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    35,
                    42,
                    4,
                    269,
                    0
                ],
                "title": "Linear Causal Representation Learning by Topological Ordering, Pruning,\n  and Disentanglement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Causal Representation Learning by Topological Ordering, Pruning,\n  and Disentanglement"
                },
                "summary": "Causal representation learning (CRL) has garnered increasing interests from\nthe causal inference and artificial intelligence community, due to its\ncapability of disentangling potentially complex data-generating mechanism into\ncausally interpretable latent features, by leveraging the heterogeneity of\nmodern datasets. In this paper, we further contribute to the CRL literature, by\nfocusing on the stylized linear structural causal model over the latent\nfeatures and assuming a linear mixing function that maps latent features to the\nobserved data or measurements. Existing linear CRL methods often rely on\nstringent assumptions, such as accessibility to single-node interventional data\nor restrictive distributional constraints on latent features and exogenous\nmeasurement noise. However, these prerequisites can be challenging to satisfy\nin certain scenarios. In this work, we propose a novel linear CRL algorithm\nthat, unlike most existing linear CRL methods, operates under weaker\nassumptions about environment heterogeneity and data-generating distributions\nwhile still recovering latent causal features up to an equivalence class. We\nfurther validate our new algorithm via synthetic experiments and an\ninterpretability analysis of large language models (LLMs), demonstrating both\nits superiority over competing methods in finite samples and its potential in\nintegrating causality into AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal representation learning (CRL) has garnered increasing interests from\nthe causal inference and artificial intelligence community, due to its\ncapability of disentangling potentially complex data-generating mechanism into\ncausally interpretable latent features, by leveraging the heterogeneity of\nmodern datasets. In this paper, we further contribute to the CRL literature, by\nfocusing on the stylized linear structural causal model over the latent\nfeatures and assuming a linear mixing function that maps latent features to the\nobserved data or measurements. Existing linear CRL methods often rely on\nstringent assumptions, such as accessibility to single-node interventional data\nor restrictive distributional constraints on latent features and exogenous\nmeasurement noise. However, these prerequisites can be challenging to satisfy\nin certain scenarios. In this work, we propose a novel linear CRL algorithm\nthat, unlike most existing linear CRL methods, operates under weaker\nassumptions about environment heterogeneity and data-generating distributions\nwhile still recovering latent causal features up to an equivalence class. We\nfurther validate our new algorithm via synthetic experiments and an\ninterpretability analysis of large language models (LLMs), demonstrating both\nits superiority over competing methods in finite samples and its potential in\nintegrating causality into AI."
                },
                "authors": [
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Yu Guang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Guang Wang"
                },
                "author": "Yu Guang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12313v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12313v2",
                "updated": "2025-09-26T16:35:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    35,
                    13,
                    4,
                    269,
                    0
                ],
                "published": "2025-05-18T08:55:46Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    8,
                    55,
                    46,
                    6,
                    138,
                    0
                ],
                "title": "ExpertSteer: Intervening in LLMs through Expert Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertSteer: Intervening in LLMs through Expert Knowledge"
                },
                "summary": "Large Language Models (LLMs) exhibit remarkable capabilities across various\ntasks, yet guiding them to follow desired behaviours during inference remains a\nsignificant challenge. Activation steering offers a promising method to control\nthe generation process of LLMs by modifying their internal activations.\nHowever, existing methods commonly intervene in the model's behaviour using\nsteering vectors generated by the model itself, which constrains their\neffectiveness to that specific model and excludes the possibility of leveraging\npowerful external expert models for steering. To address these limitations, we\npropose ExpertSteer, a novel approach that leverages arbitrary specialized\nexpert models to generate steering vectors, enabling intervention in any LLMs.\nExpertSteer transfers the knowledge from an expert model to a target LLM\nthrough a cohesive four-step process: first aligning representation dimensions\nwith auto-encoders to enable cross-model transfer, then identifying\nintervention layer pairs based on mutual information analysis, next generating\nsteering vectors from the expert model using Recursive Feature Machines, and\nfinally applying these vectors on the identified layers during inference to\nselectively guide the target LLM without updating model parameters. We conduct\ncomprehensive experiments using three LLMs on 15 popular benchmarks across four\ndistinct domains. Experiments demonstrate that ExpertSteer significantly\noutperforms established baselines across diverse tasks at minimal cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit remarkable capabilities across various\ntasks, yet guiding them to follow desired behaviours during inference remains a\nsignificant challenge. Activation steering offers a promising method to control\nthe generation process of LLMs by modifying their internal activations.\nHowever, existing methods commonly intervene in the model's behaviour using\nsteering vectors generated by the model itself, which constrains their\neffectiveness to that specific model and excludes the possibility of leveraging\npowerful external expert models for steering. To address these limitations, we\npropose ExpertSteer, a novel approach that leverages arbitrary specialized\nexpert models to generate steering vectors, enabling intervention in any LLMs.\nExpertSteer transfers the knowledge from an expert model to a target LLM\nthrough a cohesive four-step process: first aligning representation dimensions\nwith auto-encoders to enable cross-model transfer, then identifying\nintervention layer pairs based on mutual information analysis, next generating\nsteering vectors from the expert model using Recursive Feature Machines, and\nfinally applying these vectors on the identified layers during inference to\nselectively guide the target LLM without updating model parameters. We conduct\ncomprehensive experiments using three LLMs on 15 popular benchmarks across four\ndistinct domains. Experiments demonstrate that ExpertSteer significantly\noutperforms established baselines across diverse tasks at minimal cost."
                },
                "authors": [
                    {
                        "name": "Weixuan Wang"
                    },
                    {
                        "name": "Minghao Wu"
                    },
                    {
                        "name": "Barry Haddow"
                    },
                    {
                        "name": "Alexandra Birch"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Birch"
                },
                "author": "Alexandra Birch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12313v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12313v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12300v2",
                "updated": "2025-09-26T16:33:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    33,
                    17,
                    4,
                    269,
                    0
                ],
                "published": "2025-05-18T08:31:44Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    8,
                    31,
                    44,
                    6,
                    138,
                    0
                ],
                "title": "HBO: Hierarchical Balancing Optimization for Fine-Tuning Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HBO: Hierarchical Balancing Optimization for Fine-Tuning Large Language\n  Models"
                },
                "summary": "Fine-tuning large language models (LLMs) on a mixture of diverse datasets\nposes challenges due to data imbalance and heterogeneity. Existing methods\noften address these issues across datasets (globally) but overlook the\nimbalance and heterogeneity within individual datasets (locally), which limits\ntheir effectiveness. We introduce Hierarchical Balancing Optimization (HBO), a\nnovel method that enables LLMs to autonomously adjust data allocation during\nfine-tuning both across datasets (globally) and within each individual dataset\n(locally). HBO employs a bilevel optimization strategy with two types of\nactors: a Global Actor, which balances data sampling across different subsets\nof the training mixture, and several Local Actors, which optimizes data usage\nwithin each subset based on difficulty levels. These actors are guided by\nreward functions derived from the LLM's training state, which measure learning\nprogress and relative performance improvement. We evaluate HBO on three LLM\nbackbones across nine diverse tasks in multilingual and multitask setups.\nResults show that HBO consistently outperforms existing baselines, achieving\nsignificant accuracy gains. Our in-depth analysis further demonstrates that\nboth the global actor and local actors of HBO effectively adjust data usage\nduring fine-tuning. HBO provides a comprehensive solution to the challenges of\ndata imbalance and heterogeneity in LLM fine-tuning, enabling more effective\ntraining across diverse datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) on a mixture of diverse datasets\nposes challenges due to data imbalance and heterogeneity. Existing methods\noften address these issues across datasets (globally) but overlook the\nimbalance and heterogeneity within individual datasets (locally), which limits\ntheir effectiveness. We introduce Hierarchical Balancing Optimization (HBO), a\nnovel method that enables LLMs to autonomously adjust data allocation during\nfine-tuning both across datasets (globally) and within each individual dataset\n(locally). HBO employs a bilevel optimization strategy with two types of\nactors: a Global Actor, which balances data sampling across different subsets\nof the training mixture, and several Local Actors, which optimizes data usage\nwithin each subset based on difficulty levels. These actors are guided by\nreward functions derived from the LLM's training state, which measure learning\nprogress and relative performance improvement. We evaluate HBO on three LLM\nbackbones across nine diverse tasks in multilingual and multitask setups.\nResults show that HBO consistently outperforms existing baselines, achieving\nsignificant accuracy gains. Our in-depth analysis further demonstrates that\nboth the global actor and local actors of HBO effectively adjust data usage\nduring fine-tuning. HBO provides a comprehensive solution to the challenges of\ndata imbalance and heterogeneity in LLM fine-tuning, enabling more effective\ntraining across diverse datasets."
                },
                "authors": [
                    {
                        "name": "Weixuan Wang"
                    },
                    {
                        "name": "Minghao Wu"
                    },
                    {
                        "name": "Barry Haddow"
                    },
                    {
                        "name": "Alexandra Birch"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Birch"
                },
                "author": "Alexandra Birch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22550v1",
                "updated": "2025-09-26T16:31:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    31,
                    34,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:31:34Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    31,
                    34,
                    4,
                    269,
                    0
                ],
                "title": "An Intention-driven Lane Change Framework Considering Heterogeneous\n  Dynamic Cooperation in Mixed-traffic Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Intention-driven Lane Change Framework Considering Heterogeneous\n  Dynamic Cooperation in Mixed-traffic Environment"
                },
                "summary": "In mixed-traffic environments, where autonomous vehicles (AVs) interact with\ndiverse human-driven vehicles (HVs), unpredictable intentions and heterogeneous\nbehaviors make safe and efficient lane change maneuvers highly challenging.\nExisting methods often oversimplify these interactions by assuming uniform\npatterns. We propose an intention-driven lane change framework that integrates\ndriving-style recognition, cooperation-aware decision-making, and coordinated\nmotion planning. A deep learning classifier trained on the NGSIM dataset\nidentifies human driving styles in real time. A cooperation score with\nintrinsic and interactive components estimates surrounding drivers' intentions\nand quantifies their willingness to cooperate with the ego vehicle.\nDecision-making combines behavior cloning with inverse reinforcement learning\nto determine whether a lane change should be initiated. For trajectory\ngeneration, model predictive control is integrated with IRL-based intention\ninference to produce collision-free and socially compliant maneuvers.\nExperiments show that the proposed model achieves 94.2\\% accuracy and 94.3\\%\nF1-score, outperforming rule-based and learning-based baselines by 4-15\\% in\nlane change recognition. These results highlight the benefit of modeling\ninter-driver heterogeneity and demonstrate the potential of the framework to\nadvance context-aware and human-like autonomous driving in complex traffic\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In mixed-traffic environments, where autonomous vehicles (AVs) interact with\ndiverse human-driven vehicles (HVs), unpredictable intentions and heterogeneous\nbehaviors make safe and efficient lane change maneuvers highly challenging.\nExisting methods often oversimplify these interactions by assuming uniform\npatterns. We propose an intention-driven lane change framework that integrates\ndriving-style recognition, cooperation-aware decision-making, and coordinated\nmotion planning. A deep learning classifier trained on the NGSIM dataset\nidentifies human driving styles in real time. A cooperation score with\nintrinsic and interactive components estimates surrounding drivers' intentions\nand quantifies their willingness to cooperate with the ego vehicle.\nDecision-making combines behavior cloning with inverse reinforcement learning\nto determine whether a lane change should be initiated. For trajectory\ngeneration, model predictive control is integrated with IRL-based intention\ninference to produce collision-free and socially compliant maneuvers.\nExperiments show that the proposed model achieves 94.2\\% accuracy and 94.3\\%\nF1-score, outperforming rule-based and learning-based baselines by 4-15\\% in\nlane change recognition. These results highlight the benefit of modeling\ninter-driver heterogeneity and demonstrate the potential of the framework to\nadvance context-aware and human-like autonomous driving in complex traffic\nenvironments."
                },
                "authors": [
                    {
                        "name": "Xiaoyun Qiu"
                    },
                    {
                        "name": "Haichao Liu"
                    },
                    {
                        "name": "Yue Pan"
                    },
                    {
                        "name": "Jun Ma"
                    },
                    {
                        "name": "Xinhu Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Xinhu Zheng"
                },
                "author": "Xinhu Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16625v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16625v2",
                "updated": "2025-09-26T16:30:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    30,
                    43,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-20T11:02:50Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    11,
                    2,
                    50,
                    5,
                    263,
                    0
                ],
                "title": "Self-Supervised Learning of Graph Representations for Network Intrusion\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Supervised Learning of Graph Representations for Network Intrusion\n  Detection"
                },
                "summary": "Detecting intrusions in network traffic is a challenging task, particularly\nunder limited supervision and constantly evolving attack patterns. While recent\nworks have leveraged graph neural networks for network intrusion detection,\nthey often decouple representation learning from anomaly detection, limiting\nthe utility of the embeddings for identifying attacks. We propose GraphIDS, a\nself-supervised intrusion detection model that unifies these two stages by\nlearning local graph representations of normal communication patterns through a\nmasked autoencoder. An inductive graph neural network embeds each flow with its\nlocal topological context to capture typical network behavior, while a\nTransformer-based encoder-decoder reconstructs these embeddings, implicitly\nlearning global co-occurrence patterns via self-attention without requiring\nexplicit positional information. During inference, flows with unusually high\nreconstruction errors are flagged as potential intrusions. This end-to-end\nframework ensures that embeddings are directly optimized for the downstream\ntask, facilitating the recognition of malicious traffic. On diverse NetFlow\nbenchmarks, GraphIDS achieves up to 99.98% PR-AUC and 99.61% macro F1-score,\noutperforming baselines by 5-25 percentage points.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting intrusions in network traffic is a challenging task, particularly\nunder limited supervision and constantly evolving attack patterns. While recent\nworks have leveraged graph neural networks for network intrusion detection,\nthey often decouple representation learning from anomaly detection, limiting\nthe utility of the embeddings for identifying attacks. We propose GraphIDS, a\nself-supervised intrusion detection model that unifies these two stages by\nlearning local graph representations of normal communication patterns through a\nmasked autoencoder. An inductive graph neural network embeds each flow with its\nlocal topological context to capture typical network behavior, while a\nTransformer-based encoder-decoder reconstructs these embeddings, implicitly\nlearning global co-occurrence patterns via self-attention without requiring\nexplicit positional information. During inference, flows with unusually high\nreconstruction errors are flagged as potential intrusions. This end-to-end\nframework ensures that embeddings are directly optimized for the downstream\ntask, facilitating the recognition of malicious traffic. On diverse NetFlow\nbenchmarks, GraphIDS achieves up to 99.98% PR-AUC and 99.61% macro F1-score,\noutperforming baselines by 5-25 percentage points."
                },
                "authors": [
                    {
                        "name": "Lorenzo Guerra"
                    },
                    {
                        "name": "Thomas Chapuis"
                    },
                    {
                        "name": "Guillaume Duc"
                    },
                    {
                        "name": "Pavlo Mozharovskyi"
                    },
                    {
                        "name": "Van-Tam Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Van-Tam Nguyen"
                },
                "author": "Van-Tam Nguyen",
                "arxiv_comment": "Accepted at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16625v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22546v1",
                "updated": "2025-09-26T16:27:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    27,
                    29,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:27:29Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    27,
                    29,
                    4,
                    269,
                    0
                ],
                "title": "Think Socially via Cognitive Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Socially via Cognitive Reasoning"
                },
                "summary": "LLMs trained for logical reasoning excel at step-by-step deduction to reach\nverifiable answers. However, this paradigm is ill-suited for navigating social\nsituations, which induce an interpretive process of analyzing ambiguous cues\nthat rarely yield a definitive outcome. To bridge this gap, we introduce\nCognitive Reasoning, a paradigm modeled on human social cognition. It\nformulates the interpretive process into a structured cognitive flow of\ninterconnected cognitive units (e.g., observation or attribution), which\ncombine adaptively to enable effective social thinking and responses. We then\npropose CogFlow, a complete framework that instills this capability in LLMs.\nCogFlow first curates a dataset of cognitive flows by simulating the\nassociative and progressive nature of human thought via tree-structured\nplanning. After instilling the basic cognitive reasoning capability via\nsupervised fine-tuning, CogFlow adopts reinforcement learning to enable the\nmodel to improve itself via trial and error, guided by a multi-objective reward\nthat optimizes both cognitive flow and response quality. Extensive experiments\nshow that CogFlow effectively enhances the social cognitive capabilities of\nLLMs, and even humans, leading to more effective social decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs trained for logical reasoning excel at step-by-step deduction to reach\nverifiable answers. However, this paradigm is ill-suited for navigating social\nsituations, which induce an interpretive process of analyzing ambiguous cues\nthat rarely yield a definitive outcome. To bridge this gap, we introduce\nCognitive Reasoning, a paradigm modeled on human social cognition. It\nformulates the interpretive process into a structured cognitive flow of\ninterconnected cognitive units (e.g., observation or attribution), which\ncombine adaptively to enable effective social thinking and responses. We then\npropose CogFlow, a complete framework that instills this capability in LLMs.\nCogFlow first curates a dataset of cognitive flows by simulating the\nassociative and progressive nature of human thought via tree-structured\nplanning. After instilling the basic cognitive reasoning capability via\nsupervised fine-tuning, CogFlow adopts reinforcement learning to enable the\nmodel to improve itself via trial and error, guided by a multi-objective reward\nthat optimizes both cognitive flow and response quality. Extensive experiments\nshow that CogFlow effectively enhances the social cognitive capabilities of\nLLMs, and even humans, leading to more effective social decision-making."
                },
                "authors": [
                    {
                        "name": "Jinfeng Zhou"
                    },
                    {
                        "name": "Zheyu Chen"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Quanyu Dai"
                    },
                    {
                        "name": "Zhenhua Dong"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "arxiv_comment": "Repository: https://github.com/thu-coai/CogFlow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20172v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20172v2",
                "updated": "2025-09-26T16:24:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    24,
                    10,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-24T14:36:44Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    36,
                    44,
                    2,
                    267,
                    0
                ],
                "title": "Benchmarking LLMs in Web API Integration Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLMs in Web API Integration Tasks"
                },
                "summary": "API integration is a cornerstone of our digital infrastructure, enabling\nsoftware systems to connect and interact. However, as shown by many studies,\nwriting or generating correct code to invoke APIs, particularly web APIs, is\nchallenging. Although large language models (LLMs) have become popular in\nsoftware development, their effectiveness in automating the generation of web\nAPI integration code remains unexplored. In order to address this, we present a\ndataset and evaluation pipeline designed to assess the ability of LLMs to\ngenerate web API invocation code. Our experiments with several open-source LLMs\nreveal that generating API invocations poses a significant challenge, resulting\nin hallucinated endpoints, incorrect argument usage, and other errors. None of\nthe evaluated open-source models were able to solve more than 40% of the tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "API integration is a cornerstone of our digital infrastructure, enabling\nsoftware systems to connect and interact. However, as shown by many studies,\nwriting or generating correct code to invoke APIs, particularly web APIs, is\nchallenging. Although large language models (LLMs) have become popular in\nsoftware development, their effectiveness in automating the generation of web\nAPI integration code remains unexplored. In order to address this, we present a\ndataset and evaluation pipeline designed to assess the ability of LLMs to\ngenerate web API invocation code. Our experiments with several open-source LLMs\nreveal that generating API invocations poses a significant challenge, resulting\nin hallucinated endpoints, incorrect argument usage, and other errors. None of\nthe evaluated open-source models were able to solve more than 40% of the tasks."
                },
                "authors": [
                    {
                        "name": "Daniel Maninger"
                    },
                    {
                        "name": "Leon Chemnitz"
                    },
                    {
                        "name": "Amir Molzam Sharifloo"
                    },
                    {
                        "name": "Jannis Brugger"
                    },
                    {
                        "name": "Mira Mezini"
                    }
                ],
                "author_detail": {
                    "name": "Mira Mezini"
                },
                "author": "Mira Mezini",
                "arxiv_comment": "To be published in Proceedings of 2nd ACM International Conference on\n  AI-powered Software, Benchmark & Dataset Track (AIware '25); updated paper\n  title and affiliations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20172v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20172v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05563v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05563v2",
                "updated": "2025-09-26T16:21:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    21,
                    43,
                    4,
                    269,
                    0
                ],
                "published": "2025-04-07T23:34:11Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    23,
                    34,
                    11,
                    0,
                    97,
                    0
                ],
                "title": "Do Data Valuations Make Good Data Prices?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Data Valuations Make Good Data Prices?"
                },
                "summary": "As large language models increasingly rely on external data sources,\ncompensating data contributors has become a central concern. But how should\nthese payments be devised? We revisit data valuations from a\n$\\textit{market-design perspective}$ where payments serve to compensate data\nowners for the $\\textit{private}$ heterogeneous costs they incur for collecting\nand sharing data. We show that popular valuation methods-such as Leave-One-Out\nand Data Shapley-make for poor payments. They fail to ensure truthful reporting\nof the costs, leading to $\\textit{inefficient market}$ outcomes. To address\nthis, we adapt well-established payment rules from mechanism design, namely\nMyerson and Vickrey-Clarke-Groves (VCG), to the data market setting. We show\nthat Myerson payment is the minimal truthful mechanism, optimal from the\nbuyer's perspective. Additionally, we identify a condition under which both\ndata buyers and sellers are utility-satisfied, and the market achieves\nefficiency. Our findings highlight the importance of incorporating incentive\ncompatibility into data valuation design, paving the way for more robust and\nefficient data markets. Our data market framework is readily applicable to\nreal-world scenarios. We illustrate this with simulations of contributor\ncompensation in an LLM based retrieval-augmented generation (RAG) marketplace\ntasked with challenging medical question answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models increasingly rely on external data sources,\ncompensating data contributors has become a central concern. But how should\nthese payments be devised? We revisit data valuations from a\n$\\textit{market-design perspective}$ where payments serve to compensate data\nowners for the $\\textit{private}$ heterogeneous costs they incur for collecting\nand sharing data. We show that popular valuation methods-such as Leave-One-Out\nand Data Shapley-make for poor payments. They fail to ensure truthful reporting\nof the costs, leading to $\\textit{inefficient market}$ outcomes. To address\nthis, we adapt well-established payment rules from mechanism design, namely\nMyerson and Vickrey-Clarke-Groves (VCG), to the data market setting. We show\nthat Myerson payment is the minimal truthful mechanism, optimal from the\nbuyer's perspective. Additionally, we identify a condition under which both\ndata buyers and sellers are utility-satisfied, and the market achieves\nefficiency. Our findings highlight the importance of incorporating incentive\ncompatibility into data valuation design, paving the way for more robust and\nefficient data markets. Our data market framework is readily applicable to\nreal-world scenarios. We illustrate this with simulations of contributor\ncompensation in an LLM based retrieval-augmented generation (RAG) marketplace\ntasked with challenging medical question answering."
                },
                "authors": [
                    {
                        "name": "Dongyang Fan"
                    },
                    {
                        "name": "Tyler J. Rotello"
                    },
                    {
                        "name": "Sai Praneeth Karimireddy"
                    }
                ],
                "author_detail": {
                    "name": "Sai Praneeth Karimireddy"
                },
                "author": "Sai Praneeth Karimireddy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05563v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05563v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12663v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12663v2",
                "updated": "2025-09-26T16:20:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    20,
                    20,
                    4,
                    269,
                    0
                ],
                "published": "2025-02-18T09:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    11,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Demystifying Multilingual Chain-of-Thought in Process Reward Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Multilingual Chain-of-Thought in Process Reward Modeling"
                },
                "summary": "Large language models (LLMs) are designed to perform a wide range of tasks.\nTo improve their ability to solve complex problems requiring multi-step\nreasoning, recent research leverages process reward modeling to provide\nfine-grained feedback at each step of the reasoning process for reinforcement\nlearning (RL), but it predominantly focuses on English. In this paper, we\ntackle the critical challenge of extending process reward models (PRMs) to\nmultilingual settings. To achieve this, we train multilingual PRMs on a dataset\nspanning seven languages, which is translated from English. Through\ncomprehensive evaluations on two widely used reasoning benchmarks across 11\nlanguages, we demonstrate that multilingual PRMs not only improve average\naccuracy but also reduce early-stage reasoning errors. Furthermore, our results\nhighlight the sensitivity of multilingual PRMs to both the number of training\nlanguages and the volume of English data, while also uncovering the benefits\narising from more candidate responses and trainable parameters. This work opens\npromising avenues for robust multilingual applications in complex, multi-step\nreasoning tasks. In addition, we release the code to foster research along this\nline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are designed to perform a wide range of tasks.\nTo improve their ability to solve complex problems requiring multi-step\nreasoning, recent research leverages process reward modeling to provide\nfine-grained feedback at each step of the reasoning process for reinforcement\nlearning (RL), but it predominantly focuses on English. In this paper, we\ntackle the critical challenge of extending process reward models (PRMs) to\nmultilingual settings. To achieve this, we train multilingual PRMs on a dataset\nspanning seven languages, which is translated from English. Through\ncomprehensive evaluations on two widely used reasoning benchmarks across 11\nlanguages, we demonstrate that multilingual PRMs not only improve average\naccuracy but also reduce early-stage reasoning errors. Furthermore, our results\nhighlight the sensitivity of multilingual PRMs to both the number of training\nlanguages and the volume of English data, while also uncovering the benefits\narising from more candidate responses and trainable parameters. This work opens\npromising avenues for robust multilingual applications in complex, multi-step\nreasoning tasks. In addition, we release the code to foster research along this\nline."
                },
                "authors": [
                    {
                        "name": "Weixuan Wang"
                    },
                    {
                        "name": "Minghao Wu"
                    },
                    {
                        "name": "Barry Haddow"
                    },
                    {
                        "name": "Alexandra Birch"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Birch"
                },
                "author": "Alexandra Birch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12663v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12663v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22544v1",
                "updated": "2025-09-26T16:20:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    20,
                    6,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:20:06Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    20,
                    6,
                    4,
                    269,
                    0
                ],
                "title": "HyCoVAD: A Hybrid SSL-LLM Model for Complex Video Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyCoVAD: A Hybrid SSL-LLM Model for Complex Video Anomaly Detection"
                },
                "summary": "Video anomaly detection (VAD) is crucial for intelligent surveillance, but a\nsignificant challenge lies in identifying complex anomalies, which are events\ndefined by intricate relationships and temporal dependencies among multiple\nentities rather than by isolated actions. While self-supervised learning (SSL)\nmethods effectively model low-level spatiotemporal patterns, they often\nstruggle to grasp the semantic meaning of these interactions. Conversely, large\nlanguage models (LLMs) offer powerful contextual reasoning but are\ncomputationally expensive for frame-by-frame analysis and lack fine-grained\nspatial localization. We introduce HyCoVAD, Hybrid Complex Video Anomaly\nDetection, a hybrid SSL-LLM model that combines a multi-task SSL temporal\nanalyzer with LLM validator. The SSL module is built upon an nnFormer backbone\nwhich is a transformer-based model for image segmentation. It is trained with\nmultiple proxy tasks, learns from video frames to identify those suspected of\nanomaly. The selected frames are then forwarded to the LLM, which enriches the\nanalysis with semantic context by applying structured, rule-based reasoning to\nvalidate the presence of anomalies. Experiments on the challenging ComplexVAD\ndataset show that HyCoVAD achieves a 72.5% frame-level AUC, outperforming\nexisting baselines by 12.5% while reducing LLM computation. We release our\ninteraction anomaly taxonomy, adaptive thresholding protocol, and code to\nfacilitate future research in complex VAD scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video anomaly detection (VAD) is crucial for intelligent surveillance, but a\nsignificant challenge lies in identifying complex anomalies, which are events\ndefined by intricate relationships and temporal dependencies among multiple\nentities rather than by isolated actions. While self-supervised learning (SSL)\nmethods effectively model low-level spatiotemporal patterns, they often\nstruggle to grasp the semantic meaning of these interactions. Conversely, large\nlanguage models (LLMs) offer powerful contextual reasoning but are\ncomputationally expensive for frame-by-frame analysis and lack fine-grained\nspatial localization. We introduce HyCoVAD, Hybrid Complex Video Anomaly\nDetection, a hybrid SSL-LLM model that combines a multi-task SSL temporal\nanalyzer with LLM validator. The SSL module is built upon an nnFormer backbone\nwhich is a transformer-based model for image segmentation. It is trained with\nmultiple proxy tasks, learns from video frames to identify those suspected of\nanomaly. The selected frames are then forwarded to the LLM, which enriches the\nanalysis with semantic context by applying structured, rule-based reasoning to\nvalidate the presence of anomalies. Experiments on the challenging ComplexVAD\ndataset show that HyCoVAD achieves a 72.5% frame-level AUC, outperforming\nexisting baselines by 12.5% while reducing LLM computation. We release our\ninteraction anomaly taxonomy, adaptive thresholding protocol, and code to\nfacilitate future research in complex VAD scenarios."
                },
                "authors": [
                    {
                        "name": "Mohammad Mahdi Hemmatyar"
                    },
                    {
                        "name": "Mahdi Jafari"
                    },
                    {
                        "name": "Mohammad Amin Yousefi"
                    },
                    {
                        "name": "Mohammad Reza Nemati"
                    },
                    {
                        "name": "Mobin Azadani"
                    },
                    {
                        "name": "Hamid Reza Rastad"
                    },
                    {
                        "name": "Amirmohammad Akbari"
                    }
                ],
                "author_detail": {
                    "name": "Amirmohammad Akbari"
                },
                "author": "Amirmohammad Akbari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06958v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06958v2",
                "updated": "2025-09-26T16:20:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    20,
                    4,
                    4,
                    269,
                    0
                ],
                "published": "2025-06-08T00:59:02Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    0,
                    59,
                    2,
                    6,
                    159,
                    0
                ],
                "title": "Position: Simulating Society Requires Simulating Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: Simulating Society Requires Simulating Thought"
                },
                "summary": "Simulating society with large language models (LLMs), we argue, requires more\nthan generating plausible behavior; it demands cognitively grounded reasoning\nthat is structured, revisable, and traceable. LLM-based agents are increasingly\nused to emulate individual and group behavior, primarily through prompting and\nsupervised fine-tuning. Yet they often lack internal coherence, causal\nreasoning, and belief traceability, making them unreliable for simulating how\npeople reason, deliberate, and respond to interventions.\n  To address this, we present a conceptual modeling paradigm, Generative Minds\n(GenMinds), which draws from cognitive science to support structured belief\nrepresentations in generative agents. To evaluate such agents, we introduce the\nRECAP (REconstructing CAusal Paths) framework, a benchmark designed to assess\nreasoning fidelity via causal traceability, demographic grounding, and\nintervention consistency. These contributions advance a broader shift: from\nsurface-level mimicry to generative agents that simulate thought -- not just\nlanguage -- for social simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating society with large language models (LLMs), we argue, requires more\nthan generating plausible behavior; it demands cognitively grounded reasoning\nthat is structured, revisable, and traceable. LLM-based agents are increasingly\nused to emulate individual and group behavior, primarily through prompting and\nsupervised fine-tuning. Yet they often lack internal coherence, causal\nreasoning, and belief traceability, making them unreliable for simulating how\npeople reason, deliberate, and respond to interventions.\n  To address this, we present a conceptual modeling paradigm, Generative Minds\n(GenMinds), which draws from cognitive science to support structured belief\nrepresentations in generative agents. To evaluate such agents, we introduce the\nRECAP (REconstructing CAusal Paths) framework, a benchmark designed to assess\nreasoning fidelity via causal traceability, demographic grounding, and\nintervention consistency. These contributions advance a broader shift: from\nsurface-level mimicry to generative agents that simulate thought -- not just\nlanguage -- for social simulations."
                },
                "authors": [
                    {
                        "name": "Chance Jiajie Li"
                    },
                    {
                        "name": "Jiayi Wu"
                    },
                    {
                        "name": "Zhenze Mo"
                    },
                    {
                        "name": "Ao Qu"
                    },
                    {
                        "name": "Yuhan Tang"
                    },
                    {
                        "name": "Kaiya Ivy Zhao"
                    },
                    {
                        "name": "Yulu Gan"
                    },
                    {
                        "name": "Jie Fan"
                    },
                    {
                        "name": "Jiangbo Yu"
                    },
                    {
                        "name": "Jinhua Zhao"
                    },
                    {
                        "name": "Paul Liang"
                    },
                    {
                        "name": "Luis Alonso"
                    },
                    {
                        "name": "Kent Larson"
                    }
                ],
                "author_detail": {
                    "name": "Kent Larson"
                },
                "author": "Kent Larson",
                "arxiv_comment": "To appear in NeurIPS 2025 (Position Paper Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06958v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06958v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22537v1",
                "updated": "2025-09-26T16:17:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    17,
                    29,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:17:29Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    17,
                    29,
                    4,
                    269,
                    0
                ],
                "title": "The Emergence of Altruism in Large-Language-Model Agents Society",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Emergence of Altruism in Large-Language-Model Agents Society"
                },
                "summary": "Leveraging Large Language Models (LLMs) for social simulation is a frontier\nin computational social science. Understanding the social logics these agents\nembody is critical to this attempt. However, existing research has primarily\nfocused on cooperation in small-scale, task-oriented games, overlooking how\naltruism, which means sacrificing self-interest for collective benefit, emerges\nin large-scale agent societies. To address this gap, we introduce a\nSchelling-variant urban migration model that creates a social dilemma,\ncompelling over 200 LLM agents to navigate an explicit conflict between\negoistic (personal utility) and altruistic (system utility) goals. Our central\nfinding is a fundamental difference in the social tendencies of LLMs. We\nidentify two distinct archetypes: \"Adaptive Egoists\", which default to\nprioritizing self-interest but whose altruistic behaviors significantly\nincrease under the influence of a social norm-setting message board; and\n\"Altruistic Optimizers\", which exhibit an inherent altruistic logic,\nconsistently prioritizing collective benefit even at a direct cost to\nthemselves. Furthermore, to qualitatively analyze the cognitive underpinnings\nof these decisions, we introduce a method inspired by Grounded Theory to\nsystematically code agent reasoning. In summary, this research provides the\nfirst evidence of intrinsic heterogeneity in the egoistic and altruistic\ntendencies of different LLMs. We propose that for social simulation, model\nselection is not merely a matter of choosing reasoning capability, but of\nchoosing an intrinsic social action logic. While \"Adaptive Egoists\" may offer a\nmore suitable choice for simulating complex human societies, \"Altruistic\nOptimizers\" are better suited for modeling idealized pro-social actors or\nscenarios where collective welfare is the primary consideration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models (LLMs) for social simulation is a frontier\nin computational social science. Understanding the social logics these agents\nembody is critical to this attempt. However, existing research has primarily\nfocused on cooperation in small-scale, task-oriented games, overlooking how\naltruism, which means sacrificing self-interest for collective benefit, emerges\nin large-scale agent societies. To address this gap, we introduce a\nSchelling-variant urban migration model that creates a social dilemma,\ncompelling over 200 LLM agents to navigate an explicit conflict between\negoistic (personal utility) and altruistic (system utility) goals. Our central\nfinding is a fundamental difference in the social tendencies of LLMs. We\nidentify two distinct archetypes: \"Adaptive Egoists\", which default to\nprioritizing self-interest but whose altruistic behaviors significantly\nincrease under the influence of a social norm-setting message board; and\n\"Altruistic Optimizers\", which exhibit an inherent altruistic logic,\nconsistently prioritizing collective benefit even at a direct cost to\nthemselves. Furthermore, to qualitatively analyze the cognitive underpinnings\nof these decisions, we introduce a method inspired by Grounded Theory to\nsystematically code agent reasoning. In summary, this research provides the\nfirst evidence of intrinsic heterogeneity in the egoistic and altruistic\ntendencies of different LLMs. We propose that for social simulation, model\nselection is not merely a matter of choosing reasoning capability, but of\nchoosing an intrinsic social action logic. While \"Adaptive Egoists\" may offer a\nmore suitable choice for simulating complex human societies, \"Altruistic\nOptimizers\" are better suited for modeling idealized pro-social actors or\nscenarios where collective welfare is the primary consideration."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Xiao Jia"
                    },
                    {
                        "name": "Zhanzhan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhanzhan Zhao"
                },
                "author": "Zhanzhan Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22536v1",
                "updated": "2025-09-26T16:16:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    16,
                    49,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:16:49Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    16,
                    49,
                    4,
                    269,
                    0
                ],
                "title": "InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced\n  Language Models"
                },
                "summary": "The immense computational cost of training Large Language Models (LLMs)\npresents a major barrier to innovation. While FP8 training offers a promising\nsolution with significant theoretical efficiency gains, its widespread adoption\nhas been hindered by the lack of a comprehensive, open-source training recipe.\nTo bridge this gap, we introduce an end-to-end FP8 training recipe that\nseamlessly integrates continual pre-training and supervised fine-tuning. Our\nmethodology employs a fine-grained, hybrid-granularity quantization strategy to\nmaintain numerical fidelity while maximizing computational efficiency. Through\nextensive experiments, including the continue pre-training of models on a\n160B-token corpus, we demonstrate that our recipe is not only remarkably stable\nbut also essentially lossless, achieving performance on par with the BF16\nbaseline across a suite of reasoning benchmarks. Crucially, this is achieved\nwith substantial efficiency improvements, including up to a 22% reduction in\ntraining time, a 14% decrease in peak memory usage, and a 19% increase in\nthroughput. Our results establish FP8 as a practical and robust alternative to\nBF16, and we will release the accompanying code to further democratize\nlarge-scale model training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The immense computational cost of training Large Language Models (LLMs)\npresents a major barrier to innovation. While FP8 training offers a promising\nsolution with significant theoretical efficiency gains, its widespread adoption\nhas been hindered by the lack of a comprehensive, open-source training recipe.\nTo bridge this gap, we introduce an end-to-end FP8 training recipe that\nseamlessly integrates continual pre-training and supervised fine-tuning. Our\nmethodology employs a fine-grained, hybrid-granularity quantization strategy to\nmaintain numerical fidelity while maximizing computational efficiency. Through\nextensive experiments, including the continue pre-training of models on a\n160B-token corpus, we demonstrate that our recipe is not only remarkably stable\nbut also essentially lossless, achieving performance on par with the BF16\nbaseline across a suite of reasoning benchmarks. Crucially, this is achieved\nwith substantial efficiency improvements, including up to a 22% reduction in\ntraining time, a 14% decrease in peak memory usage, and a 19% increase in\nthroughput. Our results establish FP8 as a practical and robust alternative to\nBF16, and we will release the accompanying code to further democratize\nlarge-scale model training."
                },
                "authors": [
                    {
                        "name": "Wenjun Wang"
                    },
                    {
                        "name": "Shuo Cai"
                    },
                    {
                        "name": "Congkai Xie"
                    },
                    {
                        "name": "Mingfa Feng"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Kejing Yang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Jiannong Cao"
                    },
                    {
                        "name": "Yuan Xie"
                    },
                    {
                        "name": "Hongxia Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hongxia Yang"
                },
                "author": "Hongxia Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22531v1",
                "updated": "2025-09-26T16:11:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    11,
                    14,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:11:14Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    11,
                    14,
                    4,
                    269,
                    0
                ],
                "title": "Debiased Front-Door Learners for Heterogeneous Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debiased Front-Door Learners for Heterogeneous Effects"
                },
                "summary": "In observational settings where treatment and outcome share unmeasured\nconfounders but an observed mediator remains unconfounded, the front-door (FD)\nadjustment identifies causal effects through the mediator. We study the\nheterogeneous treatment effect (HTE) under FD identification and introduce two\ndebiased learners: FD-DR-Learner and FD-R-Learner. Both attain fast,\nquasi-oracle rates (i.e., performance comparable to an oracle that knows the\nnuisances) even when nuisance functions converge as slowly as n^-1/4. We\nprovide error analyses establishing debiasedness and demonstrate robust\nempirical performance in synthetic studies and a real-world case study of\nprimary seat-belt laws using Fatality Analysis Reporting System (FARS) dataset.\nTogether, these results indicate that the proposed learners deliver reliable\nand sample-efficient HTE estimates in FD scenarios. The implementation is\navailable at https://github.com/yonghanjung/FD-CATE.\n  Keywords: Front-door adjustment; Heterogeneous treatment effects; Debiased\nlearning; Quasi-oracle rates; Causal inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In observational settings where treatment and outcome share unmeasured\nconfounders but an observed mediator remains unconfounded, the front-door (FD)\nadjustment identifies causal effects through the mediator. We study the\nheterogeneous treatment effect (HTE) under FD identification and introduce two\ndebiased learners: FD-DR-Learner and FD-R-Learner. Both attain fast,\nquasi-oracle rates (i.e., performance comparable to an oracle that knows the\nnuisances) even when nuisance functions converge as slowly as n^-1/4. We\nprovide error analyses establishing debiasedness and demonstrate robust\nempirical performance in synthetic studies and a real-world case study of\nprimary seat-belt laws using Fatality Analysis Reporting System (FARS) dataset.\nTogether, these results indicate that the proposed learners deliver reliable\nand sample-efficient HTE estimates in FD scenarios. The implementation is\navailable at https://github.com/yonghanjung/FD-CATE.\n  Keywords: Front-door adjustment; Heterogeneous treatment effects; Debiased\nlearning; Quasi-oracle rates; Causal inference."
                },
                "authors": [
                    {
                        "name": "Yonghan Jung"
                    }
                ],
                "author_detail": {
                    "name": "Yonghan Jung"
                },
                "author": "Yonghan Jung",
                "arxiv_comment": "27 pages, 3 figures. Preprint. Code available at\n  https://github.com/yonghanjung/FD-CATE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22530v1",
                "updated": "2025-09-26T16:08:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    8,
                    58,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:08:58Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    8,
                    58,
                    4,
                    269,
                    0
                ],
                "title": "Boosting Pointer Analysis With Large Language Model-Enhanced Allocation\n  Function Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Pointer Analysis With Large Language Model-Enhanced Allocation\n  Function Detection"
                },
                "summary": "Pointer analysis is foundational for many static analysis tasks, yet its\neffectiveness is often hindered by imprecise modeling of heap allocations,\nparticularly in C/C++ programs where user-defined allocation functions (AFs)\nare pervasive. Existing approaches largely overlook these custom allocators,\nleading to coarse aliasing and reduced analysis precision. In this paper, we\npresent AFD, a novel technique that enhances pointer analysis by automatically\nidentifying and modeling custom allocation functions. AFD employs a hybrid\napproach: it uses value-flow analysis to detect straightforward wrappers and\nleverages Large Language Models (LLMs) to reason about more complex allocation\npatterns with side effects. This targeted enhancement enables precise modeling\nof heap objects at each call site, achieving context-sensitivity-like benefits\nwithout the associated overhead. We evaluate AFD on 15 real-world C projects,\nidentifying over 600 custom AFs. Integrating AFD into a baseline pointer\nanalysis yields a 26x increase in modeled heap objects and a 39% reduction in\nalias set sizes, with only 1.4x runtime overhead. Furthermore, our enhanced\nanalysis improves indirect call resolution and uncovers 17 previously\nundetected memory bugs. These results demonstrate that precise modeling of\ncustom allocation functions offers a scalable and practical path to improving\npointer analysis in large software systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pointer analysis is foundational for many static analysis tasks, yet its\neffectiveness is often hindered by imprecise modeling of heap allocations,\nparticularly in C/C++ programs where user-defined allocation functions (AFs)\nare pervasive. Existing approaches largely overlook these custom allocators,\nleading to coarse aliasing and reduced analysis precision. In this paper, we\npresent AFD, a novel technique that enhances pointer analysis by automatically\nidentifying and modeling custom allocation functions. AFD employs a hybrid\napproach: it uses value-flow analysis to detect straightforward wrappers and\nleverages Large Language Models (LLMs) to reason about more complex allocation\npatterns with side effects. This targeted enhancement enables precise modeling\nof heap objects at each call site, achieving context-sensitivity-like benefits\nwithout the associated overhead. We evaluate AFD on 15 real-world C projects,\nidentifying over 600 custom AFs. Integrating AFD into a baseline pointer\nanalysis yields a 26x increase in modeled heap objects and a 39% reduction in\nalias set sizes, with only 1.4x runtime overhead. Furthermore, our enhanced\nanalysis improves indirect call resolution and uncovers 17 previously\nundetected memory bugs. These results demonstrate that precise modeling of\ncustom allocation functions offers a scalable and practical path to improving\npointer analysis in large software systems."
                },
                "authors": [
                    {
                        "name": "Baijun Cheng"
                    },
                    {
                        "name": "Kailong Wang"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Peng Di"
                    },
                    {
                        "name": "Yao Guo"
                    },
                    {
                        "name": "Ding Li"
                    },
                    {
                        "name": "Xiangqun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiangqun Chen"
                },
                "author": "Xiangqun Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00907v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00907v3",
                "updated": "2025-09-26T16:06:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    6,
                    34,
                    4,
                    269,
                    0
                ],
                "published": "2025-04-01T15:41:50Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    15,
                    41,
                    50,
                    1,
                    91,
                    0
                ],
                "title": "Grounding Multimodal LLMs to Embodied Agents that Ask for Help with\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounding Multimodal LLMs to Embodied Agents that Ask for Help with\n  Reinforcement Learning"
                },
                "summary": "Embodied agents operating in household environments must interpret ambiguous\nand under-specified human instructions. A capable household robot should\nrecognize ambiguity and ask relevant clarification questions to infer the user\nintent accurately, leading to more effective task execution. To study this\nproblem, we introduce the Ask-to-Act task, where an embodied agent is tasked\nwith a single or multi-object rearrangement task using an under-specified\ninstruction in a home environment. The agent must strategically ask minimal,\nyet relevant, clarification questions to resolve ambiguity while navigating\nunder partial observability. To address this challenge, we propose a novel\napproach that fine-tunes multi-modal large language models (MLLMs) as\nvision-language-action (VLA) policies using online reinforcement learning (RL)\nwith LLM-generated rewards. Our method eliminates the need for large-scale\nhuman demonstrations or manually engineered rewards for training such agents.\nWe benchmark against strong zero-shot baselines including GPT-4o as well as\nsupervised fine-tuned MLLMs on our task. Our results show that our RL-finetuned\nMLLM outperforms all baselines by a significant margin (10.4-16.5%),\ngeneralizing well to novel scenes and tasks. To the best of our knowledge, this\nis the first demonstration of adapting MLLMs as VLA agents that can act and ask\nfor help using LLM-generated rewards with online RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied agents operating in household environments must interpret ambiguous\nand under-specified human instructions. A capable household robot should\nrecognize ambiguity and ask relevant clarification questions to infer the user\nintent accurately, leading to more effective task execution. To study this\nproblem, we introduce the Ask-to-Act task, where an embodied agent is tasked\nwith a single or multi-object rearrangement task using an under-specified\ninstruction in a home environment. The agent must strategically ask minimal,\nyet relevant, clarification questions to resolve ambiguity while navigating\nunder partial observability. To address this challenge, we propose a novel\napproach that fine-tunes multi-modal large language models (MLLMs) as\nvision-language-action (VLA) policies using online reinforcement learning (RL)\nwith LLM-generated rewards. Our method eliminates the need for large-scale\nhuman demonstrations or manually engineered rewards for training such agents.\nWe benchmark against strong zero-shot baselines including GPT-4o as well as\nsupervised fine-tuned MLLMs on our task. Our results show that our RL-finetuned\nMLLM outperforms all baselines by a significant margin (10.4-16.5%),\ngeneralizing well to novel scenes and tasks. To the best of our knowledge, this\nis the first demonstration of adapting MLLMs as VLA agents that can act and ask\nfor help using LLM-generated rewards with online RL."
                },
                "authors": [
                    {
                        "name": "Ram Ramrakhya"
                    },
                    {
                        "name": "Matthew Chang"
                    },
                    {
                        "name": "Xavier Puig"
                    },
                    {
                        "name": "Ruta Desai"
                    },
                    {
                        "name": "Zsolt Kira"
                    },
                    {
                        "name": "Roozbeh Mottaghi"
                    }
                ],
                "author_detail": {
                    "name": "Roozbeh Mottaghi"
                },
                "author": "Roozbeh Mottaghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00907v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00907v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22523v1",
                "updated": "2025-09-26T16:04:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    4,
                    9,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:04:09Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    4,
                    9,
                    4,
                    269,
                    0
                ],
                "title": "SDSS-C4 3028: The Nearest Blue Galaxy Cluster Devoid of an Intracluster\n  Medium",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SDSS-C4 3028: The Nearest Blue Galaxy Cluster Devoid of an Intracluster\n  Medium"
                },
                "summary": "SDSS-C4 3028 is a galaxy cluster at $z=0.061$, notable for its unusually high\nfraction of star-forming galaxies with 19 star-forming and 11 quiescent\nspectroscopically-confirmed member galaxies. From Subaru/HSC imaging, we\nderived a weak lensing mass of $M_{200} = (1.3 \\pm 0.9) \\times 10^{14} \\rm\nM_\\odot$, indicating a low-mass cluster. This is in excellent agreement with\nits dynamical mass of $M_{200} = (1.0\\pm0.4)\\times10^{14} \\rm M_\\odot$, derived\nfrom SDSS spectroscopic data. XMM-Newton observations reveal that its X-ray\nemission is uniform and fully consistent with the astrophysical X-ray\nbackground, with no evidence for an intracluster medium (ICM). The 3$\\sigma$\nupper limit of $L_{\\rm X}(0.1-2.4\\rm keV)=7.7\\times10^{42}$ erg s$^{-1}$ on the\ncluster's X-ray luminosity falls below the value expected from the $L_{\\rm\nX}-M_{\\rm halo}$ scaling relation of nearby galaxy clusters. We derived\nstar-formation histories for its member galaxies using the photometric spectral\nenergy distribution from SDSS, 2MASS, and WISE data. Most of its quiescent\ngalaxies reside within the central 300 kpc, while star-forming ones dominate\nthe outer region (300 kpc - 1 Mpc). The core region has formed the bulk of its\nstellar mass approximately 1.5 Gyr earlier than the outskirts. We infer a long\nquenching time of $>3$ Gyr for its quiescent galaxies, consistent with slow\nquenching mechanisms such as galaxy-galaxy interaction or strangulation. These\nfindings suggest that SDSS-C4 3028 may have undergone an \"inside-out\" formation\nand quenching process. Its ICM may have been expelled by intense AGN feedback\nafter core formation but before full cluster assembly. The high fraction\n($\\sim$0.63) of star-forming members likely results from the absence of ram\npressure stripping in this blue cluster, supporting the important role of ram\npressure stripping in quenching galaxies in clusters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SDSS-C4 3028 is a galaxy cluster at $z=0.061$, notable for its unusually high\nfraction of star-forming galaxies with 19 star-forming and 11 quiescent\nspectroscopically-confirmed member galaxies. From Subaru/HSC imaging, we\nderived a weak lensing mass of $M_{200} = (1.3 \\pm 0.9) \\times 10^{14} \\rm\nM_\\odot$, indicating a low-mass cluster. This is in excellent agreement with\nits dynamical mass of $M_{200} = (1.0\\pm0.4)\\times10^{14} \\rm M_\\odot$, derived\nfrom SDSS spectroscopic data. XMM-Newton observations reveal that its X-ray\nemission is uniform and fully consistent with the astrophysical X-ray\nbackground, with no evidence for an intracluster medium (ICM). The 3$\\sigma$\nupper limit of $L_{\\rm X}(0.1-2.4\\rm keV)=7.7\\times10^{42}$ erg s$^{-1}$ on the\ncluster's X-ray luminosity falls below the value expected from the $L_{\\rm\nX}-M_{\\rm halo}$ scaling relation of nearby galaxy clusters. We derived\nstar-formation histories for its member galaxies using the photometric spectral\nenergy distribution from SDSS, 2MASS, and WISE data. Most of its quiescent\ngalaxies reside within the central 300 kpc, while star-forming ones dominate\nthe outer region (300 kpc - 1 Mpc). The core region has formed the bulk of its\nstellar mass approximately 1.5 Gyr earlier than the outskirts. We infer a long\nquenching time of $>3$ Gyr for its quiescent galaxies, consistent with slow\nquenching mechanisms such as galaxy-galaxy interaction or strangulation. These\nfindings suggest that SDSS-C4 3028 may have undergone an \"inside-out\" formation\nand quenching process. Its ICM may have been expelled by intense AGN feedback\nafter core formation but before full cluster assembly. The high fraction\n($\\sim$0.63) of star-forming members likely results from the absence of ram\npressure stripping in this blue cluster, supporting the important role of ram\npressure stripping in quenching galaxies in clusters."
                },
                "authors": [
                    {
                        "name": "Shweta Jain"
                    },
                    {
                        "name": "Yuanyuan Su"
                    },
                    {
                        "name": "Andra Stroe"
                    },
                    {
                        "name": "Paul Nulsen"
                    },
                    {
                        "name": "Hyejeon Cho"
                    },
                    {
                        "name": "Kim HyeongHan"
                    },
                    {
                        "name": "M. James Jee"
                    },
                    {
                        "name": "Ralph P. Kraft"
                    },
                    {
                        "name": "Scott Randall"
                    },
                    {
                        "name": "Jimmy A. Irwin"
                    },
                    {
                        "name": "Ryan L. Sanders"
                    },
                    {
                        "name": "Christine Jones"
                    }
                ],
                "author_detail": {
                    "name": "Christine Jones"
                },
                "author": "Christine Jones",
                "arxiv_comment": "14 pages, 10 figures, Published in the Open Journal of Astrophysics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22518v1",
                "updated": "2025-09-26T16:02:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    2,
                    27,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:02:27Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    2,
                    27,
                    4,
                    269,
                    0
                ],
                "title": "REMA: A Unified Reasoning Manifold Framework for Interpreting Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REMA: A Unified Reasoning Manifold Framework for Interpreting Large\n  Language Model"
                },
                "summary": "Understanding how Large Language Models (LLMs) perform complex reasoning and\ntheir failure mechanisms is a challenge in interpretability research. To\nprovide a measurable geometric analysis perspective, we define the concept of\nthe Reasoning Manifold, a latent low-dimensional geometric structure formed by\nthe internal representations corresponding to all correctly reasoned\ngenerations. This structure can be conceptualized as the embodiment of the\neffective thinking paths that the model has learned to successfully solve a\ngiven task. Based on this concept, we build REMA, a framework that explains the\norigins of failures by quantitatively comparing the spatial relationships of\ninternal model representations corresponding to both erroneous and correct\nreasoning samples. Specifically, REMA first quantifies the geometric deviation\nof each erroneous representation by calculating its k-nearest neighbors\ndistance to the approximated manifold formed by correct representations,\nthereby providing a unified failure signal. It then localizes the divergence\npoints where these deviations first become significant by tracking this\ndeviation metric across the model's layers and comparing it against a baseline\nof internal fluctuations from correct representations, thus identifying where\nthe reasoning chain begins to go off-track. Our extensive experiments on\ndiverse language and multimodal models and tasks demonstrate the\nlow-dimensional nature of the reasoning manifold and the high separability\nbetween erroneous and correct reasoning representations. The results also\nvalidate the effectiveness of the REMA framework in analyzing the origins of\nreasoning failures. This research connects abstract reasoning failures to\nmeasurable geometric deviations in representations, providing new avenues for\nin-depth understanding and diagnosis of the internal computational processes of\nblack-box models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding how Large Language Models (LLMs) perform complex reasoning and\ntheir failure mechanisms is a challenge in interpretability research. To\nprovide a measurable geometric analysis perspective, we define the concept of\nthe Reasoning Manifold, a latent low-dimensional geometric structure formed by\nthe internal representations corresponding to all correctly reasoned\ngenerations. This structure can be conceptualized as the embodiment of the\neffective thinking paths that the model has learned to successfully solve a\ngiven task. Based on this concept, we build REMA, a framework that explains the\norigins of failures by quantitatively comparing the spatial relationships of\ninternal model representations corresponding to both erroneous and correct\nreasoning samples. Specifically, REMA first quantifies the geometric deviation\nof each erroneous representation by calculating its k-nearest neighbors\ndistance to the approximated manifold formed by correct representations,\nthereby providing a unified failure signal. It then localizes the divergence\npoints where these deviations first become significant by tracking this\ndeviation metric across the model's layers and comparing it against a baseline\nof internal fluctuations from correct representations, thus identifying where\nthe reasoning chain begins to go off-track. Our extensive experiments on\ndiverse language and multimodal models and tasks demonstrate the\nlow-dimensional nature of the reasoning manifold and the high separability\nbetween erroneous and correct reasoning representations. The results also\nvalidate the effectiveness of the REMA framework in analyzing the origins of\nreasoning failures. This research connects abstract reasoning failures to\nmeasurable geometric deviations in representations, providing new avenues for\nin-depth understanding and diagnosis of the internal computational processes of\nblack-box models."
                },
                "authors": [
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Guanzhi Deng"
                    },
                    {
                        "name": "Ronghao Chen"
                    },
                    {
                        "name": "Junrong Yue"
                    },
                    {
                        "name": "Shuo Zhang"
                    },
                    {
                        "name": "Qinghua Zhao"
                    },
                    {
                        "name": "Linqi Song"
                    },
                    {
                        "name": "Lijie Wen"
                    }
                ],
                "author_detail": {
                    "name": "Lijie Wen"
                },
                "author": "Lijie Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16654v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16654v2",
                "updated": "2025-09-26T16:01:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    1,
                    22,
                    4,
                    269,
                    0
                ],
                "published": "2025-03-20T19:11:06Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    19,
                    11,
                    6,
                    3,
                    79,
                    0
                ],
                "title": "Local models and Bell inequalities for the minimal triangle network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local models and Bell inequalities for the minimal triangle network"
                },
                "summary": "Nonlocal correlations created in networks with multiple independent sources\nenable surprising phenomena in quantum information and quantum foundations. The\npresence of independent sources, however, makes the analysis of network\nnonlocality challenging, and even in the simplest nontrivial scenarios a\ncomplete characterization is lacking. In this work we study one of the simplest\nof these scenarios, namely that of distributions invariant under permutations\nof parties in the minimal triangle network, which features no inputs and binary\noutcomes. We perform an exhaustive search for triangle-local models, and from\nit we infer analytic expressions for the boundaries of the set of distributions\nthat admit such models, which we conjecture to be all the tight Bell\ninequalities for the scenario. Armed with them and with improved outer\napproximations of the set, we provide insights on the existence of a\nclassical-quantum gap in the triangle network with binary outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlocal correlations created in networks with multiple independent sources\nenable surprising phenomena in quantum information and quantum foundations. The\npresence of independent sources, however, makes the analysis of network\nnonlocality challenging, and even in the simplest nontrivial scenarios a\ncomplete characterization is lacking. In this work we study one of the simplest\nof these scenarios, namely that of distributions invariant under permutations\nof parties in the minimal triangle network, which features no inputs and binary\noutcomes. We perform an exhaustive search for triangle-local models, and from\nit we infer analytic expressions for the boundaries of the set of distributions\nthat admit such models, which we conjecture to be all the tight Bell\ninequalities for the scenario. Armed with them and with improved outer\napproximations of the set, we provide insights on the existence of a\nclassical-quantum gap in the triangle network with binary outcomes."
                },
                "authors": [
                    {
                        "name": "José Mário da Silva"
                    },
                    {
                        "name": "Alejandro Pozas-Kerstjens"
                    },
                    {
                        "name": "Fernando Parisio"
                    }
                ],
                "author_detail": {
                    "name": "Fernando Parisio"
                },
                "author": "Fernando Parisio",
                "arxiv_doi": "10.1103/cg9v-wv2t",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/cg9v-wv2t",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.16654v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16654v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 5 figures. Computational appendix available at\n  https://github.com/mariofilho281/symmetric_triangle",
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03918v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03918v2",
                "updated": "2025-09-26T15:57:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    57,
                    36,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-04T06:13:28Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    6,
                    13,
                    28,
                    3,
                    247,
                    0
                ],
                "title": "Chain or tree? Re-evaluating complex reasoning from the perspective of a\n  matrix of thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain or tree? Re-evaluating complex reasoning from the perspective of a\n  matrix of thought"
                },
                "summary": "Large Language Models (LLMs) face significant accuracy degradation due to\ninsufficient reasoning ability when dealing with complex and abstract tasks.\nThought structures such as Chain of Thought (CoT) and Tree of Thought (ToT)\nfocus on enhancing the reasoning capability of LLMs. However, they suffer from\ninherent drawbacks such as redundancy within the same layer of the tree\nstructure and the singularity of the paths in the chain structure. Some studies\nhave utilized Retrieval-Augmented Generation (RAG) methods to enhance CoT and\nToT in mitigating hallucinations in LLMs, yet the fundamental shortcomings of\nthe thought structures still persist. Furthermore, when dealing with\nmulti-entity and multi-hop information, the retrieved verification knowledge\noften contains large amounts of fragmented, superficial, or even erroneous\ndata, misleading the reasoning process of LLMs. To address these issues, we\npropose the Matrix of Thought (MoT), a novel and efficient thought structure\nfor LLMs. MoT explores problems in both horizontal and vertical dimensions\nthrough a \"column-cell communication\" mechanism, enabling LLMs to actively\nengage in multi-strategy and deep thinking while reducing redundancy in the\nthought nodes within the column cells, thereby enhancing the reasoning\ncapability of LLMs. Additionally, through a fact-correction mechanism, it\nleverages the knowledge graph triples retrieved by RAG and the original text to\nconstruct knowledge units and correct erroneous answers. To validate the\neffectiveness of this method, we conducted extensive experiments in three\ntasks: 24-point game, question answering evaluation, and proposition\nwriting.The results demonstrate that our framework outperforms state-of-the-art\nmethods, with reasoning time only 14.4\\% of that of the baseline method,\nproving its efficiency and accuracy. The code for framework is available at\nhttps://github.com/lyfiter/mtqa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face significant accuracy degradation due to\ninsufficient reasoning ability when dealing with complex and abstract tasks.\nThought structures such as Chain of Thought (CoT) and Tree of Thought (ToT)\nfocus on enhancing the reasoning capability of LLMs. However, they suffer from\ninherent drawbacks such as redundancy within the same layer of the tree\nstructure and the singularity of the paths in the chain structure. Some studies\nhave utilized Retrieval-Augmented Generation (RAG) methods to enhance CoT and\nToT in mitigating hallucinations in LLMs, yet the fundamental shortcomings of\nthe thought structures still persist. Furthermore, when dealing with\nmulti-entity and multi-hop information, the retrieved verification knowledge\noften contains large amounts of fragmented, superficial, or even erroneous\ndata, misleading the reasoning process of LLMs. To address these issues, we\npropose the Matrix of Thought (MoT), a novel and efficient thought structure\nfor LLMs. MoT explores problems in both horizontal and vertical dimensions\nthrough a \"column-cell communication\" mechanism, enabling LLMs to actively\nengage in multi-strategy and deep thinking while reducing redundancy in the\nthought nodes within the column cells, thereby enhancing the reasoning\ncapability of LLMs. Additionally, through a fact-correction mechanism, it\nleverages the knowledge graph triples retrieved by RAG and the original text to\nconstruct knowledge units and correct erroneous answers. To validate the\neffectiveness of this method, we conducted extensive experiments in three\ntasks: 24-point game, question answering evaluation, and proposition\nwriting.The results demonstrate that our framework outperforms state-of-the-art\nmethods, with reasoning time only 14.4\\% of that of the baseline method,\nproving its efficiency and accuracy. The code for framework is available at\nhttps://github.com/lyfiter/mtqa."
                },
                "authors": [
                    {
                        "name": "Fengxiao Tang"
                    },
                    {
                        "name": "Yufeng Li"
                    },
                    {
                        "name": "Zongzong Wu"
                    },
                    {
                        "name": "Ming Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Ming Zhao"
                },
                "author": "Ming Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03918v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03918v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22512v1",
                "updated": "2025-09-26T15:54:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    54,
                    50,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:54:50Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    54,
                    50,
                    4,
                    269,
                    0
                ],
                "title": "AxLLM: accelerator architecture for large language models with\n  computation reuse capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AxLLM: accelerator architecture for large language models with\n  computation reuse capability"
                },
                "summary": "Large language models demand massive computational power and memory\nresources, posing significant challenges for efficient deployment. While\nquantization has been widely explored to reduce model size and computation,\nthis paper demonstrates an additional benefit: quantization increases parameter\nlocality, creating opportunities for computation reuse. Building on this\ninsight, we propose AxLLM, a hardware accelerator architecture designed for\nquantized models. Axllm introduces a novel redundancy elimination technique\nthat caches and reuses multiplication results for repeated weight values,\nsubstantially reducing redundant operations. The architecture features dual\nmultiply and reuse pipelines, efficiently supporting both base models and LoRA\nfine-tuned models without altering parameters, retraining, or requiring offline\npreprocessing. Experimental results show that AxLLM achieves up to 90%\nreduction in computations, delivering 28% lower energy consumption and a 1.7x\nspeedup over baseline execution. These results highlight Axllm as a scalable\nand efficient solution for accelerating LLMs on specialized hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models demand massive computational power and memory\nresources, posing significant challenges for efficient deployment. While\nquantization has been widely explored to reduce model size and computation,\nthis paper demonstrates an additional benefit: quantization increases parameter\nlocality, creating opportunities for computation reuse. Building on this\ninsight, we propose AxLLM, a hardware accelerator architecture designed for\nquantized models. Axllm introduces a novel redundancy elimination technique\nthat caches and reuses multiplication results for repeated weight values,\nsubstantially reducing redundant operations. The architecture features dual\nmultiply and reuse pipelines, efficiently supporting both base models and LoRA\nfine-tuned models without altering parameters, retraining, or requiring offline\npreprocessing. Experimental results show that AxLLM achieves up to 90%\nreduction in computations, delivering 28% lower energy consumption and a 1.7x\nspeedup over baseline execution. These results highlight Axllm as a scalable\nand efficient solution for accelerating LLMs on specialized hardware."
                },
                "authors": [
                    {
                        "name": "Soroush Ahadi"
                    },
                    {
                        "name": "Mehdi Modarressi"
                    },
                    {
                        "name": "Masoud Daneshtalab"
                    }
                ],
                "author_detail": {
                    "name": "Masoud Daneshtalab"
                },
                "author": "Masoud Daneshtalab",
                "arxiv_comment": "7 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "n/a",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22510v1",
                "updated": "2025-09-26T15:52:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    52,
                    21,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:52:21Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    52,
                    21,
                    4,
                    269,
                    0
                ],
                "title": "We Think, Therefore We Align LLMs to Helpful, Harmless and Honest Before\n  They Go Wrong",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We Think, Therefore We Align LLMs to Helpful, Harmless and Honest Before\n  They Go Wrong"
                },
                "summary": "Alignment of Large Language Models (LLMs) along multiple\nobjectives-helpfulness, harmlessness, and honesty (HHH)-is critical for safe\nand reliable deployment. Prior work has used steering vector-small control\nsignals injected into hidden states-to guide LLM outputs, typically via\none-to-one (1-to-1) Transformer decoders. In this setting, optimizing a single\nalignment objective can inadvertently overwrite representations learned for\nother objectives, leading to catastrophic forgetting. More recent approaches\nextend steering vectors via one-to-many (1-to-N) Transformer decoders. While\nthis alleviates catastrophic forgetting, naive multi-branch designs optimize\neach objective independently, which can cause inference fragmentation-outputs\nacross HHH objectives may become inconsistent. We propose Adaptive Multi-Branch\nSteering (AMBS), a two-stage 1-to-N framework for unified and efficient\nmulti-objective alignment. In Stage I, post-attention hidden states of the\nTransformer layer are computed once to form a shared representation. In Stage\nII, this representation is cloned into parallel branches and steered via a\npolicy-reference mechanism, enabling objective-specific control while\nmaintaining cross-objective consistency. Empirical evaluations on Alpaca,\nBeaverTails, and TruthfulQA show that AMBS consistently improves HHH alignment\nacross multiple 7B LLM backbones. For example, on DeepSeek-7B, AMBS improves\naverage alignment scores by +32.4% and reduces unsafe outputs by 11.0% compared\nto a naive 1-to-N baseline, while remaining competitive with state-of-the-art\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment of Large Language Models (LLMs) along multiple\nobjectives-helpfulness, harmlessness, and honesty (HHH)-is critical for safe\nand reliable deployment. Prior work has used steering vector-small control\nsignals injected into hidden states-to guide LLM outputs, typically via\none-to-one (1-to-1) Transformer decoders. In this setting, optimizing a single\nalignment objective can inadvertently overwrite representations learned for\nother objectives, leading to catastrophic forgetting. More recent approaches\nextend steering vectors via one-to-many (1-to-N) Transformer decoders. While\nthis alleviates catastrophic forgetting, naive multi-branch designs optimize\neach objective independently, which can cause inference fragmentation-outputs\nacross HHH objectives may become inconsistent. We propose Adaptive Multi-Branch\nSteering (AMBS), a two-stage 1-to-N framework for unified and efficient\nmulti-objective alignment. In Stage I, post-attention hidden states of the\nTransformer layer are computed once to form a shared representation. In Stage\nII, this representation is cloned into parallel branches and steered via a\npolicy-reference mechanism, enabling objective-specific control while\nmaintaining cross-objective consistency. Empirical evaluations on Alpaca,\nBeaverTails, and TruthfulQA show that AMBS consistently improves HHH alignment\nacross multiple 7B LLM backbones. For example, on DeepSeek-7B, AMBS improves\naverage alignment scores by +32.4% and reduces unsafe outputs by 11.0% compared\nto a naive 1-to-N baseline, while remaining competitive with state-of-the-art\nmethods."
                },
                "authors": [
                    {
                        "name": "Gautam Siddharth Kashyap"
                    },
                    {
                        "name": "Mark Dras"
                    },
                    {
                        "name": "Usman Naseem"
                    }
                ],
                "author_detail": {
                    "name": "Usman Naseem"
                },
                "author": "Usman Naseem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18777v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18777v3",
                "updated": "2025-09-26T15:49:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    49,
                    44,
                    4,
                    269,
                    0
                ],
                "published": "2025-05-24T16:30:13Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    16,
                    30,
                    13,
                    5,
                    144,
                    0
                ],
                "title": "HD-PiSSA: High-Rank Distributed Orthogonal Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HD-PiSSA: High-Rank Distributed Orthogonal Adaptation"
                },
                "summary": "Existing parameter-efficient fine-tuning (PEFT) methods for large language\nmodels (LLMs), such as LoRA and PiSSA, constrain model updates to low-rank\nsubspaces, limiting their expressiveness and leading to suboptimal performance\non complex tasks. To address this, we introduce High-rank Distributed PiSSA\n(HD-PiSSA), a distributed PEFT approach that initializes orthogonal adapters\nacross different devices and aggregates their delta updates collectively on W\nfor fine-tuning. Unlike Data Parallel LoRA or PiSSA, which maintain identical\nadapters across all devices, HD-PiSSA assigns different principal components of\nthe pre-trained weights to each GPU, significantly expanding the range of\nupdate directions. This results in over 16x higher effective updated ranks than\ndata-parallel LoRA or PiSSA when fine-tuning on 8 GPUs with the same per-device\nadapter rank. Empirically, we evaluate HD-PiSSA across various challenging\ndownstream tasks, including mathematics, code generation, and multi-task\nlearning. In the multi-task setting, HD-PiSSA achieves average gains of 10.0\nabsolute points (14.63%) over LoRA and 4.98 points (6.60%) over PiSSA across 12\nbenchmarks, demonstrating its benefits from the extra optimization flexibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing parameter-efficient fine-tuning (PEFT) methods for large language\nmodels (LLMs), such as LoRA and PiSSA, constrain model updates to low-rank\nsubspaces, limiting their expressiveness and leading to suboptimal performance\non complex tasks. To address this, we introduce High-rank Distributed PiSSA\n(HD-PiSSA), a distributed PEFT approach that initializes orthogonal adapters\nacross different devices and aggregates their delta updates collectively on W\nfor fine-tuning. Unlike Data Parallel LoRA or PiSSA, which maintain identical\nadapters across all devices, HD-PiSSA assigns different principal components of\nthe pre-trained weights to each GPU, significantly expanding the range of\nupdate directions. This results in over 16x higher effective updated ranks than\ndata-parallel LoRA or PiSSA when fine-tuning on 8 GPUs with the same per-device\nadapter rank. Empirically, we evaluate HD-PiSSA across various challenging\ndownstream tasks, including mathematics, code generation, and multi-task\nlearning. In the multi-task setting, HD-PiSSA achieves average gains of 10.0\nabsolute points (14.63%) over LoRA and 4.98 points (6.60%) over PiSSA across 12\nbenchmarks, demonstrating its benefits from the extra optimization flexibility."
                },
                "authors": [
                    {
                        "name": "Yiding Wang"
                    },
                    {
                        "name": "Fauxu Meng"
                    },
                    {
                        "name": "Xuefeng Zhang"
                    },
                    {
                        "name": "Fan Jiang"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18777v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18777v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22506v1",
                "updated": "2025-09-26T15:48:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    48,
                    10,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:48:10Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    48,
                    10,
                    4,
                    269,
                    0
                ],
                "title": "Representing LLMs in Prompt Semantic Task Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representing LLMs in Prompt Semantic Task Space"
                },
                "summary": "Large language models (LLMs) achieve impressive results over various tasks,\nand ever-expanding public repositories contain an abundance of pre-trained\nmodels. Therefore, identifying the best-performing LLM for a given task is a\nsignificant challenge. Previous works have suggested learning LLM\nrepresentations to address this. However, these approaches present limited\nscalability and require costly retraining to encompass additional models and\ndatasets. Moreover, the produced representation utilizes distinct spaces that\ncannot be easily interpreted. This work presents an efficient, training-free\napproach to representing LLMs as linear operators within the prompts' semantic\ntask space, thus providing a highly interpretable representation of the models'\napplication. Our method utilizes closed-form computation of geometrical\nproperties and ensures exceptional scalability and real-time adaptability to\ndynamically expanding repositories. We demonstrate our approach on success\nprediction and model selection tasks, achieving competitive or state-of-the-art\nresults with notable performance in out-of-sample scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve impressive results over various tasks,\nand ever-expanding public repositories contain an abundance of pre-trained\nmodels. Therefore, identifying the best-performing LLM for a given task is a\nsignificant challenge. Previous works have suggested learning LLM\nrepresentations to address this. However, these approaches present limited\nscalability and require costly retraining to encompass additional models and\ndatasets. Moreover, the produced representation utilizes distinct spaces that\ncannot be easily interpreted. This work presents an efficient, training-free\napproach to representing LLMs as linear operators within the prompts' semantic\ntask space, thus providing a highly interpretable representation of the models'\napplication. Our method utilizes closed-form computation of geometrical\nproperties and ensures exceptional scalability and real-time adaptability to\ndynamically expanding repositories. We demonstrate our approach on success\nprediction and model selection tasks, achieving competitive or state-of-the-art\nresults with notable performance in out-of-sample scenarios."
                },
                "authors": [
                    {
                        "name": "Idan Kashani"
                    },
                    {
                        "name": "Avi Mendelson"
                    },
                    {
                        "name": "Yaniv Nemcovsky"
                    }
                ],
                "author_detail": {
                    "name": "Yaniv Nemcovsky"
                },
                "author": "Yaniv Nemcovsky",
                "arxiv_comment": "Accepted to Findings of the Association for Computational\n  Linguistics: EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68T50, 65F20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19817v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19817v2",
                "updated": "2025-09-26T15:44:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    44,
                    51,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-24T06:56:26Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    6,
                    56,
                    26,
                    2,
                    267,
                    0
                ],
                "title": "MMedFD: A Real-world Healthcare Benchmark for Multi-turn Full-Duplex\n  Automatic Speech Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMedFD: A Real-world Healthcare Benchmark for Multi-turn Full-Duplex\n  Automatic Speech Recognition"
                },
                "summary": "Automatic speech recognition (ASR) in clinical dialogue demands robustness to\nfull-duplex interaction, speaker overlap, and low-latency constraints, yet open\nbenchmarks remain scarce. We present MMedFD, the first real-world Chinese\nhealthcare ASR corpus designed for multi-turn, full-duplex settings. Captured\nfrom a deployed AI assistant, the dataset comprises 5,805 annotated sessions\nwith synchronized user and mixed-channel views, RTTM/CTM timing, and role\nlabels. We introduce a model-agnostic pipeline for streaming segmentation,\nspeaker attribution, and dialogue memory, and fine-tune Whisper-small on\nrole-concatenated audio for long-context recognition. ASR evaluation includes\nWER, CER, and HC-WER, which measures concept-level accuracy across healthcare\nsettings. LLM-generated responses are assessed using rubric-based and pairwise\nprotocols. MMedFD establishes a reproducible framework for benchmarking\nstreaming ASR and end-to-end duplex agents in healthcare deployment. The\ndataset and related resources are publicly available at\nhttps://github.com/Kinetics-JOJO/MMedFD",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic speech recognition (ASR) in clinical dialogue demands robustness to\nfull-duplex interaction, speaker overlap, and low-latency constraints, yet open\nbenchmarks remain scarce. We present MMedFD, the first real-world Chinese\nhealthcare ASR corpus designed for multi-turn, full-duplex settings. Captured\nfrom a deployed AI assistant, the dataset comprises 5,805 annotated sessions\nwith synchronized user and mixed-channel views, RTTM/CTM timing, and role\nlabels. We introduce a model-agnostic pipeline for streaming segmentation,\nspeaker attribution, and dialogue memory, and fine-tune Whisper-small on\nrole-concatenated audio for long-context recognition. ASR evaluation includes\nWER, CER, and HC-WER, which measures concept-level accuracy across healthcare\nsettings. LLM-generated responses are assessed using rubric-based and pairwise\nprotocols. MMedFD establishes a reproducible framework for benchmarking\nstreaming ASR and end-to-end duplex agents in healthcare deployment. The\ndataset and related resources are publicly available at\nhttps://github.com/Kinetics-JOJO/MMedFD"
                },
                "authors": [
                    {
                        "name": "Hongzhao Chen"
                    },
                    {
                        "name": "XiaoYang Wang"
                    },
                    {
                        "name": "Jing Lan"
                    },
                    {
                        "name": "Hexiao Ding"
                    },
                    {
                        "name": "Yufeng Jiang"
                    },
                    {
                        "name": "MingHui Yang"
                    },
                    {
                        "name": "DanHui Xu"
                    },
                    {
                        "name": "Jun Luo"
                    },
                    {
                        "name": "Nga-Chun Ng"
                    },
                    {
                        "name": "Gerald W. Y. Cheng"
                    },
                    {
                        "name": "Yunlin Mao"
                    },
                    {
                        "name": "Jung Sun Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Jung Sun Yoo"
                },
                "author": "Jung Sun Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19817v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19817v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22502v1",
                "updated": "2025-09-26T15:44:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    44,
                    9,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:44:09Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    44,
                    9,
                    4,
                    269,
                    0
                ],
                "title": "InfiAgent: Self-Evolving Pyramid Agent Framework for Infinite Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiAgent: Self-Evolving Pyramid Agent Framework for Infinite Scenarios"
                },
                "summary": "Large Language Model (LLM) agents have demonstrated remarkable capabilities\nin organizing and executing complex tasks, and many such agents are now widely\nused in various application scenarios. However, developing these agents\nrequires carefully designed workflows, carefully crafted prompts, and iterative\ntuning, which requires LLM techniques and domain-specific expertise. These\nhand-crafted limitations hinder the scalability and cost-effectiveness of LLM\nagents across a wide range of industries. To address these challenges, we\npropose \\textbf{InfiAgent}, a Pyramid-like DAG-based Multi-Agent Framework that\ncan be applied to \\textbf{infi}nite scenarios, which introduces several key\ninnovations: a generalized \"agent-as-a-tool\" mechanism that automatically\ndecomposes complex agents into hierarchical multi-agent systems; a dual-audit\nmechanism that ensures the quality and stability of task completion; an agent\nrouting function that enables efficient task-agent matching; and an agent\nself-evolution mechanism that autonomously restructures the agent DAG based on\nnew tasks, poor performance, or optimization opportunities. Furthermore,\nInfiAgent's atomic task design supports agent parallelism, significantly\nimproving execution efficiency. This framework evolves into a versatile\npyramid-like multi-agent system capable of solving a wide range of problems.\nEvaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\\%\nhigher performance compared to ADAS (similar auto-generated agent framework),\nwhile a case study of the AI research assistant InfiHelper shows that it\ngenerates scientific papers that have received recognition from human reviewers\nat top-tier IEEE conferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents have demonstrated remarkable capabilities\nin organizing and executing complex tasks, and many such agents are now widely\nused in various application scenarios. However, developing these agents\nrequires carefully designed workflows, carefully crafted prompts, and iterative\ntuning, which requires LLM techniques and domain-specific expertise. These\nhand-crafted limitations hinder the scalability and cost-effectiveness of LLM\nagents across a wide range of industries. To address these challenges, we\npropose \\textbf{InfiAgent}, a Pyramid-like DAG-based Multi-Agent Framework that\ncan be applied to \\textbf{infi}nite scenarios, which introduces several key\ninnovations: a generalized \"agent-as-a-tool\" mechanism that automatically\ndecomposes complex agents into hierarchical multi-agent systems; a dual-audit\nmechanism that ensures the quality and stability of task completion; an agent\nrouting function that enables efficient task-agent matching; and an agent\nself-evolution mechanism that autonomously restructures the agent DAG based on\nnew tasks, poor performance, or optimization opportunities. Furthermore,\nInfiAgent's atomic task design supports agent parallelism, significantly\nimproving execution efficiency. This framework evolves into a versatile\npyramid-like multi-agent system capable of solving a wide range of problems.\nEvaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\\%\nhigher performance compared to ADAS (similar auto-generated agent framework),\nwhile a case study of the AI research assistant InfiHelper shows that it\ngenerates scientific papers that have received recognition from human reviewers\nat top-tier IEEE conferences."
                },
                "authors": [
                    {
                        "name": "Chenglin Yu"
                    },
                    {
                        "name": "Yang Yu"
                    },
                    {
                        "name": "Songmiao Wang"
                    },
                    {
                        "name": "Yucheng Wang"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Jinjia Li"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Hongxia Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hongxia Yang"
                },
                "author": "Hongxia Yang",
                "arxiv_comment": "9 pages of main content and 32 pages of others, 2 figures, under\n  review as a conference paper at ICLR 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22501v1",
                "updated": "2025-09-26T15:42:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    42,
                    57,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:42:57Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    42,
                    57,
                    4,
                    269,
                    0
                ],
                "title": "Modelling non-stationary extremal dependence through a geometric\n  approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling non-stationary extremal dependence through a geometric\n  approach"
                },
                "summary": "Non-stationary extremal dependence, whereby the relationship between the\nextremes of multiple variables evolves over time, is commonly observed in many\nenvironmental and financial data sets. However, most multivariate extreme value\nmodels are only suited to stationary data. A recent approach to multivariate\nextreme value modelling uses a geometric framework, whereby extremal dependence\nfeatures are inferred through the limiting shapes of scaled sample clouds. This\nframework can capture a wide range of dependence structures, and a variety of\ninference procedures have been proposed in the stationary setting. In this\nwork, we first extend the geometric framework to the non-stationary setting and\noutline assumptions to ensure the necessary convergence conditions hold. We\nthen introduce a flexible, semi-parametric modelling framework for obtaining\nestimates of limit sets in the non-stationary setting. Through rigorous\nsimulation studies, we demonstrate that our proposed framework can capture a\nwide range of dependence forms and is robust to different model formulations.\nWe illustrate the proposed methods on financial returns data and present\nseveral practical uses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-stationary extremal dependence, whereby the relationship between the\nextremes of multiple variables evolves over time, is commonly observed in many\nenvironmental and financial data sets. However, most multivariate extreme value\nmodels are only suited to stationary data. A recent approach to multivariate\nextreme value modelling uses a geometric framework, whereby extremal dependence\nfeatures are inferred through the limiting shapes of scaled sample clouds. This\nframework can capture a wide range of dependence structures, and a variety of\ninference procedures have been proposed in the stationary setting. In this\nwork, we first extend the geometric framework to the non-stationary setting and\noutline assumptions to ensure the necessary convergence conditions hold. We\nthen introduce a flexible, semi-parametric modelling framework for obtaining\nestimates of limit sets in the non-stationary setting. Through rigorous\nsimulation studies, we demonstrate that our proposed framework can capture a\nwide range of dependence forms and is robust to different model formulations.\nWe illustrate the proposed methods on financial returns data and present\nseveral practical uses."
                },
                "authors": [
                    {
                        "name": "C. J. R. Murphy-Barltrop"
                    },
                    {
                        "name": "J. L. Wadsworth"
                    },
                    {
                        "name": "M. de Carvalho"
                    },
                    {
                        "name": "B. D. Youngman"
                    }
                ],
                "author_detail": {
                    "name": "B. D. Youngman"
                },
                "author": "B. D. Youngman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22499v1",
                "updated": "2025-09-26T15:41:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    41,
                    15,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:41:15Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    41,
                    15,
                    4,
                    269,
                    0
                ],
                "title": "A Multiplicative Instrumental Variable Model for Data Missing\n  Not-at-Random",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multiplicative Instrumental Variable Model for Data Missing\n  Not-at-Random"
                },
                "summary": "Instrumental variable (IV) methods offer a valuable approach to account for\noutcome data missing not-at-random. A valid missing data instrument is a\nmeasured factor which (i) predicts the nonresponse process and (ii) is\nindependent of the outcome in the underlying population. For point\nidentification, all existing IV methods for missing data including the\ncelebrated Heckman selection model, a priori restrict the extent of selection\nbias on the outcome scale, therefore potentially understating uncertainty due\nto missing data. In this work, we introduce an IV framework which allows the\ndegree of selection bias on the outcome scale to remain completely\nunrestricted. The new approach instead relies for identification on (iii) a key\nmultiplicative selection model, which posits that the instrument and any hidden\ncommon correlate of selection and the outcome, do not interact on the\nmultiplicative scale. Interestingly, we establish that any regular statistical\nfunctional of the missing outcome is nonparametrically identified under\n(i)-(iii) via a single-arm Wald ratio estimand reminiscent of the standard Wald\nratio estimand in causal inference. For estimation and inference, we\ncharacterize the influence function for any functional defined on a\nnonparametric model for the observed data, which we leverage to develop\nsemiparametric multiply robust IV estimators. Several extensions of the methods\nare also considered, including the important practical setting of polytomous\nand continuous instruments. Simulation studies illustrate the favorable finite\nsample performance of proposed methods, which we further showcase in an HIV\nstudy nested within a household health survey study we conducted in Mochudi,\nBotswana, in which interviewer characteristics are used as instruments to\ncorrect for selection bias due to dependent nonresponse in the HIV component of\nthe survey study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instrumental variable (IV) methods offer a valuable approach to account for\noutcome data missing not-at-random. A valid missing data instrument is a\nmeasured factor which (i) predicts the nonresponse process and (ii) is\nindependent of the outcome in the underlying population. For point\nidentification, all existing IV methods for missing data including the\ncelebrated Heckman selection model, a priori restrict the extent of selection\nbias on the outcome scale, therefore potentially understating uncertainty due\nto missing data. In this work, we introduce an IV framework which allows the\ndegree of selection bias on the outcome scale to remain completely\nunrestricted. The new approach instead relies for identification on (iii) a key\nmultiplicative selection model, which posits that the instrument and any hidden\ncommon correlate of selection and the outcome, do not interact on the\nmultiplicative scale. Interestingly, we establish that any regular statistical\nfunctional of the missing outcome is nonparametrically identified under\n(i)-(iii) via a single-arm Wald ratio estimand reminiscent of the standard Wald\nratio estimand in causal inference. For estimation and inference, we\ncharacterize the influence function for any functional defined on a\nnonparametric model for the observed data, which we leverage to develop\nsemiparametric multiply robust IV estimators. Several extensions of the methods\nare also considered, including the important practical setting of polytomous\nand continuous instruments. Simulation studies illustrate the favorable finite\nsample performance of proposed methods, which we further showcase in an HIV\nstudy nested within a household health survey study we conducted in Mochudi,\nBotswana, in which interviewer characteristics are used as instruments to\ncorrect for selection bias due to dependent nonresponse in the HIV component of\nthe survey study."
                },
                "authors": [
                    {
                        "name": "Yunshu Zhang"
                    },
                    {
                        "name": "Chan Park"
                    },
                    {
                        "name": "Jiewen Liu"
                    },
                    {
                        "name": "Yonghoon Lee"
                    },
                    {
                        "name": "Mengxin Yu"
                    },
                    {
                        "name": "James M. Robins"
                    },
                    {
                        "name": "Eric J. Tchetgen Tchetgen"
                    }
                ],
                "author_detail": {
                    "name": "Eric J. Tchetgen Tchetgen"
                },
                "author": "Eric J. Tchetgen Tchetgen",
                "arxiv_comment": "14 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22490v1",
                "updated": "2025-09-26T15:35:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    35,
                    38,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:35:38Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    35,
                    38,
                    4,
                    269,
                    0
                ],
                "title": "JGU Mainz's Submission to the WMT25 Shared Task on LLMs with Limited\n  Resources for Slavic Languages: MT and QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JGU Mainz's Submission to the WMT25 Shared Task on LLMs with Limited\n  Resources for Slavic Languages: MT and QA"
                },
                "summary": "This paper presents the JGU Mainz submission to the WMT25 Shared Task on LLMs\nwith Limited Resources for Slavic Languages: Machine Translation and Question\nAnswering, focusing on Ukrainian, Upper Sorbian, and Lower Sorbian. For each\nlanguage, we jointly fine-tune a Qwen2.5-3B-Instruct model for both tasks with\nparameter-efficient finetuning. Our pipeline integrates additional translation\nand multiple-choice question answering (QA) data. For Ukrainian QA, we further\nuse retrieval-augmented generation. We also apply ensembling for QA in Upper\nand Lower Sorbian. Experiments show that our models outperform the baseline on\nboth tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the JGU Mainz submission to the WMT25 Shared Task on LLMs\nwith Limited Resources for Slavic Languages: Machine Translation and Question\nAnswering, focusing on Ukrainian, Upper Sorbian, and Lower Sorbian. For each\nlanguage, we jointly fine-tune a Qwen2.5-3B-Instruct model for both tasks with\nparameter-efficient finetuning. Our pipeline integrates additional translation\nand multiple-choice question answering (QA) data. For Ukrainian QA, we further\nuse retrieval-augmented generation. We also apply ensembling for QA in Upper\nand Lower Sorbian. Experiments show that our models outperform the baseline on\nboth tasks."
                },
                "authors": [
                    {
                        "name": "Hossain Shaikh Saadi"
                    },
                    {
                        "name": "Minh Duc Bui"
                    },
                    {
                        "name": "Mario Sanz-Guerrero"
                    },
                    {
                        "name": "Katharina von der Wense"
                    }
                ],
                "author_detail": {
                    "name": "Katharina von der Wense"
                },
                "author": "Katharina von der Wense",
                "arxiv_comment": "WMT 25 Shared Task LLMs with Limited Resources for Slavic Languages:\n  MT and QA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22489v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22489v1",
                "updated": "2025-09-26T15:35:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    35,
                    20,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:35:20Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    35,
                    20,
                    4,
                    269,
                    0
                ],
                "title": "Passive Learning of Lattice Automata from Recurrent Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Passive Learning of Lattice Automata from Recurrent Neural Networks"
                },
                "summary": "We present a passive automata learning algorithm that can extract automata\nfrom recurrent networks with very large or even infinite alphabets. Our method\ncombines overapproximations from the field of Abstract Interpretation and\npassive automata learning from the field of Grammatical Inference. We evaluate\nour algorithm by first comparing it with the state-of-the-art automata\nextraction algorithm from Recurrent Neural Networks trained on Tomita grammars.\nThen, we extend these experiments to regular languages with infinite alphabets,\nwhich we propose as a novel benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a passive automata learning algorithm that can extract automata\nfrom recurrent networks with very large or even infinite alphabets. Our method\ncombines overapproximations from the field of Abstract Interpretation and\npassive automata learning from the field of Grammatical Inference. We evaluate\nour algorithm by first comparing it with the state-of-the-art automata\nextraction algorithm from Recurrent Neural Networks trained on Tomita grammars.\nThen, we extend these experiments to regular languages with infinite alphabets,\nwhich we propose as a novel benchmark."
                },
                "authors": [
                    {
                        "name": "Jaouhar Slimi"
                    },
                    {
                        "name": "Tristan Le Gall"
                    },
                    {
                        "name": "Augustin Lemesle"
                    }
                ],
                "author_detail": {
                    "name": "Augustin Lemesle"
                },
                "author": "Augustin Lemesle",
                "arxiv_comment": "10 pages, 5 figures. To be published in OVERLAY 2025, 7th\n  International Workshop on Artificial Intelligence and Formal Verification,\n  Logic, Automata, and Synthesis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22489v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22489v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10739v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10739v2",
                "updated": "2025-09-26T15:28:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    28,
                    16,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-12T22:58:05Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    22,
                    58,
                    5,
                    4,
                    255,
                    0
                ],
                "title": "Reasoning Under Uncertainty: Exploring Probabilistic Reasoning\n  Capabilities of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Under Uncertainty: Exploring Probabilistic Reasoning\n  Capabilities of LLMs"
                },
                "summary": "Despite widespread success in language understanding and generation, large\nlanguage models (LLMs) exhibit unclear and often inconsistent behavior when\nfaced with tasks that require probabilistic reasoning. In this work, we present\nthe first comprehensive study of the reasoning capabilities of LLMs over\nexplicit discrete probability distributions. Given observations from a\nprobability distribution, we evaluate models on three carefully designed tasks,\nmode identification, maximum likelihood estimation, and sample generation, by\nprompting them to provide responses to queries about either the joint\ndistribution or its conditionals. These tasks thus probe a range of\nprobabilistic skills, including frequency analysis, marginalization, and\ngenerative behavior. Through comprehensive empirical evaluations, we\ndemonstrate that there exists a clear performance gap between smaller and\nlarger models, with the latter demonstrating stronger inference and surprising\ncapabilities in sample generation. Furthermore, our investigations reveal\nnotable limitations, including sensitivity to variations in the notation\nutilized to represent probabilistic outcomes and performance degradation of\nover 60% as context length increases. Together, our results provide a detailed\nunderstanding of the probabilistic reasoning abilities of LLMs and identify key\ndirections for future improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite widespread success in language understanding and generation, large\nlanguage models (LLMs) exhibit unclear and often inconsistent behavior when\nfaced with tasks that require probabilistic reasoning. In this work, we present\nthe first comprehensive study of the reasoning capabilities of LLMs over\nexplicit discrete probability distributions. Given observations from a\nprobability distribution, we evaluate models on three carefully designed tasks,\nmode identification, maximum likelihood estimation, and sample generation, by\nprompting them to provide responses to queries about either the joint\ndistribution or its conditionals. These tasks thus probe a range of\nprobabilistic skills, including frequency analysis, marginalization, and\ngenerative behavior. Through comprehensive empirical evaluations, we\ndemonstrate that there exists a clear performance gap between smaller and\nlarger models, with the latter demonstrating stronger inference and surprising\ncapabilities in sample generation. Furthermore, our investigations reveal\nnotable limitations, including sensitivity to variations in the notation\nutilized to represent probabilistic outcomes and performance degradation of\nover 60% as context length increases. Together, our results provide a detailed\nunderstanding of the probabilistic reasoning abilities of LLMs and identify key\ndirections for future improvement."
                },
                "authors": [
                    {
                        "name": "Mobina Pournemat"
                    },
                    {
                        "name": "Keivan Rezaei"
                    },
                    {
                        "name": "Gaurang Sriramanan"
                    },
                    {
                        "name": "Arman Zarei"
                    },
                    {
                        "name": "Jiaxiang Fu"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Hamid Eghbalzadeh"
                    },
                    {
                        "name": "Soheil Feizi"
                    }
                ],
                "author_detail": {
                    "name": "Soheil Feizi"
                },
                "author": "Soheil Feizi",
                "arxiv_comment": "27 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10739v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10739v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22480v1",
                "updated": "2025-09-26T15:27:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    27,
                    50,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:27:50Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    27,
                    50,
                    4,
                    269,
                    0
                ],
                "title": "Exploring Solution Divergence and Its Effect on Large Language Model\n  Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Solution Divergence and Its Effect on Large Language Model\n  Problem Solving"
                },
                "summary": "Large language models (LLMs) have been widely used for problem-solving tasks.\nMost recent work improves their performance through supervised fine-tuning\n(SFT) with labeled data or reinforcement learning (RL) from task feedback. In\nthis paper, we study a new perspective: the divergence in solutions generated\nby LLMs for a single problem. We show that higher solution divergence is\npositively related to better problem-solving abilities across various models.\nBased on this finding, we propose solution divergence as a novel metric that\ncan support both SFT and RL strategies. We test this idea on three\nrepresentative problem domains and find that using solution divergence\nconsistently improves success rates. These results suggest that solution\ndivergence is a simple but effective tool for advancing LLM training and\nevaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely used for problem-solving tasks.\nMost recent work improves their performance through supervised fine-tuning\n(SFT) with labeled data or reinforcement learning (RL) from task feedback. In\nthis paper, we study a new perspective: the divergence in solutions generated\nby LLMs for a single problem. We show that higher solution divergence is\npositively related to better problem-solving abilities across various models.\nBased on this finding, we propose solution divergence as a novel metric that\ncan support both SFT and RL strategies. We test this idea on three\nrepresentative problem domains and find that using solution divergence\nconsistently improves success rates. These results suggest that solution\ndivergence is a simple but effective tool for advancing LLM training and\nevaluation."
                },
                "authors": [
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Kaiqi Yang"
                    },
                    {
                        "name": "Yucheng Chu"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Jiliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jiliang Tang"
                },
                "author": "Jiliang Tang",
                "arxiv_comment": "17 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22478v1",
                "updated": "2025-09-26T15:23:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    23,
                    57,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:23:57Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    23,
                    57,
                    4,
                    269,
                    0
                ],
                "title": "halox: Dark matter halo properties and large-scale structure\n  calculations using JAX",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "halox: Dark matter halo properties and large-scale structure\n  calculations using JAX"
                },
                "summary": "Dark matter halos are fundamental structures in cosmology, forming the\ngravitational potential wells hosting galaxies and clusters of galaxies. Their\nproperties and statistical distribution (including the halo mass function) are\ninvaluable tools to infer the fundamental properties of the Universe. The\n\\texttt{halox} package is a JAX-powered Python library enabling differentiable\nand accelerated computations of key properties of dark matter halos, and of the\nhalo mass function. The automatic differentiation capabilities of\n\\texttt{halox} enable its usage in gradient-based workflows, e.g. in efficient\nHamiltonian Monte Carlo sampling or machine learning applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dark matter halos are fundamental structures in cosmology, forming the\ngravitational potential wells hosting galaxies and clusters of galaxies. Their\nproperties and statistical distribution (including the halo mass function) are\ninvaluable tools to infer the fundamental properties of the Universe. The\n\\texttt{halox} package is a JAX-powered Python library enabling differentiable\nand accelerated computations of key properties of dark matter halos, and of the\nhalo mass function. The automatic differentiation capabilities of\n\\texttt{halox} enable its usage in gradient-based workflows, e.g. in efficient\nHamiltonian Monte Carlo sampling or machine learning applications."
                },
                "authors": [
                    {
                        "name": "Florian Kéruzoré"
                    }
                ],
                "author_detail": {
                    "name": "Florian Kéruzoré"
                },
                "author": "Florian Kéruzoré",
                "arxiv_comment": "6 pages, submitted to the Journal of Open Source Software. halox is\n  available at https://github.com/fkeruzore/halox",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16134v2",
                "updated": "2025-09-26T15:21:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    21,
                    49,
                    4,
                    269,
                    0
                ],
                "published": "2025-05-22T02:23:00Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    2,
                    23,
                    0,
                    3,
                    142,
                    0
                ],
                "title": "Beyond Early-Token Bias: Model-Specific and Language-Specific Position\n  Effects in Multilingual LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Early-Token Bias: Model-Specific and Language-Specific Position\n  Effects in Multilingual LLMs"
                },
                "summary": "Large Language Models (LLMs) exhibit position bias - a systematic tendency to\nneglect information at specific context positions. However, the patterns of\nposition bias behavior, depending on the language or model, remain unexplored.\nWe present a multilingual study across five typologically distinct languages\n(English, Russian, German, Hindi, and Vietnamese) and five model architectures,\nexamining how position bias interacts with prompt strategies and affects output\nentropy. Our key findings are: (1) Position bias is primarily model-driven, yet\nexhibits language-specific variations. For instance, Qwen2.5-7B-Instruct and\nDeepSeek 7B Chat consistently favors late positions, challenging established\nassumptions of a universal early-token bias in LLMs. (2) Explicitly instructing\nthe model that \"the context is relevant to the query\" unexpectedly reduces\naccuracy across languages, undermining common prompt-engineering practices. (3)\nWhile the largest accuracy drop occurs when relevant information is placed in\nthe middle of the context, this is not explicitly reflected by a corresponding\npeak in output entropy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit position bias - a systematic tendency to\nneglect information at specific context positions. However, the patterns of\nposition bias behavior, depending on the language or model, remain unexplored.\nWe present a multilingual study across five typologically distinct languages\n(English, Russian, German, Hindi, and Vietnamese) and five model architectures,\nexamining how position bias interacts with prompt strategies and affects output\nentropy. Our key findings are: (1) Position bias is primarily model-driven, yet\nexhibits language-specific variations. For instance, Qwen2.5-7B-Instruct and\nDeepSeek 7B Chat consistently favors late positions, challenging established\nassumptions of a universal early-token bias in LLMs. (2) Explicitly instructing\nthe model that \"the context is relevant to the query\" unexpectedly reduces\naccuracy across languages, undermining common prompt-engineering practices. (3)\nWhile the largest accuracy drop occurs when relevant information is placed in\nthe middle of the context, this is not explicitly reflected by a corresponding\npeak in output entropy."
                },
                "authors": [
                    {
                        "name": "Mikhail Menschikov"
                    },
                    {
                        "name": "Alexander Kharitonov"
                    },
                    {
                        "name": "Maiia Kotyga"
                    },
                    {
                        "name": "Vadim Porvatov"
                    },
                    {
                        "name": "Anna Zhukovskaya"
                    },
                    {
                        "name": "David Kagramanyan"
                    },
                    {
                        "name": "Egor Shvetsov"
                    },
                    {
                        "name": "Evgeny Burnaev"
                    }
                ],
                "author_detail": {
                    "name": "Evgeny Burnaev"
                },
                "author": "Evgeny Burnaev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14364v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14364v4",
                "updated": "2025-09-26T15:19:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    19,
                    34,
                    4,
                    269,
                    0
                ],
                "published": "2024-09-22T08:51:18Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    8,
                    51,
                    18,
                    6,
                    266,
                    0
                ],
                "title": "Position IDs Matter: An Enhanced Position Layout for Efficient Context\n  Compression in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position IDs Matter: An Enhanced Position Layout for Efficient Context\n  Compression in Large Language Models"
                },
                "summary": "Using special tokens (e.g., gist, memory, or compressed tokens) to compress\ncontext information is a common practice for large language models (LLMs).\nHowever, existing approaches often neglect that position encodings inherently\ninduce local inductive biases in models, causing the compression process to\nignore holistic contextual dependencies. We propose \\textbf{Enhanced Position\nLayout (EPL)}, a simple yet effective method that improves the context\ncompression capability of LLMs by only adjusting position IDs, the numerical\nidentifiers that specify token positions. EPL minimizes the distance between\ncontext tokens and their corresponding special tokens and at the same time\nmaintains the sequence order in position IDs between context tokens, special\ntokens, and the subsequent tokens. Integrating EPL into our best performing\ncontext compression model results in a 1.9 ROUGE-1 F1 improvement on\nout-of-domain question answering datasets on average. When extended to\nmultimodal scenarios, EPL leads to an average accuracy gain of 2.6 points for\nvision compression LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using special tokens (e.g., gist, memory, or compressed tokens) to compress\ncontext information is a common practice for large language models (LLMs).\nHowever, existing approaches often neglect that position encodings inherently\ninduce local inductive biases in models, causing the compression process to\nignore holistic contextual dependencies. We propose \\textbf{Enhanced Position\nLayout (EPL)}, a simple yet effective method that improves the context\ncompression capability of LLMs by only adjusting position IDs, the numerical\nidentifiers that specify token positions. EPL minimizes the distance between\ncontext tokens and their corresponding special tokens and at the same time\nmaintains the sequence order in position IDs between context tokens, special\ntokens, and the subsequent tokens. Integrating EPL into our best performing\ncontext compression model results in a 1.9 ROUGE-1 F1 improvement on\nout-of-domain question answering datasets on average. When extended to\nmultimodal scenarios, EPL leads to an average accuracy gain of 2.6 points for\nvision compression LLMs."
                },
                "authors": [
                    {
                        "name": "Runsong Zhao"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Pengcheng Huang"
                    },
                    {
                        "name": "Chunyang Xiao"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14364v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14364v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22472v1",
                "updated": "2025-09-26T15:19:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    19,
                    12,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:19:12Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    19,
                    12,
                    4,
                    269,
                    0
                ],
                "title": "Evaluating the Limits of Large Language Models in Multilingual Legal\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Limits of Large Language Models in Multilingual Legal\n  Reasoning"
                },
                "summary": "In an era dominated by Large Language Models (LLMs), understanding their\ncapabilities and limitations, especially in high-stakes fields like law, is\ncrucial. While LLMs such as Meta's LLaMA, OpenAI's ChatGPT, Google's Gemini,\nDeepSeek, and other emerging models are increasingly integrated into legal\nworkflows, their performance in multilingual, jurisdictionally diverse, and\nadversarial contexts remains insufficiently explored. This work evaluates LLaMA\nand Gemini on multilingual legal and non-legal benchmarks, and assesses their\nadversarial robustness in legal tasks through character and word-level\nperturbations. We use an LLM-as-a-Judge approach for human-aligned evaluation.\nWe moreover present an open-source, modular evaluation pipeline designed to\nsupport multilingual, task-diverse benchmarking of any combination of LLMs and\ndatasets, with a particular focus on legal tasks, including classification,\nsummarization, open questions, and general reasoning. Our findings confirm that\nlegal tasks pose significant challenges for LLMs with accuracies often below\n50% on legal reasoning benchmarks such as LEXam, compared to over 70% on\ngeneral-purpose tasks like XNLI. In addition, while English generally yields\nmore stable results, it does not always lead to higher accuracy. Prompt\nsensitivity and adversarial vulnerability is also shown to persist across\nlanguages. Finally, a correlation is found between the performance of a\nlanguage and its syntactic similarity to English. We also observe that LLaMA is\nweaker than Gemini, with the latter showing an average advantage of about 24\npercentage points across the same task. Despite improvements in newer LLMs,\nchallenges remain in deploying them reliably for critical, multilingual legal\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an era dominated by Large Language Models (LLMs), understanding their\ncapabilities and limitations, especially in high-stakes fields like law, is\ncrucial. While LLMs such as Meta's LLaMA, OpenAI's ChatGPT, Google's Gemini,\nDeepSeek, and other emerging models are increasingly integrated into legal\nworkflows, their performance in multilingual, jurisdictionally diverse, and\nadversarial contexts remains insufficiently explored. This work evaluates LLaMA\nand Gemini on multilingual legal and non-legal benchmarks, and assesses their\nadversarial robustness in legal tasks through character and word-level\nperturbations. We use an LLM-as-a-Judge approach for human-aligned evaluation.\nWe moreover present an open-source, modular evaluation pipeline designed to\nsupport multilingual, task-diverse benchmarking of any combination of LLMs and\ndatasets, with a particular focus on legal tasks, including classification,\nsummarization, open questions, and general reasoning. Our findings confirm that\nlegal tasks pose significant challenges for LLMs with accuracies often below\n50% on legal reasoning benchmarks such as LEXam, compared to over 70% on\ngeneral-purpose tasks like XNLI. In addition, while English generally yields\nmore stable results, it does not always lead to higher accuracy. Prompt\nsensitivity and adversarial vulnerability is also shown to persist across\nlanguages. Finally, a correlation is found between the performance of a\nlanguage and its syntactic similarity to English. We also observe that LLaMA is\nweaker than Gemini, with the latter showing an average advantage of about 24\npercentage points across the same task. Despite improvements in newer LLMs,\nchallenges remain in deploying them reliably for critical, multilingual legal\napplications."
                },
                "authors": [
                    {
                        "name": "Antreas Ioannou"
                    },
                    {
                        "name": "Andreas Shiamishis"
                    },
                    {
                        "name": "Nora Hollenstein"
                    },
                    {
                        "name": "Nezihe Merve Gürel"
                    }
                ],
                "author_detail": {
                    "name": "Nezihe Merve Gürel"
                },
                "author": "Nezihe Merve Gürel",
                "arxiv_comment": "39 pages, 36 figures. Code and evaluation pipeline available at\n  https://github.com/RobustML-Lab/Legal-Multilingual-Evaluation-of-LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00666v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00666v3",
                "updated": "2025-09-26T15:18:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    18,
                    48,
                    4,
                    269,
                    0
                ],
                "published": "2025-02-02T04:40:04Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    4,
                    40,
                    4,
                    6,
                    33,
                    0
                ],
                "title": "Avoiding $\\mathbf{exp(R_{max})}$ scaling in RLHF through\n  Preference-based Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Avoiding $\\mathbf{exp(R_{max})}$ scaling in RLHF through\n  Preference-based Exploration"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal\ntechnique for large language model (LLM) alignment. This paper studies the\nsetting of online RLHF and focus on improving sample efficiency. All existing\nalgorithms in online RLHF, whether doing passive exploration or active\nexploration, suffer from a sample complexity that scales exponentially with the\nscale of the reward function. This fundamental limitation hinders their\neffectiveness in scenarios with heavily skewed preferences, e.g. questions with\na unique correct solution. To address this, we introduce Self-Exploring\nPreference-Incentive Online Preference Optimization (SE-POPO), an online RLHF\nalgorithm that for the first time achieves a sample complexity that scales\npolynomially with the reward scale, answering an open problem raised by Xie et\nal. (2024).. Theoretically, we demonstrate that the sample complexity of\nSE-POPO dominates that of existing exploration algorithms. Empirically, our\nsystematic evaluation confirms that SE-POPO is more sample-efficient than both\nexploratory and non-exploratory baselines, in two primary application scenarios\nof RLHF as well as on public benchmarks, marking a significant step forward in\nRLHF algorithm design. The code is available at\nhttps://github.com/MYC000801/SE-POPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal\ntechnique for large language model (LLM) alignment. This paper studies the\nsetting of online RLHF and focus on improving sample efficiency. All existing\nalgorithms in online RLHF, whether doing passive exploration or active\nexploration, suffer from a sample complexity that scales exponentially with the\nscale of the reward function. This fundamental limitation hinders their\neffectiveness in scenarios with heavily skewed preferences, e.g. questions with\na unique correct solution. To address this, we introduce Self-Exploring\nPreference-Incentive Online Preference Optimization (SE-POPO), an online RLHF\nalgorithm that for the first time achieves a sample complexity that scales\npolynomially with the reward scale, answering an open problem raised by Xie et\nal. (2024).. Theoretically, we demonstrate that the sample complexity of\nSE-POPO dominates that of existing exploration algorithms. Empirically, our\nsystematic evaluation confirms that SE-POPO is more sample-efficient than both\nexploratory and non-exploratory baselines, in two primary application scenarios\nof RLHF as well as on public benchmarks, marking a significant step forward in\nRLHF algorithm design. The code is available at\nhttps://github.com/MYC000801/SE-POPO."
                },
                "authors": [
                    {
                        "name": "Mingyu Chen"
                    },
                    {
                        "name": "Yiding Chen"
                    },
                    {
                        "name": "Wen Sun"
                    },
                    {
                        "name": "Xuezhou Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xuezhou Zhang"
                },
                "author": "Xuezhou Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00666v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00666v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20321v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20321v2",
                "updated": "2025-09-26T15:14:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    14,
                    24,
                    4,
                    269,
                    0
                ],
                "published": "2025-05-23T17:58:07Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    17,
                    58,
                    7,
                    4,
                    143,
                    0
                ],
                "title": "BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge\n  Bases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge\n  Bases"
                },
                "summary": "Biomedical researchers increasingly rely on large-scale structured databases\nfor complex analytical tasks. However, current text-to-SQL systems often\nstruggle to map qualitative scientific questions into executable SQL,\nparticularly when implicit domain reasoning is required. We introduce\nBiomedSQL, the first benchmark explicitly designed to evaluate scientific\nreasoning in text-to-SQL generation over a real-world biomedical knowledge\nbase. BiomedSQL comprises 68,000 question/SQL query/answer triples grounded in\na harmonized BigQuery knowledge base that integrates gene-disease associations,\ncausal inference from omics data, and drug approval records. Each question\nrequires models to infer domain-specific criteria, such as genome-wide\nsignificance thresholds, effect directionality, or trial phase filtering,\nrather than rely on syntactic translation alone. We evaluate a range of open-\nand closed-source LLMs across prompting strategies and interaction paradigms.\nOur results reveal a substantial performance gap: GPT-o3-mini achieves 59.0%\nexecution accuracy, while our custom multi-step agent, BMSQL, reaches 62.6%,\nboth well below the expert baseline of 90.0%. BiomedSQL provides a new\nfoundation for advancing text-to-SQL systems capable of supporting scientific\ndiscovery through robust reasoning over structured biomedical knowledge bases.\nOur dataset is publicly available at\nhttps://huggingface.co/datasets/NIH-CARD/BiomedSQL, and our code is open-source\nat https://github.com/NIH-CARD/biomedsql.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biomedical researchers increasingly rely on large-scale structured databases\nfor complex analytical tasks. However, current text-to-SQL systems often\nstruggle to map qualitative scientific questions into executable SQL,\nparticularly when implicit domain reasoning is required. We introduce\nBiomedSQL, the first benchmark explicitly designed to evaluate scientific\nreasoning in text-to-SQL generation over a real-world biomedical knowledge\nbase. BiomedSQL comprises 68,000 question/SQL query/answer triples grounded in\na harmonized BigQuery knowledge base that integrates gene-disease associations,\ncausal inference from omics data, and drug approval records. Each question\nrequires models to infer domain-specific criteria, such as genome-wide\nsignificance thresholds, effect directionality, or trial phase filtering,\nrather than rely on syntactic translation alone. We evaluate a range of open-\nand closed-source LLMs across prompting strategies and interaction paradigms.\nOur results reveal a substantial performance gap: GPT-o3-mini achieves 59.0%\nexecution accuracy, while our custom multi-step agent, BMSQL, reaches 62.6%,\nboth well below the expert baseline of 90.0%. BiomedSQL provides a new\nfoundation for advancing text-to-SQL systems capable of supporting scientific\ndiscovery through robust reasoning over structured biomedical knowledge bases.\nOur dataset is publicly available at\nhttps://huggingface.co/datasets/NIH-CARD/BiomedSQL, and our code is open-source\nat https://github.com/NIH-CARD/biomedsql."
                },
                "authors": [
                    {
                        "name": "Mathew J. Koretsky"
                    },
                    {
                        "name": "Maya Willey"
                    },
                    {
                        "name": "Adi Asija"
                    },
                    {
                        "name": "Owen Bianchi"
                    },
                    {
                        "name": "Chelsea X. Alvarado"
                    },
                    {
                        "name": "Tanay Nayak"
                    },
                    {
                        "name": "Nicole Kuznetsov"
                    },
                    {
                        "name": "Sungwon Kim"
                    },
                    {
                        "name": "Mike A. Nalls"
                    },
                    {
                        "name": "Daniel Khashabi"
                    },
                    {
                        "name": "Faraz Faghri"
                    }
                ],
                "author_detail": {
                    "name": "Faraz Faghri"
                },
                "author": "Faraz Faghri",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20321v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20321v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22463v1",
                "updated": "2025-09-26T15:14:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    14,
                    3,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:14:03Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    14,
                    3,
                    4,
                    269,
                    0
                ],
                "title": "IIET: Efficient Numerical Transformer via Implicit Iterative Euler\n  Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IIET: Efficient Numerical Transformer via Implicit Iterative Euler\n  Method"
                },
                "summary": "High-order numerical methods enhance Transformer performance in tasks like\nNLP and CV, but introduce a performance-efficiency trade-off due to increased\ncomputational overhead. Our analysis reveals that conventional efficiency\ntechniques, such as distillation, can be detrimental to the performance of\nthese models, exemplified by PCformer. To explore more optimizable ODE-based\nTransformer architectures, we propose the \\textbf{I}terative \\textbf{I}mplicit\n\\textbf{E}uler \\textbf{T}ransformer \\textbf{(IIET)}, which simplifies\nhigh-order methods using an iterative implicit Euler approach. This\nsimplification not only leads to superior performance but also facilitates\nmodel compression compared to PCformer. To enhance inference efficiency, we\nintroduce \\textbf{I}teration \\textbf{I}nfluence-\\textbf{A}ware\n\\textbf{D}istillation \\textbf{(IIAD)}. Through a flexible threshold, IIAD\nallows users to effectively balance the performance-efficiency trade-off. On\nlm-evaluation-harness, IIET boosts average accuracy by 2.65\\% over vanilla\nTransformers and 0.8\\% over PCformer. Its efficient variant, E-IIET,\nsignificantly cuts inference overhead by 55\\% while retaining 99.4\\% of the\noriginal task accuracy. Moreover, the most efficient IIET variant achieves an\naverage performance gain exceeding 1.6\\% over vanilla Transformer with\ncomparable speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-order numerical methods enhance Transformer performance in tasks like\nNLP and CV, but introduce a performance-efficiency trade-off due to increased\ncomputational overhead. Our analysis reveals that conventional efficiency\ntechniques, such as distillation, can be detrimental to the performance of\nthese models, exemplified by PCformer. To explore more optimizable ODE-based\nTransformer architectures, we propose the \\textbf{I}terative \\textbf{I}mplicit\n\\textbf{E}uler \\textbf{T}ransformer \\textbf{(IIET)}, which simplifies\nhigh-order methods using an iterative implicit Euler approach. This\nsimplification not only leads to superior performance but also facilitates\nmodel compression compared to PCformer. To enhance inference efficiency, we\nintroduce \\textbf{I}teration \\textbf{I}nfluence-\\textbf{A}ware\n\\textbf{D}istillation \\textbf{(IIAD)}. Through a flexible threshold, IIAD\nallows users to effectively balance the performance-efficiency trade-off. On\nlm-evaluation-harness, IIET boosts average accuracy by 2.65\\% over vanilla\nTransformers and 0.8\\% over PCformer. Its efficient variant, E-IIET,\nsignificantly cuts inference overhead by 55\\% while retaining 99.4\\% of the\noriginal task accuracy. Moreover, the most efficient IIET variant achieves an\naverage performance gain exceeding 1.6\\% over vanilla Transformer with\ncomparable speed."
                },
                "authors": [
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Jiahao Liu"
                    },
                    {
                        "name": "Junhao Ruan"
                    },
                    {
                        "name": "Kechen Jiao"
                    },
                    {
                        "name": "Hongyin Tang"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xiao Tong"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22459v1",
                "updated": "2025-09-26T15:12:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    12,
                    2,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:12:02Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    12,
                    2,
                    4,
                    269,
                    0
                ],
                "title": "Universal Inverse Distillation for Matching Models with Real-Data\n  Supervision (No GANs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Inverse Distillation for Matching Models with Real-Data\n  Supervision (No GANs)"
                },
                "summary": "While achieving exceptional generative quality, modern diffusion, flow, and\nother matching models suffer from slow inference, as they require many steps of\niterative generation. Recent distillation methods address this by training\nefficient one-step generators under the guidance of a pre-trained teacher\nmodel. However, these methods are often constrained to only one specific\nframework, e.g., only to diffusion or only to flow models. Furthermore, these\nmethods are naturally data-free, and to benefit from the usage of real data, it\nis required to use an additional complex adversarial training with an extra\ndiscriminator model. In this paper, we present RealUID, a universal\ndistillation framework for all matching models that seamlessly incorporates\nreal data into the distillation procedure without GANs. Our RealUID approach\noffers a simple theoretical foundation that covers previous distillation\nmethods for Flow Matching and Diffusion models, and is also extended to their\nmodifications, such as Bridge Matching and Stochastic Interpolants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While achieving exceptional generative quality, modern diffusion, flow, and\nother matching models suffer from slow inference, as they require many steps of\niterative generation. Recent distillation methods address this by training\nefficient one-step generators under the guidance of a pre-trained teacher\nmodel. However, these methods are often constrained to only one specific\nframework, e.g., only to diffusion or only to flow models. Furthermore, these\nmethods are naturally data-free, and to benefit from the usage of real data, it\nis required to use an additional complex adversarial training with an extra\ndiscriminator model. In this paper, we present RealUID, a universal\ndistillation framework for all matching models that seamlessly incorporates\nreal data into the distillation procedure without GANs. Our RealUID approach\noffers a simple theoretical foundation that covers previous distillation\nmethods for Flow Matching and Diffusion models, and is also extended to their\nmodifications, such as Bridge Matching and Stochastic Interpolants."
                },
                "authors": [
                    {
                        "name": "Nikita Kornilov"
                    },
                    {
                        "name": "David Li"
                    },
                    {
                        "name": "Tikhon Mavrin"
                    },
                    {
                        "name": "Aleksei Leonov"
                    },
                    {
                        "name": "Nikita Gushchin"
                    },
                    {
                        "name": "Evgeny Burnaev"
                    },
                    {
                        "name": "Iaroslav Koshelev"
                    },
                    {
                        "name": "Alexander Korotin"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Korotin"
                },
                "author": "Alexander Korotin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22458v1",
                "updated": "2025-09-26T15:09:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    9,
                    26,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:09:26Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    9,
                    26,
                    4,
                    269,
                    0
                ],
                "title": "Physics-informed GNN for medium-high voltage AC power flow with\n  edge-aware attention and line search correction operator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-informed GNN for medium-high voltage AC power flow with\n  edge-aware attention and line search correction operator"
                },
                "summary": "Physics-informed graph neural networks (PIGNNs) have emerged as fast AC\npower-flow solvers that can replace classic Newton--Raphson (NR) solvers,\nespecially when thousands of scenarios must be evaluated. However, current\nPIGNNs still need accuracy improvements at parity speed; in particular, the\nphysics loss is inoperative at inference, which can deter operational adoption.\nWe address this with PIGNN-Attn-LS, combining an edge-aware attention mechanism\nthat explicitly encodes line physics via per-edge biases, capturing the grid's\nanisotropy, with a backtracking line-search-based globalized correction\noperator that restores an operative decrease criterion at inference. Training\nand testing use a realistic High-/Medium-Voltage scenario generator, with NR\nused only to construct reference states. On held-out HV cases consisting of\n4--32-bus grids, PIGNN-Attn-LS achieves a test RMSE of 0.00033 p.u. in voltage\nand 0.08$^\\circ$ in angle, outperforming the PIGNN-MLP baseline by 99.5\\% and\n87.1\\%, respectively. With streaming micro-batches, it delivers 2--5$\\times$\nfaster batched inference than NR on 4--1024-bus grids.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-informed graph neural networks (PIGNNs) have emerged as fast AC\npower-flow solvers that can replace classic Newton--Raphson (NR) solvers,\nespecially when thousands of scenarios must be evaluated. However, current\nPIGNNs still need accuracy improvements at parity speed; in particular, the\nphysics loss is inoperative at inference, which can deter operational adoption.\nWe address this with PIGNN-Attn-LS, combining an edge-aware attention mechanism\nthat explicitly encodes line physics via per-edge biases, capturing the grid's\nanisotropy, with a backtracking line-search-based globalized correction\noperator that restores an operative decrease criterion at inference. Training\nand testing use a realistic High-/Medium-Voltage scenario generator, with NR\nused only to construct reference states. On held-out HV cases consisting of\n4--32-bus grids, PIGNN-Attn-LS achieves a test RMSE of 0.00033 p.u. in voltage\nand 0.08$^\\circ$ in angle, outperforming the PIGNN-MLP baseline by 99.5\\% and\n87.1\\%, respectively. With streaming micro-batches, it delivers 2--5$\\times$\nfaster batched inference than NR on 4--1024-bus grids."
                },
                "authors": [
                    {
                        "name": "Changhun Kim"
                    },
                    {
                        "name": "Timon Conrad"
                    },
                    {
                        "name": "Redwanul Karim"
                    },
                    {
                        "name": "Julian Oelhaf"
                    },
                    {
                        "name": "David Riebesel"
                    },
                    {
                        "name": "Tomás Arias-Vergara"
                    },
                    {
                        "name": "Andreas Maier"
                    },
                    {
                        "name": "Johann Jäger"
                    },
                    {
                        "name": "Siming Bayer"
                    }
                ],
                "author_detail": {
                    "name": "Siming Bayer"
                },
                "author": "Siming Bayer",
                "arxiv_comment": "5 pages, 2 figures. Submitted to ICASSP 2026. Code available at\n  https://github.com/Kimchangheon/PIGNN-Attn-LS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18697v2",
                "updated": "2025-09-26T15:09:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    9,
                    18,
                    4,
                    269,
                    0
                ],
                "published": "2025-05-24T13:43:29Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    13,
                    43,
                    29,
                    5,
                    144,
                    0
                ],
                "title": "Can LLMs Alleviate Catastrophic Forgetting in Graph Continual Learning?\n  A Systematic Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Alleviate Catastrophic Forgetting in Graph Continual Learning?\n  A Systematic Study"
                },
                "summary": "Nowadays, real-world data, including graph-structure data, often arrives in a\nstreaming manner, which means that learning systems need to continuously\nacquire new knowledge without forgetting previously learned information.\nAlthough substantial existing works attempt to address catastrophic forgetting\nin graph machine learning, they are all based on training from scratch with\nstreaming data. With the rise of pretrained models, an increasing number of\nstudies have leveraged their strong generalization ability for continual\nlearning. Therefore, in this work, we attempt to answer whether large language\nmodels (LLMs) can mitigate catastrophic forgetting in Graph Continual Learning\n(GCL). We first point out that current experimental setups for GCL have\nsignificant flaws, as the evaluation stage may lead to task ID leakage. Then,\nwe evaluate the performance of LLMs in more realistic scenarios and find that\neven minor modifications can lead to outstanding results. Finally, based on\nextensive experiments, we propose a simple-yet-effective method, Simple Graph\nContinual Learning (SimGCL), that surpasses the previous state-of-the-art\nGNN-based baseline by around 20% under the rehearsal-free constraint. To\nfacilitate reproducibility, we have developed an easy-to-use benchmark LLM4GCL\nfor training and evaluating existing GCL methods. The code is available at:\nhttps://github.com/ZhixunLEE/LLM4GCL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, real-world data, including graph-structure data, often arrives in a\nstreaming manner, which means that learning systems need to continuously\nacquire new knowledge without forgetting previously learned information.\nAlthough substantial existing works attempt to address catastrophic forgetting\nin graph machine learning, they are all based on training from scratch with\nstreaming data. With the rise of pretrained models, an increasing number of\nstudies have leveraged their strong generalization ability for continual\nlearning. Therefore, in this work, we attempt to answer whether large language\nmodels (LLMs) can mitigate catastrophic forgetting in Graph Continual Learning\n(GCL). We first point out that current experimental setups for GCL have\nsignificant flaws, as the evaluation stage may lead to task ID leakage. Then,\nwe evaluate the performance of LLMs in more realistic scenarios and find that\neven minor modifications can lead to outstanding results. Finally, based on\nextensive experiments, we propose a simple-yet-effective method, Simple Graph\nContinual Learning (SimGCL), that surpasses the previous state-of-the-art\nGNN-based baseline by around 20% under the rehearsal-free constraint. To\nfacilitate reproducibility, we have developed an easy-to-use benchmark LLM4GCL\nfor training and evaluating existing GCL methods. The code is available at:\nhttps://github.com/ZhixunLEE/LLM4GCL."
                },
                "authors": [
                    {
                        "name": "Ziyang Cheng"
                    },
                    {
                        "name": "Zhixun Li"
                    },
                    {
                        "name": "Yuhan Li"
                    },
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Kangyi Zhao"
                    },
                    {
                        "name": "Dawei Cheng"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Hong Cheng"
                    },
                    {
                        "name": "Jeffrey Xu Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jeffrey Xu Yu"
                },
                "author": "Jeffrey Xu Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15241v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15241v3",
                "updated": "2025-09-26T15:05:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    5,
                    35,
                    4,
                    269,
                    0
                ],
                "published": "2025-04-21T17:15:06Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    15,
                    6,
                    0,
                    111,
                    0
                ],
                "title": "MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety"
                },
                "summary": "Large Language Models (LLMs) are susceptible to adversarial attacks such as\njailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability\nis exacerbated in multilingual settings, where multilingual safety-aligned data\nis often limited. Thus, developing a guardrail capable of detecting and\nfiltering unsafe content across diverse languages is critical for deploying\nLLMs in real-world applications. In this work, we introduce a multilingual\nguardrail with reasoning for prompt classification. Our method consists of: (1)\nsynthetic multilingual data generation incorporating culturally and\nlinguistically nuanced variants, (2) supervised fine-tuning, and (3) a\ncurriculum-based Group Relative Policy Optimization (GRPO) framework that\nfurther improves performance. Experimental results demonstrate that our\nmultilingual guardrail, MrGuard, consistently outperforms recent baselines\nacross both in-domain and out-of-domain languages by more than 15%. We also\nevaluate MrGuard's robustness to multilingual variations, such as\ncode-switching and low-resource language distractors in the prompt, and\ndemonstrate that it preserves safety judgments under these challenging\nconditions. The multilingual reasoning capability of our guardrail enables it\nto generate explanations, which are particularly useful for understanding\nlanguage-specific risks and ambiguities in multilingual content moderation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are susceptible to adversarial attacks such as\njailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability\nis exacerbated in multilingual settings, where multilingual safety-aligned data\nis often limited. Thus, developing a guardrail capable of detecting and\nfiltering unsafe content across diverse languages is critical for deploying\nLLMs in real-world applications. In this work, we introduce a multilingual\nguardrail with reasoning for prompt classification. Our method consists of: (1)\nsynthetic multilingual data generation incorporating culturally and\nlinguistically nuanced variants, (2) supervised fine-tuning, and (3) a\ncurriculum-based Group Relative Policy Optimization (GRPO) framework that\nfurther improves performance. Experimental results demonstrate that our\nmultilingual guardrail, MrGuard, consistently outperforms recent baselines\nacross both in-domain and out-of-domain languages by more than 15%. We also\nevaluate MrGuard's robustness to multilingual variations, such as\ncode-switching and low-resource language distractors in the prompt, and\ndemonstrate that it preserves safety judgments under these challenging\nconditions. The multilingual reasoning capability of our guardrail enables it\nto generate explanations, which are particularly useful for understanding\nlanguage-specific risks and ambiguities in multilingual content moderation."
                },
                "authors": [
                    {
                        "name": "Yahan Yang"
                    },
                    {
                        "name": "Soham Dan"
                    },
                    {
                        "name": "Shuo Li"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Insup Lee"
                    }
                ],
                "author_detail": {
                    "name": "Insup Lee"
                },
                "author": "Insup Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15241v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15241v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22449v1",
                "updated": "2025-09-26T15:04:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    4,
                    32,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:04:32Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    4,
                    32,
                    4,
                    269,
                    0
                ],
                "title": "Detecting (Un)answerability in Large Language Models with Linear\n  Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting (Un)answerability in Large Language Models with Linear\n  Directions"
                },
                "summary": "Large language models (LLMs) often respond confidently to questions even when\nthey lack the necessary information, leading to hallucinated answers. In this\nwork, we study the problem of (un)answerability detection, focusing on\nextractive question answering (QA) where the model should determine if a\npassage contains sufficient information to answer a given question. We propose\na simple approach for identifying a direction in the model's activation space\nthat captures unanswerability and uses it for classification. This direction is\nselected by applying activation additions during inference and measuring their\nimpact on the model's abstention behavior. We show that projecting hidden\nactivations onto this direction yields a reliable score for (un)answerability\nclassification. Experiments on two open-weight LLMs and four extractive QA\nbenchmarks show that our method effectively detects unanswerable questions and\ngeneralizes better across datasets than existing prompt-based and\nclassifier-based approaches. Moreover, the obtained directions extend beyond\nextractive QA to unanswerability that stems from factors, such as lack of\nscientific consensus and subjectivity. Last, causal interventions show that\nadding or ablating the directions effectively controls the abstention behavior\nof the model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often respond confidently to questions even when\nthey lack the necessary information, leading to hallucinated answers. In this\nwork, we study the problem of (un)answerability detection, focusing on\nextractive question answering (QA) where the model should determine if a\npassage contains sufficient information to answer a given question. We propose\na simple approach for identifying a direction in the model's activation space\nthat captures unanswerability and uses it for classification. This direction is\nselected by applying activation additions during inference and measuring their\nimpact on the model's abstention behavior. We show that projecting hidden\nactivations onto this direction yields a reliable score for (un)answerability\nclassification. Experiments on two open-weight LLMs and four extractive QA\nbenchmarks show that our method effectively detects unanswerable questions and\ngeneralizes better across datasets than existing prompt-based and\nclassifier-based approaches. Moreover, the obtained directions extend beyond\nextractive QA to unanswerability that stems from factors, such as lack of\nscientific consensus and subjectivity. Last, causal interventions show that\nadding or ablating the directions effectively controls the abstention behavior\nof the model."
                },
                "authors": [
                    {
                        "name": "Maor Juliet Lavi"
                    },
                    {
                        "name": "Tova Milo"
                    },
                    {
                        "name": "Mor Geva"
                    }
                ],
                "author_detail": {
                    "name": "Mor Geva"
                },
                "author": "Mor Geva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22446v1",
                "updated": "2025-09-26T15:03:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    3,
                    18,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:03:18Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    3,
                    18,
                    4,
                    269,
                    0
                ],
                "title": "Rescuing double robustness: safe estimation under complete\n  misspecification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rescuing double robustness: safe estimation under complete\n  misspecification"
                },
                "summary": "Double robustness is a major selling point of semiparametric and missing data\nmethodology. Its virtues lie in protection against partial nuisance\nmisspecification and asymptotic semiparametric efficiency under correct\nnuisance specification. However, in many applications, complete nuisance\nmisspecification should be regarded as the norm (or at the very least the\nexpected default), and thus doubly robust estimators may behave fragilely. In\nfact, it has been amply verified empirically that these estimators can perform\npoorly when all nuisance functions are misspecified. Here, we first\ncharacterize this phenomenon of double fragility, and then propose a solution\nbased on adaptive correction clipping (ACC). We argue that our ACC proposal is\nsafe, in that it inherits the favorable properties of doubly robust estimators\nunder correct nuisance specification, but its error is guaranteed to be bounded\nby a convex combination of the individual nuisance model errors, which prevents\nthe instability caused by the compounding product of errors of doubly robust\nestimators. We also show that our proposal provides valid inference through the\nparametric bootstrap when nuisances are well-specified. We showcase the\nefficacy of our ACC estimator both through extensive simulations and by\napplying it to the analysis of Alzheimer's disease proteomics data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Double robustness is a major selling point of semiparametric and missing data\nmethodology. Its virtues lie in protection against partial nuisance\nmisspecification and asymptotic semiparametric efficiency under correct\nnuisance specification. However, in many applications, complete nuisance\nmisspecification should be regarded as the norm (or at the very least the\nexpected default), and thus doubly robust estimators may behave fragilely. In\nfact, it has been amply verified empirically that these estimators can perform\npoorly when all nuisance functions are misspecified. Here, we first\ncharacterize this phenomenon of double fragility, and then propose a solution\nbased on adaptive correction clipping (ACC). We argue that our ACC proposal is\nsafe, in that it inherits the favorable properties of doubly robust estimators\nunder correct nuisance specification, but its error is guaranteed to be bounded\nby a convex combination of the individual nuisance model errors, which prevents\nthe instability caused by the compounding product of errors of doubly robust\nestimators. We also show that our proposal provides valid inference through the\nparametric bootstrap when nuisances are well-specified. We showcase the\nefficacy of our ACC estimator both through extensive simulations and by\napplying it to the analysis of Alzheimer's disease proteomics data."
                },
                "authors": [
                    {
                        "name": "Lorenzo Testa"
                    },
                    {
                        "name": "Francesca Chiaromonte"
                    },
                    {
                        "name": "Kathryn Roeder"
                    }
                ],
                "author_detail": {
                    "name": "Kathryn Roeder"
                },
                "author": "Kathryn Roeder",
                "arxiv_comment": "24 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08123v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08123v4",
                "updated": "2025-09-26T14:57:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    57,
                    59,
                    4,
                    269,
                    0
                ],
                "published": "2025-06-09T18:24:57Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    18,
                    24,
                    57,
                    0,
                    160,
                    0
                ],
                "title": "QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA"
                },
                "summary": "Alignment of large language models (LLMs) with principles like helpfulness,\nhonesty, and harmlessness typically relies on scalar rewards that obscure which\nobjectives drive the training signal. We introduce QA-LIGN, which decomposes\nmonolithic rewards into interpretable principle-specific evaluations through\nstructured natural language programs. Models learn through a draft, critique,\nand revise pipeline, where symbolic evaluation against the rubrics provides\ntransparent feedback for both initial and revised responses during GRPO\ntraining. Applied to uncensored Llama-3.1-8B-Instruct, QA-LIGN reduces attack\nsuccess rates by up to 68.7% while maintaining a 0.67% false refusal rate,\nachieving Pareto optimal safety-helpfulness performance and outperforming both\nDPO and GRPO with state-of-the-art reward models given equivalent training.\nThese results demonstrate that making reward signals interpretable and modular\nimproves alignment effectiveness, suggesting transparency enhances LLM safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment of large language models (LLMs) with principles like helpfulness,\nhonesty, and harmlessness typically relies on scalar rewards that obscure which\nobjectives drive the training signal. We introduce QA-LIGN, which decomposes\nmonolithic rewards into interpretable principle-specific evaluations through\nstructured natural language programs. Models learn through a draft, critique,\nand revise pipeline, where symbolic evaluation against the rubrics provides\ntransparent feedback for both initial and revised responses during GRPO\ntraining. Applied to uncensored Llama-3.1-8B-Instruct, QA-LIGN reduces attack\nsuccess rates by up to 68.7% while maintaining a 0.67% false refusal rate,\nachieving Pareto optimal safety-helpfulness performance and outperforming both\nDPO and GRPO with state-of-the-art reward models given equivalent training.\nThese results demonstrate that making reward signals interpretable and modular\nimproves alignment effectiveness, suggesting transparency enhances LLM safety."
                },
                "authors": [
                    {
                        "name": "Jacob Dineen"
                    },
                    {
                        "name": "Aswin RRV"
                    },
                    {
                        "name": "Qin Liu"
                    },
                    {
                        "name": "Zhikun Xu"
                    },
                    {
                        "name": "Xiao Ye"
                    },
                    {
                        "name": "Ming Shen"
                    },
                    {
                        "name": "Zhaonan Li"
                    },
                    {
                        "name": "Shijie Lu"
                    },
                    {
                        "name": "Chitta Baral"
                    },
                    {
                        "name": "Muhao Chen"
                    },
                    {
                        "name": "Ben Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Ben Zhou"
                },
                "author": "Ben Zhou",
                "arxiv_comment": "Accepted to Findings of EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08123v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08123v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22431v1",
                "updated": "2025-09-26T14:50:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    50,
                    13,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T14:50:13Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    50,
                    13,
                    4,
                    269,
                    0
                ],
                "title": "TreeMind: Automatically Reproducing Android Bug Reports via\n  LLM-empowered Monte Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeMind: Automatically Reproducing Android Bug Reports via\n  LLM-empowered Monte Carlo Tree Search"
                },
                "summary": "Automatically reproducing Android app crashes from textual bug reports is\nchallenging, particularly when the reports are incomplete and the modern UI\nexhibits high combinatorial complexity. Existing approaches based on\nreinforcement learning or large language models (LLMs) exhibit limitations in\nsuch scenarios. They struggle to infer unobserved steps and reconstruct the\nunderlying user action sequences to navigate the vast UI interaction space,\nprimarily due to limited goal-directed reasoning and planning. We present\nTreeMind, a novel technique that integrates LLMs with a customized Monte Carlo\nTree Search (MCTS) algorithm to achieve strategic UI exploration in bug\nreproduction. To the best of our knowledge, this is the first work to combine\nexternal decision-making with LLM semantic reasoning for reliable bug\nreproduction. We formulate the reproduction task as a target-driven search\nproblem, leveraging MCTS as the core planning mechanism to iteratively refine\naction sequences. To enhance MCTS with semantic reasoning, we introduce two\nLLM-guided agents with distinct roles: Expander generates top-k promising\nactions based on the current UI state and exploration history, while Simulator\nestimates the likelihood that each action leads toward successful reproduction.\nBy incorporating multi-modal UI inputs and advanced prompting techniques,\nTreeMind conducts feedback-aware navigation that identifies missing but\nessential user actions and incrementally reconstructs the reproduction paths.\nWe evaluate TreeMind on a dataset of 93 real-world Android bug reports from\nthree widely-used benchmarks. Experimental results show that it significantly\noutperforms four state-of-the-art baselines in reproduction success rate. A\nreal-world case study indicates that integrating LLM reasoning with MCTS-based\nplanning is a compelling direction for automated bug reproduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically reproducing Android app crashes from textual bug reports is\nchallenging, particularly when the reports are incomplete and the modern UI\nexhibits high combinatorial complexity. Existing approaches based on\nreinforcement learning or large language models (LLMs) exhibit limitations in\nsuch scenarios. They struggle to infer unobserved steps and reconstruct the\nunderlying user action sequences to navigate the vast UI interaction space,\nprimarily due to limited goal-directed reasoning and planning. We present\nTreeMind, a novel technique that integrates LLMs with a customized Monte Carlo\nTree Search (MCTS) algorithm to achieve strategic UI exploration in bug\nreproduction. To the best of our knowledge, this is the first work to combine\nexternal decision-making with LLM semantic reasoning for reliable bug\nreproduction. We formulate the reproduction task as a target-driven search\nproblem, leveraging MCTS as the core planning mechanism to iteratively refine\naction sequences. To enhance MCTS with semantic reasoning, we introduce two\nLLM-guided agents with distinct roles: Expander generates top-k promising\nactions based on the current UI state and exploration history, while Simulator\nestimates the likelihood that each action leads toward successful reproduction.\nBy incorporating multi-modal UI inputs and advanced prompting techniques,\nTreeMind conducts feedback-aware navigation that identifies missing but\nessential user actions and incrementally reconstructs the reproduction paths.\nWe evaluate TreeMind on a dataset of 93 real-world Android bug reports from\nthree widely-used benchmarks. Experimental results show that it significantly\noutperforms four state-of-the-art baselines in reproduction success rate. A\nreal-world case study indicates that integrating LLM reasoning with MCTS-based\nplanning is a compelling direction for automated bug reproduction."
                },
                "authors": [
                    {
                        "name": "Zhengyu Chen"
                    },
                    {
                        "name": "Zhaoyi Meng"
                    },
                    {
                        "name": "Wenxiang Zhao"
                    },
                    {
                        "name": "Wansen Wang"
                    },
                    {
                        "name": "Haoyang Zhao"
                    },
                    {
                        "name": "Jiahao Zhan"
                    },
                    {
                        "name": "Jie Cui"
                    },
                    {
                        "name": "Hong Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Hong Zhong"
                },
                "author": "Hong Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22424v1",
                "updated": "2025-09-26T14:44:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    44,
                    45,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T14:44:45Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    44,
                    45,
                    4,
                    269,
                    0
                ],
                "title": "Desiderata for a biomedical knowledge network: opportunities, challenges\n  and future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Desiderata for a biomedical knowledge network: opportunities, challenges\n  and future Directions"
                },
                "summary": "Knowledge graphs, collectively as a knowledge network, have become critical\ntools for knowledge discovery in computable and explainable knowledge systems.\nDue to the semantic and structural complexities of biomedical data, these\nknowledge graphs need to enable dynamic reasoning over large evolving graphs\nand support fit-for-purpose abstraction, while establishing standards,\npreserving provenance and enforcing policy constraints for actionable\ndiscovery. A recent meeting of leading scientists discussed the opportunities,\nchallenges and future directions of a biomedical knowledge network. Here we\npresent six desiderata inspired by the meeting: (1) inference and reasoning in\nbiomedical knowledge graphs need domain-centric approaches; (2) harmonized and\naccessible standards are required for knowledge graph representation and\nmetadata; (3) robust validation of biomedical knowledge graphs needs\nmulti-layered, context-aware approaches that are both rigorous and scalable;\n(4) the evolving and synergistic relationship between knowledge graphs and\nlarge language models is essential in empowering AI-driven biomedical\ndiscovery; (5) integrated development environments, public repositories, and\ngovernance frameworks are essential for secure and reproducible knowledge graph\nsharing; and (6) robust validation, provenance, and ethical governance are\ncritical for trustworthy biomedical knowledge graphs. Addressing these key\nissues will be essential to realize the promises of a biomedical knowledge\nnetwork in advancing biomedicine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graphs, collectively as a knowledge network, have become critical\ntools for knowledge discovery in computable and explainable knowledge systems.\nDue to the semantic and structural complexities of biomedical data, these\nknowledge graphs need to enable dynamic reasoning over large evolving graphs\nand support fit-for-purpose abstraction, while establishing standards,\npreserving provenance and enforcing policy constraints for actionable\ndiscovery. A recent meeting of leading scientists discussed the opportunities,\nchallenges and future directions of a biomedical knowledge network. Here we\npresent six desiderata inspired by the meeting: (1) inference and reasoning in\nbiomedical knowledge graphs need domain-centric approaches; (2) harmonized and\naccessible standards are required for knowledge graph representation and\nmetadata; (3) robust validation of biomedical knowledge graphs needs\nmulti-layered, context-aware approaches that are both rigorous and scalable;\n(4) the evolving and synergistic relationship between knowledge graphs and\nlarge language models is essential in empowering AI-driven biomedical\ndiscovery; (5) integrated development environments, public repositories, and\ngovernance frameworks are essential for secure and reproducible knowledge graph\nsharing; and (6) robust validation, provenance, and ethical governance are\ncritical for trustworthy biomedical knowledge graphs. Addressing these key\nissues will be essential to realize the promises of a biomedical knowledge\nnetwork in advancing biomedicine."
                },
                "authors": [
                    {
                        "name": "Chunlei Wu"
                    },
                    {
                        "name": "Hongfang Liu"
                    },
                    {
                        "name": "Jason Flannick"
                    },
                    {
                        "name": "Mark A. Musen"
                    },
                    {
                        "name": "Andrew I. Su"
                    },
                    {
                        "name": "Lawrence Hunter"
                    },
                    {
                        "name": "Thomas M. Powers"
                    },
                    {
                        "name": "Cathy H. Wu"
                    }
                ],
                "author_detail": {
                    "name": "Cathy H. Wu"
                },
                "author": "Cathy H. Wu",
                "arxiv_comment": "6 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.OT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22415v1",
                "updated": "2025-09-26T14:39:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    39,
                    13,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T14:39:13Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    39,
                    13,
                    4,
                    269,
                    0
                ],
                "title": "Explaining multimodal LLMs via intra-modal token interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explaining multimodal LLMs via intra-modal token interactions"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success\nacross diverse vision-language tasks, yet their internal decision-making\nmechanisms remain insufficiently understood. Existing interpretability research\nhas primarily focused on cross-modal attribution, identifying which image\nregions the model attends to during output generation. However, these\napproaches often overlook intra-modal dependencies. In the visual modality,\nattributing importance to isolated image patches ignores spatial context due to\nlimited receptive fields, resulting in fragmented and noisy explanations. In\nthe textual modality, reliance on preceding tokens introduces spurious\nactivations. Failing to effectively mitigate these interference compromises\nattribution fidelity. To address these limitations, we propose enhancing\ninterpretability by leveraging intra-modal interaction. For the visual branch,\nwe introduce \\textit{Multi-Scale Explanation Aggregation} (MSEA), which\naggregates attributions over multi-scale inputs to dynamically adjust receptive\nfields, producing more holistic and spatially coherent visual explanations. For\nthe textual branch, we propose \\textit{Activation Ranking Correlation} (ARC),\nwhich measures the relevance of contextual tokens to the current token via\nalignment of their top-$k$ prediction rankings. ARC leverages this relevance to\nsuppress spurious activations from irrelevant contexts while preserving\nsemantically coherent ones. Extensive experiments across state-of-the-art MLLMs\nand benchmark datasets demonstrate that our approach consistently outperforms\nexisting interpretability methods, yielding more faithful and fine-grained\nexplanations of model behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success\nacross diverse vision-language tasks, yet their internal decision-making\nmechanisms remain insufficiently understood. Existing interpretability research\nhas primarily focused on cross-modal attribution, identifying which image\nregions the model attends to during output generation. However, these\napproaches often overlook intra-modal dependencies. In the visual modality,\nattributing importance to isolated image patches ignores spatial context due to\nlimited receptive fields, resulting in fragmented and noisy explanations. In\nthe textual modality, reliance on preceding tokens introduces spurious\nactivations. Failing to effectively mitigate these interference compromises\nattribution fidelity. To address these limitations, we propose enhancing\ninterpretability by leveraging intra-modal interaction. For the visual branch,\nwe introduce \\textit{Multi-Scale Explanation Aggregation} (MSEA), which\naggregates attributions over multi-scale inputs to dynamically adjust receptive\nfields, producing more holistic and spatially coherent visual explanations. For\nthe textual branch, we propose \\textit{Activation Ranking Correlation} (ARC),\nwhich measures the relevance of contextual tokens to the current token via\nalignment of their top-$k$ prediction rankings. ARC leverages this relevance to\nsuppress spurious activations from irrelevant contexts while preserving\nsemantically coherent ones. Extensive experiments across state-of-the-art MLLMs\nand benchmark datasets demonstrate that our approach consistently outperforms\nexisting interpretability methods, yielding more faithful and fine-grained\nexplanations of model behavior."
                },
                "authors": [
                    {
                        "name": "Jiawei Liang"
                    },
                    {
                        "name": "Ruoyu Chen"
                    },
                    {
                        "name": "Xianghao Jiao"
                    },
                    {
                        "name": "Siyuan Liang"
                    },
                    {
                        "name": "Shiming Liu"
                    },
                    {
                        "name": "Qunli Zhang"
                    },
                    {
                        "name": "Zheng Hu"
                    },
                    {
                        "name": "Xiaochun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaochun Cao"
                },
                "author": "Xiaochun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10150v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10150v3",
                "updated": "2025-09-26T14:37:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    37,
                    4,
                    4,
                    269,
                    0
                ],
                "published": "2025-03-13T08:22:31Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    8,
                    22,
                    31,
                    3,
                    72,
                    0
                ],
                "title": "Retrieval-Augmented Generation with Hierarchical Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation with Hierarchical Knowledge"
                },
                "summary": "Graph-based Retrieval-Augmented Generation (RAG) methods have significantly\nenhanced the performance of large language models (LLMs) in domain-specific\ntasks. However, existing RAG methods do not adequately utilize the naturally\ninherent hierarchical knowledge in human cognition, which limits the\ncapabilities of RAG systems. In this paper, we introduce a new RAG approach,\ncalled HiRAG, which utilizes hierarchical knowledge to enhance the semantic\nunderstanding and structure capturing capabilities of RAG systems in the\nindexing and retrieval processes. Our extensive experiments demonstrate that\nHiRAG achieves significant performance improvements over the state-of-the-art\nbaseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based Retrieval-Augmented Generation (RAG) methods have significantly\nenhanced the performance of large language models (LLMs) in domain-specific\ntasks. However, existing RAG methods do not adequately utilize the naturally\ninherent hierarchical knowledge in human cognition, which limits the\ncapabilities of RAG systems. In this paper, we introduce a new RAG approach,\ncalled HiRAG, which utilizes hierarchical knowledge to enhance the semantic\nunderstanding and structure capturing capabilities of RAG systems in the\nindexing and retrieval processes. Our extensive experiments demonstrate that\nHiRAG achieves significant performance improvements over the state-of-the-art\nbaseline methods."
                },
                "authors": [
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Junjie Yang"
                    },
                    {
                        "name": "Zhenyu Pan"
                    },
                    {
                        "name": "Yongqiang Chen"
                    },
                    {
                        "name": "Kaili Ma"
                    },
                    {
                        "name": "Hongzhi Chen"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10150v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10150v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16950v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16950v3",
                "updated": "2025-09-26T14:35:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    35,
                    4,
                    4,
                    269,
                    0
                ],
                "published": "2025-05-22T17:33:49Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    33,
                    49,
                    3,
                    142,
                    0
                ],
                "title": "Bottlenecked Transformers: Periodic KV Cache Consolidation for\n  Generalised Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bottlenecked Transformers: Periodic KV Cache Consolidation for\n  Generalised Reasoning"
                },
                "summary": "Transformer LLMs have been shown to exhibit strong reasoning ability that\nscales with inference-time compute, most prominently through token-space\n\"thinking\" chains of thought. A growing line of work pushes extra computation\ninto the model's latent space, which we term Auxiliary Latent-Space Computation\n(ALSC). Existing ALSC methods largely fall into three buckets: (i)\ntoken-mediated latent rollouts, (ii) residual/activation steering, and (iii)\nmemory (KV) compression. An underexplored alternative is memory\nconsolidation/reconsolidation, two processes in the brain that are responsible\nfor stabilising newly formed memory traces, and, upon recall, transiently\nrendering established traces plastic such they can integrate new contextual\ninformation before restabilising. In Transformer LLMs, this can be seen as\nanalogous to performing in-place rewrites of new KV segments, and rewrites of\nrecalled past segments. In this work, we give a theoretical justification as to\nwhy memory (re)consolidation via KV cache rewrites is beneficial for improved\nreasoning. We do this through the lens of Information Bottleneck (IB) theory,\nwhich posits that model generalisation emerges from an optimal balance between\ninput information compression and retention of predictive information in latent\nrepresentations. We then introduce the Bottlenecked Transformer, which augments\na backbone LLM with a Cache Processor, an auxiliary Transformer that performs\nperiodic, non-causal, in-place KV rewrites at newline-delimited reasoning step\nboundaries. The Processor consolidates recently written KV entries and\nreconsolidates a small, top-k attention-selected set of prior entries. We\nevaluate our Bottlenecked Transformer architecture on math reasoning\nbenchmarks. Our model sees consistent performance gains over vanilla\nTransformers and pause-token augmented baselines, with gains of up to +6.6pp\nfor selected tasks/backbones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer LLMs have been shown to exhibit strong reasoning ability that\nscales with inference-time compute, most prominently through token-space\n\"thinking\" chains of thought. A growing line of work pushes extra computation\ninto the model's latent space, which we term Auxiliary Latent-Space Computation\n(ALSC). Existing ALSC methods largely fall into three buckets: (i)\ntoken-mediated latent rollouts, (ii) residual/activation steering, and (iii)\nmemory (KV) compression. An underexplored alternative is memory\nconsolidation/reconsolidation, two processes in the brain that are responsible\nfor stabilising newly formed memory traces, and, upon recall, transiently\nrendering established traces plastic such they can integrate new contextual\ninformation before restabilising. In Transformer LLMs, this can be seen as\nanalogous to performing in-place rewrites of new KV segments, and rewrites of\nrecalled past segments. In this work, we give a theoretical justification as to\nwhy memory (re)consolidation via KV cache rewrites is beneficial for improved\nreasoning. We do this through the lens of Information Bottleneck (IB) theory,\nwhich posits that model generalisation emerges from an optimal balance between\ninput information compression and retention of predictive information in latent\nrepresentations. We then introduce the Bottlenecked Transformer, which augments\na backbone LLM with a Cache Processor, an auxiliary Transformer that performs\nperiodic, non-causal, in-place KV rewrites at newline-delimited reasoning step\nboundaries. The Processor consolidates recently written KV entries and\nreconsolidates a small, top-k attention-selected set of prior entries. We\nevaluate our Bottlenecked Transformer architecture on math reasoning\nbenchmarks. Our model sees consistent performance gains over vanilla\nTransformers and pause-token augmented baselines, with gains of up to +6.6pp\nfor selected tasks/backbones."
                },
                "authors": [
                    {
                        "name": "Adnan Oomerjee"
                    },
                    {
                        "name": "Zafeirios Fountas"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16950v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16950v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22403v1",
                "updated": "2025-09-26T14:31:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    31,
                    57,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T14:31:57Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    31,
                    57,
                    4,
                    269,
                    0
                ],
                "title": "MoveFM-R: Advancing Mobility Foundation Models via Language-driven\n  Semantic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoveFM-R: Advancing Mobility Foundation Models via Language-driven\n  Semantic Reasoning"
                },
                "summary": "Mobility Foundation Models (MFMs) have advanced the modeling of human\nmovement patterns, yet they face a ceiling due to limitations in data scale and\nsemantic understanding. While Large Language Models (LLMs) offer powerful\nsemantic reasoning, they lack the innate understanding of spatio-temporal\nstatistics required for generating physically plausible mobility trajectories.\nTo address these gaps, we propose MoveFM-R, a novel framework that unlocks the\nfull potential of mobility foundation models by leveraging language-driven\nsemantic reasoning capabilities. It tackles two key challenges: the vocabulary\nmismatch between continuous geographic coordinates and discrete language\ntokens, and the representation gap between the latent vectors of MFMs and the\nsemantic world of LLMs. MoveFM-R is built on three core innovations: a\nsemantically enhanced location encoding to bridge the geography-language gap, a\nprogressive curriculum to align the LLM's reasoning with mobility patterns, and\nan interactive self-reflection mechanism for conditional trajectory generation.\nExtensive experiments demonstrate that MoveFM-R significantly outperforms\nexisting MFM-based and LLM-based baselines. It also shows robust generalization\nin zero-shot settings and excels at generating realistic trajectories from\nnatural language instructions. By synthesizing the statistical power of MFMs\nwith the deep semantic understanding of LLMs, MoveFM-R pioneers a new paradigm\nthat enables a more comprehensive, interpretable, and powerful modeling of\nhuman mobility. The implementation of MoveFM-R is available online at\nhttps://anonymous.4open.science/r/MoveFM-R-CDE7/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobility Foundation Models (MFMs) have advanced the modeling of human\nmovement patterns, yet they face a ceiling due to limitations in data scale and\nsemantic understanding. While Large Language Models (LLMs) offer powerful\nsemantic reasoning, they lack the innate understanding of spatio-temporal\nstatistics required for generating physically plausible mobility trajectories.\nTo address these gaps, we propose MoveFM-R, a novel framework that unlocks the\nfull potential of mobility foundation models by leveraging language-driven\nsemantic reasoning capabilities. It tackles two key challenges: the vocabulary\nmismatch between continuous geographic coordinates and discrete language\ntokens, and the representation gap between the latent vectors of MFMs and the\nsemantic world of LLMs. MoveFM-R is built on three core innovations: a\nsemantically enhanced location encoding to bridge the geography-language gap, a\nprogressive curriculum to align the LLM's reasoning with mobility patterns, and\nan interactive self-reflection mechanism for conditional trajectory generation.\nExtensive experiments demonstrate that MoveFM-R significantly outperforms\nexisting MFM-based and LLM-based baselines. It also shows robust generalization\nin zero-shot settings and excels at generating realistic trajectories from\nnatural language instructions. By synthesizing the statistical power of MFMs\nwith the deep semantic understanding of LLMs, MoveFM-R pioneers a new paradigm\nthat enables a more comprehensive, interpretable, and powerful modeling of\nhuman mobility. The implementation of MoveFM-R is available online at\nhttps://anonymous.4open.science/r/MoveFM-R-CDE7/."
                },
                "authors": [
                    {
                        "name": "Fanjin Meng"
                    },
                    {
                        "name": "Yuan Yuan"
                    },
                    {
                        "name": "Jingtao Ding"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Chonghua Han"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22402v1",
                "updated": "2025-09-26T14:28:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    28,
                    42,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T14:28:42Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    28,
                    42,
                    4,
                    269,
                    0
                ],
                "title": "ReLAM: Learning Anticipation Model for Rewarding Visual Robotic\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReLAM: Learning Anticipation Model for Rewarding Visual Robotic\n  Manipulation"
                },
                "summary": "Reward design remains a critical bottleneck in visual reinforcement learning\n(RL) for robotic manipulation. In simulated environments, rewards are\nconventionally designed based on the distance to a target position. However,\nsuch precise positional information is often unavailable in real-world visual\nsettings due to sensory and perceptual limitations. In this study, we propose a\nmethod that implicitly infers spatial distances through keypoints extracted\nfrom images. Building on this, we introduce Reward Learning with Anticipation\nModel (ReLAM), a novel framework that automatically generates dense, structured\nrewards from action-free video demonstrations. ReLAM first learns an\nanticipation model that serves as a planner and proposes intermediate\nkeypoint-based subgoals on the optimal path to the final goal, creating a\nstructured learning curriculum directly aligned with the task's geometric\nobjectives. Based on the anticipated subgoals, a continuous reward signal is\nprovided to train a low-level, goal-conditioned policy under the hierarchical\nreinforcement learning (HRL) framework with provable sub-optimality bound.\nExtensive experiments on complex, long-horizon manipulation tasks show that\nReLAM significantly accelerates learning and achieves superior performance\ncompared to state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward design remains a critical bottleneck in visual reinforcement learning\n(RL) for robotic manipulation. In simulated environments, rewards are\nconventionally designed based on the distance to a target position. However,\nsuch precise positional information is often unavailable in real-world visual\nsettings due to sensory and perceptual limitations. In this study, we propose a\nmethod that implicitly infers spatial distances through keypoints extracted\nfrom images. Building on this, we introduce Reward Learning with Anticipation\nModel (ReLAM), a novel framework that automatically generates dense, structured\nrewards from action-free video demonstrations. ReLAM first learns an\nanticipation model that serves as a planner and proposes intermediate\nkeypoint-based subgoals on the optimal path to the final goal, creating a\nstructured learning curriculum directly aligned with the task's geometric\nobjectives. Based on the anticipated subgoals, a continuous reward signal is\nprovided to train a low-level, goal-conditioned policy under the hierarchical\nreinforcement learning (HRL) framework with provable sub-optimality bound.\nExtensive experiments on complex, long-horizon manipulation tasks show that\nReLAM significantly accelerates learning and achieves superior performance\ncompared to state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Nan Tang"
                    },
                    {
                        "name": "Jing-Cheng Pang"
                    },
                    {
                        "name": "Guanlin Li"
                    },
                    {
                        "name": "Chao Qian"
                    },
                    {
                        "name": "Yang Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Yu"
                },
                "author": "Yang Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17388v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17388v4",
                "updated": "2025-09-26T14:25:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    25,
                    50,
                    4,
                    269,
                    0
                ],
                "published": "2024-11-26T12:46:57Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    12,
                    46,
                    57,
                    1,
                    331,
                    0
                ],
                "title": "Can LLMs be Good Graph Judge for Knowledge Graph Construction?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs be Good Graph Judge for Knowledge Graph Construction?"
                },
                "summary": "In real-world scenarios, most of the data obtained from the information\nretrieval (IR) system is unstructured. Converting natural language sentences\ninto structured Knowledge Graphs (KGs) remains a critical challenge. We\nidentified three limitations with respect to existing KG construction methods:\n(1) There could be a large amount of noise in real-world documents, which could\nresult in extracting messy information. (2) Naive LLMs usually extract\ninaccurate knowledge from some domain-specific documents. (3) Hallucination\nphenomenon cannot be overlooked when directly using LLMs to construct KGs. In\nthis paper, we propose \\textbf{GraphJudge}, a KG construction framework to\naddress the aforementioned challenges. In this framework, we designed an\nentity-centric strategy to eliminate the noise information in the documents.\nAnd we fine-tuned a LLM as a graph judge to finally enhance the quality of\ngenerated KGs. Experiments conducted on two general and one domain-specific\ntext-graph pair datasets demonstrate state-of-the-art performance against\nvarious baseline methods with strong generalization abilities. Our code is\navailable at\n\\href{https://github.com/hhy-huang/GraphJudge}{https://github.com/hhy-huang/GraphJudge}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world scenarios, most of the data obtained from the information\nretrieval (IR) system is unstructured. Converting natural language sentences\ninto structured Knowledge Graphs (KGs) remains a critical challenge. We\nidentified three limitations with respect to existing KG construction methods:\n(1) There could be a large amount of noise in real-world documents, which could\nresult in extracting messy information. (2) Naive LLMs usually extract\ninaccurate knowledge from some domain-specific documents. (3) Hallucination\nphenomenon cannot be overlooked when directly using LLMs to construct KGs. In\nthis paper, we propose \\textbf{GraphJudge}, a KG construction framework to\naddress the aforementioned challenges. In this framework, we designed an\nentity-centric strategy to eliminate the noise information in the documents.\nAnd we fine-tuned a LLM as a graph judge to finally enhance the quality of\ngenerated KGs. Experiments conducted on two general and one domain-specific\ntext-graph pair datasets demonstrate state-of-the-art performance against\nvarious baseline methods with strong generalization abilities. Our code is\navailable at\n\\href{https://github.com/hhy-huang/GraphJudge}{https://github.com/hhy-huang/GraphJudge}."
                },
                "authors": [
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Chong Chen"
                    },
                    {
                        "name": "Zeang Sheng"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_comment": "EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17388v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17388v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20611v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20611v2",
                "updated": "2025-09-26T14:22:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    22,
                    42,
                    4,
                    269,
                    0
                ],
                "published": "2025-05-27T01:21:57Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    1,
                    21,
                    57,
                    1,
                    147,
                    0
                ],
                "title": "Mamba-Driven Topology Fusion for Monocular 3D Human Pose Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mamba-Driven Topology Fusion for Monocular 3D Human Pose Estimation"
                },
                "summary": "Transformer-based methods for 3D human pose estimation face significant\ncomputational challenges due to the quadratic growth of self-attention\nmechanism complexity with sequence length. Recently, the Mamba model has\nsubstantially reduced computational overhead and demonstrated outstanding\nperformance in modeling long sequences by leveraging state space model (SSM).\nHowever, the ability of SSM to process sequential data is not suitable for 3D\njoint sequences with topological structures, and the causal convolution\nstructure in Mamba also lacks insight into local joint relationships. To\naddress these issues, we propose the Mamba-Driven Topology Fusion framework in\nthis paper. Specifically, the proposed Bone Aware Module infers the direction\nand length of bone vectors in the spherical coordinate system, providing\neffective topological guidance for the Mamba model in processing joint\nsequences. Furthermore, we enhance the convolutional structure within the Mamba\nmodel by integrating forward and backward graph convolutional network, enabling\nit to better capture local joint dependencies. Finally, we design a\nSpatiotemporal Refinement Module to model both temporal and spatial\nrelationships within the sequence. Through the incorporation of skeletal\ntopology, our approach effectively alleviates Mamba's limitations in capturing\nhuman structural relationships. We conduct extensive experiments on the\nHuman3.6M and MPI-INF-3DHP datasets for testing and comparison, and the results\nshow that the proposed method greatly reduces computational cost while\nachieving higher accuracy. Ablation studies further demonstrate the\neffectiveness of each proposed module. The code and models will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based methods for 3D human pose estimation face significant\ncomputational challenges due to the quadratic growth of self-attention\nmechanism complexity with sequence length. Recently, the Mamba model has\nsubstantially reduced computational overhead and demonstrated outstanding\nperformance in modeling long sequences by leveraging state space model (SSM).\nHowever, the ability of SSM to process sequential data is not suitable for 3D\njoint sequences with topological structures, and the causal convolution\nstructure in Mamba also lacks insight into local joint relationships. To\naddress these issues, we propose the Mamba-Driven Topology Fusion framework in\nthis paper. Specifically, the proposed Bone Aware Module infers the direction\nand length of bone vectors in the spherical coordinate system, providing\neffective topological guidance for the Mamba model in processing joint\nsequences. Furthermore, we enhance the convolutional structure within the Mamba\nmodel by integrating forward and backward graph convolutional network, enabling\nit to better capture local joint dependencies. Finally, we design a\nSpatiotemporal Refinement Module to model both temporal and spatial\nrelationships within the sequence. Through the incorporation of skeletal\ntopology, our approach effectively alleviates Mamba's limitations in capturing\nhuman structural relationships. We conduct extensive experiments on the\nHuman3.6M and MPI-INF-3DHP datasets for testing and comparison, and the results\nshow that the proposed method greatly reduces computational cost while\nachieving higher accuracy. Ablation studies further demonstrate the\neffectiveness of each proposed module. The code and models will be released."
                },
                "authors": [
                    {
                        "name": "Zenghao Zheng"
                    },
                    {
                        "name": "Lianping Yang"
                    },
                    {
                        "name": "Jinshan Pan"
                    },
                    {
                        "name": "Hegui Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Hegui Zhu"
                },
                "author": "Hegui Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20611v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20611v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22394v1",
                "updated": "2025-09-26T14:22:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    22,
                    15,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T14:22:15Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    22,
                    15,
                    4,
                    269,
                    0
                ],
                "title": "Deep Learning-Based Cross-Anatomy CT Synthesis Using Adapted nnResU-Net\n  with Anatomical Feature Prioritized Loss",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning-Based Cross-Anatomy CT Synthesis Using Adapted nnResU-Net\n  with Anatomical Feature Prioritized Loss"
                },
                "summary": "We present a patch-based 3D nnUNet adaptation for MR to CT and CBCT to CT\nimage translation using the multicenter SynthRAD2025 dataset, covering head and\nneck (HN), thorax (TH), and abdomen (AB) regions. Our approach leverages two\nmain network configurations: a standard UNet and a residual UNet, both adapted\nfrom nnUNet for image synthesis. The Anatomical Feature-Prioritized (AFP) loss\nwas introduced, which compares multilayer features extracted from a compact\nsegmentation network trained on TotalSegmentator labels, enhancing\nreconstruction of clinically relevant structures. Input volumes were normalized\nper-case using zscore normalization for MRIs, and clipping plus dataset level\nzscore normalization for CBCT and CT. Training used 3D patches tailored to each\nanatomical region without additional data augmentation. Models were trained for\n1000 and 1500 epochs, with AFP fine-tuning performed for 500 epochs using a\ncombined L1+AFP objective. During inference, overlapping patches were\naggregated via mean averaging with step size of 0.3, and postprocessing\nincluded reverse zscore normalization. Both network configurations were applied\nacross all regions, allowing consistent model design while capturing local\nadaptations through residual learning and AFP loss. Qualitative and\nquantitative evaluation revealed that residual networks combined with AFP\nyielded sharper reconstructions and improved anatomical fidelity, particularly\nfor bone structures in MR to CT and lesions in CBCT to CT, while L1only\nnetworks achieved slightly better intensity-based metrics. This methodology\nprovides a stable solution for cross modality medical image synthesis,\ndemonstrating the effectiveness of combining the automatic nnUNet pipeline with\nresidual learning and anatomically guided feature losses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a patch-based 3D nnUNet adaptation for MR to CT and CBCT to CT\nimage translation using the multicenter SynthRAD2025 dataset, covering head and\nneck (HN), thorax (TH), and abdomen (AB) regions. Our approach leverages two\nmain network configurations: a standard UNet and a residual UNet, both adapted\nfrom nnUNet for image synthesis. The Anatomical Feature-Prioritized (AFP) loss\nwas introduced, which compares multilayer features extracted from a compact\nsegmentation network trained on TotalSegmentator labels, enhancing\nreconstruction of clinically relevant structures. Input volumes were normalized\nper-case using zscore normalization for MRIs, and clipping plus dataset level\nzscore normalization for CBCT and CT. Training used 3D patches tailored to each\nanatomical region without additional data augmentation. Models were trained for\n1000 and 1500 epochs, with AFP fine-tuning performed for 500 epochs using a\ncombined L1+AFP objective. During inference, overlapping patches were\naggregated via mean averaging with step size of 0.3, and postprocessing\nincluded reverse zscore normalization. Both network configurations were applied\nacross all regions, allowing consistent model design while capturing local\nadaptations through residual learning and AFP loss. Qualitative and\nquantitative evaluation revealed that residual networks combined with AFP\nyielded sharper reconstructions and improved anatomical fidelity, particularly\nfor bone structures in MR to CT and lesions in CBCT to CT, while L1only\nnetworks achieved slightly better intensity-based metrics. This methodology\nprovides a stable solution for cross modality medical image synthesis,\ndemonstrating the effectiveness of combining the automatic nnUNet pipeline with\nresidual learning and anatomically guided feature losses."
                },
                "authors": [
                    {
                        "name": "Javier Sequeiro González"
                    },
                    {
                        "name": "Arthur Longuefosse"
                    },
                    {
                        "name": "Miguel Díaz Benito"
                    },
                    {
                        "name": "Álvaro García Martín"
                    },
                    {
                        "name": "Fabien Baldacci"
                    }
                ],
                "author_detail": {
                    "name": "Fabien Baldacci"
                },
                "author": "Fabien Baldacci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22391v1",
                "updated": "2025-09-26T14:18:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    18,
                    50,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T14:18:50Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    18,
                    50,
                    4,
                    269,
                    0
                ],
                "title": "Do LLM Agents Know How to Ground, Recover, and Assess? A Benchmark for\n  Epistemic Competence in Information-Seeking Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLM Agents Know How to Ground, Recover, and Assess? A Benchmark for\n  Epistemic Competence in Information-Seeking Agents"
                },
                "summary": "Recent work has explored training Large Language Model (LLM) search agents\nwith reinforcement learning (RL) for open-domain question answering (QA).\nHowever, most evaluations focus solely on final answer accuracy, overlooking\nhow these agents reason with and act on external evidence. We introduce\nSeekBench, the first benchmark for evaluating the \\textit{epistemic competence}\nof LLM search agents through step-level analysis of their response traces.\nSeekBench comprises 190 expert-annotated traces with over 1,800 response steps\ngenerated by LLM search agents, each enriched with evidence annotations for\ngranular analysis of whether agents (1) generate reasoning steps grounded in\nobserved evidence, (2) adaptively reformulate searches to recover from\nlow-quality results, and (3) have proper calibration to correctly assess\nwhether the current evidence is sufficient for providing an answer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has explored training Large Language Model (LLM) search agents\nwith reinforcement learning (RL) for open-domain question answering (QA).\nHowever, most evaluations focus solely on final answer accuracy, overlooking\nhow these agents reason with and act on external evidence. We introduce\nSeekBench, the first benchmark for evaluating the \\textit{epistemic competence}\nof LLM search agents through step-level analysis of their response traces.\nSeekBench comprises 190 expert-annotated traces with over 1,800 response steps\ngenerated by LLM search agents, each enriched with evidence annotations for\ngranular analysis of whether agents (1) generate reasoning steps grounded in\nobserved evidence, (2) adaptively reformulate searches to recover from\nlow-quality results, and (3) have proper calibration to correctly assess\nwhether the current evidence is sufficient for providing an answer."
                },
                "authors": [
                    {
                        "name": "Jiaqi Shao"
                    },
                    {
                        "name": "Yuxiang Lin"
                    },
                    {
                        "name": "Munish Prasad Lohani"
                    },
                    {
                        "name": "Yufeng Miao"
                    },
                    {
                        "name": "Bing Luo"
                    }
                ],
                "author_detail": {
                    "name": "Bing Luo"
                },
                "author": "Bing Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22389v1",
                "updated": "2025-09-26T14:17:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    17,
                    13,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T14:17:13Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    17,
                    13,
                    4,
                    269,
                    0
                ],
                "title": "SensIAT: An R Package for Conducting Sensitivity Analysis of Randomized\n  Trials with Irregular Assessment Times",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SensIAT: An R Package for Conducting Sensitivity Analysis of Randomized\n  Trials with Irregular Assessment Times"
                },
                "summary": "This paper introduces an R package SensIAT that implements a sensitivity\nanalysis methodology, based on augmented inverse intensity weighting, for\nrandomized trials with irregular and potentially informative assessment times.\nTargets of inference involve the population mean outcome in each treatment arm\nas well as the difference in these means (i.e., treatment effect) at specified\ntimes after randomization. This methodology is useful in settings where there\nis concern that study participants are either more, or less, likely to have\nassessments at times when their outcomes are worse. In such settings,\nunadjusted estimates can be biased. The methodology allows researchers to see\nhow inferences are impacted by a range of assumptions about the strength and\ndirection of informative timing in each arm, while incorporating flexible\nsemi-parametric modeling. We describe the functions implemented in SensIAT and\nillustrate them through an analysis of a synthetic dataset motivated by the\nHAP2 asthma randomized clinical trial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an R package SensIAT that implements a sensitivity\nanalysis methodology, based on augmented inverse intensity weighting, for\nrandomized trials with irregular and potentially informative assessment times.\nTargets of inference involve the population mean outcome in each treatment arm\nas well as the difference in these means (i.e., treatment effect) at specified\ntimes after randomization. This methodology is useful in settings where there\nis concern that study participants are either more, or less, likely to have\nassessments at times when their outcomes are worse. In such settings,\nunadjusted estimates can be biased. The methodology allows researchers to see\nhow inferences are impacted by a range of assumptions about the strength and\ndirection of informative timing in each arm, while incorporating flexible\nsemi-parametric modeling. We describe the functions implemented in SensIAT and\nillustrate them through an analysis of a synthetic dataset motivated by the\nHAP2 asthma randomized clinical trial."
                },
                "authors": [
                    {
                        "name": "Andrew Redd"
                    },
                    {
                        "name": "Yujing Gao"
                    },
                    {
                        "name": "Bonnie B. Smith"
                    },
                    {
                        "name": "Ravi Varadhan"
                    },
                    {
                        "name": "Andrea J. Apter"
                    },
                    {
                        "name": "Daniel O. Scharfstein"
                    }
                ],
                "author_detail": {
                    "name": "Daniel O. Scharfstein"
                },
                "author": "Daniel O. Scharfstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22387v1",
                "updated": "2025-09-26T14:15:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    15,
                    44,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T14:15:44Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    15,
                    44,
                    4,
                    269,
                    0
                ],
                "title": "SpinGPT: A Large-Language-Model Approach to Playing Poker Correctly",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpinGPT: A Large-Language-Model Approach to Playing Poker Correctly"
                },
                "summary": "The Counterfactual Regret Minimization (CFR) algorithm and its variants have\nenabled the development of pokerbots capable of beating the best human players\nin heads-up (1v1) cash games and competing with them in six-player formats.\nHowever, CFR's computational complexity rises exponentially with the number of\nplayers. Furthermore, in games with three or more players, following Nash\nequilibrium no longer guarantees a non-losing outcome. These limitations, along\nwith others, significantly restrict the applicability of CFR to the most\npopular formats: tournaments. Motivated by the recent success of Large Language\nModels (LLM) in chess and Diplomacy, we present SpinGPT, the first LLM tailored\nto Spin & Go, a popular three-player online poker format. SpinGPT is trained in\ntwo stages: (1) Supervised Fine-Tuning on 320k high-stakes expert decisions;\n(2) Reinforcement Learning on 270k solver-generated hands. Our results show\nthat SpinGPT matches the solver's actions in 78% of decisions (tolerant\naccuracy). With a simple deep-stack heuristic, it achieves 13.4 +/- 12.9 BB/100\nversus Slumbot in heads-up over 30,000 hands (95% CI). These results suggest\nthat LLMs could be a new way to deal with multi-player imperfect-information\ngames like poker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Counterfactual Regret Minimization (CFR) algorithm and its variants have\nenabled the development of pokerbots capable of beating the best human players\nin heads-up (1v1) cash games and competing with them in six-player formats.\nHowever, CFR's computational complexity rises exponentially with the number of\nplayers. Furthermore, in games with three or more players, following Nash\nequilibrium no longer guarantees a non-losing outcome. These limitations, along\nwith others, significantly restrict the applicability of CFR to the most\npopular formats: tournaments. Motivated by the recent success of Large Language\nModels (LLM) in chess and Diplomacy, we present SpinGPT, the first LLM tailored\nto Spin & Go, a popular three-player online poker format. SpinGPT is trained in\ntwo stages: (1) Supervised Fine-Tuning on 320k high-stakes expert decisions;\n(2) Reinforcement Learning on 270k solver-generated hands. Our results show\nthat SpinGPT matches the solver's actions in 78% of decisions (tolerant\naccuracy). With a simple deep-stack heuristic, it achieves 13.4 +/- 12.9 BB/100\nversus Slumbot in heads-up over 30,000 hands (95% CI). These results suggest\nthat LLMs could be a new way to deal with multi-player imperfect-information\ngames like poker."
                },
                "authors": [
                    {
                        "name": "Narada Maugin"
                    },
                    {
                        "name": "Tristan Cazenave"
                    }
                ],
                "author_detail": {
                    "name": "Tristan Cazenave"
                },
                "author": "Tristan Cazenave",
                "arxiv_comment": "Accepted at Advances in Computer Games (ACG) 2025, LNCS (Springer)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11973v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11973v2",
                "updated": "2025-09-26T14:07:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    7,
                    34,
                    4,
                    269,
                    0
                ],
                "published": "2024-04-18T08:01:20Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    8,
                    1,
                    20,
                    3,
                    109,
                    0
                ],
                "title": "A critical review of methods and challenges in large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical review of methods and challenges in large language models"
                },
                "summary": "This critical review provides an in-depth analysis of Large Language Models\n(LLMs), encompassing their foundational principles, diverse applications, and\nadvanced training methodologies. We critically examine the evolution from\nRecurrent Neural Networks (RNNs) to Transformer models, highlighting the\nsignificant advancements and innovations in LLM architectures. The review\nexplores state-of-the-art techniques such as in-context learning and various\nfine-tuning approaches, with an emphasis on optimizing parameter efficiency. We\nalso discuss methods for aligning LLMs with human preferences, including\nreinforcement learning frameworks and human feedback mechanisms. The emerging\ntechnique of retrieval-augmented generation, which integrates external\nknowledge into LLMs, is also evaluated. Additionally, we address the ethical\nconsiderations of deploying LLMs, stressing the importance of responsible and\nmindful application. By identifying current gaps and suggesting future research\ndirections, this review provides a comprehensive and critical overview of the\npresent state and potential advancements in LLMs. This work serves as an\ninsightful guide for researchers and practitioners in artificial intelligence,\noffering a unified perspective on the strengths, limitations, and future\nprospects of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This critical review provides an in-depth analysis of Large Language Models\n(LLMs), encompassing their foundational principles, diverse applications, and\nadvanced training methodologies. We critically examine the evolution from\nRecurrent Neural Networks (RNNs) to Transformer models, highlighting the\nsignificant advancements and innovations in LLM architectures. The review\nexplores state-of-the-art techniques such as in-context learning and various\nfine-tuning approaches, with an emphasis on optimizing parameter efficiency. We\nalso discuss methods for aligning LLMs with human preferences, including\nreinforcement learning frameworks and human feedback mechanisms. The emerging\ntechnique of retrieval-augmented generation, which integrates external\nknowledge into LLMs, is also evaluated. Additionally, we address the ethical\nconsiderations of deploying LLMs, stressing the importance of responsible and\nmindful application. By identifying current gaps and suggesting future research\ndirections, this review provides a comprehensive and critical overview of the\npresent state and potential advancements in LLMs. This work serves as an\ninsightful guide for researchers and practitioners in artificial intelligence,\noffering a unified perspective on the strengths, limitations, and future\nprospects of LLMs."
                },
                "authors": [
                    {
                        "name": "Milad Moradi"
                    },
                    {
                        "name": "Ke Yan"
                    },
                    {
                        "name": "David Colwell"
                    },
                    {
                        "name": "Matthias Samwald"
                    },
                    {
                        "name": "Rhona Asgari"
                    }
                ],
                "author_detail": {
                    "name": "Rhona Asgari"
                },
                "author": "Rhona Asgari",
                "arxiv_doi": "10.32604/cmc.2025.061263",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.32604/cmc.2025.061263",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.11973v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11973v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04349v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04349v5",
                "updated": "2025-09-26T14:04:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    4,
                    7,
                    4,
                    269,
                    0
                ],
                "published": "2025-08-06T11:42:47Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    42,
                    47,
                    2,
                    218,
                    0
                ],
                "title": "GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy\n  Entropy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy\n  Entropy"
                },
                "summary": "Reinforcement learning (RL) is a pivotal task for enhancing Large Language\nModel (LLM) reasoning. Conventional algorithms, however, typically adhere to a\ncoarse-grained credit assignment paradigm, applying a uniform reward to all\ntokens in a sequence, a critical flaw in long-chain reasoning tasks. In this\npaper, we address this challenge and propose Dynamic Entropy Weighting, a novel\nmechanism that facilitates fine-grained rewards through two new algorithms:\nGroup Token Policy Optimization (GTPO), which assigns an entropy-weighted\nreward to each token, and the analogous algorithm Sequence-Level GRPO (GRPO-S).\nOur approach is founded on the hypothesis that high policy entropy within a\nreasoning path is a powerful heuristic for cognitive effort at pivotal\njunctures, which can be repurposed into a learning signal. By repurposing\npolicy entropy for reward shaping, we achieve true per-token credit assignment.\nExperimental results across challenging reasoning benchmarks validate the\nsuperiority of our approach, showing our methods significantly outperform a\nstrong DAPO baseline and confirming our entropy-weighting mechanism as the key\ndriver of this performance boost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) is a pivotal task for enhancing Large Language\nModel (LLM) reasoning. Conventional algorithms, however, typically adhere to a\ncoarse-grained credit assignment paradigm, applying a uniform reward to all\ntokens in a sequence, a critical flaw in long-chain reasoning tasks. In this\npaper, we address this challenge and propose Dynamic Entropy Weighting, a novel\nmechanism that facilitates fine-grained rewards through two new algorithms:\nGroup Token Policy Optimization (GTPO), which assigns an entropy-weighted\nreward to each token, and the analogous algorithm Sequence-Level GRPO (GRPO-S).\nOur approach is founded on the hypothesis that high policy entropy within a\nreasoning path is a powerful heuristic for cognitive effort at pivotal\njunctures, which can be repurposed into a learning signal. By repurposing\npolicy entropy for reward shaping, we achieve true per-token credit assignment.\nExperimental results across challenging reasoning benchmarks validate the\nsuperiority of our approach, showing our methods significantly outperform a\nstrong DAPO baseline and confirming our entropy-weighting mechanism as the key\ndriver of this performance boost."
                },
                "authors": [
                    {
                        "name": "Hongze Tan"
                    },
                    {
                        "name": "Jianfei Pan"
                    },
                    {
                        "name": "Jinghao Lin"
                    },
                    {
                        "name": "Tao Chen"
                    },
                    {
                        "name": "Zhihang Zheng"
                    },
                    {
                        "name": "Zhihao Tang"
                    },
                    {
                        "name": "Haihua Yang"
                    }
                ],
                "author_detail": {
                    "name": "Haihua Yang"
                },
                "author": "Haihua Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04349v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04349v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22369v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22369v1",
                "updated": "2025-09-26T14:02:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    2,
                    20,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T14:02:20Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    2,
                    20,
                    4,
                    269,
                    0
                ],
                "title": "Role-Aware Multi-modal federated learning system for detecting phishing\n  webpages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role-Aware Multi-modal federated learning system for detecting phishing\n  webpages"
                },
                "summary": "We present a federated, multi-modal phishing website detector that supports\nURL, HTML, and IMAGE inputs without binding clients to a fixed modality at\ninference: any client can invoke any modality head trained elsewhere.\nMethodologically, we propose role-aware bucket aggregation on top of FedProx,\ninspired by Mixture-of-Experts and FedMM. We drop learnable routing and use\nhard gating (selecting the IMAGE/HTML/URL expert by sample modality), enabling\nseparate aggregation of modality-specific parameters to isolate cross-embedding\nconflicts and stabilize convergence. On TR-OP, the Fusion head reaches Acc\n97.5% with FPR 2.4% across two data types; on the image subset (ablation) it\nattains Acc 95.5% with FPR 5.9%. For text, we use GraphCodeBERT for URLs and an\nearly three-way embedding for raw, noisy HTML. On WebPhish (HTML) we obtain Acc\n96.5% / FPR 1.8%; on TR-OP (raw HTML) we obtain Acc 95.1% / FPR 4.6%. Results\nindicate that bucket aggregation with hard-gated experts enables stable\nfederated training under strict privacy, while improving the usability and\nflexibility of multi-modal phishing detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a federated, multi-modal phishing website detector that supports\nURL, HTML, and IMAGE inputs without binding clients to a fixed modality at\ninference: any client can invoke any modality head trained elsewhere.\nMethodologically, we propose role-aware bucket aggregation on top of FedProx,\ninspired by Mixture-of-Experts and FedMM. We drop learnable routing and use\nhard gating (selecting the IMAGE/HTML/URL expert by sample modality), enabling\nseparate aggregation of modality-specific parameters to isolate cross-embedding\nconflicts and stabilize convergence. On TR-OP, the Fusion head reaches Acc\n97.5% with FPR 2.4% across two data types; on the image subset (ablation) it\nattains Acc 95.5% with FPR 5.9%. For text, we use GraphCodeBERT for URLs and an\nearly three-way embedding for raw, noisy HTML. On WebPhish (HTML) we obtain Acc\n96.5% / FPR 1.8%; on TR-OP (raw HTML) we obtain Acc 95.1% / FPR 4.6%. Results\nindicate that bucket aggregation with hard-gated experts enables stable\nfederated training under strict privacy, while improving the usability and\nflexibility of multi-modal phishing detection."
                },
                "authors": [
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Imran Khan"
                    },
                    {
                        "name": "Martin White"
                    },
                    {
                        "name": "Natalia Beloff"
                    }
                ],
                "author_detail": {
                    "name": "Natalia Beloff"
                },
                "author": "Natalia Beloff",
                "arxiv_comment": "22 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22369v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22369v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22367v1",
                "updated": "2025-09-26T14:00:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    0,
                    51,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T14:00:51Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    0,
                    51,
                    4,
                    269,
                    0
                ],
                "title": "What Is The Political Content in LLMs' Pre- and Post-Training Data?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Is The Political Content in LLMs' Pre- and Post-Training Data?"
                },
                "summary": "Large language models (LLMs) are known to generate politically biased text,\nyet how such biases arise remains unclear. A crucial step toward answering this\nquestion is the analysis of training data, whose political content remains\nlargely underexplored in current LLM research. To address this gap, we present\nin this paper an analysis of the pre- and post-training corpora of OLMO2, the\nlargest fully open-source model released together with its complete dataset.\nFrom these corpora, we draw large random samples, automatically annotate\ndocuments for political orientation, and analyze their source domains and\ncontent. We then assess how political content in the training data correlates\nwith models' stance on specific policy issues. Our analysis shows that\nleft-leaning documents predominate across datasets, with pre-training corpora\ncontaining significantly more politically engaged content than post-training\ndata. We also find that left- and right-leaning documents frame similar topics\nthrough distinct values and sources of legitimacy. Finally, the predominant\nstance in the training data strongly correlates with models' political biases\nwhen evaluated on policy issues. These findings underscore the need to\nintegrate political content analysis into future data curation pipelines as\nwell as in-depth documentation of filtering strategies for transparency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are known to generate politically biased text,\nyet how such biases arise remains unclear. A crucial step toward answering this\nquestion is the analysis of training data, whose political content remains\nlargely underexplored in current LLM research. To address this gap, we present\nin this paper an analysis of the pre- and post-training corpora of OLMO2, the\nlargest fully open-source model released together with its complete dataset.\nFrom these corpora, we draw large random samples, automatically annotate\ndocuments for political orientation, and analyze their source domains and\ncontent. We then assess how political content in the training data correlates\nwith models' stance on specific policy issues. Our analysis shows that\nleft-leaning documents predominate across datasets, with pre-training corpora\ncontaining significantly more politically engaged content than post-training\ndata. We also find that left- and right-leaning documents frame similar topics\nthrough distinct values and sources of legitimacy. Finally, the predominant\nstance in the training data strongly correlates with models' political biases\nwhen evaluated on policy issues. These findings underscore the need to\nintegrate political content analysis into future data curation pipelines as\nwell as in-depth documentation of filtering strategies for transparency."
                },
                "authors": [
                    {
                        "name": "Tanise Ceron"
                    },
                    {
                        "name": "Dmitry Nikolaev"
                    },
                    {
                        "name": "Dominik Stammbach"
                    },
                    {
                        "name": "Debora Nozza"
                    }
                ],
                "author_detail": {
                    "name": "Debora Nozza"
                },
                "author": "Debora Nozza",
                "arxiv_comment": "9 pages, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22366v1",
                "updated": "2025-09-26T14:00:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    0,
                    20,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T14:00:20Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    0,
                    20,
                    4,
                    269,
                    0
                ],
                "title": "Exploratory Semantic Reliability Analysis of Wind Turbine Maintenance\n  Logs using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploratory Semantic Reliability Analysis of Wind Turbine Maintenance\n  Logs using Large Language Models"
                },
                "summary": "A wealth of operational intelligence is locked within the unstructured\nfree-text of wind turbine maintenance logs, a resource largely inaccessible to\ntraditional quantitative reliability analysis. While machine learning has been\napplied to this data, existing approaches typically stop at classification,\ncategorising text into predefined labels. This paper addresses the gap in\nleveraging modern large language models (LLMs) for more complex reasoning\ntasks. We introduce an exploratory framework that uses LLMs to move beyond\nclassification and perform deep semantic analysis. We apply this framework to a\nlarge industrial dataset to execute four analytical workflows: failure mode\nidentification, causal chain inference, comparative site analysis, and data\nquality auditing. The results demonstrate that LLMs can function as powerful\n\"reliability co-pilots,\" moving beyond labelling to synthesise textual\ninformation and generate actionable, expert-level hypotheses. This work\ncontributes a novel and reproducible methodology for using LLMs as a reasoning\ntool, offering a new pathway to enhance operational intelligence in the wind\nenergy sector by unlocking insights previously obscured in unstructured data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A wealth of operational intelligence is locked within the unstructured\nfree-text of wind turbine maintenance logs, a resource largely inaccessible to\ntraditional quantitative reliability analysis. While machine learning has been\napplied to this data, existing approaches typically stop at classification,\ncategorising text into predefined labels. This paper addresses the gap in\nleveraging modern large language models (LLMs) for more complex reasoning\ntasks. We introduce an exploratory framework that uses LLMs to move beyond\nclassification and perform deep semantic analysis. We apply this framework to a\nlarge industrial dataset to execute four analytical workflows: failure mode\nidentification, causal chain inference, comparative site analysis, and data\nquality auditing. The results demonstrate that LLMs can function as powerful\n\"reliability co-pilots,\" moving beyond labelling to synthesise textual\ninformation and generate actionable, expert-level hypotheses. This work\ncontributes a novel and reproducible methodology for using LLMs as a reasoning\ntool, offering a new pathway to enhance operational intelligence in the wind\nenergy sector by unlocking insights previously obscured in unstructured data."
                },
                "authors": [
                    {
                        "name": "Max Malyi"
                    },
                    {
                        "name": "Jonathan Shek"
                    },
                    {
                        "name": "Andre Biscaya"
                    }
                ],
                "author_detail": {
                    "name": "Andre Biscaya"
                },
                "author": "Andre Biscaya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.22647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22647v1",
                "updated": "2025-09-26T17:59:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    55,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:59:55Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    55,
                    4,
                    269,
                    0
                ],
                "title": "CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement\n  Learning"
                },
                "summary": "Image captioning is a fundamental task that bridges the visual and linguistic\ndomains, playing a critical role in pre-training Large Vision-Language Models\n(LVLMs). Current state-of-the-art captioning models are typically trained with\nSupervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable\ndata annotated by humans or proprietary models. This approach often leads to\nmodels that memorize specific ground-truth answers, limiting their generality\nand ability to generate diverse, creative descriptions. To overcome the\nlimitation of SFT, we propose applying the Reinforcement Learning with\nVerifiable Rewards (RLVR) paradigm to the open-ended task of image captioning.\nA primary challenge, however, is designing an objective reward function for the\ninherently subjective nature of what constitutes a \"good\" caption. We introduce\nCaptioning Reinforcement Learning (CapRL), a novel training framework that\nredefines caption quality through its utility: a high-quality caption should\nenable a non-visual language model to accurately answer questions about the\ncorresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM\ngenerates a caption, and the objective reward is derived from the accuracy of a\nseparate, vision-free LLM answering Multiple-Choice Questions based solely on\nthat caption. As the first study to apply RLVR to the subjective image\ncaptioning task, we demonstrate that CapRL significantly enhances multiple\nsettings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B\nresults in substantial gains across 12 benchmarks. Moreover, within the Prism\nFramework for caption quality evaluation, CapRL achieves performance comparable\nto Qwen2.5-VL-72B, while exceeding the baseline by an average margin of 8.4%.\nCode is available here: https://github.com/InternLM/CapRL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image captioning is a fundamental task that bridges the visual and linguistic\ndomains, playing a critical role in pre-training Large Vision-Language Models\n(LVLMs). Current state-of-the-art captioning models are typically trained with\nSupervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable\ndata annotated by humans or proprietary models. This approach often leads to\nmodels that memorize specific ground-truth answers, limiting their generality\nand ability to generate diverse, creative descriptions. To overcome the\nlimitation of SFT, we propose applying the Reinforcement Learning with\nVerifiable Rewards (RLVR) paradigm to the open-ended task of image captioning.\nA primary challenge, however, is designing an objective reward function for the\ninherently subjective nature of what constitutes a \"good\" caption. We introduce\nCaptioning Reinforcement Learning (CapRL), a novel training framework that\nredefines caption quality through its utility: a high-quality caption should\nenable a non-visual language model to accurately answer questions about the\ncorresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM\ngenerates a caption, and the objective reward is derived from the accuracy of a\nseparate, vision-free LLM answering Multiple-Choice Questions based solely on\nthat caption. As the first study to apply RLVR to the subjective image\ncaptioning task, we demonstrate that CapRL significantly enhances multiple\nsettings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B\nresults in substantial gains across 12 benchmarks. Moreover, within the Prism\nFramework for caption quality evaluation, CapRL achieves performance comparable\nto Qwen2.5-VL-72B, while exceeding the baseline by an average margin of 8.4%.\nCode is available here: https://github.com/InternLM/CapRL."
                },
                "authors": [
                    {
                        "name": "Long Xing"
                    },
                    {
                        "name": "Xiaoyi Dong"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Yuhang Cao"
                    },
                    {
                        "name": "Jianze Liang"
                    },
                    {
                        "name": "Qidong Huang"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Feng Wu"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "arxiv_comment": "Code is available at https://github.com/InternLM/CapRL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08799v2",
                "updated": "2025-09-26T17:59:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    54,
                    4,
                    269,
                    0
                ],
                "published": "2025-07-11T17:59:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "KV Cache Steering for Controlling Frozen LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Steering for Controlling Frozen LLMs"
                },
                "summary": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach constructs\nsteering vectors from reasoning traces, obtained either from teacher models\n(e.g., GPT-4o) or existing human annotations, that shift model behavior toward\nmore explicit, multi-step reasoning without fine-tuning or prompt\nmodifications. Experimental evaluations on diverse reasoning benchmarks\ndemonstrate that cache steering improves both the qualitative structure of\nmodel reasoning and quantitative task performance. Additional experiments show\nthat the method also scales to larger models and yields further gains on\nchallenging datasets such as GPQA and MATH. Compared to prior activation\nsteering techniques that require continuous interventions, our one-shot cache\nsteering offers substantial advantages in terms of inference latency,\nhyperparameter stability, and ease of integration with existing inference APIs.\nBeyond mere reasoning induction, we show that cache steering enables\ncontrollable transfer of reasoning styles (e.g., stepwise, causal, analogical),\nmaking it a practical tool for behavior-level guidance of language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach constructs\nsteering vectors from reasoning traces, obtained either from teacher models\n(e.g., GPT-4o) or existing human annotations, that shift model behavior toward\nmore explicit, multi-step reasoning without fine-tuning or prompt\nmodifications. Experimental evaluations on diverse reasoning benchmarks\ndemonstrate that cache steering improves both the qualitative structure of\nmodel reasoning and quantitative task performance. Additional experiments show\nthat the method also scales to larger models and yields further gains on\nchallenging datasets such as GPQA and MATH. Compared to prior activation\nsteering techniques that require continuous interventions, our one-shot cache\nsteering offers substantial advantages in terms of inference latency,\nhyperparameter stability, and ease of integration with existing inference APIs.\nBeyond mere reasoning induction, we show that cache steering enables\ncontrollable transfer of reasoning styles (e.g., stepwise, causal, analogical),\nmaking it a practical tool for behavior-level guidance of language models."
                },
                "authors": [
                    {
                        "name": "Max Belitsky"
                    },
                    {
                        "name": "Dawid J. Kopiczko"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "M. Jehanzeb Mirza"
                    },
                    {
                        "name": "James R. Glass"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22646v1",
                "updated": "2025-09-26T17:59:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    54,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:59:54Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    54,
                    4,
                    269,
                    0
                ],
                "title": "Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal\n  LLMs"
                },
                "summary": "Can humans identify AI-generated (fake) videos and provide grounded reasons?\nWhile video generation models have advanced rapidly, a critical dimension --\nwhether humans can detect deepfake traces within a generated video, i.e.,\nspatiotemporal grounded visual artifacts that reveal a video as machine\ngenerated -- has been largely overlooked. We introduce DeeptraceReward, the\nfirst fine-grained, spatially- and temporally- aware benchmark that annotates\nhuman-perceived fake traces for video generation reward. The dataset comprises\n4.3K detailed annotations across 3.3K high-quality generated videos. Each\nannotation provides a natural-language explanation, pinpoints a bounding-box\nregion containing the perceived trace, and marks precise onset and offset\ntimestamps. We consolidate these annotations into 9 major categories of\ndeepfake traces that lead humans to identify a video as AI-generated, and train\nmultimodal language models (LMs) as reward models to mimic human judgments and\nlocalizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by\n34.7% on average across fake clue identification, grounding, and explanation.\nInterestingly, we observe a consistent difficulty gradient: binary fake v.s.\nreal classification is substantially easier than fine-grained deepfake trace\ndetection; within the latter, performance degrades from natural language\nexplanations (easiest), to spatial grounding, to temporal labeling (hardest).\nBy foregrounding human-perceived deepfake traces, DeeptraceReward provides a\nrigorous testbed and training signal for socially aware and trustworthy video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can humans identify AI-generated (fake) videos and provide grounded reasons?\nWhile video generation models have advanced rapidly, a critical dimension --\nwhether humans can detect deepfake traces within a generated video, i.e.,\nspatiotemporal grounded visual artifacts that reveal a video as machine\ngenerated -- has been largely overlooked. We introduce DeeptraceReward, the\nfirst fine-grained, spatially- and temporally- aware benchmark that annotates\nhuman-perceived fake traces for video generation reward. The dataset comprises\n4.3K detailed annotations across 3.3K high-quality generated videos. Each\nannotation provides a natural-language explanation, pinpoints a bounding-box\nregion containing the perceived trace, and marks precise onset and offset\ntimestamps. We consolidate these annotations into 9 major categories of\ndeepfake traces that lead humans to identify a video as AI-generated, and train\nmultimodal language models (LMs) as reward models to mimic human judgments and\nlocalizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by\n34.7% on average across fake clue identification, grounding, and explanation.\nInterestingly, we observe a consistent difficulty gradient: binary fake v.s.\nreal classification is substantially easier than fine-grained deepfake trace\ndetection; within the latter, performance degrades from natural language\nexplanations (easiest), to spatial grounding, to temporal labeling (hardest).\nBy foregrounding human-perceived deepfake traces, DeeptraceReward provides a\nrigorous testbed and training signal for socially aware and trustworthy video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Xingyu Fu"
                    },
                    {
                        "name": "Siyi Liu"
                    },
                    {
                        "name": "Yinuo Xu"
                    },
                    {
                        "name": "Pan Lu"
                    },
                    {
                        "name": "Guangqiuse Hu"
                    },
                    {
                        "name": "Tianbo Yang"
                    },
                    {
                        "name": "Taran Anantasagar"
                    },
                    {
                        "name": "Christopher Shen"
                    },
                    {
                        "name": "Yikai Mao"
                    },
                    {
                        "name": "Yuanzhe Liu"
                    },
                    {
                        "name": "Keyush Shah"
                    },
                    {
                        "name": "Chung Un Lee"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    }
                ],
                "author_detail": {
                    "name": "Chris Callison-Burch"
                },
                "author": "Chris Callison-Burch",
                "arxiv_comment": "Project Page: https://deeptracereward.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22644v1",
                "updated": "2025-09-26T17:59:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    51,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:59:51Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    51,
                    4,
                    269,
                    0
                ],
                "title": "WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level\n  Feedback and Step-Level Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level\n  Feedback and Step-Level Reinforcement Learning"
                },
                "summary": "Agent systems powered by large language models (LLMs) have demonstrated\nimpressive performance on repository-level code-generation tasks. However, for\ntasks such as website codebase generation, which depend heavily on visual\neffects and user-interaction feedback, current code agents rely only on simple\ncode execution for feedback and verification. This approach fails to capture\nthe actual quality of the generated code. In this paper, we propose\nWebGen-Agent, a novel website-generation agent that leverages comprehensive and\nmulti-level visual feedback to iteratively generate and refine the website\ncodebase. Detailed and expressive text descriptions and suggestions regarding\nthe screenshots and GUI-agent testing of the websites are generated by a visual\nlanguage model (VLM), together with scores that quantify their quality. The\nscreenshot and GUI-agent scores are further integrated with a backtracking and\nselect-best mechanism, enhancing the performance of the agent. Utilizing the\naccurate visual scores inherent in the WebGen-Agent workflow, we further\nintroduce \\textit{Step-GRPO with Screenshot and GUI-agent Feedback} to improve\nthe ability of LLMs to act as the reasoning engine of WebGen-Agent. By using\nthe screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we\nprovide a dense and reliable process supervision signal, which effectively\nimproves the model's website-generation ability. On the WebGen-Bench dataset,\nWebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9%\nand its appearance score from 3.0 to 3.9, outperforming the previous\nstate-of-the-art agent system. Additionally, our Step-GRPO training approach\nincreases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and\nraises the appearance score from 3.4 to 3.7.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent systems powered by large language models (LLMs) have demonstrated\nimpressive performance on repository-level code-generation tasks. However, for\ntasks such as website codebase generation, which depend heavily on visual\neffects and user-interaction feedback, current code agents rely only on simple\ncode execution for feedback and verification. This approach fails to capture\nthe actual quality of the generated code. In this paper, we propose\nWebGen-Agent, a novel website-generation agent that leverages comprehensive and\nmulti-level visual feedback to iteratively generate and refine the website\ncodebase. Detailed and expressive text descriptions and suggestions regarding\nthe screenshots and GUI-agent testing of the websites are generated by a visual\nlanguage model (VLM), together with scores that quantify their quality. The\nscreenshot and GUI-agent scores are further integrated with a backtracking and\nselect-best mechanism, enhancing the performance of the agent. Utilizing the\naccurate visual scores inherent in the WebGen-Agent workflow, we further\nintroduce \\textit{Step-GRPO with Screenshot and GUI-agent Feedback} to improve\nthe ability of LLMs to act as the reasoning engine of WebGen-Agent. By using\nthe screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we\nprovide a dense and reliable process supervision signal, which effectively\nimproves the model's website-generation ability. On the WebGen-Bench dataset,\nWebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9%\nand its appearance score from 3.0 to 3.9, outperforming the previous\nstate-of-the-art agent system. Additionally, our Step-GRPO training approach\nincreases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and\nraises the appearance score from 3.4 to 3.7."
                },
                "authors": [
                    {
                        "name": "Zimu Lu"
                    },
                    {
                        "name": "Houxing Ren"
                    },
                    {
                        "name": "Yunqiao Yang"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Zhuofan Zong"
                    },
                    {
                        "name": "Junting Pan"
                    },
                    {
                        "name": "Mingjie Zhan"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22645v1",
                "updated": "2025-09-26T17:59:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    51,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:59:51Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    51,
                    4,
                    269,
                    0
                ],
                "title": "Hierarchical Representation Matching for CLIP-based Class-Incremental\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Representation Matching for CLIP-based Class-Incremental\n  Learning"
                },
                "summary": "Class-Incremental Learning (CIL) aims to endow models with the ability to\ncontinuously adapt to evolving data streams. Recent advances in pre-trained\nvision-language models (e.g., CLIP) provide a powerful foundation for this\ntask. However, existing approaches often rely on simplistic templates, such as\n\"a photo of a [CLASS]\", which overlook the hierarchical nature of visual\nconcepts. For example, recognizing \"cat\" versus \"car\" depends on coarse-grained\ncues, while distinguishing \"cat\" from \"lion\" requires fine-grained details.\nSimilarly, the current feature mapping in CLIP relies solely on the\nrepresentation from the last layer, neglecting the hierarchical information\ncontained in earlier layers. In this work, we introduce HiErarchical\nRepresentation MAtchiNg (HERMAN) for CLIP-based CIL. Our approach leverages\nLLMs to recursively generate discriminative textual descriptors, thereby\naugmenting the semantic space with explicit hierarchical cues. These\ndescriptors are matched to different levels of the semantic hierarchy and\nadaptively routed based on task-specific requirements, enabling precise\ndiscrimination while alleviating catastrophic forgetting in incremental tasks.\nExtensive experiments on multiple benchmarks demonstrate that our method\nconsistently achieves state-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Class-Incremental Learning (CIL) aims to endow models with the ability to\ncontinuously adapt to evolving data streams. Recent advances in pre-trained\nvision-language models (e.g., CLIP) provide a powerful foundation for this\ntask. However, existing approaches often rely on simplistic templates, such as\n\"a photo of a [CLASS]\", which overlook the hierarchical nature of visual\nconcepts. For example, recognizing \"cat\" versus \"car\" depends on coarse-grained\ncues, while distinguishing \"cat\" from \"lion\" requires fine-grained details.\nSimilarly, the current feature mapping in CLIP relies solely on the\nrepresentation from the last layer, neglecting the hierarchical information\ncontained in earlier layers. In this work, we introduce HiErarchical\nRepresentation MAtchiNg (HERMAN) for CLIP-based CIL. Our approach leverages\nLLMs to recursively generate discriminative textual descriptors, thereby\naugmenting the semantic space with explicit hierarchical cues. These\ndescriptors are matched to different levels of the semantic hierarchy and\nadaptively routed based on task-specific requirements, enabling precise\ndiscrimination while alleviating catastrophic forgetting in incremental tasks.\nExtensive experiments on multiple benchmarks demonstrate that our method\nconsistently achieves state-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Zhen-Hao Wen"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Ji Feng"
                    },
                    {
                        "name": "Han-Jia Ye"
                    },
                    {
                        "name": "De-Chuan Zhan"
                    },
                    {
                        "name": "Da-Wei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Da-Wei Zhou"
                },
                "author": "Da-Wei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22641v1",
                "updated": "2025-09-26T17:59:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    5,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:59:05Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    5,
                    4,
                    269,
                    0
                ],
                "title": "Death of the Novel(ty): Beyond n-Gram Novelty as a Metric for Textual\n  Creativity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Death of the Novel(ty): Beyond n-Gram Novelty as a Metric for Textual\n  Creativity"
                },
                "summary": "N-gram novelty is widely used to evaluate language models' ability to\ngenerate text outside of their training data. More recently, it has also been\nadopted as a metric for measuring textual creativity. However, theoretical work\non creativity suggests that this approach may be inadequate, as it does not\naccount for creativity's dual nature: novelty (how original the text is) and\nappropriateness (how sensical and pragmatic it is). We investigate the\nrelationship between this notion of creativity and n-gram novelty through 7542\nexpert writer annotations (n=26) of novelty, pragmaticality, and sensicality\nvia close reading of human and AI-generated text. We find that while n-gram\nnovelty is positively associated with expert writer-judged creativity, ~91% of\ntop-quartile expressions by n-gram novelty are not judged as creative,\ncautioning against relying on n-gram novelty alone. Furthermore, unlike\nhuman-written text, higher n-gram novelty in open-source LLMs correlates with\nlower pragmaticality. In an exploratory study with frontier close-source\nmodels, we additionally confirm that they are less likely to produce creative\nexpressions than humans. Using our dataset, we test whether zero-shot,\nfew-shot, and finetuned models are able to identify creative expressions (a\npositive aspect of writing) and non-pragmatic ones (a negative aspect).\nOverall, frontier LLMs exhibit performance much higher than random but leave\nroom for improvement, especially struggling to identify non-pragmatic\nexpressions. We further find that LLM-as-a-Judge novelty scores from the\nbest-performing model were predictive of expert writer preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "N-gram novelty is widely used to evaluate language models' ability to\ngenerate text outside of their training data. More recently, it has also been\nadopted as a metric for measuring textual creativity. However, theoretical work\non creativity suggests that this approach may be inadequate, as it does not\naccount for creativity's dual nature: novelty (how original the text is) and\nappropriateness (how sensical and pragmatic it is). We investigate the\nrelationship between this notion of creativity and n-gram novelty through 7542\nexpert writer annotations (n=26) of novelty, pragmaticality, and sensicality\nvia close reading of human and AI-generated text. We find that while n-gram\nnovelty is positively associated with expert writer-judged creativity, ~91% of\ntop-quartile expressions by n-gram novelty are not judged as creative,\ncautioning against relying on n-gram novelty alone. Furthermore, unlike\nhuman-written text, higher n-gram novelty in open-source LLMs correlates with\nlower pragmaticality. In an exploratory study with frontier close-source\nmodels, we additionally confirm that they are less likely to produce creative\nexpressions than humans. Using our dataset, we test whether zero-shot,\nfew-shot, and finetuned models are able to identify creative expressions (a\npositive aspect of writing) and non-pragmatic ones (a negative aspect).\nOverall, frontier LLMs exhibit performance much higher than random but leave\nroom for improvement, especially struggling to identify non-pragmatic\nexpressions. We further find that LLM-as-a-Judge novelty scores from the\nbest-performing model were predictive of expert writer preferences."
                },
                "authors": [
                    {
                        "name": "Arkadiy Saakyan"
                    },
                    {
                        "name": "Najoung Kim"
                    },
                    {
                        "name": "Smaranda Muresan"
                    },
                    {
                        "name": "Tuhin Chakrabarty"
                    }
                ],
                "author_detail": {
                    "name": "Tuhin Chakrabarty"
                },
                "author": "Tuhin Chakrabarty",
                "arxiv_comment": "26 pages, 10 figures, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22638v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22638v1",
                "updated": "2025-09-26T17:58:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    58,
                    27,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:58:27Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    58,
                    27,
                    4,
                    269,
                    0
                ],
                "title": "Language Models Can Learn from Verbal Feedback Without Scalar Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models Can Learn from Verbal Feedback Without Scalar Rewards"
                },
                "summary": "LLMs are often trained with RL from human or AI feedback, yet such methods\ntypically compress nuanced feedback into scalar rewards, discarding much of\ntheir richness and inducing scale imbalance. We propose treating verbal\nfeedback as a conditioning signal. Inspired by language priors in text-to-image\ngeneration, which enable novel outputs from unseen prompts, we introduce the\nfeedback-conditional policy (FCP). FCP learns directly from response-feedback\npairs, approximating the feedback-conditional posterior through maximum\nlikelihood training on offline data. We further develop an online bootstrapping\nstage where the policy generates under positive conditions and receives fresh\nfeedback to refine itself. This reframes feedback-driven learning as\nconditional generation rather than reward optimization, offering a more\nexpressive way for LLMs to directly learn from verbal feedback. Our code is\navailable at https://github.com/sail-sg/feedback-conditional-policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are often trained with RL from human or AI feedback, yet such methods\ntypically compress nuanced feedback into scalar rewards, discarding much of\ntheir richness and inducing scale imbalance. We propose treating verbal\nfeedback as a conditioning signal. Inspired by language priors in text-to-image\ngeneration, which enable novel outputs from unseen prompts, we introduce the\nfeedback-conditional policy (FCP). FCP learns directly from response-feedback\npairs, approximating the feedback-conditional posterior through maximum\nlikelihood training on offline data. We further develop an online bootstrapping\nstage where the policy generates under positive conditions and receives fresh\nfeedback to refine itself. This reframes feedback-driven learning as\nconditional generation rather than reward optimization, offering a more\nexpressive way for LLMs to directly learn from verbal feedback. Our code is\navailable at https://github.com/sail-sg/feedback-conditional-policy."
                },
                "authors": [
                    {
                        "name": "Renjie Luo"
                    },
                    {
                        "name": "Zichen Liu"
                    },
                    {
                        "name": "Xiangyan Liu"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Min Lin"
                    },
                    {
                        "name": "Wenhu Chen"
                    },
                    {
                        "name": "Wei Lu"
                    },
                    {
                        "name": "Tianyu Pang"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Pang"
                },
                "author": "Tianyu Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22638v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22633v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22633v1",
                "updated": "2025-09-26T17:57:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    57,
                    17,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:57:17Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    57,
                    17,
                    4,
                    269,
                    0
                ],
                "title": "Towards Efficient Online Exploration for Reinforcement Learning with\n  Human Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient Online Exploration for Reinforcement Learning with\n  Human Feedback"
                },
                "summary": "Reinforcement learning with human feedback (RLHF), which learns a reward\nmodel from human preference data and then optimizes a policy to favor preferred\nresponses, has emerged as a central paradigm for aligning large language models\n(LLMs) with human preferences. In this paper, we investigate exploration\nprinciples for online RLHF, where one seeks to adaptively collect new\npreference data to refine both the reward model and the policy in a\ndata-efficient manner. By examining existing optimism-based exploration\nalgorithms, we identify a drawback in their sampling protocol: they tend to\ngather comparisons that fail to reduce the most informative uncertainties in\nreward differences, and we prove lower bounds showing that such methods can\nincur linear regret over exponentially long horizons. Motivated by this\ninsight, we propose a new exploration scheme that directs preference queries\ntoward reducing uncertainty in reward differences most relevant to policy\nimprovement. Under a multi-armed bandit model of RLHF, we establish regret\nbounds of order $T^{(\\beta+1)/(\\beta+2)}$, where $\\beta>0$ is a hyperparameter\nthat balances reward maximization against mitigating distribution shift. To our\nknowledge, this is the first online RLHF algorithm with regret scaling\npolynomially in all model parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with human feedback (RLHF), which learns a reward\nmodel from human preference data and then optimizes a policy to favor preferred\nresponses, has emerged as a central paradigm for aligning large language models\n(LLMs) with human preferences. In this paper, we investigate exploration\nprinciples for online RLHF, where one seeks to adaptively collect new\npreference data to refine both the reward model and the policy in a\ndata-efficient manner. By examining existing optimism-based exploration\nalgorithms, we identify a drawback in their sampling protocol: they tend to\ngather comparisons that fail to reduce the most informative uncertainties in\nreward differences, and we prove lower bounds showing that such methods can\nincur linear regret over exponentially long horizons. Motivated by this\ninsight, we propose a new exploration scheme that directs preference queries\ntoward reducing uncertainty in reward differences most relevant to policy\nimprovement. Under a multi-armed bandit model of RLHF, we establish regret\nbounds of order $T^{(\\beta+1)/(\\beta+2)}$, where $\\beta>0$ is a hyperparameter\nthat balances reward maximization against mitigating distribution shift. To our\nknowledge, this is the first online RLHF algorithm with regret scaling\npolynomially in all model parameters."
                },
                "authors": [
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Yuling Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yuling Yan"
                },
                "author": "Yuling Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22633v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22633v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05418v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05418v2",
                "updated": "2025-09-26T17:57:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    57,
                    11,
                    4,
                    269,
                    0
                ],
                "published": "2025-07-07T19:04:36Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    19,
                    4,
                    36,
                    0,
                    188,
                    0
                ],
                "title": "Learn Globally, Speak Locally: Bridging the Gaps in Multilingual\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learn Globally, Speak Locally: Bridging the Gaps in Multilingual\n  Reasoning"
                },
                "summary": "Large Language Models (LLMs) have achieved strong performance in domains like\nmathematics, factual question answering, and code generation, yet their ability\nto reason on these tasks in different languages remains underdeveloped.\nEspecially for low-resource languages such as Swahili or Thai, LLMs can often\nmisinterpret prompts or default to reasoning in English. This implicit bias\ntoward high-resource languages undermines factual accuracy, interpretability,\nand trust. We propose M2A, a novel method that combines multi-scale\nmultilingual alignment with language-consistency rewards on machine-translated\nquestions, training models to reason directly and accurately in the target\nlanguage. Furthermore, existing multilingual benchmarks only evaluate on final\nanswers, overlooking whether reasoning occurs in the intended language. To\nclose this gap, we introduce GeoFact-X, a geography-based multilingual factual\nreasoning benchmark together with reasoning traces in five languages: English,\nHindi, Japanese, Swahili, and Thai. Our results show that M2A significantly\nenhances multilingual reasoning fidelity in both mathematical and factual\nreasoning tasks, highlighting that reasoning-aware multilingual reinforcement\nlearning is crucial for robust cross-lingual generalization.\nhttps://jd730.github.io/projects/M2A_GeoFact-X",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved strong performance in domains like\nmathematics, factual question answering, and code generation, yet their ability\nto reason on these tasks in different languages remains underdeveloped.\nEspecially for low-resource languages such as Swahili or Thai, LLMs can often\nmisinterpret prompts or default to reasoning in English. This implicit bias\ntoward high-resource languages undermines factual accuracy, interpretability,\nand trust. We propose M2A, a novel method that combines multi-scale\nmultilingual alignment with language-consistency rewards on machine-translated\nquestions, training models to reason directly and accurately in the target\nlanguage. Furthermore, existing multilingual benchmarks only evaluate on final\nanswers, overlooking whether reasoning occurs in the intended language. To\nclose this gap, we introduce GeoFact-X, a geography-based multilingual factual\nreasoning benchmark together with reasoning traces in five languages: English,\nHindi, Japanese, Swahili, and Thai. Our results show that M2A significantly\nenhances multilingual reasoning fidelity in both mathematical and factual\nreasoning tasks, highlighting that reasoning-aware multilingual reinforcement\nlearning is crucial for robust cross-lingual generalization.\nhttps://jd730.github.io/projects/M2A_GeoFact-X"
                },
                "authors": [
                    {
                        "name": "Jaedong Hwang"
                    },
                    {
                        "name": "Kumar Tanmay"
                    },
                    {
                        "name": "Seok-Jin Lee"
                    },
                    {
                        "name": "Ayush Agrawal"
                    },
                    {
                        "name": "Hamid Palangi"
                    },
                    {
                        "name": "Kumar Ayush"
                    },
                    {
                        "name": "Ila Fiete"
                    },
                    {
                        "name": "Paul Pu Liang"
                    }
                ],
                "author_detail": {
                    "name": "Paul Pu Liang"
                },
                "author": "Paul Pu Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05418v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05418v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22628v1",
                "updated": "2025-09-26T17:51:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    51,
                    46,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:51:46Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    51,
                    46,
                    4,
                    269,
                    0
                ],
                "title": "UML-CoT: Structured Reasoning and Planning with Unified Modeling\n  Language for Robotic Room Cleaning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UML-CoT: Structured Reasoning and Planning with Unified Modeling\n  Language for Robotic Room Cleaning"
                },
                "summary": "Chain-of-Thought (CoT) prompting improves reasoning in large language models\n(LLMs), but its reliance on unstructured text limits interpretability and\nexecutability in embodied tasks. Prior work has explored structured CoTs using\nscene or logic graphs, yet these remain fundamentally limited: they model only\nlow-order relations, lack constructs like inheritance or behavioral\nabstraction, and provide no standardized semantics for sequential or\nconditional planning. We propose UML-CoT, a structured reasoning and planning\nframework that leverages Unified Modeling Language (UML) to generate symbolic\nCoTs and executable action plans. UML class diagrams capture compositional\nobject semantics, while activity diagrams model procedural control flow. Our\nthree-stage training pipeline combines supervised fine-tuning with Group\nRelative Policy Optimization (GRPO), including reward learning from answer-only\ndata. We evaluate UML-CoT on MRoom-30k, a new benchmark of cluttered\nroom-cleaning scenarios. UML-CoT outperforms unstructured CoTs in\ninterpretability, planning coherence, and execution success, highlighting UML\nas a more expressive and actionable structured reasoning formalism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting improves reasoning in large language models\n(LLMs), but its reliance on unstructured text limits interpretability and\nexecutability in embodied tasks. Prior work has explored structured CoTs using\nscene or logic graphs, yet these remain fundamentally limited: they model only\nlow-order relations, lack constructs like inheritance or behavioral\nabstraction, and provide no standardized semantics for sequential or\nconditional planning. We propose UML-CoT, a structured reasoning and planning\nframework that leverages Unified Modeling Language (UML) to generate symbolic\nCoTs and executable action plans. UML class diagrams capture compositional\nobject semantics, while activity diagrams model procedural control flow. Our\nthree-stage training pipeline combines supervised fine-tuning with Group\nRelative Policy Optimization (GRPO), including reward learning from answer-only\ndata. We evaluate UML-CoT on MRoom-30k, a new benchmark of cluttered\nroom-cleaning scenarios. UML-CoT outperforms unstructured CoTs in\ninterpretability, planning coherence, and execution success, highlighting UML\nas a more expressive and actionable structured reasoning formalism."
                },
                "authors": [
                    {
                        "name": "Hongyu Chen"
                    },
                    {
                        "name": "Guangrun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangrun Wang"
                },
                "author": "Guangrun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7; I.2.8; I.4.8; I.5.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22624v1",
                "updated": "2025-09-26T17:50:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    50,
                    12,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:50:12Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    50,
                    12,
                    4,
                    269,
                    0
                ],
                "title": "SPARK: Synergistic Policy And Reward Co-Evolving Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPARK: Synergistic Policy And Reward Co-Evolving Framework"
                },
                "summary": "Recent Large Language Models (LLMs) and Large Vision-Language Models (LVLMs)\nincreasingly use Reinforcement Learning (RL) for post-pretraining, such as RL\nwith Verifiable Rewards (RLVR) for objective tasks and RL from Human Feedback\n(RLHF) for subjective tasks. However, RLHF incurs high costs and potential\nreward-policy mismatch due to reliance on human preferences, while RLVR still\nwastes supervision by discarding rollouts and correctness signals after each\nupdate. To address these challenges, we introduce the Synergistic Policy And\nReward Co-Evolving Framework (SPARK), an efficient, on-policy, and stable\nmethod that builds on RLVR. Instead of discarding rollouts and correctness\ndata, SPARK recycles this valuable information to simultaneously train the\nmodel itself as a generative reward model. This auxiliary training uses a mix\nof objectives, such as pointwise reward score, pairwise comparison, and\nevaluation conditioned on further-reflection responses, to teach the model to\nevaluate and improve its own responses. Our process eliminates the need for a\nseparate reward model and costly human preference data. SPARK creates a\npositive co-evolving feedback loop: improved reward accuracy yields better\npolicy gradients, which in turn produce higher-quality rollouts that further\nrefine the reward model. Our unified framework supports test-time scaling via\nself-reflection without external reward models and their associated costs. We\nshow that SPARK achieves significant performance gains on multiple LLM and LVLM\nmodels and multiple reasoning, reward models, and general benchmarks. For\nexample, SPARK-VL-7B achieves an average 9.7% gain on 7 reasoning benchmarks,\n12.1% on 2 reward benchmarks, and 1.5% on 8 general benchmarks over the\nbaselines, demonstrating robustness and broad generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Large Language Models (LLMs) and Large Vision-Language Models (LVLMs)\nincreasingly use Reinforcement Learning (RL) for post-pretraining, such as RL\nwith Verifiable Rewards (RLVR) for objective tasks and RL from Human Feedback\n(RLHF) for subjective tasks. However, RLHF incurs high costs and potential\nreward-policy mismatch due to reliance on human preferences, while RLVR still\nwastes supervision by discarding rollouts and correctness signals after each\nupdate. To address these challenges, we introduce the Synergistic Policy And\nReward Co-Evolving Framework (SPARK), an efficient, on-policy, and stable\nmethod that builds on RLVR. Instead of discarding rollouts and correctness\ndata, SPARK recycles this valuable information to simultaneously train the\nmodel itself as a generative reward model. This auxiliary training uses a mix\nof objectives, such as pointwise reward score, pairwise comparison, and\nevaluation conditioned on further-reflection responses, to teach the model to\nevaluate and improve its own responses. Our process eliminates the need for a\nseparate reward model and costly human preference data. SPARK creates a\npositive co-evolving feedback loop: improved reward accuracy yields better\npolicy gradients, which in turn produce higher-quality rollouts that further\nrefine the reward model. Our unified framework supports test-time scaling via\nself-reflection without external reward models and their associated costs. We\nshow that SPARK achieves significant performance gains on multiple LLM and LVLM\nmodels and multiple reasoning, reward models, and general benchmarks. For\nexample, SPARK-VL-7B achieves an average 9.7% gain on 7 reasoning benchmarks,\n12.1% on 2 reward benchmarks, and 1.5% on 8 general benchmarks over the\nbaselines, demonstrating robustness and broad generalization."
                },
                "authors": [
                    {
                        "name": "Ziyu Liu"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Shengyuan Ding"
                    },
                    {
                        "name": "Yuhang Cao"
                    },
                    {
                        "name": "Xiaoyi Dong"
                    },
                    {
                        "name": "Haodong Duan"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiaqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Wang"
                },
                "author": "Jiaqi Wang",
                "arxiv_comment": "Project:https://github.com/InternLM/Spark",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17967v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17967v3",
                "updated": "2025-09-26T17:42:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    42,
                    23,
                    4,
                    269,
                    0
                ],
                "published": "2025-05-23T14:37:00Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    14,
                    37,
                    0,
                    4,
                    143,
                    0
                ],
                "title": "FFT-based Dynamic Subspace Selection for Low-Rank Adaptive Optimization\n  of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FFT-based Dynamic Subspace Selection for Low-Rank Adaptive Optimization\n  of Large Language Models"
                },
                "summary": "Low-rank optimization has emerged as a promising direction in training large\nlanguage models (LLMs) to improve running time and reduce the memory usage of\nadaptive optimizers by constraining learning to a lower-dimensional space.\nPrior work typically projects gradients of linear layers using approaches based\non Singular Value Decomposition (SVD) or QR-decomposition. Applying these\ntechniques individually to each layer in large models is computationally\nexpensive and incurs additional memory costs due to storing the projection\nmatrices. In this work, we propose a computationally efficient and conceptually\nsimple, two-step procedure to approximate SVD/QR-based gradient projections\ninto lower-dimensional spaces by using a predefined orthogonal matrix of the\nDiscrete Cosine Transform (DCT). We dynamically select columns from the DCT\nmatrix based on their alignment with the gradient of each layer. The effective\nprojection matrices are obtained via a simple matmul with the DCT matrix in\n$O(n^3)$ time, followed by a lightweight sorting step to identify the most\nrelevant basis vectors. For large layers, DCT can be computed via Makhoul's\n$N$-point algorithm based on Fast Fourier Transform (FFT) in $O(n^2 \\log(n))$\ntime. Due to the predefined nature of the orthogonal bases, they are computed\nonce at the start of training. Our numerical experiments on both pre-training\nand fine-tuning tasks demonstrate the effectiveness of our dual strategy in\napproximating optimal low-rank projections, obtaining an approach with\nrank-independent running time that matches the performance of costly\nSVD/QR-based methods while achieving faster runtime and reduced memory usage by\nup to $25\\%$ across different model sizes. Our code is available at\n\\href{https://github.com/IST-DASLab/ISTA-DASLab-Optimizers/tree/main/ista_daslab_optimizers/fft_low_rank}{ISTA-DASLab-Optimizers}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rank optimization has emerged as a promising direction in training large\nlanguage models (LLMs) to improve running time and reduce the memory usage of\nadaptive optimizers by constraining learning to a lower-dimensional space.\nPrior work typically projects gradients of linear layers using approaches based\non Singular Value Decomposition (SVD) or QR-decomposition. Applying these\ntechniques individually to each layer in large models is computationally\nexpensive and incurs additional memory costs due to storing the projection\nmatrices. In this work, we propose a computationally efficient and conceptually\nsimple, two-step procedure to approximate SVD/QR-based gradient projections\ninto lower-dimensional spaces by using a predefined orthogonal matrix of the\nDiscrete Cosine Transform (DCT). We dynamically select columns from the DCT\nmatrix based on their alignment with the gradient of each layer. The effective\nprojection matrices are obtained via a simple matmul with the DCT matrix in\n$O(n^3)$ time, followed by a lightweight sorting step to identify the most\nrelevant basis vectors. For large layers, DCT can be computed via Makhoul's\n$N$-point algorithm based on Fast Fourier Transform (FFT) in $O(n^2 \\log(n))$\ntime. Due to the predefined nature of the orthogonal bases, they are computed\nonce at the start of training. Our numerical experiments on both pre-training\nand fine-tuning tasks demonstrate the effectiveness of our dual strategy in\napproximating optimal low-rank projections, obtaining an approach with\nrank-independent running time that matches the performance of costly\nSVD/QR-based methods while achieving faster runtime and reduced memory usage by\nup to $25\\%$ across different model sizes. Our code is available at\n\\href{https://github.com/IST-DASLab/ISTA-DASLab-Optimizers/tree/main/ista_daslab_optimizers/fft_low_rank}{ISTA-DASLab-Optimizers}."
                },
                "authors": [
                    {
                        "name": "Ionut-Vlad Modoranu"
                    },
                    {
                        "name": "Mher Safaryan"
                    },
                    {
                        "name": "Erik Schultheis"
                    },
                    {
                        "name": "Max Ryabinin"
                    },
                    {
                        "name": "Artem Chumachenko"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17967v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17967v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10823v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10823v3",
                "updated": "2025-09-26T17:40:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    40,
                    31,
                    4,
                    269,
                    0
                ],
                "published": "2025-04-15T02:54:16Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    2,
                    54,
                    16,
                    1,
                    105,
                    0
                ],
                "title": "CLASH: Evaluating Language Models on Judging High-Stakes Dilemmas from\n  Multiple Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLASH: Evaluating Language Models on Judging High-Stakes Dilemmas from\n  Multiple Perspectives"
                },
                "summary": "Navigating dilemmas involving conflicting values is challenging even for\nhumans in high-stakes domains, let alone for AI, yet prior work has been\nlimited to everyday scenarios. To close this gap, we introduce CLASH (Character\nperspective-based LLM Assessments in Situations with High-stakes), a\nmeticulously curated dataset consisting of 345 high-impact dilemmas along with\n3,795 individual perspectives of diverse values. CLASH enables the study of\ncritical yet underexplored aspects of value-based decision-making processes,\nincluding understanding of decision ambivalence and psychological discomfort as\nwell as capturing the temporal shifts of values in the perspectives of\ncharacters. By benchmarking 14 non-thinking and thinking models, we uncover\nseveral key findings. (1) Even strong proprietary models, such as GPT-5 and\nClaude-4-Sonnet, struggle with ambivalent decisions, achieving only 24.06 and\n51.01 accuracy. (2) Although LLMs reasonably predict psychological discomfort,\nthey do not adequately comprehend perspectives involving value shifts. (3)\nCognitive behaviors that are effective in the math-solving and game strategy\ndomains do not transfer to value reasoning. Instead, new failure patterns\nemerge, including early commitment and overcommitment. (4) The steerability of\nLLMs towards a given value is significantly correlated with their value\npreferences. (5) Finally, LLMs exhibit greater steerability when reasoning from\na third-party perspective, although certain values (e.g., safety) benefit\nuniquely from first-person framing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating dilemmas involving conflicting values is challenging even for\nhumans in high-stakes domains, let alone for AI, yet prior work has been\nlimited to everyday scenarios. To close this gap, we introduce CLASH (Character\nperspective-based LLM Assessments in Situations with High-stakes), a\nmeticulously curated dataset consisting of 345 high-impact dilemmas along with\n3,795 individual perspectives of diverse values. CLASH enables the study of\ncritical yet underexplored aspects of value-based decision-making processes,\nincluding understanding of decision ambivalence and psychological discomfort as\nwell as capturing the temporal shifts of values in the perspectives of\ncharacters. By benchmarking 14 non-thinking and thinking models, we uncover\nseveral key findings. (1) Even strong proprietary models, such as GPT-5 and\nClaude-4-Sonnet, struggle with ambivalent decisions, achieving only 24.06 and\n51.01 accuracy. (2) Although LLMs reasonably predict psychological discomfort,\nthey do not adequately comprehend perspectives involving value shifts. (3)\nCognitive behaviors that are effective in the math-solving and game strategy\ndomains do not transfer to value reasoning. Instead, new failure patterns\nemerge, including early commitment and overcommitment. (4) The steerability of\nLLMs towards a given value is significantly correlated with their value\npreferences. (5) Finally, LLMs exhibit greater steerability when reasoning from\na third-party perspective, although certain values (e.g., safety) benefit\nuniquely from first-person framing."
                },
                "authors": [
                    {
                        "name": "Ayoung Lee"
                    },
                    {
                        "name": "Ryan Sungmo Kwon"
                    },
                    {
                        "name": "Peter Railton"
                    },
                    {
                        "name": "Lu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Wang"
                },
                "author": "Lu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10823v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10823v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22613v1",
                "updated": "2025-09-26T17:39:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    39,
                    48,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:39:48Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    39,
                    48,
                    4,
                    269,
                    0
                ],
                "title": "Benefits and Pitfalls of Reinforcement Learning for Language Model\n  Planning: A Theoretical Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benefits and Pitfalls of Reinforcement Learning for Language Model\n  Planning: A Theoretical Perspective"
                },
                "summary": "Recent reinforcement learning (RL) methods have substantially enhanced the\nplanning capabilities of Large Language Models (LLMs), yet the theoretical\nbasis for their effectiveness remains elusive. In this work, we investigate\nRL's benefits and limitations through a tractable graph-based abstraction,\nfocusing on policy gradient (PG) and Q-learning methods. Our theoretical\nanalyses reveal that supervised fine-tuning (SFT) may introduce\nco-occurrence-based spurious solutions, whereas RL achieves correct planning\nprimarily through exploration, underscoring exploration's role in enabling\nbetter generalization. However, we also show that PG suffers from diversity\ncollapse, where output diversity decreases during training and persists even\nafter perfect accuracy is attained. By contrast, Q-learning provides two key\nadvantages: off-policy learning and diversity preservation at convergence. We\nfurther demonstrate that careful reward design is necessary to prevent reward\nhacking in Q-learning. Finally, applying our framework to the real-world\nplanning benchmark Blocksworld, we confirm that these behaviors manifest in\npractice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent reinforcement learning (RL) methods have substantially enhanced the\nplanning capabilities of Large Language Models (LLMs), yet the theoretical\nbasis for their effectiveness remains elusive. In this work, we investigate\nRL's benefits and limitations through a tractable graph-based abstraction,\nfocusing on policy gradient (PG) and Q-learning methods. Our theoretical\nanalyses reveal that supervised fine-tuning (SFT) may introduce\nco-occurrence-based spurious solutions, whereas RL achieves correct planning\nprimarily through exploration, underscoring exploration's role in enabling\nbetter generalization. However, we also show that PG suffers from diversity\ncollapse, where output diversity decreases during training and persists even\nafter perfect accuracy is attained. By contrast, Q-learning provides two key\nadvantages: off-policy learning and diversity preservation at convergence. We\nfurther demonstrate that careful reward design is necessary to prevent reward\nhacking in Q-learning. Finally, applying our framework to the real-world\nplanning benchmark Blocksworld, we confirm that these behaviors manifest in\npractice."
                },
                "authors": [
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "Yifei Shen"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Shi Feng"
                    },
                    {
                        "name": "Shang-Hua Teng"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Yaru Hao"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15735v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15735v2",
                "updated": "2025-09-26T17:38:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    38,
                    41,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-19T08:05:28Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    8,
                    5,
                    28,
                    4,
                    262,
                    0
                ],
                "title": "EigenTrack: Spectral Activation Feature Tracking for Hallucination and\n  Out-of-Distribution Detection in LLMs and VLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EigenTrack: Spectral Activation Feature Tracking for Hallucination and\n  Out-of-Distribution Detection in LLMs and VLMs"
                },
                "summary": "Large language models (LLMs) offer broad utility but remain prone to\nhallucination and out-of-distribution (OOD) errors. We propose EigenTrack, an\ninterpretable real-time detector that uses the spectral geometry of hidden\nactivations, a compact global signature of model dynamics. By streaming\ncovariance-spectrum statistics such as entropy, eigenvalue gaps, and KL\ndivergence from random baselines into a lightweight recurrent classifier,\nEigenTrack tracks temporal shifts in representation structure that signal\nhallucination and OOD drift before surface errors appear. Unlike black- and\ngrey-box methods, it needs only a single forward pass without resampling.\nUnlike existing white-box detectors, it preserves temporal context, aggregates\nglobal signals, and offers interpretable accuracy-latency trade-offs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) offer broad utility but remain prone to\nhallucination and out-of-distribution (OOD) errors. We propose EigenTrack, an\ninterpretable real-time detector that uses the spectral geometry of hidden\nactivations, a compact global signature of model dynamics. By streaming\ncovariance-spectrum statistics such as entropy, eigenvalue gaps, and KL\ndivergence from random baselines into a lightweight recurrent classifier,\nEigenTrack tracks temporal shifts in representation structure that signal\nhallucination and OOD drift before surface errors appear. Unlike black- and\ngrey-box methods, it needs only a single forward pass without resampling.\nUnlike existing white-box detectors, it preserves temporal context, aggregates\nglobal signals, and offers interpretable accuracy-latency trade-offs."
                },
                "authors": [
                    {
                        "name": "Davide Ettori"
                    },
                    {
                        "name": "Nastaran Darabi"
                    },
                    {
                        "name": "Sina Tayebati"
                    },
                    {
                        "name": "Ranganath Krishnan"
                    },
                    {
                        "name": "Mahesh Subedar"
                    },
                    {
                        "name": "Omesh Tickoo"
                    },
                    {
                        "name": "Amit Ranjan Trivedi"
                    }
                ],
                "author_detail": {
                    "name": "Amit Ranjan Trivedi"
                },
                "author": "Amit Ranjan Trivedi",
                "arxiv_comment": "5 pages, submitted to ICASSP 2026, September 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15735v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15735v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22611v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22611v1",
                "updated": "2025-09-26T17:37:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    37,
                    52,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:37:52Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    37,
                    52,
                    4,
                    269,
                    0
                ],
                "title": "Quantile Advantage Estimation for Entropy-Safe Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantile Advantage Estimation for Entropy-Safe Reasoning"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) strengthens LLM\nreasoning, but training often oscillates between {entropy collapse} and\n{entropy explosion}. We trace both hazards to the mean baseline used in\nvalue-free RL (e.g., GRPO and DAPO), which improperly penalizes\nnegative-advantage samples under reward outliers. We propose {Quantile\nAdvantage Estimation} (QAE), replacing the mean with a group-wise K-quantile\nbaseline. QAE induces a response-level, two-regime gate: on hard queries (p <=\n1 - K) it reinforces rare successes, while on easy queries (p > 1 - K) it\ntargets remaining failures. Under first-order softmax updates, we prove\n{two-sided entropy safety}, giving lower and upper bounds on one-step entropy\nchange that curb explosion and prevent collapse. Empirically, this minimal\nmodification stabilizes entropy, sparsifies credit assignment (with tuned K,\nroughly 80% of responses receive zero advantage), and yields sustained pass@1\ngains on Qwen3-8B/14B-Base across AIME 2024/2025 and AMC 2023. These results\nidentify {baseline design} -- rather than token-level heuristics -- as the\nprimary mechanism for scaling RLVR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) strengthens LLM\nreasoning, but training often oscillates between {entropy collapse} and\n{entropy explosion}. We trace both hazards to the mean baseline used in\nvalue-free RL (e.g., GRPO and DAPO), which improperly penalizes\nnegative-advantage samples under reward outliers. We propose {Quantile\nAdvantage Estimation} (QAE), replacing the mean with a group-wise K-quantile\nbaseline. QAE induces a response-level, two-regime gate: on hard queries (p <=\n1 - K) it reinforces rare successes, while on easy queries (p > 1 - K) it\ntargets remaining failures. Under first-order softmax updates, we prove\n{two-sided entropy safety}, giving lower and upper bounds on one-step entropy\nchange that curb explosion and prevent collapse. Empirically, this minimal\nmodification stabilizes entropy, sparsifies credit assignment (with tuned K,\nroughly 80% of responses receive zero advantage), and yields sustained pass@1\ngains on Qwen3-8B/14B-Base across AIME 2024/2025 and AMC 2023. These results\nidentify {baseline design} -- rather than token-level heuristics -- as the\nprimary mechanism for scaling RLVR."
                },
                "authors": [
                    {
                        "name": "Junkang Wu"
                    },
                    {
                        "name": "Kexin Huang"
                    },
                    {
                        "name": "Jiancan Wu"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22611v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05723v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05723v3",
                "updated": "2025-09-26T17:37:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    37,
                    0,
                    4,
                    269,
                    0
                ],
                "published": "2024-12-07T18:49:27Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    18,
                    49,
                    27,
                    5,
                    342,
                    0
                ],
                "title": "Training-Free Bayesianization for Low-Rank Adapters of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Bayesianization for Low-Rank Adapters of Large Language\n  Models"
                },
                "summary": "Estimating the uncertainty of responses from Large Language Models (LLMs)\nremains a critical challenge. While recent Bayesian methods have demonstrated\neffectiveness in quantifying uncertainty through low-rank weight updates, they\ntypically require complex fine-tuning or post-training procedures. In this\npaper, we propose Training-Free Bayesianization (TFB), a simple yet\ntheoretically grounded framework that efficiently transforms trained low-rank\nadapters into Bayesian ones without additional training. TFB systematically\nsearches for the maximally acceptable level of variance in the weight\nposterior, constrained within a family of low-rank isotropic Gaussian\ndistributions. Our theoretical analysis shows that under mild conditions, this\nsearch process is equivalent to KL-regularized variational optimization, a\ngeneralized form of variational inference. Through comprehensive experiments,\nwe show that TFB achieves superior uncertainty estimation and generalization\ncompared to existing methods while eliminating the need for complex\nBayesianization training procedures. Code will be available at\nhttps://github.com/Wang-ML-Lab/bayesian-peft.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating the uncertainty of responses from Large Language Models (LLMs)\nremains a critical challenge. While recent Bayesian methods have demonstrated\neffectiveness in quantifying uncertainty through low-rank weight updates, they\ntypically require complex fine-tuning or post-training procedures. In this\npaper, we propose Training-Free Bayesianization (TFB), a simple yet\ntheoretically grounded framework that efficiently transforms trained low-rank\nadapters into Bayesian ones without additional training. TFB systematically\nsearches for the maximally acceptable level of variance in the weight\nposterior, constrained within a family of low-rank isotropic Gaussian\ndistributions. Our theoretical analysis shows that under mild conditions, this\nsearch process is equivalent to KL-regularized variational optimization, a\ngeneralized form of variational inference. Through comprehensive experiments,\nwe show that TFB achieves superior uncertainty estimation and generalization\ncompared to existing methods while eliminating the need for complex\nBayesianization training procedures. Code will be available at\nhttps://github.com/Wang-ML-Lab/bayesian-peft."
                },
                "authors": [
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Yibin Wang"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Huan Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "arxiv_comment": "Accepted at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05723v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05723v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16355v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16355v2",
                "updated": "2025-09-26T17:21:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    21,
                    50,
                    4,
                    269,
                    0
                ],
                "published": "2025-01-20T01:39:03Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    1,
                    39,
                    3,
                    0,
                    20,
                    0
                ],
                "title": "How Strategic Agents Respond: Comparing Analytical Models with\n  LLM-Generated Responses in Strategic Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Strategic Agents Respond: Comparing Analytical Models with\n  LLM-Generated Responses in Strategic Classification"
                },
                "summary": "When ML algorithms are deployed to automate human-related decisions, human\nagents may learn the underlying decision policies and adapt their behavior.\nStrategic Classification (SC) has emerged as a framework for studying this\ninteraction between agents and decision-makers to design more trustworthy ML\nsystems. Prior theoretical models in SC assume that agents are perfectly or\napproximately rational and respond to decision policies by optimizing their\nutility. However, the growing prevalence of LLMs raises the possibility that\nreal-world agents may instead rely on these tools for strategic advice. This\nshift prompts two questions: (i) Can LLMs generate effective and socially\nresponsible strategies in SC settings? (ii) Can existing SC theoretical models\naccurately capture agent behavior when agents follow LLM-generated advice? To\ninvestigate these questions, we examine five critical SC scenarios: hiring,\nloan applications, school admissions, personal income, and public assistance\nprograms. We simulate agents with diverse profiles who interact with three\ncommercial LLMs (GPT-4o, GPT-4.1, and GPT-5), following their suggestions on\neffort allocations on features. We compare the resulting agent behaviors with\nthe best responses in existing SC models. Our findings show that: (i) Even\nwithout access to the decision policy, LLMs can generate effective strategies\nthat improve both agents' scores and qualification; (ii) At the population\nlevel, LLM-guided effort allocation strategies yield similar or even higher\nscore improvements, qualification rates, and fairness metrics as those\npredicted by the SC theoretical model, suggesting that the theoretical model\nmay still serve as a reasonable proxy for LLM-influenced behavior; and (iii) At\nthe individual level, LLMs tend to produce more diverse and balanced effort\nallocations than theoretical models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When ML algorithms are deployed to automate human-related decisions, human\nagents may learn the underlying decision policies and adapt their behavior.\nStrategic Classification (SC) has emerged as a framework for studying this\ninteraction between agents and decision-makers to design more trustworthy ML\nsystems. Prior theoretical models in SC assume that agents are perfectly or\napproximately rational and respond to decision policies by optimizing their\nutility. However, the growing prevalence of LLMs raises the possibility that\nreal-world agents may instead rely on these tools for strategic advice. This\nshift prompts two questions: (i) Can LLMs generate effective and socially\nresponsible strategies in SC settings? (ii) Can existing SC theoretical models\naccurately capture agent behavior when agents follow LLM-generated advice? To\ninvestigate these questions, we examine five critical SC scenarios: hiring,\nloan applications, school admissions, personal income, and public assistance\nprograms. We simulate agents with diverse profiles who interact with three\ncommercial LLMs (GPT-4o, GPT-4.1, and GPT-5), following their suggestions on\neffort allocations on features. We compare the resulting agent behaviors with\nthe best responses in existing SC models. Our findings show that: (i) Even\nwithout access to the decision policy, LLMs can generate effective strategies\nthat improve both agents' scores and qualification; (ii) At the population\nlevel, LLM-guided effort allocation strategies yield similar or even higher\nscore improvements, qualification rates, and fairness metrics as those\npredicted by the SC theoretical model, suggesting that the theoretical model\nmay still serve as a reasonable proxy for LLM-influenced behavior; and (iii) At\nthe individual level, LLMs tend to produce more diverse and balanced effort\nallocations than theoretical models."
                },
                "authors": [
                    {
                        "name": "Tian Xie"
                    },
                    {
                        "name": "Pavan Rauch"
                    },
                    {
                        "name": "Xueru Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xueru Zhang"
                },
                "author": "Xueru Zhang",
                "arxiv_comment": "Add GPT 5 experiments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16355v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16355v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22601v1",
                "updated": "2025-09-26T17:20:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    20,
                    38,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:20:38Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    20,
                    38,
                    4,
                    269,
                    0
                ],
                "title": "Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive\n  Exploration for Agentic Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive\n  Exploration for Agentic Reinforcement Learning"
                },
                "summary": "Reinforcement learning (RL) is the dominant paradigm for sharpening strategic\ntool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks,\nyet it faces a fundamental challenge of exploration-exploitation trade-off.\nExisting studies stimulate exploration through the lens of policy entropy, but\nsuch mechanical entropy maximization is prone to RL training instability due to\nthe multi-turn distribution shifting. In this paper, we target the progressive\nexploration-exploitation balance under the guidance of the agent own\nexperiences without succumbing to either entropy collapsing or runaway\ndivergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL)\nrecipe for training agentic LLMs. It extends the vanilla SIL framework, where a\nreplay buffer stores self-generated promising trajectories for off-policy\nupdate, by gradually steering the policy evolution within a well-balanced range\nof entropy across stages. Specifically, our approach incorporates a curriculum\nto manage the exploration process, utilizing intrinsic rewards to foster\nskill-level exploration and facilitating action-level exploration through SIL.\nAt first, the auxiliary tool call reward plays a critical role in the\naccumulation of tool-use skills, enabling broad exposure to the unfamiliar\ndistributions of the environment feedback with an upward entropy trend. As\ntraining progresses, self-imitation gets strengthened to exploit existing\nsuccessful patterns from replayed experiences for comparative action-level\nexploration, accelerating solution iteration without unbounded entropy growth.\nTo further stabilize training, we recalibrate the advantages of experiences in\nthe replay buffer to address the potential policy drift. Reugularizations such\nas the clipping of tokens with high covariance between probability and\nadvantage are introduced to the trajectory-level entropy control to curb\nover-confidence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) is the dominant paradigm for sharpening strategic\ntool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks,\nyet it faces a fundamental challenge of exploration-exploitation trade-off.\nExisting studies stimulate exploration through the lens of policy entropy, but\nsuch mechanical entropy maximization is prone to RL training instability due to\nthe multi-turn distribution shifting. In this paper, we target the progressive\nexploration-exploitation balance under the guidance of the agent own\nexperiences without succumbing to either entropy collapsing or runaway\ndivergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL)\nrecipe for training agentic LLMs. It extends the vanilla SIL framework, where a\nreplay buffer stores self-generated promising trajectories for off-policy\nupdate, by gradually steering the policy evolution within a well-balanced range\nof entropy across stages. Specifically, our approach incorporates a curriculum\nto manage the exploration process, utilizing intrinsic rewards to foster\nskill-level exploration and facilitating action-level exploration through SIL.\nAt first, the auxiliary tool call reward plays a critical role in the\naccumulation of tool-use skills, enabling broad exposure to the unfamiliar\ndistributions of the environment feedback with an upward entropy trend. As\ntraining progresses, self-imitation gets strengthened to exploit existing\nsuccessful patterns from replayed experiences for comparative action-level\nexploration, accelerating solution iteration without unbounded entropy growth.\nTo further stabilize training, we recalibrate the advantages of experiences in\nthe replay buffer to address the potential policy drift. Reugularizations such\nas the clipping of tokens with high covariance between probability and\nadvantage are introduced to the trajectory-level entropy control to curb\nover-confidence."
                },
                "authors": [
                    {
                        "name": "Yulei Qin"
                    },
                    {
                        "name": "Xiaoyu Tan"
                    },
                    {
                        "name": "Zhengbao He"
                    },
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Haojia Lin"
                    },
                    {
                        "name": "Zongyi Li"
                    },
                    {
                        "name": "Zihan Xu"
                    },
                    {
                        "name": "Yuchen Shi"
                    },
                    {
                        "name": "Siqi Cai"
                    },
                    {
                        "name": "Renting Rui"
                    },
                    {
                        "name": "Shaofei Cai"
                    },
                    {
                        "name": "Yuzheng Cai"
                    },
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Sheng Ye"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "26 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.13428v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.13428v3",
                "updated": "2025-09-26T17:17:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    17,
                    1,
                    4,
                    269,
                    0
                ],
                "published": "2023-12-20T21:14:28Z",
                "published_parsed": [
                    2023,
                    12,
                    20,
                    21,
                    14,
                    28,
                    2,
                    354,
                    0
                ],
                "title": "IPU: Flexible Hardware Introspection Units",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IPU: Flexible Hardware Introspection Units"
                },
                "summary": "Modern chip designs are increasingly complex, making it difficult for\ndevelopers to glean meaningful insights about hardware behavior while real\nworkloads are running. Hardware introspection aims to solve this by enabling\nthe hardware itself to observe and report on its internal operation -\nespecially in the field, where the chip is executing real-world software and\nworkloads. Three key problems are now imminent that hardware introspection can\nsolve: A/B testing of hardware in the field, obfuscated hardware, and\nobfuscated software which prevents chip designers from gleaning insights on in\nthe field behavior of their chips. To this end, the goal is to enable\nmonitoring chip hardware behavior in the field, at real-time speeds with no\nslowdowns, with minimal power overheads, and thereby obtain insights on chip\nbehavior and workloads. This paper implements the system architecture for and\nintroduces the Introspection Processing Unit (IPU) - one solution to said goal.\nWe perform case studies exemplifying the application of hardware introspection\nto the three problems through an IPU and implement an RTL level prototype.\nAcross the case studies, we show that an IPU with area overhead less than 1\npercent at 7nm, and overall power consumption of less than 25 mW is able to\ncreate previously inconceivable analysis: evaluating instruction prefetchers in\nthe field before deployment, creating per-instruction cycles stacks of\narbitrary programs, and detailing fine-grained cycle-by-cycle utilization of\nhardware modules.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern chip designs are increasingly complex, making it difficult for\ndevelopers to glean meaningful insights about hardware behavior while real\nworkloads are running. Hardware introspection aims to solve this by enabling\nthe hardware itself to observe and report on its internal operation -\nespecially in the field, where the chip is executing real-world software and\nworkloads. Three key problems are now imminent that hardware introspection can\nsolve: A/B testing of hardware in the field, obfuscated hardware, and\nobfuscated software which prevents chip designers from gleaning insights on in\nthe field behavior of their chips. To this end, the goal is to enable\nmonitoring chip hardware behavior in the field, at real-time speeds with no\nslowdowns, with minimal power overheads, and thereby obtain insights on chip\nbehavior and workloads. This paper implements the system architecture for and\nintroduces the Introspection Processing Unit (IPU) - one solution to said goal.\nWe perform case studies exemplifying the application of hardware introspection\nto the three problems through an IPU and implement an RTL level prototype.\nAcross the case studies, we show that an IPU with area overhead less than 1\npercent at 7nm, and overall power consumption of less than 25 mW is able to\ncreate previously inconceivable analysis: evaluating instruction prefetchers in\nthe field before deployment, creating per-instruction cycles stacks of\narbitrary programs, and detailing fine-grained cycle-by-cycle utilization of\nhardware modules."
                },
                "authors": [
                    {
                        "name": "Ian McDougall"
                    },
                    {
                        "name": "Shayne Wadle"
                    },
                    {
                        "name": "Harish Batchu"
                    },
                    {
                        "name": "Karthikeyan Sankaralingam"
                    }
                ],
                "author_detail": {
                    "name": "Karthikeyan Sankaralingam"
                },
                "author": "Karthikeyan Sankaralingam",
                "arxiv_comment": "14 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.13428v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.13428v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.m; B.m; C.m",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03814v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03814v5",
                "updated": "2025-09-26T17:11:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    11,
                    25,
                    4,
                    269,
                    0
                ],
                "published": "2025-04-04T14:41:41Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    41,
                    41,
                    4,
                    94,
                    0
                ],
                "title": "Recursive Training Loops in LLMs: How training data properties modulate\n  distribution shift in generated data?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recursive Training Loops in LLMs: How training data properties modulate\n  distribution shift in generated data?"
                },
                "summary": "Large language models (LLMs) are increasingly used in the creation of online\ncontent, creating feedback loops as subsequent generations of models will be\ntrained on this synthetic data. Such loops were shown to lead to distribution\nshifts - models misrepresenting the true underlying distributions of human data\n(also called model collapse). However, how human data properties affect such\nshifts remains poorly understood. In this paper, we provide the first empirical\nexamination of the effect of such properties on the outcome of recursive\ntraining. We first confirm that using different human datasets leads to\ndistribution shifts of different magnitudes. Through exhaustive manipulation of\ndataset properties combined with regression analyses, we then identify a set of\nproperties predicting distribution shift magnitudes. Lexical diversity is found\nto amplify these shifts, while semantic diversity and data quality mitigate\nthem. Furthermore, we find that these influences are highly modular: data\nscrapped from a given internet domain has little influence on the content\ngenerated for another domain. Finally, experiments on political bias reveal\nthat human data properties affect whether the initial bias will be amplified or\nreduced. Overall, our results portray a novel view, where different parts of\ninternet may undergo different types of distribution shift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used in the creation of online\ncontent, creating feedback loops as subsequent generations of models will be\ntrained on this synthetic data. Such loops were shown to lead to distribution\nshifts - models misrepresenting the true underlying distributions of human data\n(also called model collapse). However, how human data properties affect such\nshifts remains poorly understood. In this paper, we provide the first empirical\nexamination of the effect of such properties on the outcome of recursive\ntraining. We first confirm that using different human datasets leads to\ndistribution shifts of different magnitudes. Through exhaustive manipulation of\ndataset properties combined with regression analyses, we then identify a set of\nproperties predicting distribution shift magnitudes. Lexical diversity is found\nto amplify these shifts, while semantic diversity and data quality mitigate\nthem. Furthermore, we find that these influences are highly modular: data\nscrapped from a given internet domain has little influence on the content\ngenerated for another domain. Finally, experiments on political bias reveal\nthat human data properties affect whether the initial bias will be amplified or\nreduced. Overall, our results portray a novel view, where different parts of\ninternet may undergo different types of distribution shift."
                },
                "authors": [
                    {
                        "name": "Grgur Kovač"
                    },
                    {
                        "name": "Jérémy Perez"
                    },
                    {
                        "name": "Rémy Portelas"
                    },
                    {
                        "name": "Peter Ford Dominey"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-Yves Oudeyer"
                },
                "author": "Pierre-Yves Oudeyer",
                "arxiv_comment": "Accepted to EMNLP 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03814v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03814v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22582v1",
                "updated": "2025-09-26T17:03:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    3,
                    24,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:03:24Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    3,
                    24,
                    4,
                    269,
                    0
                ],
                "title": "Fine-Grained Detection of Context-Grounded Hallucinations Using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained Detection of Context-Grounded Hallucinations Using LLMs"
                },
                "summary": "Context-grounded hallucinations are cases where model outputs contain\ninformation not verifiable against the source text. We study the applicability\nof LLMs for localizing such hallucinations, as a more practical alternative to\nexisting complex evaluation pipelines. In the absence of established benchmarks\nfor meta-evaluation of hallucinations localization, we construct one tailored\nto LLMs, involving a challenging human annotation of over 1,000 examples. We\ncomplement the benchmark with an LLM-based evaluation protocol, verifying its\nquality in a human evaluation. Since existing representations of hallucinations\nlimit the types of errors that can be expressed, we propose a new\nrepresentation based on free-form textual descriptions, capturing the full\nrange of possible errors. We conduct a comprehensive study, evaluating four\nlarge-scale LLMs, which highlights the benchmark's difficulty, as the best\nmodel achieves an F1 score of only 0.67. Through careful analysis, we offer\ninsights into optimal prompting strategies for the task and identify the main\nfactors that make it challenging for LLMs: (1) a tendency to incorrectly flag\nmissing details as inconsistent, despite being instructed to check only facts\nin the output; and (2) difficulty with outputs containing factually correct\ninformation absent from the source - and thus not verifiable - due to alignment\nwith the model's parametric knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-grounded hallucinations are cases where model outputs contain\ninformation not verifiable against the source text. We study the applicability\nof LLMs for localizing such hallucinations, as a more practical alternative to\nexisting complex evaluation pipelines. In the absence of established benchmarks\nfor meta-evaluation of hallucinations localization, we construct one tailored\nto LLMs, involving a challenging human annotation of over 1,000 examples. We\ncomplement the benchmark with an LLM-based evaluation protocol, verifying its\nquality in a human evaluation. Since existing representations of hallucinations\nlimit the types of errors that can be expressed, we propose a new\nrepresentation based on free-form textual descriptions, capturing the full\nrange of possible errors. We conduct a comprehensive study, evaluating four\nlarge-scale LLMs, which highlights the benchmark's difficulty, as the best\nmodel achieves an F1 score of only 0.67. Through careful analysis, we offer\ninsights into optimal prompting strategies for the task and identify the main\nfactors that make it challenging for LLMs: (1) a tendency to incorrectly flag\nmissing details as inconsistent, despite being instructed to check only facts\nin the output; and (2) difficulty with outputs containing factually correct\ninformation absent from the source - and thus not verifiable - due to alignment\nwith the model's parametric knowledge."
                },
                "authors": [
                    {
                        "name": "Yehonatan Pesiakhovsky"
                    },
                    {
                        "name": "Zorik Gekhman"
                    },
                    {
                        "name": "Yosi Mass"
                    },
                    {
                        "name": "Liat Ein-Dor"
                    },
                    {
                        "name": "Roi Reichart"
                    }
                ],
                "author_detail": {
                    "name": "Roi Reichart"
                },
                "author": "Roi Reichart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22576v1",
                "updated": "2025-09-26T16:51:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    51,
                    44,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:51:44Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    51,
                    44,
                    4,
                    269,
                    0
                ],
                "title": "EPO: Entropy-regularized Policy Optimization for LLM Agents\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPO: Entropy-regularized Policy Optimization for LLM Agents\n  Reinforcement Learning"
                },
                "summary": "Training LLM agents in multi-turn environments with sparse rewards, where\ncompleting a single task requires 30+ turns of interaction within an episode,\npresents a fundamental challenge for reinforcement learning. We identify a\ncritical failure mode unique to this setting: the exploration-exploitation\ncascade failure. This cascade begins with early-stage policy premature\nconvergence, where sparse feedback causes agents to commit to flawed,\nlow-entropy strategies. Subsequently, agents enter late-stage policy collapse,\nwhere conventional entropy regularization becomes counterproductive, promoting\nchaotic exploration that destabilizes training. We propose Entropy-regularized\nPolicy Optimization (EPO), a general framework that breaks this failure cycle\nthrough three synergistic mechanisms: (1) adopting entropy regularization in\nmulti-turn settings to enhance exploration, (2) an entropy smoothing\nregularizer that bounds policy entropy within historical averages to prevent\nabrupt fluctuations, and (3) adaptive phase-based weighting that balances\nexploration and exploitation across training. Our analysis justifies that EPO\nguarantees monotonically decreasing entropy variance while maintaining\nconvergence. EPO achieves up to 152% performance improvement on ScienceWorld\nand up to 19.8% on ALFWorld. Our work demonstrates that multi-turn\nsparse-reward settings require fundamentally different entropy control than\ntraditional RL, with broad implications for LLM agent training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training LLM agents in multi-turn environments with sparse rewards, where\ncompleting a single task requires 30+ turns of interaction within an episode,\npresents a fundamental challenge for reinforcement learning. We identify a\ncritical failure mode unique to this setting: the exploration-exploitation\ncascade failure. This cascade begins with early-stage policy premature\nconvergence, where sparse feedback causes agents to commit to flawed,\nlow-entropy strategies. Subsequently, agents enter late-stage policy collapse,\nwhere conventional entropy regularization becomes counterproductive, promoting\nchaotic exploration that destabilizes training. We propose Entropy-regularized\nPolicy Optimization (EPO), a general framework that breaks this failure cycle\nthrough three synergistic mechanisms: (1) adopting entropy regularization in\nmulti-turn settings to enhance exploration, (2) an entropy smoothing\nregularizer that bounds policy entropy within historical averages to prevent\nabrupt fluctuations, and (3) adaptive phase-based weighting that balances\nexploration and exploitation across training. Our analysis justifies that EPO\nguarantees monotonically decreasing entropy variance while maintaining\nconvergence. EPO achieves up to 152% performance improvement on ScienceWorld\nand up to 19.8% on ALFWorld. Our work demonstrates that multi-turn\nsparse-reward settings require fundamentally different entropy control than\ntraditional RL, with broad implications for LLM agent training."
                },
                "authors": [
                    {
                        "name": "Xu Wujiang"
                    },
                    {
                        "name": "Wentian Zhao"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Li Yu-Jhe"
                    },
                    {
                        "name": "Jin Can"
                    },
                    {
                        "name": "Jin Mingyu"
                    },
                    {
                        "name": "Mei Kai"
                    },
                    {
                        "name": "Wan Kun"
                    },
                    {
                        "name": "Metaxas Dimitris"
                    }
                ],
                "author_detail": {
                    "name": "Metaxas Dimitris"
                },
                "author": "Metaxas Dimitris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22572v1",
                "updated": "2025-09-26T16:49:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    49,
                    10,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:49:10Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    49,
                    10,
                    4,
                    269,
                    0
                ],
                "title": "Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs\n  at Test Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs\n  at Test Time"
                },
                "summary": "Test-Time Scaling (TTS) enhances the reasoning ability of large language\nmodels (LLMs) by allocating additional computation during inference. However,\nexisting approaches primarily rely on output-level sampling while overlooking\nthe role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we\nobserve that varying the number of activated experts yields complementary\nsolution sets with stable accuracy, revealing a new and underexplored source of\ndiversity. Motivated by this observation, we propose Dynamic Experts Search\n(DES), a TTS strategy that elevates expert activation into a controllable\ndimension of the search space. DES integrates two key components: (1) Dynamic\nMoE, which enables direct control of expert counts during inference to generate\ndiverse reasoning trajectories without additional cost; and (2) Expert\nConfiguration Inheritance, which preserves consistent expert counts within a\nreasoning path while varying them across runs, thereby balancing stability and\ndiversity throughout the search. Extensive experiments across MoE\narchitectures, verifiers and reasoning benchmarks (i.e., math, code and\nknowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing\naccuracy and stability without additional cost. These results highlight DES as\na practical and scalable form of architecture-aware TTS, illustrating how\nstructural flexibility in modern LLMs can advance reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Scaling (TTS) enhances the reasoning ability of large language\nmodels (LLMs) by allocating additional computation during inference. However,\nexisting approaches primarily rely on output-level sampling while overlooking\nthe role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we\nobserve that varying the number of activated experts yields complementary\nsolution sets with stable accuracy, revealing a new and underexplored source of\ndiversity. Motivated by this observation, we propose Dynamic Experts Search\n(DES), a TTS strategy that elevates expert activation into a controllable\ndimension of the search space. DES integrates two key components: (1) Dynamic\nMoE, which enables direct control of expert counts during inference to generate\ndiverse reasoning trajectories without additional cost; and (2) Expert\nConfiguration Inheritance, which preserves consistent expert counts within a\nreasoning path while varying them across runs, thereby balancing stability and\ndiversity throughout the search. Extensive experiments across MoE\narchitectures, verifiers and reasoning benchmarks (i.e., math, code and\nknowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing\naccuracy and stability without additional cost. These results highlight DES as\na practical and scalable form of architecture-aware TTS, illustrating how\nstructural flexibility in modern LLMs can advance reasoning."
                },
                "authors": [
                    {
                        "name": "Yixuan Han"
                    },
                    {
                        "name": "Fan Ma"
                    },
                    {
                        "name": "Ruijie Quan"
                    },
                    {
                        "name": "Yi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Yang"
                },
                "author": "Yi Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22565v1",
                "updated": "2025-09-26T16:42:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    42,
                    43,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:42:43Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    42,
                    43,
                    4,
                    269,
                    0
                ],
                "title": "Retrieval-Augmented Guardrails for AI-Drafted Patient-Portal Messages:\n  Error Taxonomy Construction and Large-Scale Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Guardrails for AI-Drafted Patient-Portal Messages:\n  Error Taxonomy Construction and Large-Scale Evaluation"
                },
                "summary": "Asynchronous patient-clinician messaging via EHR portals is a growing source\nof clinician workload, prompting interest in large language models (LLMs) to\nassist with draft responses. However, LLM outputs may contain clinical\ninaccuracies, omissions, or tone mismatches, making robust evaluation\nessential. Our contributions are threefold: (1) we introduce a clinically\ngrounded error ontology comprising 5 domains and 59 granular error codes,\ndeveloped through inductive coding and expert adjudication; (2) we develop a\nretrieval-augmented evaluation pipeline (RAEC) that leverages semantically\nsimilar historical message-response pairs to improve judgment quality; and (3)\nwe provide a two-stage prompting architecture using DSPy to enable scalable,\ninterpretable, and hierarchical error detection. Our approach assesses the\nquality of drafts both in isolation and with reference to similar past\nmessage-response pairs retrieved from institutional archives. Using a two-stage\nDSPy pipeline, we compared baseline and reference-enhanced evaluations on over\n1,500 patient messages. Retrieval context improved error identification in\ndomains such as clinical completeness and workflow appropriateness. Human\nvalidation on 100 messages demonstrated superior agreement (concordance = 50%\nvs. 33%) and performance (F1 = 0.500 vs. 0.256) of context-enhanced labels vs.\nbaseline, supporting the use of our RAEC pipeline as AI guardrails for patient\nmessaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asynchronous patient-clinician messaging via EHR portals is a growing source\nof clinician workload, prompting interest in large language models (LLMs) to\nassist with draft responses. However, LLM outputs may contain clinical\ninaccuracies, omissions, or tone mismatches, making robust evaluation\nessential. Our contributions are threefold: (1) we introduce a clinically\ngrounded error ontology comprising 5 domains and 59 granular error codes,\ndeveloped through inductive coding and expert adjudication; (2) we develop a\nretrieval-augmented evaluation pipeline (RAEC) that leverages semantically\nsimilar historical message-response pairs to improve judgment quality; and (3)\nwe provide a two-stage prompting architecture using DSPy to enable scalable,\ninterpretable, and hierarchical error detection. Our approach assesses the\nquality of drafts both in isolation and with reference to similar past\nmessage-response pairs retrieved from institutional archives. Using a two-stage\nDSPy pipeline, we compared baseline and reference-enhanced evaluations on over\n1,500 patient messages. Retrieval context improved error identification in\ndomains such as clinical completeness and workflow appropriateness. Human\nvalidation on 100 messages demonstrated superior agreement (concordance = 50%\nvs. 33%) and performance (F1 = 0.500 vs. 0.256) of context-enhanced labels vs.\nbaseline, supporting the use of our RAEC pipeline as AI guardrails for patient\nmessaging."
                },
                "authors": [
                    {
                        "name": "Wenyuan Chen"
                    },
                    {
                        "name": "Fateme Nateghi Haredasht"
                    },
                    {
                        "name": "Kameron C. Black"
                    },
                    {
                        "name": "Francois Grolleau"
                    },
                    {
                        "name": "Emily Alsentzer"
                    },
                    {
                        "name": "Jonathan H. Chen"
                    },
                    {
                        "name": "Stephen P. Ma"
                    }
                ],
                "author_detail": {
                    "name": "Stephen P. Ma"
                },
                "author": "Stephen P. Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22560v1",
                "updated": "2025-09-26T16:40:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    40,
                    44,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:40:44Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    40,
                    44,
                    4,
                    269,
                    0
                ],
                "title": "LLM-Augmented and Fair Machine Learning Framework for University\n  Admission Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Augmented and Fair Machine Learning Framework for University\n  Admission Prediction"
                },
                "summary": "Universities face surging applications and heightened expectations for\nfairness, making accurate admission prediction increasingly vital. This work\npresents a comprehensive framework that fuses machine learning, deep learning,\nand large language model techniques to combine structured academic and\ndemographic variables with unstructured text signals. Drawing on more than\n2,000 student records, the study benchmarks logistic regression, Naive Bayes,\nrandom forests, deep neural networks, and a stacked ensemble. Logistic\nregression offers a strong, interpretable baseline at 89.5% accuracy, while the\nstacked ensemble achieves the best performance at 91.0%, with Naive Bayes and\nrandom forests close behind. To probe text integration, GPT-4-simulated\nevaluations of personal statements are added as features, yielding modest gains\nbut demonstrating feasibility for authentic essays and recommendation letters.\nTransparency is ensured through feature-importance visualizations and fairness\naudits. The audits reveal a 9% gender gap (67% male vs. 76% female) and an 11%\ngap by parental education, underscoring the need for continued monitoring. The\nframework is interpretable, fairness-aware, and deployable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universities face surging applications and heightened expectations for\nfairness, making accurate admission prediction increasingly vital. This work\npresents a comprehensive framework that fuses machine learning, deep learning,\nand large language model techniques to combine structured academic and\ndemographic variables with unstructured text signals. Drawing on more than\n2,000 student records, the study benchmarks logistic regression, Naive Bayes,\nrandom forests, deep neural networks, and a stacked ensemble. Logistic\nregression offers a strong, interpretable baseline at 89.5% accuracy, while the\nstacked ensemble achieves the best performance at 91.0%, with Naive Bayes and\nrandom forests close behind. To probe text integration, GPT-4-simulated\nevaluations of personal statements are added as features, yielding modest gains\nbut demonstrating feasibility for authentic essays and recommendation letters.\nTransparency is ensured through feature-importance visualizations and fairness\naudits. The audits reveal a 9% gender gap (67% male vs. 76% female) and an 11%\ngap by parental education, underscoring the need for continued monitoring. The\nframework is interpretable, fairness-aware, and deployable."
                },
                "authors": [
                    {
                        "name": "Mohammad Abbadi"
                    },
                    {
                        "name": "Yassine Himeur"
                    },
                    {
                        "name": "Shadi Atalla"
                    },
                    {
                        "name": "Dahlia Mansoor"
                    },
                    {
                        "name": "Wathiq Mansoor"
                    }
                ],
                "author_detail": {
                    "name": "Wathiq Mansoor"
                },
                "author": "Wathiq Mansoor",
                "arxiv_comment": "The 9th International Symposium on Multidisciplinary Studies and\n  Innovative Technologies (ISMSIT)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22558v1",
                "updated": "2025-09-26T16:39:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    39,
                    10,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:39:10Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    39,
                    10,
                    4,
                    269,
                    0
                ],
                "title": "StepORLM: A Self-Evolving Framework With Generative Process Supervision\n  For Operations Research Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StepORLM: A Self-Evolving Framework With Generative Process Supervision\n  For Operations Research Language Models"
                },
                "summary": "Large Language Models (LLMs) have shown promising capabilities for solving\nOperations Research (OR) problems. While reinforcement learning serves as a\npowerful paradigm for LLM training on OR problems, existing works generally\nface two key limitations. First, outcome reward suffers from the credit\nassignment problem, where correct final answers can reinforce flawed reasoning.\nSecond, conventional discriminative process supervision is myopic, failing to\nevaluate the interdependent steps of OR modeling holistically. To this end, we\nintroduce StepORLM, a novel self-evolving framework with generative process\nsupervision. At its core, StepORLM features a co-evolutionary loop where a\npolicy model and a generative process reward model (GenPRM) iteratively improve\non each other. This loop is driven by a dual-feedback mechanism: definitive,\noutcome-based verification from an external solver, and nuanced, holistic\nprocess evaluation from the GenPRM. The combined signal is used to align the\npolicy via Weighted Direct Preference Optimization (W-DPO) and simultaneously\nrefine the GenPRM. Our resulting 8B-parameter StepORLM establishes a new\nstate-of-the-art across six benchmarks, significantly outperforming vastly\nlarger generalist models, agentic methods, and specialized baselines. Moreover,\nthe co-evolved GenPRM is able to act as a powerful and universally applicable\nprocess verifier, substantially boosting the inference scaling performance of\nboth our own model and other existing LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown promising capabilities for solving\nOperations Research (OR) problems. While reinforcement learning serves as a\npowerful paradigm for LLM training on OR problems, existing works generally\nface two key limitations. First, outcome reward suffers from the credit\nassignment problem, where correct final answers can reinforce flawed reasoning.\nSecond, conventional discriminative process supervision is myopic, failing to\nevaluate the interdependent steps of OR modeling holistically. To this end, we\nintroduce StepORLM, a novel self-evolving framework with generative process\nsupervision. At its core, StepORLM features a co-evolutionary loop where a\npolicy model and a generative process reward model (GenPRM) iteratively improve\non each other. This loop is driven by a dual-feedback mechanism: definitive,\noutcome-based verification from an external solver, and nuanced, holistic\nprocess evaluation from the GenPRM. The combined signal is used to align the\npolicy via Weighted Direct Preference Optimization (W-DPO) and simultaneously\nrefine the GenPRM. Our resulting 8B-parameter StepORLM establishes a new\nstate-of-the-art across six benchmarks, significantly outperforming vastly\nlarger generalist models, agentic methods, and specialized baselines. Moreover,\nthe co-evolved GenPRM is able to act as a powerful and universally applicable\nprocess verifier, substantially boosting the inference scaling performance of\nboth our own model and other existing LLMs."
                },
                "authors": [
                    {
                        "name": "Chenyu Zhou"
                    },
                    {
                        "name": "Tianyi Xu"
                    },
                    {
                        "name": "Jianghao Lin"
                    },
                    {
                        "name": "Dongdong Ge"
                    }
                ],
                "author_detail": {
                    "name": "Dongdong Ge"
                },
                "author": "Dongdong Ge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20900v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20900v2",
                "updated": "2025-09-26T16:37:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    37,
                    45,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-25T08:36:19Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    8,
                    36,
                    19,
                    3,
                    268,
                    0
                ],
                "title": "Learning to Summarize by Learning to Quiz: Adversarial Agentic\n  Collaboration for Long Document Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Summarize by Learning to Quiz: Adversarial Agentic\n  Collaboration for Long Document Summarization"
                },
                "summary": "Long document summarization remains a significant challenge for current large\nlanguage models (LLMs), as existing approaches commonly struggle with\ninformation loss, factual inconsistencies, and coherence issues when processing\nexcessively long documents. We propose SummQ, a novel adversarial multi-agent\nframework that addresses these limitations through collaborative intelligence\nbetween specialized agents operating in two complementary domains:\nsummarization and quizzing. Our approach employs summary generators and\nreviewers that work collaboratively to create and evaluate comprehensive\nsummaries, while quiz generators and reviewers create comprehension questions\nthat serve as continuous quality checks for the summarization process. This\nadversarial dynamic, enhanced by an examinee agent that validates whether the\ngenerated summary contains the information needed to answer the quiz questions,\nenables iterative refinement through multifaceted feedback mechanisms. We\nevaluate SummQ on three widely used long document summarization benchmarks.\nExperimental results demonstrate that our framework significantly outperforms\nexisting state-of-the-art methods across ROUGE and BERTScore metrics, as well\nas in LLM-as-a-Judge and human evaluations. Our comprehensive analyses reveal\nthe effectiveness of the multi-agent collaboration dynamics, the influence of\ndifferent agent configurations, and the impact of the quizzing mechanism. This\nwork establishes a new approach for long document summarization that uses\nadversarial agentic collaboration to improve summarization quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long document summarization remains a significant challenge for current large\nlanguage models (LLMs), as existing approaches commonly struggle with\ninformation loss, factual inconsistencies, and coherence issues when processing\nexcessively long documents. We propose SummQ, a novel adversarial multi-agent\nframework that addresses these limitations through collaborative intelligence\nbetween specialized agents operating in two complementary domains:\nsummarization and quizzing. Our approach employs summary generators and\nreviewers that work collaboratively to create and evaluate comprehensive\nsummaries, while quiz generators and reviewers create comprehension questions\nthat serve as continuous quality checks for the summarization process. This\nadversarial dynamic, enhanced by an examinee agent that validates whether the\ngenerated summary contains the information needed to answer the quiz questions,\nenables iterative refinement through multifaceted feedback mechanisms. We\nevaluate SummQ on three widely used long document summarization benchmarks.\nExperimental results demonstrate that our framework significantly outperforms\nexisting state-of-the-art methods across ROUGE and BERTScore metrics, as well\nas in LLM-as-a-Judge and human evaluations. Our comprehensive analyses reveal\nthe effectiveness of the multi-agent collaboration dynamics, the influence of\ndifferent agent configurations, and the impact of the quizzing mechanism. This\nwork establishes a new approach for long document summarization that uses\nadversarial agentic collaboration to improve summarization quality."
                },
                "authors": [
                    {
                        "name": "Weixuan Wang"
                    },
                    {
                        "name": "Minghao Wu"
                    },
                    {
                        "name": "Barry Haddow"
                    },
                    {
                        "name": "Alexandra Birch"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Birch"
                },
                "author": "Alexandra Birch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20900v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20900v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22553v1",
                "updated": "2025-09-26T16:35:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    35,
                    42,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:35:42Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    35,
                    42,
                    4,
                    269,
                    0
                ],
                "title": "Linear Causal Representation Learning by Topological Ordering, Pruning,\n  and Disentanglement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Causal Representation Learning by Topological Ordering, Pruning,\n  and Disentanglement"
                },
                "summary": "Causal representation learning (CRL) has garnered increasing interests from\nthe causal inference and artificial intelligence community, due to its\ncapability of disentangling potentially complex data-generating mechanism into\ncausally interpretable latent features, by leveraging the heterogeneity of\nmodern datasets. In this paper, we further contribute to the CRL literature, by\nfocusing on the stylized linear structural causal model over the latent\nfeatures and assuming a linear mixing function that maps latent features to the\nobserved data or measurements. Existing linear CRL methods often rely on\nstringent assumptions, such as accessibility to single-node interventional data\nor restrictive distributional constraints on latent features and exogenous\nmeasurement noise. However, these prerequisites can be challenging to satisfy\nin certain scenarios. In this work, we propose a novel linear CRL algorithm\nthat, unlike most existing linear CRL methods, operates under weaker\nassumptions about environment heterogeneity and data-generating distributions\nwhile still recovering latent causal features up to an equivalence class. We\nfurther validate our new algorithm via synthetic experiments and an\ninterpretability analysis of large language models (LLMs), demonstrating both\nits superiority over competing methods in finite samples and its potential in\nintegrating causality into AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal representation learning (CRL) has garnered increasing interests from\nthe causal inference and artificial intelligence community, due to its\ncapability of disentangling potentially complex data-generating mechanism into\ncausally interpretable latent features, by leveraging the heterogeneity of\nmodern datasets. In this paper, we further contribute to the CRL literature, by\nfocusing on the stylized linear structural causal model over the latent\nfeatures and assuming a linear mixing function that maps latent features to the\nobserved data or measurements. Existing linear CRL methods often rely on\nstringent assumptions, such as accessibility to single-node interventional data\nor restrictive distributional constraints on latent features and exogenous\nmeasurement noise. However, these prerequisites can be challenging to satisfy\nin certain scenarios. In this work, we propose a novel linear CRL algorithm\nthat, unlike most existing linear CRL methods, operates under weaker\nassumptions about environment heterogeneity and data-generating distributions\nwhile still recovering latent causal features up to an equivalence class. We\nfurther validate our new algorithm via synthetic experiments and an\ninterpretability analysis of large language models (LLMs), demonstrating both\nits superiority over competing methods in finite samples and its potential in\nintegrating causality into AI."
                },
                "authors": [
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Yu Guang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Guang Wang"
                },
                "author": "Yu Guang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12313v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12313v2",
                "updated": "2025-09-26T16:35:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    35,
                    13,
                    4,
                    269,
                    0
                ],
                "published": "2025-05-18T08:55:46Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    8,
                    55,
                    46,
                    6,
                    138,
                    0
                ],
                "title": "ExpertSteer: Intervening in LLMs through Expert Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertSteer: Intervening in LLMs through Expert Knowledge"
                },
                "summary": "Large Language Models (LLMs) exhibit remarkable capabilities across various\ntasks, yet guiding them to follow desired behaviours during inference remains a\nsignificant challenge. Activation steering offers a promising method to control\nthe generation process of LLMs by modifying their internal activations.\nHowever, existing methods commonly intervene in the model's behaviour using\nsteering vectors generated by the model itself, which constrains their\neffectiveness to that specific model and excludes the possibility of leveraging\npowerful external expert models for steering. To address these limitations, we\npropose ExpertSteer, a novel approach that leverages arbitrary specialized\nexpert models to generate steering vectors, enabling intervention in any LLMs.\nExpertSteer transfers the knowledge from an expert model to a target LLM\nthrough a cohesive four-step process: first aligning representation dimensions\nwith auto-encoders to enable cross-model transfer, then identifying\nintervention layer pairs based on mutual information analysis, next generating\nsteering vectors from the expert model using Recursive Feature Machines, and\nfinally applying these vectors on the identified layers during inference to\nselectively guide the target LLM without updating model parameters. We conduct\ncomprehensive experiments using three LLMs on 15 popular benchmarks across four\ndistinct domains. Experiments demonstrate that ExpertSteer significantly\noutperforms established baselines across diverse tasks at minimal cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit remarkable capabilities across various\ntasks, yet guiding them to follow desired behaviours during inference remains a\nsignificant challenge. Activation steering offers a promising method to control\nthe generation process of LLMs by modifying their internal activations.\nHowever, existing methods commonly intervene in the model's behaviour using\nsteering vectors generated by the model itself, which constrains their\neffectiveness to that specific model and excludes the possibility of leveraging\npowerful external expert models for steering. To address these limitations, we\npropose ExpertSteer, a novel approach that leverages arbitrary specialized\nexpert models to generate steering vectors, enabling intervention in any LLMs.\nExpertSteer transfers the knowledge from an expert model to a target LLM\nthrough a cohesive four-step process: first aligning representation dimensions\nwith auto-encoders to enable cross-model transfer, then identifying\nintervention layer pairs based on mutual information analysis, next generating\nsteering vectors from the expert model using Recursive Feature Machines, and\nfinally applying these vectors on the identified layers during inference to\nselectively guide the target LLM without updating model parameters. We conduct\ncomprehensive experiments using three LLMs on 15 popular benchmarks across four\ndistinct domains. Experiments demonstrate that ExpertSteer significantly\noutperforms established baselines across diverse tasks at minimal cost."
                },
                "authors": [
                    {
                        "name": "Weixuan Wang"
                    },
                    {
                        "name": "Minghao Wu"
                    },
                    {
                        "name": "Barry Haddow"
                    },
                    {
                        "name": "Alexandra Birch"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Birch"
                },
                "author": "Alexandra Birch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12313v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12313v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12300v2",
                "updated": "2025-09-26T16:33:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    33,
                    17,
                    4,
                    269,
                    0
                ],
                "published": "2025-05-18T08:31:44Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    8,
                    31,
                    44,
                    6,
                    138,
                    0
                ],
                "title": "HBO: Hierarchical Balancing Optimization for Fine-Tuning Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HBO: Hierarchical Balancing Optimization for Fine-Tuning Large Language\n  Models"
                },
                "summary": "Fine-tuning large language models (LLMs) on a mixture of diverse datasets\nposes challenges due to data imbalance and heterogeneity. Existing methods\noften address these issues across datasets (globally) but overlook the\nimbalance and heterogeneity within individual datasets (locally), which limits\ntheir effectiveness. We introduce Hierarchical Balancing Optimization (HBO), a\nnovel method that enables LLMs to autonomously adjust data allocation during\nfine-tuning both across datasets (globally) and within each individual dataset\n(locally). HBO employs a bilevel optimization strategy with two types of\nactors: a Global Actor, which balances data sampling across different subsets\nof the training mixture, and several Local Actors, which optimizes data usage\nwithin each subset based on difficulty levels. These actors are guided by\nreward functions derived from the LLM's training state, which measure learning\nprogress and relative performance improvement. We evaluate HBO on three LLM\nbackbones across nine diverse tasks in multilingual and multitask setups.\nResults show that HBO consistently outperforms existing baselines, achieving\nsignificant accuracy gains. Our in-depth analysis further demonstrates that\nboth the global actor and local actors of HBO effectively adjust data usage\nduring fine-tuning. HBO provides a comprehensive solution to the challenges of\ndata imbalance and heterogeneity in LLM fine-tuning, enabling more effective\ntraining across diverse datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) on a mixture of diverse datasets\nposes challenges due to data imbalance and heterogeneity. Existing methods\noften address these issues across datasets (globally) but overlook the\nimbalance and heterogeneity within individual datasets (locally), which limits\ntheir effectiveness. We introduce Hierarchical Balancing Optimization (HBO), a\nnovel method that enables LLMs to autonomously adjust data allocation during\nfine-tuning both across datasets (globally) and within each individual dataset\n(locally). HBO employs a bilevel optimization strategy with two types of\nactors: a Global Actor, which balances data sampling across different subsets\nof the training mixture, and several Local Actors, which optimizes data usage\nwithin each subset based on difficulty levels. These actors are guided by\nreward functions derived from the LLM's training state, which measure learning\nprogress and relative performance improvement. We evaluate HBO on three LLM\nbackbones across nine diverse tasks in multilingual and multitask setups.\nResults show that HBO consistently outperforms existing baselines, achieving\nsignificant accuracy gains. Our in-depth analysis further demonstrates that\nboth the global actor and local actors of HBO effectively adjust data usage\nduring fine-tuning. HBO provides a comprehensive solution to the challenges of\ndata imbalance and heterogeneity in LLM fine-tuning, enabling more effective\ntraining across diverse datasets."
                },
                "authors": [
                    {
                        "name": "Weixuan Wang"
                    },
                    {
                        "name": "Minghao Wu"
                    },
                    {
                        "name": "Barry Haddow"
                    },
                    {
                        "name": "Alexandra Birch"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Birch"
                },
                "author": "Alexandra Birch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22546v1",
                "updated": "2025-09-26T16:27:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    27,
                    29,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:27:29Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    27,
                    29,
                    4,
                    269,
                    0
                ],
                "title": "Think Socially via Cognitive Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Socially via Cognitive Reasoning"
                },
                "summary": "LLMs trained for logical reasoning excel at step-by-step deduction to reach\nverifiable answers. However, this paradigm is ill-suited for navigating social\nsituations, which induce an interpretive process of analyzing ambiguous cues\nthat rarely yield a definitive outcome. To bridge this gap, we introduce\nCognitive Reasoning, a paradigm modeled on human social cognition. It\nformulates the interpretive process into a structured cognitive flow of\ninterconnected cognitive units (e.g., observation or attribution), which\ncombine adaptively to enable effective social thinking and responses. We then\npropose CogFlow, a complete framework that instills this capability in LLMs.\nCogFlow first curates a dataset of cognitive flows by simulating the\nassociative and progressive nature of human thought via tree-structured\nplanning. After instilling the basic cognitive reasoning capability via\nsupervised fine-tuning, CogFlow adopts reinforcement learning to enable the\nmodel to improve itself via trial and error, guided by a multi-objective reward\nthat optimizes both cognitive flow and response quality. Extensive experiments\nshow that CogFlow effectively enhances the social cognitive capabilities of\nLLMs, and even humans, leading to more effective social decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs trained for logical reasoning excel at step-by-step deduction to reach\nverifiable answers. However, this paradigm is ill-suited for navigating social\nsituations, which induce an interpretive process of analyzing ambiguous cues\nthat rarely yield a definitive outcome. To bridge this gap, we introduce\nCognitive Reasoning, a paradigm modeled on human social cognition. It\nformulates the interpretive process into a structured cognitive flow of\ninterconnected cognitive units (e.g., observation or attribution), which\ncombine adaptively to enable effective social thinking and responses. We then\npropose CogFlow, a complete framework that instills this capability in LLMs.\nCogFlow first curates a dataset of cognitive flows by simulating the\nassociative and progressive nature of human thought via tree-structured\nplanning. After instilling the basic cognitive reasoning capability via\nsupervised fine-tuning, CogFlow adopts reinforcement learning to enable the\nmodel to improve itself via trial and error, guided by a multi-objective reward\nthat optimizes both cognitive flow and response quality. Extensive experiments\nshow that CogFlow effectively enhances the social cognitive capabilities of\nLLMs, and even humans, leading to more effective social decision-making."
                },
                "authors": [
                    {
                        "name": "Jinfeng Zhou"
                    },
                    {
                        "name": "Zheyu Chen"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Quanyu Dai"
                    },
                    {
                        "name": "Zhenhua Dong"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "arxiv_comment": "Repository: https://github.com/thu-coai/CogFlow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20172v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20172v2",
                "updated": "2025-09-26T16:24:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    24,
                    10,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-24T14:36:44Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    36,
                    44,
                    2,
                    267,
                    0
                ],
                "title": "Benchmarking LLMs in Web API Integration Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLMs in Web API Integration Tasks"
                },
                "summary": "API integration is a cornerstone of our digital infrastructure, enabling\nsoftware systems to connect and interact. However, as shown by many studies,\nwriting or generating correct code to invoke APIs, particularly web APIs, is\nchallenging. Although large language models (LLMs) have become popular in\nsoftware development, their effectiveness in automating the generation of web\nAPI integration code remains unexplored. In order to address this, we present a\ndataset and evaluation pipeline designed to assess the ability of LLMs to\ngenerate web API invocation code. Our experiments with several open-source LLMs\nreveal that generating API invocations poses a significant challenge, resulting\nin hallucinated endpoints, incorrect argument usage, and other errors. None of\nthe evaluated open-source models were able to solve more than 40% of the tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "API integration is a cornerstone of our digital infrastructure, enabling\nsoftware systems to connect and interact. However, as shown by many studies,\nwriting or generating correct code to invoke APIs, particularly web APIs, is\nchallenging. Although large language models (LLMs) have become popular in\nsoftware development, their effectiveness in automating the generation of web\nAPI integration code remains unexplored. In order to address this, we present a\ndataset and evaluation pipeline designed to assess the ability of LLMs to\ngenerate web API invocation code. Our experiments with several open-source LLMs\nreveal that generating API invocations poses a significant challenge, resulting\nin hallucinated endpoints, incorrect argument usage, and other errors. None of\nthe evaluated open-source models were able to solve more than 40% of the tasks."
                },
                "authors": [
                    {
                        "name": "Daniel Maninger"
                    },
                    {
                        "name": "Leon Chemnitz"
                    },
                    {
                        "name": "Amir Molzam Sharifloo"
                    },
                    {
                        "name": "Jannis Brugger"
                    },
                    {
                        "name": "Mira Mezini"
                    }
                ],
                "author_detail": {
                    "name": "Mira Mezini"
                },
                "author": "Mira Mezini",
                "arxiv_comment": "To be published in Proceedings of 2nd ACM International Conference on\n  AI-powered Software, Benchmark & Dataset Track (AIware '25); updated paper\n  title and affiliations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20172v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20172v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05563v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05563v2",
                "updated": "2025-09-26T16:21:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    21,
                    43,
                    4,
                    269,
                    0
                ],
                "published": "2025-04-07T23:34:11Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    23,
                    34,
                    11,
                    0,
                    97,
                    0
                ],
                "title": "Do Data Valuations Make Good Data Prices?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Data Valuations Make Good Data Prices?"
                },
                "summary": "As large language models increasingly rely on external data sources,\ncompensating data contributors has become a central concern. But how should\nthese payments be devised? We revisit data valuations from a\n$\\textit{market-design perspective}$ where payments serve to compensate data\nowners for the $\\textit{private}$ heterogeneous costs they incur for collecting\nand sharing data. We show that popular valuation methods-such as Leave-One-Out\nand Data Shapley-make for poor payments. They fail to ensure truthful reporting\nof the costs, leading to $\\textit{inefficient market}$ outcomes. To address\nthis, we adapt well-established payment rules from mechanism design, namely\nMyerson and Vickrey-Clarke-Groves (VCG), to the data market setting. We show\nthat Myerson payment is the minimal truthful mechanism, optimal from the\nbuyer's perspective. Additionally, we identify a condition under which both\ndata buyers and sellers are utility-satisfied, and the market achieves\nefficiency. Our findings highlight the importance of incorporating incentive\ncompatibility into data valuation design, paving the way for more robust and\nefficient data markets. Our data market framework is readily applicable to\nreal-world scenarios. We illustrate this with simulations of contributor\ncompensation in an LLM based retrieval-augmented generation (RAG) marketplace\ntasked with challenging medical question answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models increasingly rely on external data sources,\ncompensating data contributors has become a central concern. But how should\nthese payments be devised? We revisit data valuations from a\n$\\textit{market-design perspective}$ where payments serve to compensate data\nowners for the $\\textit{private}$ heterogeneous costs they incur for collecting\nand sharing data. We show that popular valuation methods-such as Leave-One-Out\nand Data Shapley-make for poor payments. They fail to ensure truthful reporting\nof the costs, leading to $\\textit{inefficient market}$ outcomes. To address\nthis, we adapt well-established payment rules from mechanism design, namely\nMyerson and Vickrey-Clarke-Groves (VCG), to the data market setting. We show\nthat Myerson payment is the minimal truthful mechanism, optimal from the\nbuyer's perspective. Additionally, we identify a condition under which both\ndata buyers and sellers are utility-satisfied, and the market achieves\nefficiency. Our findings highlight the importance of incorporating incentive\ncompatibility into data valuation design, paving the way for more robust and\nefficient data markets. Our data market framework is readily applicable to\nreal-world scenarios. We illustrate this with simulations of contributor\ncompensation in an LLM based retrieval-augmented generation (RAG) marketplace\ntasked with challenging medical question answering."
                },
                "authors": [
                    {
                        "name": "Dongyang Fan"
                    },
                    {
                        "name": "Tyler J. Rotello"
                    },
                    {
                        "name": "Sai Praneeth Karimireddy"
                    }
                ],
                "author_detail": {
                    "name": "Sai Praneeth Karimireddy"
                },
                "author": "Sai Praneeth Karimireddy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05563v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05563v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12663v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12663v2",
                "updated": "2025-09-26T16:20:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    20,
                    20,
                    4,
                    269,
                    0
                ],
                "published": "2025-02-18T09:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    11,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Demystifying Multilingual Chain-of-Thought in Process Reward Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Multilingual Chain-of-Thought in Process Reward Modeling"
                },
                "summary": "Large language models (LLMs) are designed to perform a wide range of tasks.\nTo improve their ability to solve complex problems requiring multi-step\nreasoning, recent research leverages process reward modeling to provide\nfine-grained feedback at each step of the reasoning process for reinforcement\nlearning (RL), but it predominantly focuses on English. In this paper, we\ntackle the critical challenge of extending process reward models (PRMs) to\nmultilingual settings. To achieve this, we train multilingual PRMs on a dataset\nspanning seven languages, which is translated from English. Through\ncomprehensive evaluations on two widely used reasoning benchmarks across 11\nlanguages, we demonstrate that multilingual PRMs not only improve average\naccuracy but also reduce early-stage reasoning errors. Furthermore, our results\nhighlight the sensitivity of multilingual PRMs to both the number of training\nlanguages and the volume of English data, while also uncovering the benefits\narising from more candidate responses and trainable parameters. This work opens\npromising avenues for robust multilingual applications in complex, multi-step\nreasoning tasks. In addition, we release the code to foster research along this\nline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are designed to perform a wide range of tasks.\nTo improve their ability to solve complex problems requiring multi-step\nreasoning, recent research leverages process reward modeling to provide\nfine-grained feedback at each step of the reasoning process for reinforcement\nlearning (RL), but it predominantly focuses on English. In this paper, we\ntackle the critical challenge of extending process reward models (PRMs) to\nmultilingual settings. To achieve this, we train multilingual PRMs on a dataset\nspanning seven languages, which is translated from English. Through\ncomprehensive evaluations on two widely used reasoning benchmarks across 11\nlanguages, we demonstrate that multilingual PRMs not only improve average\naccuracy but also reduce early-stage reasoning errors. Furthermore, our results\nhighlight the sensitivity of multilingual PRMs to both the number of training\nlanguages and the volume of English data, while also uncovering the benefits\narising from more candidate responses and trainable parameters. This work opens\npromising avenues for robust multilingual applications in complex, multi-step\nreasoning tasks. In addition, we release the code to foster research along this\nline."
                },
                "authors": [
                    {
                        "name": "Weixuan Wang"
                    },
                    {
                        "name": "Minghao Wu"
                    },
                    {
                        "name": "Barry Haddow"
                    },
                    {
                        "name": "Alexandra Birch"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Birch"
                },
                "author": "Alexandra Birch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12663v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12663v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22544v1",
                "updated": "2025-09-26T16:20:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    20,
                    6,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:20:06Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    20,
                    6,
                    4,
                    269,
                    0
                ],
                "title": "HyCoVAD: A Hybrid SSL-LLM Model for Complex Video Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyCoVAD: A Hybrid SSL-LLM Model for Complex Video Anomaly Detection"
                },
                "summary": "Video anomaly detection (VAD) is crucial for intelligent surveillance, but a\nsignificant challenge lies in identifying complex anomalies, which are events\ndefined by intricate relationships and temporal dependencies among multiple\nentities rather than by isolated actions. While self-supervised learning (SSL)\nmethods effectively model low-level spatiotemporal patterns, they often\nstruggle to grasp the semantic meaning of these interactions. Conversely, large\nlanguage models (LLMs) offer powerful contextual reasoning but are\ncomputationally expensive for frame-by-frame analysis and lack fine-grained\nspatial localization. We introduce HyCoVAD, Hybrid Complex Video Anomaly\nDetection, a hybrid SSL-LLM model that combines a multi-task SSL temporal\nanalyzer with LLM validator. The SSL module is built upon an nnFormer backbone\nwhich is a transformer-based model for image segmentation. It is trained with\nmultiple proxy tasks, learns from video frames to identify those suspected of\nanomaly. The selected frames are then forwarded to the LLM, which enriches the\nanalysis with semantic context by applying structured, rule-based reasoning to\nvalidate the presence of anomalies. Experiments on the challenging ComplexVAD\ndataset show that HyCoVAD achieves a 72.5% frame-level AUC, outperforming\nexisting baselines by 12.5% while reducing LLM computation. We release our\ninteraction anomaly taxonomy, adaptive thresholding protocol, and code to\nfacilitate future research in complex VAD scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video anomaly detection (VAD) is crucial for intelligent surveillance, but a\nsignificant challenge lies in identifying complex anomalies, which are events\ndefined by intricate relationships and temporal dependencies among multiple\nentities rather than by isolated actions. While self-supervised learning (SSL)\nmethods effectively model low-level spatiotemporal patterns, they often\nstruggle to grasp the semantic meaning of these interactions. Conversely, large\nlanguage models (LLMs) offer powerful contextual reasoning but are\ncomputationally expensive for frame-by-frame analysis and lack fine-grained\nspatial localization. We introduce HyCoVAD, Hybrid Complex Video Anomaly\nDetection, a hybrid SSL-LLM model that combines a multi-task SSL temporal\nanalyzer with LLM validator. The SSL module is built upon an nnFormer backbone\nwhich is a transformer-based model for image segmentation. It is trained with\nmultiple proxy tasks, learns from video frames to identify those suspected of\nanomaly. The selected frames are then forwarded to the LLM, which enriches the\nanalysis with semantic context by applying structured, rule-based reasoning to\nvalidate the presence of anomalies. Experiments on the challenging ComplexVAD\ndataset show that HyCoVAD achieves a 72.5% frame-level AUC, outperforming\nexisting baselines by 12.5% while reducing LLM computation. We release our\ninteraction anomaly taxonomy, adaptive thresholding protocol, and code to\nfacilitate future research in complex VAD scenarios."
                },
                "authors": [
                    {
                        "name": "Mohammad Mahdi Hemmatyar"
                    },
                    {
                        "name": "Mahdi Jafari"
                    },
                    {
                        "name": "Mohammad Amin Yousefi"
                    },
                    {
                        "name": "Mohammad Reza Nemati"
                    },
                    {
                        "name": "Mobin Azadani"
                    },
                    {
                        "name": "Hamid Reza Rastad"
                    },
                    {
                        "name": "Amirmohammad Akbari"
                    }
                ],
                "author_detail": {
                    "name": "Amirmohammad Akbari"
                },
                "author": "Amirmohammad Akbari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06958v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06958v2",
                "updated": "2025-09-26T16:20:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    20,
                    4,
                    4,
                    269,
                    0
                ],
                "published": "2025-06-08T00:59:02Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    0,
                    59,
                    2,
                    6,
                    159,
                    0
                ],
                "title": "Position: Simulating Society Requires Simulating Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: Simulating Society Requires Simulating Thought"
                },
                "summary": "Simulating society with large language models (LLMs), we argue, requires more\nthan generating plausible behavior; it demands cognitively grounded reasoning\nthat is structured, revisable, and traceable. LLM-based agents are increasingly\nused to emulate individual and group behavior, primarily through prompting and\nsupervised fine-tuning. Yet they often lack internal coherence, causal\nreasoning, and belief traceability, making them unreliable for simulating how\npeople reason, deliberate, and respond to interventions.\n  To address this, we present a conceptual modeling paradigm, Generative Minds\n(GenMinds), which draws from cognitive science to support structured belief\nrepresentations in generative agents. To evaluate such agents, we introduce the\nRECAP (REconstructing CAusal Paths) framework, a benchmark designed to assess\nreasoning fidelity via causal traceability, demographic grounding, and\nintervention consistency. These contributions advance a broader shift: from\nsurface-level mimicry to generative agents that simulate thought -- not just\nlanguage -- for social simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating society with large language models (LLMs), we argue, requires more\nthan generating plausible behavior; it demands cognitively grounded reasoning\nthat is structured, revisable, and traceable. LLM-based agents are increasingly\nused to emulate individual and group behavior, primarily through prompting and\nsupervised fine-tuning. Yet they often lack internal coherence, causal\nreasoning, and belief traceability, making them unreliable for simulating how\npeople reason, deliberate, and respond to interventions.\n  To address this, we present a conceptual modeling paradigm, Generative Minds\n(GenMinds), which draws from cognitive science to support structured belief\nrepresentations in generative agents. To evaluate such agents, we introduce the\nRECAP (REconstructing CAusal Paths) framework, a benchmark designed to assess\nreasoning fidelity via causal traceability, demographic grounding, and\nintervention consistency. These contributions advance a broader shift: from\nsurface-level mimicry to generative agents that simulate thought -- not just\nlanguage -- for social simulations."
                },
                "authors": [
                    {
                        "name": "Chance Jiajie Li"
                    },
                    {
                        "name": "Jiayi Wu"
                    },
                    {
                        "name": "Zhenze Mo"
                    },
                    {
                        "name": "Ao Qu"
                    },
                    {
                        "name": "Yuhan Tang"
                    },
                    {
                        "name": "Kaiya Ivy Zhao"
                    },
                    {
                        "name": "Yulu Gan"
                    },
                    {
                        "name": "Jie Fan"
                    },
                    {
                        "name": "Jiangbo Yu"
                    },
                    {
                        "name": "Jinhua Zhao"
                    },
                    {
                        "name": "Paul Liang"
                    },
                    {
                        "name": "Luis Alonso"
                    },
                    {
                        "name": "Kent Larson"
                    }
                ],
                "author_detail": {
                    "name": "Kent Larson"
                },
                "author": "Kent Larson",
                "arxiv_comment": "To appear in NeurIPS 2025 (Position Paper Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06958v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06958v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22537v1",
                "updated": "2025-09-26T16:17:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    17,
                    29,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:17:29Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    17,
                    29,
                    4,
                    269,
                    0
                ],
                "title": "The Emergence of Altruism in Large-Language-Model Agents Society",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Emergence of Altruism in Large-Language-Model Agents Society"
                },
                "summary": "Leveraging Large Language Models (LLMs) for social simulation is a frontier\nin computational social science. Understanding the social logics these agents\nembody is critical to this attempt. However, existing research has primarily\nfocused on cooperation in small-scale, task-oriented games, overlooking how\naltruism, which means sacrificing self-interest for collective benefit, emerges\nin large-scale agent societies. To address this gap, we introduce a\nSchelling-variant urban migration model that creates a social dilemma,\ncompelling over 200 LLM agents to navigate an explicit conflict between\negoistic (personal utility) and altruistic (system utility) goals. Our central\nfinding is a fundamental difference in the social tendencies of LLMs. We\nidentify two distinct archetypes: \"Adaptive Egoists\", which default to\nprioritizing self-interest but whose altruistic behaviors significantly\nincrease under the influence of a social norm-setting message board; and\n\"Altruistic Optimizers\", which exhibit an inherent altruistic logic,\nconsistently prioritizing collective benefit even at a direct cost to\nthemselves. Furthermore, to qualitatively analyze the cognitive underpinnings\nof these decisions, we introduce a method inspired by Grounded Theory to\nsystematically code agent reasoning. In summary, this research provides the\nfirst evidence of intrinsic heterogeneity in the egoistic and altruistic\ntendencies of different LLMs. We propose that for social simulation, model\nselection is not merely a matter of choosing reasoning capability, but of\nchoosing an intrinsic social action logic. While \"Adaptive Egoists\" may offer a\nmore suitable choice for simulating complex human societies, \"Altruistic\nOptimizers\" are better suited for modeling idealized pro-social actors or\nscenarios where collective welfare is the primary consideration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models (LLMs) for social simulation is a frontier\nin computational social science. Understanding the social logics these agents\nembody is critical to this attempt. However, existing research has primarily\nfocused on cooperation in small-scale, task-oriented games, overlooking how\naltruism, which means sacrificing self-interest for collective benefit, emerges\nin large-scale agent societies. To address this gap, we introduce a\nSchelling-variant urban migration model that creates a social dilemma,\ncompelling over 200 LLM agents to navigate an explicit conflict between\negoistic (personal utility) and altruistic (system utility) goals. Our central\nfinding is a fundamental difference in the social tendencies of LLMs. We\nidentify two distinct archetypes: \"Adaptive Egoists\", which default to\nprioritizing self-interest but whose altruistic behaviors significantly\nincrease under the influence of a social norm-setting message board; and\n\"Altruistic Optimizers\", which exhibit an inherent altruistic logic,\nconsistently prioritizing collective benefit even at a direct cost to\nthemselves. Furthermore, to qualitatively analyze the cognitive underpinnings\nof these decisions, we introduce a method inspired by Grounded Theory to\nsystematically code agent reasoning. In summary, this research provides the\nfirst evidence of intrinsic heterogeneity in the egoistic and altruistic\ntendencies of different LLMs. We propose that for social simulation, model\nselection is not merely a matter of choosing reasoning capability, but of\nchoosing an intrinsic social action logic. While \"Adaptive Egoists\" may offer a\nmore suitable choice for simulating complex human societies, \"Altruistic\nOptimizers\" are better suited for modeling idealized pro-social actors or\nscenarios where collective welfare is the primary consideration."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Xiao Jia"
                    },
                    {
                        "name": "Zhanzhan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhanzhan Zhao"
                },
                "author": "Zhanzhan Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22536v1",
                "updated": "2025-09-26T16:16:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    16,
                    49,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:16:49Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    16,
                    49,
                    4,
                    269,
                    0
                ],
                "title": "InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced\n  Language Models"
                },
                "summary": "The immense computational cost of training Large Language Models (LLMs)\npresents a major barrier to innovation. While FP8 training offers a promising\nsolution with significant theoretical efficiency gains, its widespread adoption\nhas been hindered by the lack of a comprehensive, open-source training recipe.\nTo bridge this gap, we introduce an end-to-end FP8 training recipe that\nseamlessly integrates continual pre-training and supervised fine-tuning. Our\nmethodology employs a fine-grained, hybrid-granularity quantization strategy to\nmaintain numerical fidelity while maximizing computational efficiency. Through\nextensive experiments, including the continue pre-training of models on a\n160B-token corpus, we demonstrate that our recipe is not only remarkably stable\nbut also essentially lossless, achieving performance on par with the BF16\nbaseline across a suite of reasoning benchmarks. Crucially, this is achieved\nwith substantial efficiency improvements, including up to a 22% reduction in\ntraining time, a 14% decrease in peak memory usage, and a 19% increase in\nthroughput. Our results establish FP8 as a practical and robust alternative to\nBF16, and we will release the accompanying code to further democratize\nlarge-scale model training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The immense computational cost of training Large Language Models (LLMs)\npresents a major barrier to innovation. While FP8 training offers a promising\nsolution with significant theoretical efficiency gains, its widespread adoption\nhas been hindered by the lack of a comprehensive, open-source training recipe.\nTo bridge this gap, we introduce an end-to-end FP8 training recipe that\nseamlessly integrates continual pre-training and supervised fine-tuning. Our\nmethodology employs a fine-grained, hybrid-granularity quantization strategy to\nmaintain numerical fidelity while maximizing computational efficiency. Through\nextensive experiments, including the continue pre-training of models on a\n160B-token corpus, we demonstrate that our recipe is not only remarkably stable\nbut also essentially lossless, achieving performance on par with the BF16\nbaseline across a suite of reasoning benchmarks. Crucially, this is achieved\nwith substantial efficiency improvements, including up to a 22% reduction in\ntraining time, a 14% decrease in peak memory usage, and a 19% increase in\nthroughput. Our results establish FP8 as a practical and robust alternative to\nBF16, and we will release the accompanying code to further democratize\nlarge-scale model training."
                },
                "authors": [
                    {
                        "name": "Wenjun Wang"
                    },
                    {
                        "name": "Shuo Cai"
                    },
                    {
                        "name": "Congkai Xie"
                    },
                    {
                        "name": "Mingfa Feng"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Kejing Yang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Jiannong Cao"
                    },
                    {
                        "name": "Yuan Xie"
                    },
                    {
                        "name": "Hongxia Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hongxia Yang"
                },
                "author": "Hongxia Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22530v1",
                "updated": "2025-09-26T16:08:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    8,
                    58,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:08:58Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    8,
                    58,
                    4,
                    269,
                    0
                ],
                "title": "Boosting Pointer Analysis With Large Language Model-Enhanced Allocation\n  Function Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Pointer Analysis With Large Language Model-Enhanced Allocation\n  Function Detection"
                },
                "summary": "Pointer analysis is foundational for many static analysis tasks, yet its\neffectiveness is often hindered by imprecise modeling of heap allocations,\nparticularly in C/C++ programs where user-defined allocation functions (AFs)\nare pervasive. Existing approaches largely overlook these custom allocators,\nleading to coarse aliasing and reduced analysis precision. In this paper, we\npresent AFD, a novel technique that enhances pointer analysis by automatically\nidentifying and modeling custom allocation functions. AFD employs a hybrid\napproach: it uses value-flow analysis to detect straightforward wrappers and\nleverages Large Language Models (LLMs) to reason about more complex allocation\npatterns with side effects. This targeted enhancement enables precise modeling\nof heap objects at each call site, achieving context-sensitivity-like benefits\nwithout the associated overhead. We evaluate AFD on 15 real-world C projects,\nidentifying over 600 custom AFs. Integrating AFD into a baseline pointer\nanalysis yields a 26x increase in modeled heap objects and a 39% reduction in\nalias set sizes, with only 1.4x runtime overhead. Furthermore, our enhanced\nanalysis improves indirect call resolution and uncovers 17 previously\nundetected memory bugs. These results demonstrate that precise modeling of\ncustom allocation functions offers a scalable and practical path to improving\npointer analysis in large software systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pointer analysis is foundational for many static analysis tasks, yet its\neffectiveness is often hindered by imprecise modeling of heap allocations,\nparticularly in C/C++ programs where user-defined allocation functions (AFs)\nare pervasive. Existing approaches largely overlook these custom allocators,\nleading to coarse aliasing and reduced analysis precision. In this paper, we\npresent AFD, a novel technique that enhances pointer analysis by automatically\nidentifying and modeling custom allocation functions. AFD employs a hybrid\napproach: it uses value-flow analysis to detect straightforward wrappers and\nleverages Large Language Models (LLMs) to reason about more complex allocation\npatterns with side effects. This targeted enhancement enables precise modeling\nof heap objects at each call site, achieving context-sensitivity-like benefits\nwithout the associated overhead. We evaluate AFD on 15 real-world C projects,\nidentifying over 600 custom AFs. Integrating AFD into a baseline pointer\nanalysis yields a 26x increase in modeled heap objects and a 39% reduction in\nalias set sizes, with only 1.4x runtime overhead. Furthermore, our enhanced\nanalysis improves indirect call resolution and uncovers 17 previously\nundetected memory bugs. These results demonstrate that precise modeling of\ncustom allocation functions offers a scalable and practical path to improving\npointer analysis in large software systems."
                },
                "authors": [
                    {
                        "name": "Baijun Cheng"
                    },
                    {
                        "name": "Kailong Wang"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Peng Di"
                    },
                    {
                        "name": "Yao Guo"
                    },
                    {
                        "name": "Ding Li"
                    },
                    {
                        "name": "Xiangqun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiangqun Chen"
                },
                "author": "Xiangqun Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00907v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00907v3",
                "updated": "2025-09-26T16:06:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    6,
                    34,
                    4,
                    269,
                    0
                ],
                "published": "2025-04-01T15:41:50Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    15,
                    41,
                    50,
                    1,
                    91,
                    0
                ],
                "title": "Grounding Multimodal LLMs to Embodied Agents that Ask for Help with\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounding Multimodal LLMs to Embodied Agents that Ask for Help with\n  Reinforcement Learning"
                },
                "summary": "Embodied agents operating in household environments must interpret ambiguous\nand under-specified human instructions. A capable household robot should\nrecognize ambiguity and ask relevant clarification questions to infer the user\nintent accurately, leading to more effective task execution. To study this\nproblem, we introduce the Ask-to-Act task, where an embodied agent is tasked\nwith a single or multi-object rearrangement task using an under-specified\ninstruction in a home environment. The agent must strategically ask minimal,\nyet relevant, clarification questions to resolve ambiguity while navigating\nunder partial observability. To address this challenge, we propose a novel\napproach that fine-tunes multi-modal large language models (MLLMs) as\nvision-language-action (VLA) policies using online reinforcement learning (RL)\nwith LLM-generated rewards. Our method eliminates the need for large-scale\nhuman demonstrations or manually engineered rewards for training such agents.\nWe benchmark against strong zero-shot baselines including GPT-4o as well as\nsupervised fine-tuned MLLMs on our task. Our results show that our RL-finetuned\nMLLM outperforms all baselines by a significant margin (10.4-16.5%),\ngeneralizing well to novel scenes and tasks. To the best of our knowledge, this\nis the first demonstration of adapting MLLMs as VLA agents that can act and ask\nfor help using LLM-generated rewards with online RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied agents operating in household environments must interpret ambiguous\nand under-specified human instructions. A capable household robot should\nrecognize ambiguity and ask relevant clarification questions to infer the user\nintent accurately, leading to more effective task execution. To study this\nproblem, we introduce the Ask-to-Act task, where an embodied agent is tasked\nwith a single or multi-object rearrangement task using an under-specified\ninstruction in a home environment. The agent must strategically ask minimal,\nyet relevant, clarification questions to resolve ambiguity while navigating\nunder partial observability. To address this challenge, we propose a novel\napproach that fine-tunes multi-modal large language models (MLLMs) as\nvision-language-action (VLA) policies using online reinforcement learning (RL)\nwith LLM-generated rewards. Our method eliminates the need for large-scale\nhuman demonstrations or manually engineered rewards for training such agents.\nWe benchmark against strong zero-shot baselines including GPT-4o as well as\nsupervised fine-tuned MLLMs on our task. Our results show that our RL-finetuned\nMLLM outperforms all baselines by a significant margin (10.4-16.5%),\ngeneralizing well to novel scenes and tasks. To the best of our knowledge, this\nis the first demonstration of adapting MLLMs as VLA agents that can act and ask\nfor help using LLM-generated rewards with online RL."
                },
                "authors": [
                    {
                        "name": "Ram Ramrakhya"
                    },
                    {
                        "name": "Matthew Chang"
                    },
                    {
                        "name": "Xavier Puig"
                    },
                    {
                        "name": "Ruta Desai"
                    },
                    {
                        "name": "Zsolt Kira"
                    },
                    {
                        "name": "Roozbeh Mottaghi"
                    }
                ],
                "author_detail": {
                    "name": "Roozbeh Mottaghi"
                },
                "author": "Roozbeh Mottaghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00907v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00907v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18788v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18788v2",
                "updated": "2025-09-26T16:03:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    3,
                    0,
                    4,
                    269,
                    0
                ],
                "published": "2024-09-27T14:36:20Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    14,
                    36,
                    20,
                    4,
                    271,
                    0
                ],
                "title": "Excavating in the Wild: The GOOSE-Ex Dataset for Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Excavating in the Wild: The GOOSE-Ex Dataset for Semantic Segmentation"
                },
                "summary": "The successful deployment of deep learning-based techniques for autonomous\nsystems is highly dependent on the data availability for the respective system\nin its deployment environment. Especially for unstructured outdoor\nenvironments, very few datasets exist for even fewer robotic platforms and\nscenarios. In an earlier work, we presented the German Outdoor and Offroad\nDataset (GOOSE) framework along with 10000 multimodal frames from an offroad\nvehicle to enhance the perception capabilities in unstructured environments. In\nthis work, we address the generalizability of the GOOSE framework. To\naccomplish this, we open-source the GOOSE-Ex dataset, which contains additional\n5000 labeled multimodal frames from various completely different environments,\nrecorded on a robotic excavator and a quadruped platform. We perform a\ncomprehensive analysis of the semantic segmentation performance on different\nplatforms and sensor modalities in unseen environments. In addition, we\ndemonstrate how the combined datasets can be utilized for different downstream\napplications or competitions such as offroad navigation, object manipulation or\nscene completion. The dataset, its platform documentation and pre-trained\nstate-of-the-art models for offroad perception will be made available on\nhttps://goose-dataset.de/.\n  \\",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The successful deployment of deep learning-based techniques for autonomous\nsystems is highly dependent on the data availability for the respective system\nin its deployment environment. Especially for unstructured outdoor\nenvironments, very few datasets exist for even fewer robotic platforms and\nscenarios. In an earlier work, we presented the German Outdoor and Offroad\nDataset (GOOSE) framework along with 10000 multimodal frames from an offroad\nvehicle to enhance the perception capabilities in unstructured environments. In\nthis work, we address the generalizability of the GOOSE framework. To\naccomplish this, we open-source the GOOSE-Ex dataset, which contains additional\n5000 labeled multimodal frames from various completely different environments,\nrecorded on a robotic excavator and a quadruped platform. We perform a\ncomprehensive analysis of the semantic segmentation performance on different\nplatforms and sensor modalities in unseen environments. In addition, we\ndemonstrate how the combined datasets can be utilized for different downstream\napplications or competitions such as offroad navigation, object manipulation or\nscene completion. The dataset, its platform documentation and pre-trained\nstate-of-the-art models for offroad perception will be made available on\nhttps://goose-dataset.de/.\n  \\"
                },
                "authors": [
                    {
                        "name": "Raphael Hagmanns"
                    },
                    {
                        "name": "Peter Mortimer"
                    },
                    {
                        "name": "Miguel Granero"
                    },
                    {
                        "name": "Thorsten Luettel"
                    },
                    {
                        "name": "Janko Petereit"
                    }
                ],
                "author_detail": {
                    "name": "Janko Petereit"
                },
                "author": "Janko Petereit",
                "arxiv_doi": "10.1109/ICRA55743.2025.11127604",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICRA55743.2025.11127604",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.18788v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18788v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication at 2025 IEEE International Conference on\n  Robotics and Automation (ICRA)",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22518v1",
                "updated": "2025-09-26T16:02:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    2,
                    27,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:02:27Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    2,
                    27,
                    4,
                    269,
                    0
                ],
                "title": "REMA: A Unified Reasoning Manifold Framework for Interpreting Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REMA: A Unified Reasoning Manifold Framework for Interpreting Large\n  Language Model"
                },
                "summary": "Understanding how Large Language Models (LLMs) perform complex reasoning and\ntheir failure mechanisms is a challenge in interpretability research. To\nprovide a measurable geometric analysis perspective, we define the concept of\nthe Reasoning Manifold, a latent low-dimensional geometric structure formed by\nthe internal representations corresponding to all correctly reasoned\ngenerations. This structure can be conceptualized as the embodiment of the\neffective thinking paths that the model has learned to successfully solve a\ngiven task. Based on this concept, we build REMA, a framework that explains the\norigins of failures by quantitatively comparing the spatial relationships of\ninternal model representations corresponding to both erroneous and correct\nreasoning samples. Specifically, REMA first quantifies the geometric deviation\nof each erroneous representation by calculating its k-nearest neighbors\ndistance to the approximated manifold formed by correct representations,\nthereby providing a unified failure signal. It then localizes the divergence\npoints where these deviations first become significant by tracking this\ndeviation metric across the model's layers and comparing it against a baseline\nof internal fluctuations from correct representations, thus identifying where\nthe reasoning chain begins to go off-track. Our extensive experiments on\ndiverse language and multimodal models and tasks demonstrate the\nlow-dimensional nature of the reasoning manifold and the high separability\nbetween erroneous and correct reasoning representations. The results also\nvalidate the effectiveness of the REMA framework in analyzing the origins of\nreasoning failures. This research connects abstract reasoning failures to\nmeasurable geometric deviations in representations, providing new avenues for\nin-depth understanding and diagnosis of the internal computational processes of\nblack-box models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding how Large Language Models (LLMs) perform complex reasoning and\ntheir failure mechanisms is a challenge in interpretability research. To\nprovide a measurable geometric analysis perspective, we define the concept of\nthe Reasoning Manifold, a latent low-dimensional geometric structure formed by\nthe internal representations corresponding to all correctly reasoned\ngenerations. This structure can be conceptualized as the embodiment of the\neffective thinking paths that the model has learned to successfully solve a\ngiven task. Based on this concept, we build REMA, a framework that explains the\norigins of failures by quantitatively comparing the spatial relationships of\ninternal model representations corresponding to both erroneous and correct\nreasoning samples. Specifically, REMA first quantifies the geometric deviation\nof each erroneous representation by calculating its k-nearest neighbors\ndistance to the approximated manifold formed by correct representations,\nthereby providing a unified failure signal. It then localizes the divergence\npoints where these deviations first become significant by tracking this\ndeviation metric across the model's layers and comparing it against a baseline\nof internal fluctuations from correct representations, thus identifying where\nthe reasoning chain begins to go off-track. Our extensive experiments on\ndiverse language and multimodal models and tasks demonstrate the\nlow-dimensional nature of the reasoning manifold and the high separability\nbetween erroneous and correct reasoning representations. The results also\nvalidate the effectiveness of the REMA framework in analyzing the origins of\nreasoning failures. This research connects abstract reasoning failures to\nmeasurable geometric deviations in representations, providing new avenues for\nin-depth understanding and diagnosis of the internal computational processes of\nblack-box models."
                },
                "authors": [
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Guanzhi Deng"
                    },
                    {
                        "name": "Ronghao Chen"
                    },
                    {
                        "name": "Junrong Yue"
                    },
                    {
                        "name": "Shuo Zhang"
                    },
                    {
                        "name": "Qinghua Zhao"
                    },
                    {
                        "name": "Linqi Song"
                    },
                    {
                        "name": "Lijie Wen"
                    }
                ],
                "author_detail": {
                    "name": "Lijie Wen"
                },
                "author": "Lijie Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03918v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03918v2",
                "updated": "2025-09-26T15:57:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    57,
                    36,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-04T06:13:28Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    6,
                    13,
                    28,
                    3,
                    247,
                    0
                ],
                "title": "Chain or tree? Re-evaluating complex reasoning from the perspective of a\n  matrix of thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain or tree? Re-evaluating complex reasoning from the perspective of a\n  matrix of thought"
                },
                "summary": "Large Language Models (LLMs) face significant accuracy degradation due to\ninsufficient reasoning ability when dealing with complex and abstract tasks.\nThought structures such as Chain of Thought (CoT) and Tree of Thought (ToT)\nfocus on enhancing the reasoning capability of LLMs. However, they suffer from\ninherent drawbacks such as redundancy within the same layer of the tree\nstructure and the singularity of the paths in the chain structure. Some studies\nhave utilized Retrieval-Augmented Generation (RAG) methods to enhance CoT and\nToT in mitigating hallucinations in LLMs, yet the fundamental shortcomings of\nthe thought structures still persist. Furthermore, when dealing with\nmulti-entity and multi-hop information, the retrieved verification knowledge\noften contains large amounts of fragmented, superficial, or even erroneous\ndata, misleading the reasoning process of LLMs. To address these issues, we\npropose the Matrix of Thought (MoT), a novel and efficient thought structure\nfor LLMs. MoT explores problems in both horizontal and vertical dimensions\nthrough a \"column-cell communication\" mechanism, enabling LLMs to actively\nengage in multi-strategy and deep thinking while reducing redundancy in the\nthought nodes within the column cells, thereby enhancing the reasoning\ncapability of LLMs. Additionally, through a fact-correction mechanism, it\nleverages the knowledge graph triples retrieved by RAG and the original text to\nconstruct knowledge units and correct erroneous answers. To validate the\neffectiveness of this method, we conducted extensive experiments in three\ntasks: 24-point game, question answering evaluation, and proposition\nwriting.The results demonstrate that our framework outperforms state-of-the-art\nmethods, with reasoning time only 14.4\\% of that of the baseline method,\nproving its efficiency and accuracy. The code for framework is available at\nhttps://github.com/lyfiter/mtqa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face significant accuracy degradation due to\ninsufficient reasoning ability when dealing with complex and abstract tasks.\nThought structures such as Chain of Thought (CoT) and Tree of Thought (ToT)\nfocus on enhancing the reasoning capability of LLMs. However, they suffer from\ninherent drawbacks such as redundancy within the same layer of the tree\nstructure and the singularity of the paths in the chain structure. Some studies\nhave utilized Retrieval-Augmented Generation (RAG) methods to enhance CoT and\nToT in mitigating hallucinations in LLMs, yet the fundamental shortcomings of\nthe thought structures still persist. Furthermore, when dealing with\nmulti-entity and multi-hop information, the retrieved verification knowledge\noften contains large amounts of fragmented, superficial, or even erroneous\ndata, misleading the reasoning process of LLMs. To address these issues, we\npropose the Matrix of Thought (MoT), a novel and efficient thought structure\nfor LLMs. MoT explores problems in both horizontal and vertical dimensions\nthrough a \"column-cell communication\" mechanism, enabling LLMs to actively\nengage in multi-strategy and deep thinking while reducing redundancy in the\nthought nodes within the column cells, thereby enhancing the reasoning\ncapability of LLMs. Additionally, through a fact-correction mechanism, it\nleverages the knowledge graph triples retrieved by RAG and the original text to\nconstruct knowledge units and correct erroneous answers. To validate the\neffectiveness of this method, we conducted extensive experiments in three\ntasks: 24-point game, question answering evaluation, and proposition\nwriting.The results demonstrate that our framework outperforms state-of-the-art\nmethods, with reasoning time only 14.4\\% of that of the baseline method,\nproving its efficiency and accuracy. The code for framework is available at\nhttps://github.com/lyfiter/mtqa."
                },
                "authors": [
                    {
                        "name": "Fengxiao Tang"
                    },
                    {
                        "name": "Yufeng Li"
                    },
                    {
                        "name": "Zongzong Wu"
                    },
                    {
                        "name": "Ming Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Ming Zhao"
                },
                "author": "Ming Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03918v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03918v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22512v1",
                "updated": "2025-09-26T15:54:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    54,
                    50,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:54:50Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    54,
                    50,
                    4,
                    269,
                    0
                ],
                "title": "AxLLM: accelerator architecture for large language models with\n  computation reuse capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AxLLM: accelerator architecture for large language models with\n  computation reuse capability"
                },
                "summary": "Large language models demand massive computational power and memory\nresources, posing significant challenges for efficient deployment. While\nquantization has been widely explored to reduce model size and computation,\nthis paper demonstrates an additional benefit: quantization increases parameter\nlocality, creating opportunities for computation reuse. Building on this\ninsight, we propose AxLLM, a hardware accelerator architecture designed for\nquantized models. Axllm introduces a novel redundancy elimination technique\nthat caches and reuses multiplication results for repeated weight values,\nsubstantially reducing redundant operations. The architecture features dual\nmultiply and reuse pipelines, efficiently supporting both base models and LoRA\nfine-tuned models without altering parameters, retraining, or requiring offline\npreprocessing. Experimental results show that AxLLM achieves up to 90%\nreduction in computations, delivering 28% lower energy consumption and a 1.7x\nspeedup over baseline execution. These results highlight Axllm as a scalable\nand efficient solution for accelerating LLMs on specialized hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models demand massive computational power and memory\nresources, posing significant challenges for efficient deployment. While\nquantization has been widely explored to reduce model size and computation,\nthis paper demonstrates an additional benefit: quantization increases parameter\nlocality, creating opportunities for computation reuse. Building on this\ninsight, we propose AxLLM, a hardware accelerator architecture designed for\nquantized models. Axllm introduces a novel redundancy elimination technique\nthat caches and reuses multiplication results for repeated weight values,\nsubstantially reducing redundant operations. The architecture features dual\nmultiply and reuse pipelines, efficiently supporting both base models and LoRA\nfine-tuned models without altering parameters, retraining, or requiring offline\npreprocessing. Experimental results show that AxLLM achieves up to 90%\nreduction in computations, delivering 28% lower energy consumption and a 1.7x\nspeedup over baseline execution. These results highlight Axllm as a scalable\nand efficient solution for accelerating LLMs on specialized hardware."
                },
                "authors": [
                    {
                        "name": "Soroush Ahadi"
                    },
                    {
                        "name": "Mehdi Modarressi"
                    },
                    {
                        "name": "Masoud Daneshtalab"
                    }
                ],
                "author_detail": {
                    "name": "Masoud Daneshtalab"
                },
                "author": "Masoud Daneshtalab",
                "arxiv_comment": "7 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "n/a",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22510v1",
                "updated": "2025-09-26T15:52:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    52,
                    21,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:52:21Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    52,
                    21,
                    4,
                    269,
                    0
                ],
                "title": "We Think, Therefore We Align LLMs to Helpful, Harmless and Honest Before\n  They Go Wrong",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We Think, Therefore We Align LLMs to Helpful, Harmless and Honest Before\n  They Go Wrong"
                },
                "summary": "Alignment of Large Language Models (LLMs) along multiple\nobjectives-helpfulness, harmlessness, and honesty (HHH)-is critical for safe\nand reliable deployment. Prior work has used steering vector-small control\nsignals injected into hidden states-to guide LLM outputs, typically via\none-to-one (1-to-1) Transformer decoders. In this setting, optimizing a single\nalignment objective can inadvertently overwrite representations learned for\nother objectives, leading to catastrophic forgetting. More recent approaches\nextend steering vectors via one-to-many (1-to-N) Transformer decoders. While\nthis alleviates catastrophic forgetting, naive multi-branch designs optimize\neach objective independently, which can cause inference fragmentation-outputs\nacross HHH objectives may become inconsistent. We propose Adaptive Multi-Branch\nSteering (AMBS), a two-stage 1-to-N framework for unified and efficient\nmulti-objective alignment. In Stage I, post-attention hidden states of the\nTransformer layer are computed once to form a shared representation. In Stage\nII, this representation is cloned into parallel branches and steered via a\npolicy-reference mechanism, enabling objective-specific control while\nmaintaining cross-objective consistency. Empirical evaluations on Alpaca,\nBeaverTails, and TruthfulQA show that AMBS consistently improves HHH alignment\nacross multiple 7B LLM backbones. For example, on DeepSeek-7B, AMBS improves\naverage alignment scores by +32.4% and reduces unsafe outputs by 11.0% compared\nto a naive 1-to-N baseline, while remaining competitive with state-of-the-art\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment of Large Language Models (LLMs) along multiple\nobjectives-helpfulness, harmlessness, and honesty (HHH)-is critical for safe\nand reliable deployment. Prior work has used steering vector-small control\nsignals injected into hidden states-to guide LLM outputs, typically via\none-to-one (1-to-1) Transformer decoders. In this setting, optimizing a single\nalignment objective can inadvertently overwrite representations learned for\nother objectives, leading to catastrophic forgetting. More recent approaches\nextend steering vectors via one-to-many (1-to-N) Transformer decoders. While\nthis alleviates catastrophic forgetting, naive multi-branch designs optimize\neach objective independently, which can cause inference fragmentation-outputs\nacross HHH objectives may become inconsistent. We propose Adaptive Multi-Branch\nSteering (AMBS), a two-stage 1-to-N framework for unified and efficient\nmulti-objective alignment. In Stage I, post-attention hidden states of the\nTransformer layer are computed once to form a shared representation. In Stage\nII, this representation is cloned into parallel branches and steered via a\npolicy-reference mechanism, enabling objective-specific control while\nmaintaining cross-objective consistency. Empirical evaluations on Alpaca,\nBeaverTails, and TruthfulQA show that AMBS consistently improves HHH alignment\nacross multiple 7B LLM backbones. For example, on DeepSeek-7B, AMBS improves\naverage alignment scores by +32.4% and reduces unsafe outputs by 11.0% compared\nto a naive 1-to-N baseline, while remaining competitive with state-of-the-art\nmethods."
                },
                "authors": [
                    {
                        "name": "Gautam Siddharth Kashyap"
                    },
                    {
                        "name": "Mark Dras"
                    },
                    {
                        "name": "Usman Naseem"
                    }
                ],
                "author_detail": {
                    "name": "Usman Naseem"
                },
                "author": "Usman Naseem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18777v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18777v3",
                "updated": "2025-09-26T15:49:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    49,
                    44,
                    4,
                    269,
                    0
                ],
                "published": "2025-05-24T16:30:13Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    16,
                    30,
                    13,
                    5,
                    144,
                    0
                ],
                "title": "HD-PiSSA: High-Rank Distributed Orthogonal Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HD-PiSSA: High-Rank Distributed Orthogonal Adaptation"
                },
                "summary": "Existing parameter-efficient fine-tuning (PEFT) methods for large language\nmodels (LLMs), such as LoRA and PiSSA, constrain model updates to low-rank\nsubspaces, limiting their expressiveness and leading to suboptimal performance\non complex tasks. To address this, we introduce High-rank Distributed PiSSA\n(HD-PiSSA), a distributed PEFT approach that initializes orthogonal adapters\nacross different devices and aggregates their delta updates collectively on W\nfor fine-tuning. Unlike Data Parallel LoRA or PiSSA, which maintain identical\nadapters across all devices, HD-PiSSA assigns different principal components of\nthe pre-trained weights to each GPU, significantly expanding the range of\nupdate directions. This results in over 16x higher effective updated ranks than\ndata-parallel LoRA or PiSSA when fine-tuning on 8 GPUs with the same per-device\nadapter rank. Empirically, we evaluate HD-PiSSA across various challenging\ndownstream tasks, including mathematics, code generation, and multi-task\nlearning. In the multi-task setting, HD-PiSSA achieves average gains of 10.0\nabsolute points (14.63%) over LoRA and 4.98 points (6.60%) over PiSSA across 12\nbenchmarks, demonstrating its benefits from the extra optimization flexibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing parameter-efficient fine-tuning (PEFT) methods for large language\nmodels (LLMs), such as LoRA and PiSSA, constrain model updates to low-rank\nsubspaces, limiting their expressiveness and leading to suboptimal performance\non complex tasks. To address this, we introduce High-rank Distributed PiSSA\n(HD-PiSSA), a distributed PEFT approach that initializes orthogonal adapters\nacross different devices and aggregates their delta updates collectively on W\nfor fine-tuning. Unlike Data Parallel LoRA or PiSSA, which maintain identical\nadapters across all devices, HD-PiSSA assigns different principal components of\nthe pre-trained weights to each GPU, significantly expanding the range of\nupdate directions. This results in over 16x higher effective updated ranks than\ndata-parallel LoRA or PiSSA when fine-tuning on 8 GPUs with the same per-device\nadapter rank. Empirically, we evaluate HD-PiSSA across various challenging\ndownstream tasks, including mathematics, code generation, and multi-task\nlearning. In the multi-task setting, HD-PiSSA achieves average gains of 10.0\nabsolute points (14.63%) over LoRA and 4.98 points (6.60%) over PiSSA across 12\nbenchmarks, demonstrating its benefits from the extra optimization flexibility."
                },
                "authors": [
                    {
                        "name": "Yiding Wang"
                    },
                    {
                        "name": "Fauxu Meng"
                    },
                    {
                        "name": "Xuefeng Zhang"
                    },
                    {
                        "name": "Fan Jiang"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18777v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18777v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22506v1",
                "updated": "2025-09-26T15:48:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    48,
                    10,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:48:10Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    48,
                    10,
                    4,
                    269,
                    0
                ],
                "title": "Representing LLMs in Prompt Semantic Task Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representing LLMs in Prompt Semantic Task Space"
                },
                "summary": "Large language models (LLMs) achieve impressive results over various tasks,\nand ever-expanding public repositories contain an abundance of pre-trained\nmodels. Therefore, identifying the best-performing LLM for a given task is a\nsignificant challenge. Previous works have suggested learning LLM\nrepresentations to address this. However, these approaches present limited\nscalability and require costly retraining to encompass additional models and\ndatasets. Moreover, the produced representation utilizes distinct spaces that\ncannot be easily interpreted. This work presents an efficient, training-free\napproach to representing LLMs as linear operators within the prompts' semantic\ntask space, thus providing a highly interpretable representation of the models'\napplication. Our method utilizes closed-form computation of geometrical\nproperties and ensures exceptional scalability and real-time adaptability to\ndynamically expanding repositories. We demonstrate our approach on success\nprediction and model selection tasks, achieving competitive or state-of-the-art\nresults with notable performance in out-of-sample scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve impressive results over various tasks,\nand ever-expanding public repositories contain an abundance of pre-trained\nmodels. Therefore, identifying the best-performing LLM for a given task is a\nsignificant challenge. Previous works have suggested learning LLM\nrepresentations to address this. However, these approaches present limited\nscalability and require costly retraining to encompass additional models and\ndatasets. Moreover, the produced representation utilizes distinct spaces that\ncannot be easily interpreted. This work presents an efficient, training-free\napproach to representing LLMs as linear operators within the prompts' semantic\ntask space, thus providing a highly interpretable representation of the models'\napplication. Our method utilizes closed-form computation of geometrical\nproperties and ensures exceptional scalability and real-time adaptability to\ndynamically expanding repositories. We demonstrate our approach on success\nprediction and model selection tasks, achieving competitive or state-of-the-art\nresults with notable performance in out-of-sample scenarios."
                },
                "authors": [
                    {
                        "name": "Idan Kashani"
                    },
                    {
                        "name": "Avi Mendelson"
                    },
                    {
                        "name": "Yaniv Nemcovsky"
                    }
                ],
                "author_detail": {
                    "name": "Yaniv Nemcovsky"
                },
                "author": "Yaniv Nemcovsky",
                "arxiv_comment": "Accepted to Findings of the Association for Computational\n  Linguistics: EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68T50, 65F20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19817v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19817v2",
                "updated": "2025-09-26T15:44:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    44,
                    51,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-24T06:56:26Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    6,
                    56,
                    26,
                    2,
                    267,
                    0
                ],
                "title": "MMedFD: A Real-world Healthcare Benchmark for Multi-turn Full-Duplex\n  Automatic Speech Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMedFD: A Real-world Healthcare Benchmark for Multi-turn Full-Duplex\n  Automatic Speech Recognition"
                },
                "summary": "Automatic speech recognition (ASR) in clinical dialogue demands robustness to\nfull-duplex interaction, speaker overlap, and low-latency constraints, yet open\nbenchmarks remain scarce. We present MMedFD, the first real-world Chinese\nhealthcare ASR corpus designed for multi-turn, full-duplex settings. Captured\nfrom a deployed AI assistant, the dataset comprises 5,805 annotated sessions\nwith synchronized user and mixed-channel views, RTTM/CTM timing, and role\nlabels. We introduce a model-agnostic pipeline for streaming segmentation,\nspeaker attribution, and dialogue memory, and fine-tune Whisper-small on\nrole-concatenated audio for long-context recognition. ASR evaluation includes\nWER, CER, and HC-WER, which measures concept-level accuracy across healthcare\nsettings. LLM-generated responses are assessed using rubric-based and pairwise\nprotocols. MMedFD establishes a reproducible framework for benchmarking\nstreaming ASR and end-to-end duplex agents in healthcare deployment. The\ndataset and related resources are publicly available at\nhttps://github.com/Kinetics-JOJO/MMedFD",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic speech recognition (ASR) in clinical dialogue demands robustness to\nfull-duplex interaction, speaker overlap, and low-latency constraints, yet open\nbenchmarks remain scarce. We present MMedFD, the first real-world Chinese\nhealthcare ASR corpus designed for multi-turn, full-duplex settings. Captured\nfrom a deployed AI assistant, the dataset comprises 5,805 annotated sessions\nwith synchronized user and mixed-channel views, RTTM/CTM timing, and role\nlabels. We introduce a model-agnostic pipeline for streaming segmentation,\nspeaker attribution, and dialogue memory, and fine-tune Whisper-small on\nrole-concatenated audio for long-context recognition. ASR evaluation includes\nWER, CER, and HC-WER, which measures concept-level accuracy across healthcare\nsettings. LLM-generated responses are assessed using rubric-based and pairwise\nprotocols. MMedFD establishes a reproducible framework for benchmarking\nstreaming ASR and end-to-end duplex agents in healthcare deployment. The\ndataset and related resources are publicly available at\nhttps://github.com/Kinetics-JOJO/MMedFD"
                },
                "authors": [
                    {
                        "name": "Hongzhao Chen"
                    },
                    {
                        "name": "XiaoYang Wang"
                    },
                    {
                        "name": "Jing Lan"
                    },
                    {
                        "name": "Hexiao Ding"
                    },
                    {
                        "name": "Yufeng Jiang"
                    },
                    {
                        "name": "MingHui Yang"
                    },
                    {
                        "name": "DanHui Xu"
                    },
                    {
                        "name": "Jun Luo"
                    },
                    {
                        "name": "Nga-Chun Ng"
                    },
                    {
                        "name": "Gerald W. Y. Cheng"
                    },
                    {
                        "name": "Yunlin Mao"
                    },
                    {
                        "name": "Jung Sun Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Jung Sun Yoo"
                },
                "author": "Jung Sun Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19817v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19817v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22502v1",
                "updated": "2025-09-26T15:44:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    44,
                    9,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:44:09Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    44,
                    9,
                    4,
                    269,
                    0
                ],
                "title": "InfiAgent: Self-Evolving Pyramid Agent Framework for Infinite Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiAgent: Self-Evolving Pyramid Agent Framework for Infinite Scenarios"
                },
                "summary": "Large Language Model (LLM) agents have demonstrated remarkable capabilities\nin organizing and executing complex tasks, and many such agents are now widely\nused in various application scenarios. However, developing these agents\nrequires carefully designed workflows, carefully crafted prompts, and iterative\ntuning, which requires LLM techniques and domain-specific expertise. These\nhand-crafted limitations hinder the scalability and cost-effectiveness of LLM\nagents across a wide range of industries. To address these challenges, we\npropose \\textbf{InfiAgent}, a Pyramid-like DAG-based Multi-Agent Framework that\ncan be applied to \\textbf{infi}nite scenarios, which introduces several key\ninnovations: a generalized \"agent-as-a-tool\" mechanism that automatically\ndecomposes complex agents into hierarchical multi-agent systems; a dual-audit\nmechanism that ensures the quality and stability of task completion; an agent\nrouting function that enables efficient task-agent matching; and an agent\nself-evolution mechanism that autonomously restructures the agent DAG based on\nnew tasks, poor performance, or optimization opportunities. Furthermore,\nInfiAgent's atomic task design supports agent parallelism, significantly\nimproving execution efficiency. This framework evolves into a versatile\npyramid-like multi-agent system capable of solving a wide range of problems.\nEvaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\\%\nhigher performance compared to ADAS (similar auto-generated agent framework),\nwhile a case study of the AI research assistant InfiHelper shows that it\ngenerates scientific papers that have received recognition from human reviewers\nat top-tier IEEE conferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents have demonstrated remarkable capabilities\nin organizing and executing complex tasks, and many such agents are now widely\nused in various application scenarios. However, developing these agents\nrequires carefully designed workflows, carefully crafted prompts, and iterative\ntuning, which requires LLM techniques and domain-specific expertise. These\nhand-crafted limitations hinder the scalability and cost-effectiveness of LLM\nagents across a wide range of industries. To address these challenges, we\npropose \\textbf{InfiAgent}, a Pyramid-like DAG-based Multi-Agent Framework that\ncan be applied to \\textbf{infi}nite scenarios, which introduces several key\ninnovations: a generalized \"agent-as-a-tool\" mechanism that automatically\ndecomposes complex agents into hierarchical multi-agent systems; a dual-audit\nmechanism that ensures the quality and stability of task completion; an agent\nrouting function that enables efficient task-agent matching; and an agent\nself-evolution mechanism that autonomously restructures the agent DAG based on\nnew tasks, poor performance, or optimization opportunities. Furthermore,\nInfiAgent's atomic task design supports agent parallelism, significantly\nimproving execution efficiency. This framework evolves into a versatile\npyramid-like multi-agent system capable of solving a wide range of problems.\nEvaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\\%\nhigher performance compared to ADAS (similar auto-generated agent framework),\nwhile a case study of the AI research assistant InfiHelper shows that it\ngenerates scientific papers that have received recognition from human reviewers\nat top-tier IEEE conferences."
                },
                "authors": [
                    {
                        "name": "Chenglin Yu"
                    },
                    {
                        "name": "Yang Yu"
                    },
                    {
                        "name": "Songmiao Wang"
                    },
                    {
                        "name": "Yucheng Wang"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Jinjia Li"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Hongxia Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hongxia Yang"
                },
                "author": "Hongxia Yang",
                "arxiv_comment": "9 pages of main content and 32 pages of others, 2 figures, under\n  review as a conference paper at ICLR 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22490v1",
                "updated": "2025-09-26T15:35:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    35,
                    38,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:35:38Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    35,
                    38,
                    4,
                    269,
                    0
                ],
                "title": "JGU Mainz's Submission to the WMT25 Shared Task on LLMs with Limited\n  Resources for Slavic Languages: MT and QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JGU Mainz's Submission to the WMT25 Shared Task on LLMs with Limited\n  Resources for Slavic Languages: MT and QA"
                },
                "summary": "This paper presents the JGU Mainz submission to the WMT25 Shared Task on LLMs\nwith Limited Resources for Slavic Languages: Machine Translation and Question\nAnswering, focusing on Ukrainian, Upper Sorbian, and Lower Sorbian. For each\nlanguage, we jointly fine-tune a Qwen2.5-3B-Instruct model for both tasks with\nparameter-efficient finetuning. Our pipeline integrates additional translation\nand multiple-choice question answering (QA) data. For Ukrainian QA, we further\nuse retrieval-augmented generation. We also apply ensembling for QA in Upper\nand Lower Sorbian. Experiments show that our models outperform the baseline on\nboth tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the JGU Mainz submission to the WMT25 Shared Task on LLMs\nwith Limited Resources for Slavic Languages: Machine Translation and Question\nAnswering, focusing on Ukrainian, Upper Sorbian, and Lower Sorbian. For each\nlanguage, we jointly fine-tune a Qwen2.5-3B-Instruct model for both tasks with\nparameter-efficient finetuning. Our pipeline integrates additional translation\nand multiple-choice question answering (QA) data. For Ukrainian QA, we further\nuse retrieval-augmented generation. We also apply ensembling for QA in Upper\nand Lower Sorbian. Experiments show that our models outperform the baseline on\nboth tasks."
                },
                "authors": [
                    {
                        "name": "Hossain Shaikh Saadi"
                    },
                    {
                        "name": "Minh Duc Bui"
                    },
                    {
                        "name": "Mario Sanz-Guerrero"
                    },
                    {
                        "name": "Katharina von der Wense"
                    }
                ],
                "author_detail": {
                    "name": "Katharina von der Wense"
                },
                "author": "Katharina von der Wense",
                "arxiv_comment": "WMT 25 Shared Task LLMs with Limited Resources for Slavic Languages:\n  MT and QA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22486v1",
                "updated": "2025-09-26T15:33:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    33,
                    36,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:33:36Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    33,
                    36,
                    4,
                    269,
                    0
                ],
                "title": "Your RAG is Unfair: Exposing Fairness Vulnerabilities in\n  Retrieval-Augmented Generation via Backdoor Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your RAG is Unfair: Exposing Fairness Vulnerabilities in\n  Retrieval-Augmented Generation via Backdoor Attacks"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances factual grounding by\nintegrating retrieval mechanisms with generative models but introduces new\nattack surfaces, particularly through backdoor attacks. While prior research\nhas largely focused on disinformation threats, fairness vulnerabilities remain\nunderexplored. Unlike conventional backdoors that rely on direct\ntrigger-to-target mappings, fairness-driven attacks exploit the interaction\nbetween retrieval and generation models, manipulating semantic relationships\nbetween target groups and social biases to establish a persistent and covert\ninfluence on content generation.\n  This paper introduces BiasRAG, a systematic framework that exposes fairness\nvulnerabilities in RAG through a two-phase backdoor attack. During the\npre-training phase, the query encoder is compromised to align the target group\nwith the intended social bias, ensuring long-term persistence. In the\npost-deployment phase, adversarial documents are injected into knowledge bases\nto reinforce the backdoor, subtly influencing retrieved content while remaining\nundetectable under standard fairness evaluations. Together, BiasRAG ensures\nprecise target alignment over sensitive attributes, stealthy execution, and\nresilience. Empirical evaluations demonstrate that BiasRAG achieves high attack\nsuccess rates while preserving contextual relevance and utility, establishing a\npersistent and evolving threat to fairness in RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances factual grounding by\nintegrating retrieval mechanisms with generative models but introduces new\nattack surfaces, particularly through backdoor attacks. While prior research\nhas largely focused on disinformation threats, fairness vulnerabilities remain\nunderexplored. Unlike conventional backdoors that rely on direct\ntrigger-to-target mappings, fairness-driven attacks exploit the interaction\nbetween retrieval and generation models, manipulating semantic relationships\nbetween target groups and social biases to establish a persistent and covert\ninfluence on content generation.\n  This paper introduces BiasRAG, a systematic framework that exposes fairness\nvulnerabilities in RAG through a two-phase backdoor attack. During the\npre-training phase, the query encoder is compromised to align the target group\nwith the intended social bias, ensuring long-term persistence. In the\npost-deployment phase, adversarial documents are injected into knowledge bases\nto reinforce the backdoor, subtly influencing retrieved content while remaining\nundetectable under standard fairness evaluations. Together, BiasRAG ensures\nprecise target alignment over sensitive attributes, stealthy execution, and\nresilience. Empirical evaluations demonstrate that BiasRAG achieves high attack\nsuccess rates while preserving contextual relevance and utility, establishing a\npersistent and evolving threat to fairness in RAG."
                },
                "authors": [
                    {
                        "name": "Gaurav Bagwe"
                    },
                    {
                        "name": "Saket S. Chaturvedi"
                    },
                    {
                        "name": "Xiaolong Ma"
                    },
                    {
                        "name": "Xiaoyong Yuan"
                    },
                    {
                        "name": "Kuang-Ching Wang"
                    },
                    {
                        "name": "Lan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lan Zhang"
                },
                "author": "Lan Zhang",
                "arxiv_comment": "Accepted by EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10739v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10739v2",
                "updated": "2025-09-26T15:28:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    28,
                    16,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-12T22:58:05Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    22,
                    58,
                    5,
                    4,
                    255,
                    0
                ],
                "title": "Reasoning Under Uncertainty: Exploring Probabilistic Reasoning\n  Capabilities of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Under Uncertainty: Exploring Probabilistic Reasoning\n  Capabilities of LLMs"
                },
                "summary": "Despite widespread success in language understanding and generation, large\nlanguage models (LLMs) exhibit unclear and often inconsistent behavior when\nfaced with tasks that require probabilistic reasoning. In this work, we present\nthe first comprehensive study of the reasoning capabilities of LLMs over\nexplicit discrete probability distributions. Given observations from a\nprobability distribution, we evaluate models on three carefully designed tasks,\nmode identification, maximum likelihood estimation, and sample generation, by\nprompting them to provide responses to queries about either the joint\ndistribution or its conditionals. These tasks thus probe a range of\nprobabilistic skills, including frequency analysis, marginalization, and\ngenerative behavior. Through comprehensive empirical evaluations, we\ndemonstrate that there exists a clear performance gap between smaller and\nlarger models, with the latter demonstrating stronger inference and surprising\ncapabilities in sample generation. Furthermore, our investigations reveal\nnotable limitations, including sensitivity to variations in the notation\nutilized to represent probabilistic outcomes and performance degradation of\nover 60% as context length increases. Together, our results provide a detailed\nunderstanding of the probabilistic reasoning abilities of LLMs and identify key\ndirections for future improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite widespread success in language understanding and generation, large\nlanguage models (LLMs) exhibit unclear and often inconsistent behavior when\nfaced with tasks that require probabilistic reasoning. In this work, we present\nthe first comprehensive study of the reasoning capabilities of LLMs over\nexplicit discrete probability distributions. Given observations from a\nprobability distribution, we evaluate models on three carefully designed tasks,\nmode identification, maximum likelihood estimation, and sample generation, by\nprompting them to provide responses to queries about either the joint\ndistribution or its conditionals. These tasks thus probe a range of\nprobabilistic skills, including frequency analysis, marginalization, and\ngenerative behavior. Through comprehensive empirical evaluations, we\ndemonstrate that there exists a clear performance gap between smaller and\nlarger models, with the latter demonstrating stronger inference and surprising\ncapabilities in sample generation. Furthermore, our investigations reveal\nnotable limitations, including sensitivity to variations in the notation\nutilized to represent probabilistic outcomes and performance degradation of\nover 60% as context length increases. Together, our results provide a detailed\nunderstanding of the probabilistic reasoning abilities of LLMs and identify key\ndirections for future improvement."
                },
                "authors": [
                    {
                        "name": "Mobina Pournemat"
                    },
                    {
                        "name": "Keivan Rezaei"
                    },
                    {
                        "name": "Gaurang Sriramanan"
                    },
                    {
                        "name": "Arman Zarei"
                    },
                    {
                        "name": "Jiaxiang Fu"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Hamid Eghbalzadeh"
                    },
                    {
                        "name": "Soheil Feizi"
                    }
                ],
                "author_detail": {
                    "name": "Soheil Feizi"
                },
                "author": "Soheil Feizi",
                "arxiv_comment": "27 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10739v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10739v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22480v1",
                "updated": "2025-09-26T15:27:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    27,
                    50,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:27:50Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    27,
                    50,
                    4,
                    269,
                    0
                ],
                "title": "Exploring Solution Divergence and Its Effect on Large Language Model\n  Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Solution Divergence and Its Effect on Large Language Model\n  Problem Solving"
                },
                "summary": "Large language models (LLMs) have been widely used for problem-solving tasks.\nMost recent work improves their performance through supervised fine-tuning\n(SFT) with labeled data or reinforcement learning (RL) from task feedback. In\nthis paper, we study a new perspective: the divergence in solutions generated\nby LLMs for a single problem. We show that higher solution divergence is\npositively related to better problem-solving abilities across various models.\nBased on this finding, we propose solution divergence as a novel metric that\ncan support both SFT and RL strategies. We test this idea on three\nrepresentative problem domains and find that using solution divergence\nconsistently improves success rates. These results suggest that solution\ndivergence is a simple but effective tool for advancing LLM training and\nevaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely used for problem-solving tasks.\nMost recent work improves their performance through supervised fine-tuning\n(SFT) with labeled data or reinforcement learning (RL) from task feedback. In\nthis paper, we study a new perspective: the divergence in solutions generated\nby LLMs for a single problem. We show that higher solution divergence is\npositively related to better problem-solving abilities across various models.\nBased on this finding, we propose solution divergence as a novel metric that\ncan support both SFT and RL strategies. We test this idea on three\nrepresentative problem domains and find that using solution divergence\nconsistently improves success rates. These results suggest that solution\ndivergence is a simple but effective tool for advancing LLM training and\nevaluation."
                },
                "authors": [
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Kaiqi Yang"
                    },
                    {
                        "name": "Yucheng Chu"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Jiliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jiliang Tang"
                },
                "author": "Jiliang Tang",
                "arxiv_comment": "17 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16134v2",
                "updated": "2025-09-26T15:21:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    21,
                    49,
                    4,
                    269,
                    0
                ],
                "published": "2025-05-22T02:23:00Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    2,
                    23,
                    0,
                    3,
                    142,
                    0
                ],
                "title": "Beyond Early-Token Bias: Model-Specific and Language-Specific Position\n  Effects in Multilingual LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Early-Token Bias: Model-Specific and Language-Specific Position\n  Effects in Multilingual LLMs"
                },
                "summary": "Large Language Models (LLMs) exhibit position bias - a systematic tendency to\nneglect information at specific context positions. However, the patterns of\nposition bias behavior, depending on the language or model, remain unexplored.\nWe present a multilingual study across five typologically distinct languages\n(English, Russian, German, Hindi, and Vietnamese) and five model architectures,\nexamining how position bias interacts with prompt strategies and affects output\nentropy. Our key findings are: (1) Position bias is primarily model-driven, yet\nexhibits language-specific variations. For instance, Qwen2.5-7B-Instruct and\nDeepSeek 7B Chat consistently favors late positions, challenging established\nassumptions of a universal early-token bias in LLMs. (2) Explicitly instructing\nthe model that \"the context is relevant to the query\" unexpectedly reduces\naccuracy across languages, undermining common prompt-engineering practices. (3)\nWhile the largest accuracy drop occurs when relevant information is placed in\nthe middle of the context, this is not explicitly reflected by a corresponding\npeak in output entropy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit position bias - a systematic tendency to\nneglect information at specific context positions. However, the patterns of\nposition bias behavior, depending on the language or model, remain unexplored.\nWe present a multilingual study across five typologically distinct languages\n(English, Russian, German, Hindi, and Vietnamese) and five model architectures,\nexamining how position bias interacts with prompt strategies and affects output\nentropy. Our key findings are: (1) Position bias is primarily model-driven, yet\nexhibits language-specific variations. For instance, Qwen2.5-7B-Instruct and\nDeepSeek 7B Chat consistently favors late positions, challenging established\nassumptions of a universal early-token bias in LLMs. (2) Explicitly instructing\nthe model that \"the context is relevant to the query\" unexpectedly reduces\naccuracy across languages, undermining common prompt-engineering practices. (3)\nWhile the largest accuracy drop occurs when relevant information is placed in\nthe middle of the context, this is not explicitly reflected by a corresponding\npeak in output entropy."
                },
                "authors": [
                    {
                        "name": "Mikhail Menschikov"
                    },
                    {
                        "name": "Alexander Kharitonov"
                    },
                    {
                        "name": "Maiia Kotyga"
                    },
                    {
                        "name": "Vadim Porvatov"
                    },
                    {
                        "name": "Anna Zhukovskaya"
                    },
                    {
                        "name": "David Kagramanyan"
                    },
                    {
                        "name": "Egor Shvetsov"
                    },
                    {
                        "name": "Evgeny Burnaev"
                    }
                ],
                "author_detail": {
                    "name": "Evgeny Burnaev"
                },
                "author": "Evgeny Burnaev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14364v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14364v4",
                "updated": "2025-09-26T15:19:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    19,
                    34,
                    4,
                    269,
                    0
                ],
                "published": "2024-09-22T08:51:18Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    8,
                    51,
                    18,
                    6,
                    266,
                    0
                ],
                "title": "Position IDs Matter: An Enhanced Position Layout for Efficient Context\n  Compression in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position IDs Matter: An Enhanced Position Layout for Efficient Context\n  Compression in Large Language Models"
                },
                "summary": "Using special tokens (e.g., gist, memory, or compressed tokens) to compress\ncontext information is a common practice for large language models (LLMs).\nHowever, existing approaches often neglect that position encodings inherently\ninduce local inductive biases in models, causing the compression process to\nignore holistic contextual dependencies. We propose \\textbf{Enhanced Position\nLayout (EPL)}, a simple yet effective method that improves the context\ncompression capability of LLMs by only adjusting position IDs, the numerical\nidentifiers that specify token positions. EPL minimizes the distance between\ncontext tokens and their corresponding special tokens and at the same time\nmaintains the sequence order in position IDs between context tokens, special\ntokens, and the subsequent tokens. Integrating EPL into our best performing\ncontext compression model results in a 1.9 ROUGE-1 F1 improvement on\nout-of-domain question answering datasets on average. When extended to\nmultimodal scenarios, EPL leads to an average accuracy gain of 2.6 points for\nvision compression LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using special tokens (e.g., gist, memory, or compressed tokens) to compress\ncontext information is a common practice for large language models (LLMs).\nHowever, existing approaches often neglect that position encodings inherently\ninduce local inductive biases in models, causing the compression process to\nignore holistic contextual dependencies. We propose \\textbf{Enhanced Position\nLayout (EPL)}, a simple yet effective method that improves the context\ncompression capability of LLMs by only adjusting position IDs, the numerical\nidentifiers that specify token positions. EPL minimizes the distance between\ncontext tokens and their corresponding special tokens and at the same time\nmaintains the sequence order in position IDs between context tokens, special\ntokens, and the subsequent tokens. Integrating EPL into our best performing\ncontext compression model results in a 1.9 ROUGE-1 F1 improvement on\nout-of-domain question answering datasets on average. When extended to\nmultimodal scenarios, EPL leads to an average accuracy gain of 2.6 points for\nvision compression LLMs."
                },
                "authors": [
                    {
                        "name": "Runsong Zhao"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Pengcheng Huang"
                    },
                    {
                        "name": "Chunyang Xiao"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14364v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14364v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22472v1",
                "updated": "2025-09-26T15:19:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    19,
                    12,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:19:12Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    19,
                    12,
                    4,
                    269,
                    0
                ],
                "title": "Evaluating the Limits of Large Language Models in Multilingual Legal\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Limits of Large Language Models in Multilingual Legal\n  Reasoning"
                },
                "summary": "In an era dominated by Large Language Models (LLMs), understanding their\ncapabilities and limitations, especially in high-stakes fields like law, is\ncrucial. While LLMs such as Meta's LLaMA, OpenAI's ChatGPT, Google's Gemini,\nDeepSeek, and other emerging models are increasingly integrated into legal\nworkflows, their performance in multilingual, jurisdictionally diverse, and\nadversarial contexts remains insufficiently explored. This work evaluates LLaMA\nand Gemini on multilingual legal and non-legal benchmarks, and assesses their\nadversarial robustness in legal tasks through character and word-level\nperturbations. We use an LLM-as-a-Judge approach for human-aligned evaluation.\nWe moreover present an open-source, modular evaluation pipeline designed to\nsupport multilingual, task-diverse benchmarking of any combination of LLMs and\ndatasets, with a particular focus on legal tasks, including classification,\nsummarization, open questions, and general reasoning. Our findings confirm that\nlegal tasks pose significant challenges for LLMs with accuracies often below\n50% on legal reasoning benchmarks such as LEXam, compared to over 70% on\ngeneral-purpose tasks like XNLI. In addition, while English generally yields\nmore stable results, it does not always lead to higher accuracy. Prompt\nsensitivity and adversarial vulnerability is also shown to persist across\nlanguages. Finally, a correlation is found between the performance of a\nlanguage and its syntactic similarity to English. We also observe that LLaMA is\nweaker than Gemini, with the latter showing an average advantage of about 24\npercentage points across the same task. Despite improvements in newer LLMs,\nchallenges remain in deploying them reliably for critical, multilingual legal\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an era dominated by Large Language Models (LLMs), understanding their\ncapabilities and limitations, especially in high-stakes fields like law, is\ncrucial. While LLMs such as Meta's LLaMA, OpenAI's ChatGPT, Google's Gemini,\nDeepSeek, and other emerging models are increasingly integrated into legal\nworkflows, their performance in multilingual, jurisdictionally diverse, and\nadversarial contexts remains insufficiently explored. This work evaluates LLaMA\nand Gemini on multilingual legal and non-legal benchmarks, and assesses their\nadversarial robustness in legal tasks through character and word-level\nperturbations. We use an LLM-as-a-Judge approach for human-aligned evaluation.\nWe moreover present an open-source, modular evaluation pipeline designed to\nsupport multilingual, task-diverse benchmarking of any combination of LLMs and\ndatasets, with a particular focus on legal tasks, including classification,\nsummarization, open questions, and general reasoning. Our findings confirm that\nlegal tasks pose significant challenges for LLMs with accuracies often below\n50% on legal reasoning benchmarks such as LEXam, compared to over 70% on\ngeneral-purpose tasks like XNLI. In addition, while English generally yields\nmore stable results, it does not always lead to higher accuracy. Prompt\nsensitivity and adversarial vulnerability is also shown to persist across\nlanguages. Finally, a correlation is found between the performance of a\nlanguage and its syntactic similarity to English. We also observe that LLaMA is\nweaker than Gemini, with the latter showing an average advantage of about 24\npercentage points across the same task. Despite improvements in newer LLMs,\nchallenges remain in deploying them reliably for critical, multilingual legal\napplications."
                },
                "authors": [
                    {
                        "name": "Antreas Ioannou"
                    },
                    {
                        "name": "Andreas Shiamishis"
                    },
                    {
                        "name": "Nora Hollenstein"
                    },
                    {
                        "name": "Nezihe Merve Gürel"
                    }
                ],
                "author_detail": {
                    "name": "Nezihe Merve Gürel"
                },
                "author": "Nezihe Merve Gürel",
                "arxiv_comment": "39 pages, 36 figures. Code and evaluation pipeline available at\n  https://github.com/RobustML-Lab/Legal-Multilingual-Evaluation-of-LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00666v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00666v3",
                "updated": "2025-09-26T15:18:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    18,
                    48,
                    4,
                    269,
                    0
                ],
                "published": "2025-02-02T04:40:04Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    4,
                    40,
                    4,
                    6,
                    33,
                    0
                ],
                "title": "Avoiding $\\mathbf{exp(R_{max})}$ scaling in RLHF through\n  Preference-based Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Avoiding $\\mathbf{exp(R_{max})}$ scaling in RLHF through\n  Preference-based Exploration"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal\ntechnique for large language model (LLM) alignment. This paper studies the\nsetting of online RLHF and focus on improving sample efficiency. All existing\nalgorithms in online RLHF, whether doing passive exploration or active\nexploration, suffer from a sample complexity that scales exponentially with the\nscale of the reward function. This fundamental limitation hinders their\neffectiveness in scenarios with heavily skewed preferences, e.g. questions with\na unique correct solution. To address this, we introduce Self-Exploring\nPreference-Incentive Online Preference Optimization (SE-POPO), an online RLHF\nalgorithm that for the first time achieves a sample complexity that scales\npolynomially with the reward scale, answering an open problem raised by Xie et\nal. (2024).. Theoretically, we demonstrate that the sample complexity of\nSE-POPO dominates that of existing exploration algorithms. Empirically, our\nsystematic evaluation confirms that SE-POPO is more sample-efficient than both\nexploratory and non-exploratory baselines, in two primary application scenarios\nof RLHF as well as on public benchmarks, marking a significant step forward in\nRLHF algorithm design. The code is available at\nhttps://github.com/MYC000801/SE-POPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal\ntechnique for large language model (LLM) alignment. This paper studies the\nsetting of online RLHF and focus on improving sample efficiency. All existing\nalgorithms in online RLHF, whether doing passive exploration or active\nexploration, suffer from a sample complexity that scales exponentially with the\nscale of the reward function. This fundamental limitation hinders their\neffectiveness in scenarios with heavily skewed preferences, e.g. questions with\na unique correct solution. To address this, we introduce Self-Exploring\nPreference-Incentive Online Preference Optimization (SE-POPO), an online RLHF\nalgorithm that for the first time achieves a sample complexity that scales\npolynomially with the reward scale, answering an open problem raised by Xie et\nal. (2024).. Theoretically, we demonstrate that the sample complexity of\nSE-POPO dominates that of existing exploration algorithms. Empirically, our\nsystematic evaluation confirms that SE-POPO is more sample-efficient than both\nexploratory and non-exploratory baselines, in two primary application scenarios\nof RLHF as well as on public benchmarks, marking a significant step forward in\nRLHF algorithm design. The code is available at\nhttps://github.com/MYC000801/SE-POPO."
                },
                "authors": [
                    {
                        "name": "Mingyu Chen"
                    },
                    {
                        "name": "Yiding Chen"
                    },
                    {
                        "name": "Wen Sun"
                    },
                    {
                        "name": "Xuezhou Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xuezhou Zhang"
                },
                "author": "Xuezhou Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00666v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00666v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20321v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20321v2",
                "updated": "2025-09-26T15:14:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    14,
                    24,
                    4,
                    269,
                    0
                ],
                "published": "2025-05-23T17:58:07Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    17,
                    58,
                    7,
                    4,
                    143,
                    0
                ],
                "title": "BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge\n  Bases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge\n  Bases"
                },
                "summary": "Biomedical researchers increasingly rely on large-scale structured databases\nfor complex analytical tasks. However, current text-to-SQL systems often\nstruggle to map qualitative scientific questions into executable SQL,\nparticularly when implicit domain reasoning is required. We introduce\nBiomedSQL, the first benchmark explicitly designed to evaluate scientific\nreasoning in text-to-SQL generation over a real-world biomedical knowledge\nbase. BiomedSQL comprises 68,000 question/SQL query/answer triples grounded in\na harmonized BigQuery knowledge base that integrates gene-disease associations,\ncausal inference from omics data, and drug approval records. Each question\nrequires models to infer domain-specific criteria, such as genome-wide\nsignificance thresholds, effect directionality, or trial phase filtering,\nrather than rely on syntactic translation alone. We evaluate a range of open-\nand closed-source LLMs across prompting strategies and interaction paradigms.\nOur results reveal a substantial performance gap: GPT-o3-mini achieves 59.0%\nexecution accuracy, while our custom multi-step agent, BMSQL, reaches 62.6%,\nboth well below the expert baseline of 90.0%. BiomedSQL provides a new\nfoundation for advancing text-to-SQL systems capable of supporting scientific\ndiscovery through robust reasoning over structured biomedical knowledge bases.\nOur dataset is publicly available at\nhttps://huggingface.co/datasets/NIH-CARD/BiomedSQL, and our code is open-source\nat https://github.com/NIH-CARD/biomedsql.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biomedical researchers increasingly rely on large-scale structured databases\nfor complex analytical tasks. However, current text-to-SQL systems often\nstruggle to map qualitative scientific questions into executable SQL,\nparticularly when implicit domain reasoning is required. We introduce\nBiomedSQL, the first benchmark explicitly designed to evaluate scientific\nreasoning in text-to-SQL generation over a real-world biomedical knowledge\nbase. BiomedSQL comprises 68,000 question/SQL query/answer triples grounded in\na harmonized BigQuery knowledge base that integrates gene-disease associations,\ncausal inference from omics data, and drug approval records. Each question\nrequires models to infer domain-specific criteria, such as genome-wide\nsignificance thresholds, effect directionality, or trial phase filtering,\nrather than rely on syntactic translation alone. We evaluate a range of open-\nand closed-source LLMs across prompting strategies and interaction paradigms.\nOur results reveal a substantial performance gap: GPT-o3-mini achieves 59.0%\nexecution accuracy, while our custom multi-step agent, BMSQL, reaches 62.6%,\nboth well below the expert baseline of 90.0%. BiomedSQL provides a new\nfoundation for advancing text-to-SQL systems capable of supporting scientific\ndiscovery through robust reasoning over structured biomedical knowledge bases.\nOur dataset is publicly available at\nhttps://huggingface.co/datasets/NIH-CARD/BiomedSQL, and our code is open-source\nat https://github.com/NIH-CARD/biomedsql."
                },
                "authors": [
                    {
                        "name": "Mathew J. Koretsky"
                    },
                    {
                        "name": "Maya Willey"
                    },
                    {
                        "name": "Adi Asija"
                    },
                    {
                        "name": "Owen Bianchi"
                    },
                    {
                        "name": "Chelsea X. Alvarado"
                    },
                    {
                        "name": "Tanay Nayak"
                    },
                    {
                        "name": "Nicole Kuznetsov"
                    },
                    {
                        "name": "Sungwon Kim"
                    },
                    {
                        "name": "Mike A. Nalls"
                    },
                    {
                        "name": "Daniel Khashabi"
                    },
                    {
                        "name": "Faraz Faghri"
                    }
                ],
                "author_detail": {
                    "name": "Faraz Faghri"
                },
                "author": "Faraz Faghri",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20321v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20321v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18697v2",
                "updated": "2025-09-26T15:09:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    9,
                    18,
                    4,
                    269,
                    0
                ],
                "published": "2025-05-24T13:43:29Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    13,
                    43,
                    29,
                    5,
                    144,
                    0
                ],
                "title": "Can LLMs Alleviate Catastrophic Forgetting in Graph Continual Learning?\n  A Systematic Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Alleviate Catastrophic Forgetting in Graph Continual Learning?\n  A Systematic Study"
                },
                "summary": "Nowadays, real-world data, including graph-structure data, often arrives in a\nstreaming manner, which means that learning systems need to continuously\nacquire new knowledge without forgetting previously learned information.\nAlthough substantial existing works attempt to address catastrophic forgetting\nin graph machine learning, they are all based on training from scratch with\nstreaming data. With the rise of pretrained models, an increasing number of\nstudies have leveraged their strong generalization ability for continual\nlearning. Therefore, in this work, we attempt to answer whether large language\nmodels (LLMs) can mitigate catastrophic forgetting in Graph Continual Learning\n(GCL). We first point out that current experimental setups for GCL have\nsignificant flaws, as the evaluation stage may lead to task ID leakage. Then,\nwe evaluate the performance of LLMs in more realistic scenarios and find that\neven minor modifications can lead to outstanding results. Finally, based on\nextensive experiments, we propose a simple-yet-effective method, Simple Graph\nContinual Learning (SimGCL), that surpasses the previous state-of-the-art\nGNN-based baseline by around 20% under the rehearsal-free constraint. To\nfacilitate reproducibility, we have developed an easy-to-use benchmark LLM4GCL\nfor training and evaluating existing GCL methods. The code is available at:\nhttps://github.com/ZhixunLEE/LLM4GCL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, real-world data, including graph-structure data, often arrives in a\nstreaming manner, which means that learning systems need to continuously\nacquire new knowledge without forgetting previously learned information.\nAlthough substantial existing works attempt to address catastrophic forgetting\nin graph machine learning, they are all based on training from scratch with\nstreaming data. With the rise of pretrained models, an increasing number of\nstudies have leveraged their strong generalization ability for continual\nlearning. Therefore, in this work, we attempt to answer whether large language\nmodels (LLMs) can mitigate catastrophic forgetting in Graph Continual Learning\n(GCL). We first point out that current experimental setups for GCL have\nsignificant flaws, as the evaluation stage may lead to task ID leakage. Then,\nwe evaluate the performance of LLMs in more realistic scenarios and find that\neven minor modifications can lead to outstanding results. Finally, based on\nextensive experiments, we propose a simple-yet-effective method, Simple Graph\nContinual Learning (SimGCL), that surpasses the previous state-of-the-art\nGNN-based baseline by around 20% under the rehearsal-free constraint. To\nfacilitate reproducibility, we have developed an easy-to-use benchmark LLM4GCL\nfor training and evaluating existing GCL methods. The code is available at:\nhttps://github.com/ZhixunLEE/LLM4GCL."
                },
                "authors": [
                    {
                        "name": "Ziyang Cheng"
                    },
                    {
                        "name": "Zhixun Li"
                    },
                    {
                        "name": "Yuhan Li"
                    },
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Kangyi Zhao"
                    },
                    {
                        "name": "Dawei Cheng"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Hong Cheng"
                    },
                    {
                        "name": "Jeffrey Xu Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jeffrey Xu Yu"
                },
                "author": "Jeffrey Xu Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15241v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15241v3",
                "updated": "2025-09-26T15:05:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    5,
                    35,
                    4,
                    269,
                    0
                ],
                "published": "2025-04-21T17:15:06Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    15,
                    6,
                    0,
                    111,
                    0
                ],
                "title": "MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety"
                },
                "summary": "Large Language Models (LLMs) are susceptible to adversarial attacks such as\njailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability\nis exacerbated in multilingual settings, where multilingual safety-aligned data\nis often limited. Thus, developing a guardrail capable of detecting and\nfiltering unsafe content across diverse languages is critical for deploying\nLLMs in real-world applications. In this work, we introduce a multilingual\nguardrail with reasoning for prompt classification. Our method consists of: (1)\nsynthetic multilingual data generation incorporating culturally and\nlinguistically nuanced variants, (2) supervised fine-tuning, and (3) a\ncurriculum-based Group Relative Policy Optimization (GRPO) framework that\nfurther improves performance. Experimental results demonstrate that our\nmultilingual guardrail, MrGuard, consistently outperforms recent baselines\nacross both in-domain and out-of-domain languages by more than 15%. We also\nevaluate MrGuard's robustness to multilingual variations, such as\ncode-switching and low-resource language distractors in the prompt, and\ndemonstrate that it preserves safety judgments under these challenging\nconditions. The multilingual reasoning capability of our guardrail enables it\nto generate explanations, which are particularly useful for understanding\nlanguage-specific risks and ambiguities in multilingual content moderation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are susceptible to adversarial attacks such as\njailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability\nis exacerbated in multilingual settings, where multilingual safety-aligned data\nis often limited. Thus, developing a guardrail capable of detecting and\nfiltering unsafe content across diverse languages is critical for deploying\nLLMs in real-world applications. In this work, we introduce a multilingual\nguardrail with reasoning for prompt classification. Our method consists of: (1)\nsynthetic multilingual data generation incorporating culturally and\nlinguistically nuanced variants, (2) supervised fine-tuning, and (3) a\ncurriculum-based Group Relative Policy Optimization (GRPO) framework that\nfurther improves performance. Experimental results demonstrate that our\nmultilingual guardrail, MrGuard, consistently outperforms recent baselines\nacross both in-domain and out-of-domain languages by more than 15%. We also\nevaluate MrGuard's robustness to multilingual variations, such as\ncode-switching and low-resource language distractors in the prompt, and\ndemonstrate that it preserves safety judgments under these challenging\nconditions. The multilingual reasoning capability of our guardrail enables it\nto generate explanations, which are particularly useful for understanding\nlanguage-specific risks and ambiguities in multilingual content moderation."
                },
                "authors": [
                    {
                        "name": "Yahan Yang"
                    },
                    {
                        "name": "Soham Dan"
                    },
                    {
                        "name": "Shuo Li"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Insup Lee"
                    }
                ],
                "author_detail": {
                    "name": "Insup Lee"
                },
                "author": "Insup Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15241v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15241v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22449v1",
                "updated": "2025-09-26T15:04:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    4,
                    32,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:04:32Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    4,
                    32,
                    4,
                    269,
                    0
                ],
                "title": "Detecting (Un)answerability in Large Language Models with Linear\n  Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting (Un)answerability in Large Language Models with Linear\n  Directions"
                },
                "summary": "Large language models (LLMs) often respond confidently to questions even when\nthey lack the necessary information, leading to hallucinated answers. In this\nwork, we study the problem of (un)answerability detection, focusing on\nextractive question answering (QA) where the model should determine if a\npassage contains sufficient information to answer a given question. We propose\na simple approach for identifying a direction in the model's activation space\nthat captures unanswerability and uses it for classification. This direction is\nselected by applying activation additions during inference and measuring their\nimpact on the model's abstention behavior. We show that projecting hidden\nactivations onto this direction yields a reliable score for (un)answerability\nclassification. Experiments on two open-weight LLMs and four extractive QA\nbenchmarks show that our method effectively detects unanswerable questions and\ngeneralizes better across datasets than existing prompt-based and\nclassifier-based approaches. Moreover, the obtained directions extend beyond\nextractive QA to unanswerability that stems from factors, such as lack of\nscientific consensus and subjectivity. Last, causal interventions show that\nadding or ablating the directions effectively controls the abstention behavior\nof the model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often respond confidently to questions even when\nthey lack the necessary information, leading to hallucinated answers. In this\nwork, we study the problem of (un)answerability detection, focusing on\nextractive question answering (QA) where the model should determine if a\npassage contains sufficient information to answer a given question. We propose\na simple approach for identifying a direction in the model's activation space\nthat captures unanswerability and uses it for classification. This direction is\nselected by applying activation additions during inference and measuring their\nimpact on the model's abstention behavior. We show that projecting hidden\nactivations onto this direction yields a reliable score for (un)answerability\nclassification. Experiments on two open-weight LLMs and four extractive QA\nbenchmarks show that our method effectively detects unanswerable questions and\ngeneralizes better across datasets than existing prompt-based and\nclassifier-based approaches. Moreover, the obtained directions extend beyond\nextractive QA to unanswerability that stems from factors, such as lack of\nscientific consensus and subjectivity. Last, causal interventions show that\nadding or ablating the directions effectively controls the abstention behavior\nof the model."
                },
                "authors": [
                    {
                        "name": "Maor Juliet Lavi"
                    },
                    {
                        "name": "Tova Milo"
                    },
                    {
                        "name": "Mor Geva"
                    }
                ],
                "author_detail": {
                    "name": "Mor Geva"
                },
                "author": "Mor Geva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08123v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08123v4",
                "updated": "2025-09-26T14:57:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    57,
                    59,
                    4,
                    269,
                    0
                ],
                "published": "2025-06-09T18:24:57Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    18,
                    24,
                    57,
                    0,
                    160,
                    0
                ],
                "title": "QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA"
                },
                "summary": "Alignment of large language models (LLMs) with principles like helpfulness,\nhonesty, and harmlessness typically relies on scalar rewards that obscure which\nobjectives drive the training signal. We introduce QA-LIGN, which decomposes\nmonolithic rewards into interpretable principle-specific evaluations through\nstructured natural language programs. Models learn through a draft, critique,\nand revise pipeline, where symbolic evaluation against the rubrics provides\ntransparent feedback for both initial and revised responses during GRPO\ntraining. Applied to uncensored Llama-3.1-8B-Instruct, QA-LIGN reduces attack\nsuccess rates by up to 68.7% while maintaining a 0.67% false refusal rate,\nachieving Pareto optimal safety-helpfulness performance and outperforming both\nDPO and GRPO with state-of-the-art reward models given equivalent training.\nThese results demonstrate that making reward signals interpretable and modular\nimproves alignment effectiveness, suggesting transparency enhances LLM safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment of large language models (LLMs) with principles like helpfulness,\nhonesty, and harmlessness typically relies on scalar rewards that obscure which\nobjectives drive the training signal. We introduce QA-LIGN, which decomposes\nmonolithic rewards into interpretable principle-specific evaluations through\nstructured natural language programs. Models learn through a draft, critique,\nand revise pipeline, where symbolic evaluation against the rubrics provides\ntransparent feedback for both initial and revised responses during GRPO\ntraining. Applied to uncensored Llama-3.1-8B-Instruct, QA-LIGN reduces attack\nsuccess rates by up to 68.7% while maintaining a 0.67% false refusal rate,\nachieving Pareto optimal safety-helpfulness performance and outperforming both\nDPO and GRPO with state-of-the-art reward models given equivalent training.\nThese results demonstrate that making reward signals interpretable and modular\nimproves alignment effectiveness, suggesting transparency enhances LLM safety."
                },
                "authors": [
                    {
                        "name": "Jacob Dineen"
                    },
                    {
                        "name": "Aswin RRV"
                    },
                    {
                        "name": "Qin Liu"
                    },
                    {
                        "name": "Zhikun Xu"
                    },
                    {
                        "name": "Xiao Ye"
                    },
                    {
                        "name": "Ming Shen"
                    },
                    {
                        "name": "Zhaonan Li"
                    },
                    {
                        "name": "Shijie Lu"
                    },
                    {
                        "name": "Chitta Baral"
                    },
                    {
                        "name": "Muhao Chen"
                    },
                    {
                        "name": "Ben Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Ben Zhou"
                },
                "author": "Ben Zhou",
                "arxiv_comment": "Accepted to Findings of EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08123v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08123v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22434v1",
                "updated": "2025-09-26T14:53:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    53,
                    8,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T14:53:08Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    53,
                    8,
                    4,
                    269,
                    0
                ],
                "title": "An Ontology for Unified Modeling of Tasks, Actions, Environments, and\n  Capabilities in Personal Service Robotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Ontology for Unified Modeling of Tasks, Actions, Environments, and\n  Capabilities in Personal Service Robotics"
                },
                "summary": "Personal service robots are increasingly used in domestic settings to assist\nolder adults and people requiring support. Effective operation involves not\nonly physical interaction but also the ability to interpret dynamic\nenvironments, understand tasks, and choose appropriate actions based on\ncontext. This requires integrating both hardware components (e.g. sensors,\nactuators) and software systems capable of reasoning about tasks, environments,\nand robot capabilities. Frameworks such as the Robot Operating System (ROS)\nprovide open-source tools that help connect low-level hardware with\nhigher-level functionalities. However, real-world deployments remain tightly\ncoupled to specific platforms. As a result, solutions are often isolated and\nhard-coded, limiting interoperability, reusability, and knowledge sharing.\nOntologies and knowledge graphs offer a structured way to represent tasks,\nenvironments, and robot capabilities. Existing ontologies, such as the\nSocio-physical Model of Activities (SOMA) and the Descriptive Ontology for\nLinguistic and Cognitive Engineering (DOLCE), provide models for activities,\nspatial relationships, and reasoning structures. However, they often focus on\nspecific domains and do not fully capture the connection between environment,\naction, robot capabilities, and system-level integration. In this work, we\npropose the Ontology for roBOts and acTions (OntoBOT), which extends existing\nontologies to provide a unified representation of tasks, actions, environments,\nand capabilities. Our contributions are twofold: (1) we unify these aspects\ninto a cohesive ontology to support formal reasoning about task execution, and\n(2) we demonstrate its generalizability by evaluating competency questions\nacross four embodied agents - TIAGo, HSR, UR3, and Stretch - showing how\nOntoBOT enables context-aware reasoning, task-oriented execution, and knowledge\nsharing in service robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personal service robots are increasingly used in domestic settings to assist\nolder adults and people requiring support. Effective operation involves not\nonly physical interaction but also the ability to interpret dynamic\nenvironments, understand tasks, and choose appropriate actions based on\ncontext. This requires integrating both hardware components (e.g. sensors,\nactuators) and software systems capable of reasoning about tasks, environments,\nand robot capabilities. Frameworks such as the Robot Operating System (ROS)\nprovide open-source tools that help connect low-level hardware with\nhigher-level functionalities. However, real-world deployments remain tightly\ncoupled to specific platforms. As a result, solutions are often isolated and\nhard-coded, limiting interoperability, reusability, and knowledge sharing.\nOntologies and knowledge graphs offer a structured way to represent tasks,\nenvironments, and robot capabilities. Existing ontologies, such as the\nSocio-physical Model of Activities (SOMA) and the Descriptive Ontology for\nLinguistic and Cognitive Engineering (DOLCE), provide models for activities,\nspatial relationships, and reasoning structures. However, they often focus on\nspecific domains and do not fully capture the connection between environment,\naction, robot capabilities, and system-level integration. In this work, we\npropose the Ontology for roBOts and acTions (OntoBOT), which extends existing\nontologies to provide a unified representation of tasks, actions, environments,\nand capabilities. Our contributions are twofold: (1) we unify these aspects\ninto a cohesive ontology to support formal reasoning about task execution, and\n(2) we demonstrate its generalizability by evaluating competency questions\nacross four embodied agents - TIAGo, HSR, UR3, and Stretch - showing how\nOntoBOT enables context-aware reasoning, task-oriented execution, and knowledge\nsharing in service robotics."
                },
                "authors": [
                    {
                        "name": "Margherita Martorana"
                    },
                    {
                        "name": "Francesca Urgese"
                    },
                    {
                        "name": "Ilaria Tiddi"
                    },
                    {
                        "name": "Stefan Schlobach"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Schlobach"
                },
                "author": "Stefan Schlobach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22431v1",
                "updated": "2025-09-26T14:50:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    50,
                    13,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T14:50:13Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    50,
                    13,
                    4,
                    269,
                    0
                ],
                "title": "TreeMind: Automatically Reproducing Android Bug Reports via\n  LLM-empowered Monte Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeMind: Automatically Reproducing Android Bug Reports via\n  LLM-empowered Monte Carlo Tree Search"
                },
                "summary": "Automatically reproducing Android app crashes from textual bug reports is\nchallenging, particularly when the reports are incomplete and the modern UI\nexhibits high combinatorial complexity. Existing approaches based on\nreinforcement learning or large language models (LLMs) exhibit limitations in\nsuch scenarios. They struggle to infer unobserved steps and reconstruct the\nunderlying user action sequences to navigate the vast UI interaction space,\nprimarily due to limited goal-directed reasoning and planning. We present\nTreeMind, a novel technique that integrates LLMs with a customized Monte Carlo\nTree Search (MCTS) algorithm to achieve strategic UI exploration in bug\nreproduction. To the best of our knowledge, this is the first work to combine\nexternal decision-making with LLM semantic reasoning for reliable bug\nreproduction. We formulate the reproduction task as a target-driven search\nproblem, leveraging MCTS as the core planning mechanism to iteratively refine\naction sequences. To enhance MCTS with semantic reasoning, we introduce two\nLLM-guided agents with distinct roles: Expander generates top-k promising\nactions based on the current UI state and exploration history, while Simulator\nestimates the likelihood that each action leads toward successful reproduction.\nBy incorporating multi-modal UI inputs and advanced prompting techniques,\nTreeMind conducts feedback-aware navigation that identifies missing but\nessential user actions and incrementally reconstructs the reproduction paths.\nWe evaluate TreeMind on a dataset of 93 real-world Android bug reports from\nthree widely-used benchmarks. Experimental results show that it significantly\noutperforms four state-of-the-art baselines in reproduction success rate. A\nreal-world case study indicates that integrating LLM reasoning with MCTS-based\nplanning is a compelling direction for automated bug reproduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically reproducing Android app crashes from textual bug reports is\nchallenging, particularly when the reports are incomplete and the modern UI\nexhibits high combinatorial complexity. Existing approaches based on\nreinforcement learning or large language models (LLMs) exhibit limitations in\nsuch scenarios. They struggle to infer unobserved steps and reconstruct the\nunderlying user action sequences to navigate the vast UI interaction space,\nprimarily due to limited goal-directed reasoning and planning. We present\nTreeMind, a novel technique that integrates LLMs with a customized Monte Carlo\nTree Search (MCTS) algorithm to achieve strategic UI exploration in bug\nreproduction. To the best of our knowledge, this is the first work to combine\nexternal decision-making with LLM semantic reasoning for reliable bug\nreproduction. We formulate the reproduction task as a target-driven search\nproblem, leveraging MCTS as the core planning mechanism to iteratively refine\naction sequences. To enhance MCTS with semantic reasoning, we introduce two\nLLM-guided agents with distinct roles: Expander generates top-k promising\nactions based on the current UI state and exploration history, while Simulator\nestimates the likelihood that each action leads toward successful reproduction.\nBy incorporating multi-modal UI inputs and advanced prompting techniques,\nTreeMind conducts feedback-aware navigation that identifies missing but\nessential user actions and incrementally reconstructs the reproduction paths.\nWe evaluate TreeMind on a dataset of 93 real-world Android bug reports from\nthree widely-used benchmarks. Experimental results show that it significantly\noutperforms four state-of-the-art baselines in reproduction success rate. A\nreal-world case study indicates that integrating LLM reasoning with MCTS-based\nplanning is a compelling direction for automated bug reproduction."
                },
                "authors": [
                    {
                        "name": "Zhengyu Chen"
                    },
                    {
                        "name": "Zhaoyi Meng"
                    },
                    {
                        "name": "Wenxiang Zhao"
                    },
                    {
                        "name": "Wansen Wang"
                    },
                    {
                        "name": "Haoyang Zhao"
                    },
                    {
                        "name": "Jiahao Zhan"
                    },
                    {
                        "name": "Jie Cui"
                    },
                    {
                        "name": "Hong Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Hong Zhong"
                },
                "author": "Hong Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13514v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13514v3",
                "updated": "2025-09-26T14:44:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    44,
                    53,
                    4,
                    269,
                    0
                ],
                "published": "2024-10-17T13:02:06Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    13,
                    2,
                    6,
                    3,
                    291,
                    0
                ],
                "title": "GraphSCENE: On-Demand Critical Scenario Generation for Autonomous\n  Vehicles in Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSCENE: On-Demand Critical Scenario Generation for Autonomous\n  Vehicles in Simulation"
                },
                "summary": "Testing and validating Autonomous Vehicle (AV) performance in safety-critical\nand diverse scenarios is crucial before real-world deployment. However,\nmanually creating such scenarios in simulation remains a significant and\ntime-consuming challenge. This work introduces a novel method that generates\ndynamic temporal scene graphs corresponding to diverse traffic scenarios,\non-demand, tailored to user-defined preferences, such as AV actions, sets of\ndynamic agents, and criticality levels. A temporal Graph Neural Network (GNN)\nmodel learns to predict relationships between ego-vehicle, agents, and static\nstructures, guided by real-world spatiotemporal interaction patterns and\nconstrained by an ontology that restricts predictions to semantically valid\nlinks. Our model consistently outperforms the baselines in accurately\ngenerating links corresponding to the requested scenarios. We render the\npredicted scenarios in simulation to further demonstrate their effectiveness as\ntesting environments for AV agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing and validating Autonomous Vehicle (AV) performance in safety-critical\nand diverse scenarios is crucial before real-world deployment. However,\nmanually creating such scenarios in simulation remains a significant and\ntime-consuming challenge. This work introduces a novel method that generates\ndynamic temporal scene graphs corresponding to diverse traffic scenarios,\non-demand, tailored to user-defined preferences, such as AV actions, sets of\ndynamic agents, and criticality levels. A temporal Graph Neural Network (GNN)\nmodel learns to predict relationships between ego-vehicle, agents, and static\nstructures, guided by real-world spatiotemporal interaction patterns and\nconstrained by an ontology that restricts predictions to semantically valid\nlinks. Our model consistently outperforms the baselines in accurately\ngenerating links corresponding to the requested scenarios. We render the\npredicted scenarios in simulation to further demonstrate their effectiveness as\ntesting environments for AV agents."
                },
                "authors": [
                    {
                        "name": "Efimia Panagiotaki"
                    },
                    {
                        "name": "Georgi Pramatarov"
                    },
                    {
                        "name": "Lars Kunze"
                    },
                    {
                        "name": "Daniele De Martini"
                    }
                ],
                "author_detail": {
                    "name": "Daniele De Martini"
                },
                "author": "Daniele De Martini",
                "arxiv_comment": "Accepted to the IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13514v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13514v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22415v1",
                "updated": "2025-09-26T14:39:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    39,
                    13,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T14:39:13Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    39,
                    13,
                    4,
                    269,
                    0
                ],
                "title": "Explaining multimodal LLMs via intra-modal token interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explaining multimodal LLMs via intra-modal token interactions"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success\nacross diverse vision-language tasks, yet their internal decision-making\nmechanisms remain insufficiently understood. Existing interpretability research\nhas primarily focused on cross-modal attribution, identifying which image\nregions the model attends to during output generation. However, these\napproaches often overlook intra-modal dependencies. In the visual modality,\nattributing importance to isolated image patches ignores spatial context due to\nlimited receptive fields, resulting in fragmented and noisy explanations. In\nthe textual modality, reliance on preceding tokens introduces spurious\nactivations. Failing to effectively mitigate these interference compromises\nattribution fidelity. To address these limitations, we propose enhancing\ninterpretability by leveraging intra-modal interaction. For the visual branch,\nwe introduce \\textit{Multi-Scale Explanation Aggregation} (MSEA), which\naggregates attributions over multi-scale inputs to dynamically adjust receptive\nfields, producing more holistic and spatially coherent visual explanations. For\nthe textual branch, we propose \\textit{Activation Ranking Correlation} (ARC),\nwhich measures the relevance of contextual tokens to the current token via\nalignment of their top-$k$ prediction rankings. ARC leverages this relevance to\nsuppress spurious activations from irrelevant contexts while preserving\nsemantically coherent ones. Extensive experiments across state-of-the-art MLLMs\nand benchmark datasets demonstrate that our approach consistently outperforms\nexisting interpretability methods, yielding more faithful and fine-grained\nexplanations of model behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success\nacross diverse vision-language tasks, yet their internal decision-making\nmechanisms remain insufficiently understood. Existing interpretability research\nhas primarily focused on cross-modal attribution, identifying which image\nregions the model attends to during output generation. However, these\napproaches often overlook intra-modal dependencies. In the visual modality,\nattributing importance to isolated image patches ignores spatial context due to\nlimited receptive fields, resulting in fragmented and noisy explanations. In\nthe textual modality, reliance on preceding tokens introduces spurious\nactivations. Failing to effectively mitigate these interference compromises\nattribution fidelity. To address these limitations, we propose enhancing\ninterpretability by leveraging intra-modal interaction. For the visual branch,\nwe introduce \\textit{Multi-Scale Explanation Aggregation} (MSEA), which\naggregates attributions over multi-scale inputs to dynamically adjust receptive\nfields, producing more holistic and spatially coherent visual explanations. For\nthe textual branch, we propose \\textit{Activation Ranking Correlation} (ARC),\nwhich measures the relevance of contextual tokens to the current token via\nalignment of their top-$k$ prediction rankings. ARC leverages this relevance to\nsuppress spurious activations from irrelevant contexts while preserving\nsemantically coherent ones. Extensive experiments across state-of-the-art MLLMs\nand benchmark datasets demonstrate that our approach consistently outperforms\nexisting interpretability methods, yielding more faithful and fine-grained\nexplanations of model behavior."
                },
                "authors": [
                    {
                        "name": "Jiawei Liang"
                    },
                    {
                        "name": "Ruoyu Chen"
                    },
                    {
                        "name": "Xianghao Jiao"
                    },
                    {
                        "name": "Siyuan Liang"
                    },
                    {
                        "name": "Shiming Liu"
                    },
                    {
                        "name": "Qunli Zhang"
                    },
                    {
                        "name": "Zheng Hu"
                    },
                    {
                        "name": "Xiaochun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaochun Cao"
                },
                "author": "Xiaochun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10150v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10150v3",
                "updated": "2025-09-26T14:37:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    37,
                    4,
                    4,
                    269,
                    0
                ],
                "published": "2025-03-13T08:22:31Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    8,
                    22,
                    31,
                    3,
                    72,
                    0
                ],
                "title": "Retrieval-Augmented Generation with Hierarchical Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation with Hierarchical Knowledge"
                },
                "summary": "Graph-based Retrieval-Augmented Generation (RAG) methods have significantly\nenhanced the performance of large language models (LLMs) in domain-specific\ntasks. However, existing RAG methods do not adequately utilize the naturally\ninherent hierarchical knowledge in human cognition, which limits the\ncapabilities of RAG systems. In this paper, we introduce a new RAG approach,\ncalled HiRAG, which utilizes hierarchical knowledge to enhance the semantic\nunderstanding and structure capturing capabilities of RAG systems in the\nindexing and retrieval processes. Our extensive experiments demonstrate that\nHiRAG achieves significant performance improvements over the state-of-the-art\nbaseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based Retrieval-Augmented Generation (RAG) methods have significantly\nenhanced the performance of large language models (LLMs) in domain-specific\ntasks. However, existing RAG methods do not adequately utilize the naturally\ninherent hierarchical knowledge in human cognition, which limits the\ncapabilities of RAG systems. In this paper, we introduce a new RAG approach,\ncalled HiRAG, which utilizes hierarchical knowledge to enhance the semantic\nunderstanding and structure capturing capabilities of RAG systems in the\nindexing and retrieval processes. Our extensive experiments demonstrate that\nHiRAG achieves significant performance improvements over the state-of-the-art\nbaseline methods."
                },
                "authors": [
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Junjie Yang"
                    },
                    {
                        "name": "Zhenyu Pan"
                    },
                    {
                        "name": "Yongqiang Chen"
                    },
                    {
                        "name": "Kaili Ma"
                    },
                    {
                        "name": "Hongzhi Chen"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10150v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10150v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16950v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16950v3",
                "updated": "2025-09-26T14:35:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    35,
                    4,
                    4,
                    269,
                    0
                ],
                "published": "2025-05-22T17:33:49Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    33,
                    49,
                    3,
                    142,
                    0
                ],
                "title": "Bottlenecked Transformers: Periodic KV Cache Consolidation for\n  Generalised Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bottlenecked Transformers: Periodic KV Cache Consolidation for\n  Generalised Reasoning"
                },
                "summary": "Transformer LLMs have been shown to exhibit strong reasoning ability that\nscales with inference-time compute, most prominently through token-space\n\"thinking\" chains of thought. A growing line of work pushes extra computation\ninto the model's latent space, which we term Auxiliary Latent-Space Computation\n(ALSC). Existing ALSC methods largely fall into three buckets: (i)\ntoken-mediated latent rollouts, (ii) residual/activation steering, and (iii)\nmemory (KV) compression. An underexplored alternative is memory\nconsolidation/reconsolidation, two processes in the brain that are responsible\nfor stabilising newly formed memory traces, and, upon recall, transiently\nrendering established traces plastic such they can integrate new contextual\ninformation before restabilising. In Transformer LLMs, this can be seen as\nanalogous to performing in-place rewrites of new KV segments, and rewrites of\nrecalled past segments. In this work, we give a theoretical justification as to\nwhy memory (re)consolidation via KV cache rewrites is beneficial for improved\nreasoning. We do this through the lens of Information Bottleneck (IB) theory,\nwhich posits that model generalisation emerges from an optimal balance between\ninput information compression and retention of predictive information in latent\nrepresentations. We then introduce the Bottlenecked Transformer, which augments\na backbone LLM with a Cache Processor, an auxiliary Transformer that performs\nperiodic, non-causal, in-place KV rewrites at newline-delimited reasoning step\nboundaries. The Processor consolidates recently written KV entries and\nreconsolidates a small, top-k attention-selected set of prior entries. We\nevaluate our Bottlenecked Transformer architecture on math reasoning\nbenchmarks. Our model sees consistent performance gains over vanilla\nTransformers and pause-token augmented baselines, with gains of up to +6.6pp\nfor selected tasks/backbones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer LLMs have been shown to exhibit strong reasoning ability that\nscales with inference-time compute, most prominently through token-space\n\"thinking\" chains of thought. A growing line of work pushes extra computation\ninto the model's latent space, which we term Auxiliary Latent-Space Computation\n(ALSC). Existing ALSC methods largely fall into three buckets: (i)\ntoken-mediated latent rollouts, (ii) residual/activation steering, and (iii)\nmemory (KV) compression. An underexplored alternative is memory\nconsolidation/reconsolidation, two processes in the brain that are responsible\nfor stabilising newly formed memory traces, and, upon recall, transiently\nrendering established traces plastic such they can integrate new contextual\ninformation before restabilising. In Transformer LLMs, this can be seen as\nanalogous to performing in-place rewrites of new KV segments, and rewrites of\nrecalled past segments. In this work, we give a theoretical justification as to\nwhy memory (re)consolidation via KV cache rewrites is beneficial for improved\nreasoning. We do this through the lens of Information Bottleneck (IB) theory,\nwhich posits that model generalisation emerges from an optimal balance between\ninput information compression and retention of predictive information in latent\nrepresentations. We then introduce the Bottlenecked Transformer, which augments\na backbone LLM with a Cache Processor, an auxiliary Transformer that performs\nperiodic, non-causal, in-place KV rewrites at newline-delimited reasoning step\nboundaries. The Processor consolidates recently written KV entries and\nreconsolidates a small, top-k attention-selected set of prior entries. We\nevaluate our Bottlenecked Transformer architecture on math reasoning\nbenchmarks. Our model sees consistent performance gains over vanilla\nTransformers and pause-token augmented baselines, with gains of up to +6.6pp\nfor selected tasks/backbones."
                },
                "authors": [
                    {
                        "name": "Adnan Oomerjee"
                    },
                    {
                        "name": "Zafeirios Fountas"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16950v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16950v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22403v1",
                "updated": "2025-09-26T14:31:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    31,
                    57,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T14:31:57Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    31,
                    57,
                    4,
                    269,
                    0
                ],
                "title": "MoveFM-R: Advancing Mobility Foundation Models via Language-driven\n  Semantic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoveFM-R: Advancing Mobility Foundation Models via Language-driven\n  Semantic Reasoning"
                },
                "summary": "Mobility Foundation Models (MFMs) have advanced the modeling of human\nmovement patterns, yet they face a ceiling due to limitations in data scale and\nsemantic understanding. While Large Language Models (LLMs) offer powerful\nsemantic reasoning, they lack the innate understanding of spatio-temporal\nstatistics required for generating physically plausible mobility trajectories.\nTo address these gaps, we propose MoveFM-R, a novel framework that unlocks the\nfull potential of mobility foundation models by leveraging language-driven\nsemantic reasoning capabilities. It tackles two key challenges: the vocabulary\nmismatch between continuous geographic coordinates and discrete language\ntokens, and the representation gap between the latent vectors of MFMs and the\nsemantic world of LLMs. MoveFM-R is built on three core innovations: a\nsemantically enhanced location encoding to bridge the geography-language gap, a\nprogressive curriculum to align the LLM's reasoning with mobility patterns, and\nan interactive self-reflection mechanism for conditional trajectory generation.\nExtensive experiments demonstrate that MoveFM-R significantly outperforms\nexisting MFM-based and LLM-based baselines. It also shows robust generalization\nin zero-shot settings and excels at generating realistic trajectories from\nnatural language instructions. By synthesizing the statistical power of MFMs\nwith the deep semantic understanding of LLMs, MoveFM-R pioneers a new paradigm\nthat enables a more comprehensive, interpretable, and powerful modeling of\nhuman mobility. The implementation of MoveFM-R is available online at\nhttps://anonymous.4open.science/r/MoveFM-R-CDE7/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobility Foundation Models (MFMs) have advanced the modeling of human\nmovement patterns, yet they face a ceiling due to limitations in data scale and\nsemantic understanding. While Large Language Models (LLMs) offer powerful\nsemantic reasoning, they lack the innate understanding of spatio-temporal\nstatistics required for generating physically plausible mobility trajectories.\nTo address these gaps, we propose MoveFM-R, a novel framework that unlocks the\nfull potential of mobility foundation models by leveraging language-driven\nsemantic reasoning capabilities. It tackles two key challenges: the vocabulary\nmismatch between continuous geographic coordinates and discrete language\ntokens, and the representation gap between the latent vectors of MFMs and the\nsemantic world of LLMs. MoveFM-R is built on three core innovations: a\nsemantically enhanced location encoding to bridge the geography-language gap, a\nprogressive curriculum to align the LLM's reasoning with mobility patterns, and\nan interactive self-reflection mechanism for conditional trajectory generation.\nExtensive experiments demonstrate that MoveFM-R significantly outperforms\nexisting MFM-based and LLM-based baselines. It also shows robust generalization\nin zero-shot settings and excels at generating realistic trajectories from\nnatural language instructions. By synthesizing the statistical power of MFMs\nwith the deep semantic understanding of LLMs, MoveFM-R pioneers a new paradigm\nthat enables a more comprehensive, interpretable, and powerful modeling of\nhuman mobility. The implementation of MoveFM-R is available online at\nhttps://anonymous.4open.science/r/MoveFM-R-CDE7/."
                },
                "authors": [
                    {
                        "name": "Fanjin Meng"
                    },
                    {
                        "name": "Yuan Yuan"
                    },
                    {
                        "name": "Jingtao Ding"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Chonghua Han"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17388v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17388v4",
                "updated": "2025-09-26T14:25:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    25,
                    50,
                    4,
                    269,
                    0
                ],
                "published": "2024-11-26T12:46:57Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    12,
                    46,
                    57,
                    1,
                    331,
                    0
                ],
                "title": "Can LLMs be Good Graph Judge for Knowledge Graph Construction?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs be Good Graph Judge for Knowledge Graph Construction?"
                },
                "summary": "In real-world scenarios, most of the data obtained from the information\nretrieval (IR) system is unstructured. Converting natural language sentences\ninto structured Knowledge Graphs (KGs) remains a critical challenge. We\nidentified three limitations with respect to existing KG construction methods:\n(1) There could be a large amount of noise in real-world documents, which could\nresult in extracting messy information. (2) Naive LLMs usually extract\ninaccurate knowledge from some domain-specific documents. (3) Hallucination\nphenomenon cannot be overlooked when directly using LLMs to construct KGs. In\nthis paper, we propose \\textbf{GraphJudge}, a KG construction framework to\naddress the aforementioned challenges. In this framework, we designed an\nentity-centric strategy to eliminate the noise information in the documents.\nAnd we fine-tuned a LLM as a graph judge to finally enhance the quality of\ngenerated KGs. Experiments conducted on two general and one domain-specific\ntext-graph pair datasets demonstrate state-of-the-art performance against\nvarious baseline methods with strong generalization abilities. Our code is\navailable at\n\\href{https://github.com/hhy-huang/GraphJudge}{https://github.com/hhy-huang/GraphJudge}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world scenarios, most of the data obtained from the information\nretrieval (IR) system is unstructured. Converting natural language sentences\ninto structured Knowledge Graphs (KGs) remains a critical challenge. We\nidentified three limitations with respect to existing KG construction methods:\n(1) There could be a large amount of noise in real-world documents, which could\nresult in extracting messy information. (2) Naive LLMs usually extract\ninaccurate knowledge from some domain-specific documents. (3) Hallucination\nphenomenon cannot be overlooked when directly using LLMs to construct KGs. In\nthis paper, we propose \\textbf{GraphJudge}, a KG construction framework to\naddress the aforementioned challenges. In this framework, we designed an\nentity-centric strategy to eliminate the noise information in the documents.\nAnd we fine-tuned a LLM as a graph judge to finally enhance the quality of\ngenerated KGs. Experiments conducted on two general and one domain-specific\ntext-graph pair datasets demonstrate state-of-the-art performance against\nvarious baseline methods with strong generalization abilities. Our code is\navailable at\n\\href{https://github.com/hhy-huang/GraphJudge}{https://github.com/hhy-huang/GraphJudge}."
                },
                "authors": [
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Chong Chen"
                    },
                    {
                        "name": "Zeang Sheng"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_comment": "EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17388v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17388v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22391v1",
                "updated": "2025-09-26T14:18:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    18,
                    50,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T14:18:50Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    18,
                    50,
                    4,
                    269,
                    0
                ],
                "title": "Do LLM Agents Know How to Ground, Recover, and Assess? A Benchmark for\n  Epistemic Competence in Information-Seeking Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLM Agents Know How to Ground, Recover, and Assess? A Benchmark for\n  Epistemic Competence in Information-Seeking Agents"
                },
                "summary": "Recent work has explored training Large Language Model (LLM) search agents\nwith reinforcement learning (RL) for open-domain question answering (QA).\nHowever, most evaluations focus solely on final answer accuracy, overlooking\nhow these agents reason with and act on external evidence. We introduce\nSeekBench, the first benchmark for evaluating the \\textit{epistemic competence}\nof LLM search agents through step-level analysis of their response traces.\nSeekBench comprises 190 expert-annotated traces with over 1,800 response steps\ngenerated by LLM search agents, each enriched with evidence annotations for\ngranular analysis of whether agents (1) generate reasoning steps grounded in\nobserved evidence, (2) adaptively reformulate searches to recover from\nlow-quality results, and (3) have proper calibration to correctly assess\nwhether the current evidence is sufficient for providing an answer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has explored training Large Language Model (LLM) search agents\nwith reinforcement learning (RL) for open-domain question answering (QA).\nHowever, most evaluations focus solely on final answer accuracy, overlooking\nhow these agents reason with and act on external evidence. We introduce\nSeekBench, the first benchmark for evaluating the \\textit{epistemic competence}\nof LLM search agents through step-level analysis of their response traces.\nSeekBench comprises 190 expert-annotated traces with over 1,800 response steps\ngenerated by LLM search agents, each enriched with evidence annotations for\ngranular analysis of whether agents (1) generate reasoning steps grounded in\nobserved evidence, (2) adaptively reformulate searches to recover from\nlow-quality results, and (3) have proper calibration to correctly assess\nwhether the current evidence is sufficient for providing an answer."
                },
                "authors": [
                    {
                        "name": "Jiaqi Shao"
                    },
                    {
                        "name": "Yuxiang Lin"
                    },
                    {
                        "name": "Munish Prasad Lohani"
                    },
                    {
                        "name": "Yufeng Miao"
                    },
                    {
                        "name": "Bing Luo"
                    }
                ],
                "author_detail": {
                    "name": "Bing Luo"
                },
                "author": "Bing Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22387v1",
                "updated": "2025-09-26T14:15:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    15,
                    44,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T14:15:44Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    15,
                    44,
                    4,
                    269,
                    0
                ],
                "title": "SpinGPT: A Large-Language-Model Approach to Playing Poker Correctly",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpinGPT: A Large-Language-Model Approach to Playing Poker Correctly"
                },
                "summary": "The Counterfactual Regret Minimization (CFR) algorithm and its variants have\nenabled the development of pokerbots capable of beating the best human players\nin heads-up (1v1) cash games and competing with them in six-player formats.\nHowever, CFR's computational complexity rises exponentially with the number of\nplayers. Furthermore, in games with three or more players, following Nash\nequilibrium no longer guarantees a non-losing outcome. These limitations, along\nwith others, significantly restrict the applicability of CFR to the most\npopular formats: tournaments. Motivated by the recent success of Large Language\nModels (LLM) in chess and Diplomacy, we present SpinGPT, the first LLM tailored\nto Spin & Go, a popular three-player online poker format. SpinGPT is trained in\ntwo stages: (1) Supervised Fine-Tuning on 320k high-stakes expert decisions;\n(2) Reinforcement Learning on 270k solver-generated hands. Our results show\nthat SpinGPT matches the solver's actions in 78% of decisions (tolerant\naccuracy). With a simple deep-stack heuristic, it achieves 13.4 +/- 12.9 BB/100\nversus Slumbot in heads-up over 30,000 hands (95% CI). These results suggest\nthat LLMs could be a new way to deal with multi-player imperfect-information\ngames like poker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Counterfactual Regret Minimization (CFR) algorithm and its variants have\nenabled the development of pokerbots capable of beating the best human players\nin heads-up (1v1) cash games and competing with them in six-player formats.\nHowever, CFR's computational complexity rises exponentially with the number of\nplayers. Furthermore, in games with three or more players, following Nash\nequilibrium no longer guarantees a non-losing outcome. These limitations, along\nwith others, significantly restrict the applicability of CFR to the most\npopular formats: tournaments. Motivated by the recent success of Large Language\nModels (LLM) in chess and Diplomacy, we present SpinGPT, the first LLM tailored\nto Spin & Go, a popular three-player online poker format. SpinGPT is trained in\ntwo stages: (1) Supervised Fine-Tuning on 320k high-stakes expert decisions;\n(2) Reinforcement Learning on 270k solver-generated hands. Our results show\nthat SpinGPT matches the solver's actions in 78% of decisions (tolerant\naccuracy). With a simple deep-stack heuristic, it achieves 13.4 +/- 12.9 BB/100\nversus Slumbot in heads-up over 30,000 hands (95% CI). These results suggest\nthat LLMs could be a new way to deal with multi-player imperfect-information\ngames like poker."
                },
                "authors": [
                    {
                        "name": "Narada Maugin"
                    },
                    {
                        "name": "Tristan Cazenave"
                    }
                ],
                "author_detail": {
                    "name": "Tristan Cazenave"
                },
                "author": "Tristan Cazenave",
                "arxiv_comment": "Accepted at Advances in Computer Games (ACG) 2025, LNCS (Springer)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22382v1",
                "updated": "2025-09-26T14:09:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    9,
                    25,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T14:09:25Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    9,
                    25,
                    4,
                    269,
                    0
                ],
                "title": "The Simons Observatory: Characterization of the 220/280 GHz TES Detector\n  Modules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Simons Observatory: Characterization of the 220/280 GHz TES Detector\n  Modules"
                },
                "summary": "The Simons Observatory (SO) is a new suite of cosmic microwave background\ntelescopes in the Chilean Atacama Desert with an extensive science program\nspanning cosmology, Galactic and extragalactic astrophysics, and particle\nphysics. SO will survey the millimeter-wave sky over a wide range of angular\nscales using six spectral bands across three types of dichroic,\npolarization-sensitive transition-edge sensor (TES) detector modules:\nLow-Frequency (LF) modules with bandpasses centered near 30 and 40 GHz,\nMid-Frequency (MF) modules near 90 and 150 GHz, and Ultra-High-Frequency (UHF)\nmodules near 220 and 280 GHz. Twenty-five UHF detector modules, each containing\n1720 optically-coupled TESs connected to microwave SQUID multiplexing readout,\nhave now been produced. This work summarizes the pre-deployment\ncharacterization of these detector modules in laboratory cryostats. Across all\nUHF modules, we find an average operable TES yield of 83%, equating to over\n36,000 devices tested. The distributions of (220, 280) GHz saturation powers\nhave medians of (24, 26) pW, near the centers of their target ranges. For both\nbands, the median optical efficiency is 0.6, the median effective time constant\nis 0.4 ms, and the median dark noise-equivalent power (NEP) is ~40 aW/rtHz. The\nexpected photon NEPs at (220, 280) GHz are (64, 99) aW/rtHz, indicating these\ndetectors will achieve background-limited performance on the sky. Thirty-nine\nUHF and MF detector modules are currently operating in fielded SO instruments,\nwhich are transitioning from the commissioning stage to full science\nobservations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Simons Observatory (SO) is a new suite of cosmic microwave background\ntelescopes in the Chilean Atacama Desert with an extensive science program\nspanning cosmology, Galactic and extragalactic astrophysics, and particle\nphysics. SO will survey the millimeter-wave sky over a wide range of angular\nscales using six spectral bands across three types of dichroic,\npolarization-sensitive transition-edge sensor (TES) detector modules:\nLow-Frequency (LF) modules with bandpasses centered near 30 and 40 GHz,\nMid-Frequency (MF) modules near 90 and 150 GHz, and Ultra-High-Frequency (UHF)\nmodules near 220 and 280 GHz. Twenty-five UHF detector modules, each containing\n1720 optically-coupled TESs connected to microwave SQUID multiplexing readout,\nhave now been produced. This work summarizes the pre-deployment\ncharacterization of these detector modules in laboratory cryostats. Across all\nUHF modules, we find an average operable TES yield of 83%, equating to over\n36,000 devices tested. The distributions of (220, 280) GHz saturation powers\nhave medians of (24, 26) pW, near the centers of their target ranges. For both\nbands, the median optical efficiency is 0.6, the median effective time constant\nis 0.4 ms, and the median dark noise-equivalent power (NEP) is ~40 aW/rtHz. The\nexpected photon NEPs at (220, 280) GHz are (64, 99) aW/rtHz, indicating these\ndetectors will achieve background-limited performance on the sky. Thirty-nine\nUHF and MF detector modules are currently operating in fielded SO instruments,\nwhich are transitioning from the commissioning stage to full science\nobservations."
                },
                "authors": [
                    {
                        "name": "Daniel Dutcher"
                    },
                    {
                        "name": "Peter Dow"
                    },
                    {
                        "name": "Shannon M. Duff"
                    },
                    {
                        "name": "Shawn W. Henderson"
                    },
                    {
                        "name": "Johannes Hubmayr"
                    },
                    {
                        "name": "Bradley R. Johnson"
                    },
                    {
                        "name": "Michael J. Link"
                    },
                    {
                        "name": "Tammy J. Lucas"
                    },
                    {
                        "name": "Michael D. Niemack"
                    },
                    {
                        "name": "Yudai Seino"
                    },
                    {
                        "name": "Rita F. Sonka"
                    },
                    {
                        "name": "Suzanne Staggs"
                    },
                    {
                        "name": "Yuhan Wang"
                    },
                    {
                        "name": "Kaiwen Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Kaiwen Zheng"
                },
                "author": "Kaiwen Zheng",
                "arxiv_comment": "6 pages, 7 figures. Proceedings of the 21st International Conference\n  on Low Temperature Detectors (LTD2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11973v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11973v2",
                "updated": "2025-09-26T14:07:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    7,
                    34,
                    4,
                    269,
                    0
                ],
                "published": "2024-04-18T08:01:20Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    8,
                    1,
                    20,
                    3,
                    109,
                    0
                ],
                "title": "A critical review of methods and challenges in large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical review of methods and challenges in large language models"
                },
                "summary": "This critical review provides an in-depth analysis of Large Language Models\n(LLMs), encompassing their foundational principles, diverse applications, and\nadvanced training methodologies. We critically examine the evolution from\nRecurrent Neural Networks (RNNs) to Transformer models, highlighting the\nsignificant advancements and innovations in LLM architectures. The review\nexplores state-of-the-art techniques such as in-context learning and various\nfine-tuning approaches, with an emphasis on optimizing parameter efficiency. We\nalso discuss methods for aligning LLMs with human preferences, including\nreinforcement learning frameworks and human feedback mechanisms. The emerging\ntechnique of retrieval-augmented generation, which integrates external\nknowledge into LLMs, is also evaluated. Additionally, we address the ethical\nconsiderations of deploying LLMs, stressing the importance of responsible and\nmindful application. By identifying current gaps and suggesting future research\ndirections, this review provides a comprehensive and critical overview of the\npresent state and potential advancements in LLMs. This work serves as an\ninsightful guide for researchers and practitioners in artificial intelligence,\noffering a unified perspective on the strengths, limitations, and future\nprospects of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This critical review provides an in-depth analysis of Large Language Models\n(LLMs), encompassing their foundational principles, diverse applications, and\nadvanced training methodologies. We critically examine the evolution from\nRecurrent Neural Networks (RNNs) to Transformer models, highlighting the\nsignificant advancements and innovations in LLM architectures. The review\nexplores state-of-the-art techniques such as in-context learning and various\nfine-tuning approaches, with an emphasis on optimizing parameter efficiency. We\nalso discuss methods for aligning LLMs with human preferences, including\nreinforcement learning frameworks and human feedback mechanisms. The emerging\ntechnique of retrieval-augmented generation, which integrates external\nknowledge into LLMs, is also evaluated. Additionally, we address the ethical\nconsiderations of deploying LLMs, stressing the importance of responsible and\nmindful application. By identifying current gaps and suggesting future research\ndirections, this review provides a comprehensive and critical overview of the\npresent state and potential advancements in LLMs. This work serves as an\ninsightful guide for researchers and practitioners in artificial intelligence,\noffering a unified perspective on the strengths, limitations, and future\nprospects of LLMs."
                },
                "authors": [
                    {
                        "name": "Milad Moradi"
                    },
                    {
                        "name": "Ke Yan"
                    },
                    {
                        "name": "David Colwell"
                    },
                    {
                        "name": "Matthias Samwald"
                    },
                    {
                        "name": "Rhona Asgari"
                    }
                ],
                "author_detail": {
                    "name": "Rhona Asgari"
                },
                "author": "Rhona Asgari",
                "arxiv_doi": "10.32604/cmc.2025.061263",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.32604/cmc.2025.061263",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.11973v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11973v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04349v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04349v5",
                "updated": "2025-09-26T14:04:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    4,
                    7,
                    4,
                    269,
                    0
                ],
                "published": "2025-08-06T11:42:47Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    42,
                    47,
                    2,
                    218,
                    0
                ],
                "title": "GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy\n  Entropy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy\n  Entropy"
                },
                "summary": "Reinforcement learning (RL) is a pivotal task for enhancing Large Language\nModel (LLM) reasoning. Conventional algorithms, however, typically adhere to a\ncoarse-grained credit assignment paradigm, applying a uniform reward to all\ntokens in a sequence, a critical flaw in long-chain reasoning tasks. In this\npaper, we address this challenge and propose Dynamic Entropy Weighting, a novel\nmechanism that facilitates fine-grained rewards through two new algorithms:\nGroup Token Policy Optimization (GTPO), which assigns an entropy-weighted\nreward to each token, and the analogous algorithm Sequence-Level GRPO (GRPO-S).\nOur approach is founded on the hypothesis that high policy entropy within a\nreasoning path is a powerful heuristic for cognitive effort at pivotal\njunctures, which can be repurposed into a learning signal. By repurposing\npolicy entropy for reward shaping, we achieve true per-token credit assignment.\nExperimental results across challenging reasoning benchmarks validate the\nsuperiority of our approach, showing our methods significantly outperform a\nstrong DAPO baseline and confirming our entropy-weighting mechanism as the key\ndriver of this performance boost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) is a pivotal task for enhancing Large Language\nModel (LLM) reasoning. Conventional algorithms, however, typically adhere to a\ncoarse-grained credit assignment paradigm, applying a uniform reward to all\ntokens in a sequence, a critical flaw in long-chain reasoning tasks. In this\npaper, we address this challenge and propose Dynamic Entropy Weighting, a novel\nmechanism that facilitates fine-grained rewards through two new algorithms:\nGroup Token Policy Optimization (GTPO), which assigns an entropy-weighted\nreward to each token, and the analogous algorithm Sequence-Level GRPO (GRPO-S).\nOur approach is founded on the hypothesis that high policy entropy within a\nreasoning path is a powerful heuristic for cognitive effort at pivotal\njunctures, which can be repurposed into a learning signal. By repurposing\npolicy entropy for reward shaping, we achieve true per-token credit assignment.\nExperimental results across challenging reasoning benchmarks validate the\nsuperiority of our approach, showing our methods significantly outperform a\nstrong DAPO baseline and confirming our entropy-weighting mechanism as the key\ndriver of this performance boost."
                },
                "authors": [
                    {
                        "name": "Hongze Tan"
                    },
                    {
                        "name": "Jianfei Pan"
                    },
                    {
                        "name": "Jinghao Lin"
                    },
                    {
                        "name": "Tao Chen"
                    },
                    {
                        "name": "Zhihang Zheng"
                    },
                    {
                        "name": "Zhihao Tang"
                    },
                    {
                        "name": "Haihua Yang"
                    }
                ],
                "author_detail": {
                    "name": "Haihua Yang"
                },
                "author": "Haihua Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04349v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04349v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22367v1",
                "updated": "2025-09-26T14:00:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    0,
                    51,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T14:00:51Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    0,
                    51,
                    4,
                    269,
                    0
                ],
                "title": "What Is The Political Content in LLMs' Pre- and Post-Training Data?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Is The Political Content in LLMs' Pre- and Post-Training Data?"
                },
                "summary": "Large language models (LLMs) are known to generate politically biased text,\nyet how such biases arise remains unclear. A crucial step toward answering this\nquestion is the analysis of training data, whose political content remains\nlargely underexplored in current LLM research. To address this gap, we present\nin this paper an analysis of the pre- and post-training corpora of OLMO2, the\nlargest fully open-source model released together with its complete dataset.\nFrom these corpora, we draw large random samples, automatically annotate\ndocuments for political orientation, and analyze their source domains and\ncontent. We then assess how political content in the training data correlates\nwith models' stance on specific policy issues. Our analysis shows that\nleft-leaning documents predominate across datasets, with pre-training corpora\ncontaining significantly more politically engaged content than post-training\ndata. We also find that left- and right-leaning documents frame similar topics\nthrough distinct values and sources of legitimacy. Finally, the predominant\nstance in the training data strongly correlates with models' political biases\nwhen evaluated on policy issues. These findings underscore the need to\nintegrate political content analysis into future data curation pipelines as\nwell as in-depth documentation of filtering strategies for transparency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are known to generate politically biased text,\nyet how such biases arise remains unclear. A crucial step toward answering this\nquestion is the analysis of training data, whose political content remains\nlargely underexplored in current LLM research. To address this gap, we present\nin this paper an analysis of the pre- and post-training corpora of OLMO2, the\nlargest fully open-source model released together with its complete dataset.\nFrom these corpora, we draw large random samples, automatically annotate\ndocuments for political orientation, and analyze their source domains and\ncontent. We then assess how political content in the training data correlates\nwith models' stance on specific policy issues. Our analysis shows that\nleft-leaning documents predominate across datasets, with pre-training corpora\ncontaining significantly more politically engaged content than post-training\ndata. We also find that left- and right-leaning documents frame similar topics\nthrough distinct values and sources of legitimacy. Finally, the predominant\nstance in the training data strongly correlates with models' political biases\nwhen evaluated on policy issues. These findings underscore the need to\nintegrate political content analysis into future data curation pipelines as\nwell as in-depth documentation of filtering strategies for transparency."
                },
                "authors": [
                    {
                        "name": "Tanise Ceron"
                    },
                    {
                        "name": "Dmitry Nikolaev"
                    },
                    {
                        "name": "Dominik Stammbach"
                    },
                    {
                        "name": "Debora Nozza"
                    }
                ],
                "author_detail": {
                    "name": "Debora Nozza"
                },
                "author": "Debora Nozza",
                "arxiv_comment": "9 pages, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22366v1",
                "updated": "2025-09-26T14:00:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    0,
                    20,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T14:00:20Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    0,
                    20,
                    4,
                    269,
                    0
                ],
                "title": "Exploratory Semantic Reliability Analysis of Wind Turbine Maintenance\n  Logs using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploratory Semantic Reliability Analysis of Wind Turbine Maintenance\n  Logs using Large Language Models"
                },
                "summary": "A wealth of operational intelligence is locked within the unstructured\nfree-text of wind turbine maintenance logs, a resource largely inaccessible to\ntraditional quantitative reliability analysis. While machine learning has been\napplied to this data, existing approaches typically stop at classification,\ncategorising text into predefined labels. This paper addresses the gap in\nleveraging modern large language models (LLMs) for more complex reasoning\ntasks. We introduce an exploratory framework that uses LLMs to move beyond\nclassification and perform deep semantic analysis. We apply this framework to a\nlarge industrial dataset to execute four analytical workflows: failure mode\nidentification, causal chain inference, comparative site analysis, and data\nquality auditing. The results demonstrate that LLMs can function as powerful\n\"reliability co-pilots,\" moving beyond labelling to synthesise textual\ninformation and generate actionable, expert-level hypotheses. This work\ncontributes a novel and reproducible methodology for using LLMs as a reasoning\ntool, offering a new pathway to enhance operational intelligence in the wind\nenergy sector by unlocking insights previously obscured in unstructured data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A wealth of operational intelligence is locked within the unstructured\nfree-text of wind turbine maintenance logs, a resource largely inaccessible to\ntraditional quantitative reliability analysis. While machine learning has been\napplied to this data, existing approaches typically stop at classification,\ncategorising text into predefined labels. This paper addresses the gap in\nleveraging modern large language models (LLMs) for more complex reasoning\ntasks. We introduce an exploratory framework that uses LLMs to move beyond\nclassification and perform deep semantic analysis. We apply this framework to a\nlarge industrial dataset to execute four analytical workflows: failure mode\nidentification, causal chain inference, comparative site analysis, and data\nquality auditing. The results demonstrate that LLMs can function as powerful\n\"reliability co-pilots,\" moving beyond labelling to synthesise textual\ninformation and generate actionable, expert-level hypotheses. This work\ncontributes a novel and reproducible methodology for using LLMs as a reasoning\ntool, offering a new pathway to enhance operational intelligence in the wind\nenergy sector by unlocking insights previously obscured in unstructured data."
                },
                "authors": [
                    {
                        "name": "Max Malyi"
                    },
                    {
                        "name": "Jonathan Shek"
                    },
                    {
                        "name": "Andre Biscaya"
                    }
                ],
                "author_detail": {
                    "name": "Andre Biscaya"
                },
                "author": "Andre Biscaya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22363v1",
                "updated": "2025-09-26T13:58:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    58,
                    22,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T13:58:22Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    58,
                    22,
                    4,
                    269,
                    0
                ],
                "title": "Investigating Faithfulness in Large Audio Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Faithfulness in Large Audio Language Models"
                },
                "summary": "Faithfulness measures whether chain-of-thought (CoT) representations\naccurately reflect a model's decision process and can be used as reliable\nexplanations. Prior work has shown that CoTs from text-based LLMs are often\nunfaithful. This question has not been explored for large audio-language models\n(LALMs), where faithfulness is critical for safety-sensitive applications.\nReasoning in LALMs is also more challenging, as models must first extract\nrelevant clues from audio before reasoning over them. In this paper, we\ninvestigate the faithfulness of CoTs produced by several LALMs by applying\ntargeted interventions, including paraphrasing, filler token injection, early\nanswering, and introducing mistakes, on two challenging reasoning datasets:\nSAKURA and MMAR. After going through the aforementioned interventions across\nseveral datasets and tasks, our experiments suggest that, LALMs generally\nproduce CoTs that appear to be faithful to their underlying decision processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faithfulness measures whether chain-of-thought (CoT) representations\naccurately reflect a model's decision process and can be used as reliable\nexplanations. Prior work has shown that CoTs from text-based LLMs are often\nunfaithful. This question has not been explored for large audio-language models\n(LALMs), where faithfulness is critical for safety-sensitive applications.\nReasoning in LALMs is also more challenging, as models must first extract\nrelevant clues from audio before reasoning over them. In this paper, we\ninvestigate the faithfulness of CoTs produced by several LALMs by applying\ntargeted interventions, including paraphrasing, filler token injection, early\nanswering, and introducing mistakes, on two challenging reasoning datasets:\nSAKURA and MMAR. After going through the aforementioned interventions across\nseveral datasets and tasks, our experiments suggest that, LALMs generally\nproduce CoTs that appear to be faithful to their underlying decision processes."
                },
                "authors": [
                    {
                        "name": "Lovenya Jain"
                    },
                    {
                        "name": "Pooneh Mousavi"
                    },
                    {
                        "name": "Mirco Ravanelli"
                    },
                    {
                        "name": "Cem Subakan"
                    }
                ],
                "author_detail": {
                    "name": "Cem Subakan"
                },
                "author": "Cem Subakan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22360v1",
                "updated": "2025-09-26T13:56:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    56,
                    16,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T13:56:16Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    56,
                    16,
                    4,
                    269,
                    0
                ],
                "title": "CHRONOBERG: Capturing Language Evolution and Temporal Awareness in\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHRONOBERG: Capturing Language Evolution and Temporal Awareness in\n  Foundation Models"
                },
                "summary": "Large language models (LLMs) excel at operating at scale by leveraging social\nmedia and various data crawled from the web. Whereas existing corpora are\ndiverse, their frequent lack of long-term temporal structure may however limit\nan LLM's ability to contextualize semantic and normative evolution of language\nand to capture diachronic variation. To support analysis and training for the\nlatter, we introduce CHRONOBERG, a temporally structured corpus of English book\ntexts spanning 250 years, curated from Project Gutenberg and enriched with a\nvariety of temporal annotations. First, the edited nature of books enables us\nto quantify lexical semantic change through time-sensitive\nValence-Arousal-Dominance (VAD) analysis and to construct historically\ncalibrated affective lexicons to support temporally grounded interpretation.\nWith the lexicons at hand, we demonstrate a need for modern LLM-based tools to\nbetter situate their detection of discriminatory language and contextualization\nof sentiment across various time-periods. In fact, we show how language models\ntrained sequentially on CHRONOBERG struggle to encode diachronic shifts in\nmeaning, emphasizing the need for temporally aware training and evaluation\npipelines, and positioning CHRONOBERG as a scalable resource for the study of\nlinguistic change and temporal generalization. Disclaimer: This paper includes\nlanguage and display of samples that could be offensive to readers. Open\nAccess: Chronoberg is available publicly on HuggingFace at (\nhttps://huggingface.co/datasets/spaul25/Chronoberg). Code is available at\n(https://github.com/paulsubarna/Chronoberg).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at operating at scale by leveraging social\nmedia and various data crawled from the web. Whereas existing corpora are\ndiverse, their frequent lack of long-term temporal structure may however limit\nan LLM's ability to contextualize semantic and normative evolution of language\nand to capture diachronic variation. To support analysis and training for the\nlatter, we introduce CHRONOBERG, a temporally structured corpus of English book\ntexts spanning 250 years, curated from Project Gutenberg and enriched with a\nvariety of temporal annotations. First, the edited nature of books enables us\nto quantify lexical semantic change through time-sensitive\nValence-Arousal-Dominance (VAD) analysis and to construct historically\ncalibrated affective lexicons to support temporally grounded interpretation.\nWith the lexicons at hand, we demonstrate a need for modern LLM-based tools to\nbetter situate their detection of discriminatory language and contextualization\nof sentiment across various time-periods. In fact, we show how language models\ntrained sequentially on CHRONOBERG struggle to encode diachronic shifts in\nmeaning, emphasizing the need for temporally aware training and evaluation\npipelines, and positioning CHRONOBERG as a scalable resource for the study of\nlinguistic change and temporal generalization. Disclaimer: This paper includes\nlanguage and display of samples that could be offensive to readers. Open\nAccess: Chronoberg is available publicly on HuggingFace at (\nhttps://huggingface.co/datasets/spaul25/Chronoberg). Code is available at\n(https://github.com/paulsubarna/Chronoberg)."
                },
                "authors": [
                    {
                        "name": "Niharika Hegde"
                    },
                    {
                        "name": "Subarnaduti Paul"
                    },
                    {
                        "name": "Lars Joel-Frey"
                    },
                    {
                        "name": "Manuel Brack"
                    },
                    {
                        "name": "Kristian Kersting"
                    },
                    {
                        "name": "Martin Mundt"
                    },
                    {
                        "name": "Patrick Schramowski"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Schramowski"
                },
                "author": "Patrick Schramowski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22345v1",
                "updated": "2025-09-26T13:42:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    42,
                    32,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T13:42:32Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    42,
                    32,
                    4,
                    269,
                    0
                ],
                "title": "The InviTE Corpus: Annotating Invectives in Tudor English Texts for\n  Computational Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The InviTE Corpus: Annotating Invectives in Tudor English Texts for\n  Computational Modeling"
                },
                "summary": "In this paper, we aim at the application of Natural Language Processing (NLP)\ntechniques to historical research endeavors, particularly addressing the study\nof religious invectives in the context of the Protestant Reformation in Tudor\nEngland. We outline a workflow spanning from raw data, through pre-processing\nand data selection, to an iterative annotation process. As a result, we\nintroduce the InviTE corpus -- a corpus of almost 2000 Early Modern English\n(EModE) sentences, which are enriched with expert annotations regarding\ninvective language throughout 16th-century England. Subsequently, we assess and\ncompare the performance of fine-tuned BERT-based models and zero-shot prompted\ninstruction-tuned large language models (LLMs), which highlights the\nsuperiority of models pre-trained on historical data and fine-tuned to\ninvective detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we aim at the application of Natural Language Processing (NLP)\ntechniques to historical research endeavors, particularly addressing the study\nof religious invectives in the context of the Protestant Reformation in Tudor\nEngland. We outline a workflow spanning from raw data, through pre-processing\nand data selection, to an iterative annotation process. As a result, we\nintroduce the InviTE corpus -- a corpus of almost 2000 Early Modern English\n(EModE) sentences, which are enriched with expert annotations regarding\ninvective language throughout 16th-century England. Subsequently, we assess and\ncompare the performance of fine-tuned BERT-based models and zero-shot prompted\ninstruction-tuned large language models (LLMs), which highlights the\nsuperiority of models pre-trained on historical data and fine-tuned to\ninvective detection."
                },
                "authors": [
                    {
                        "name": "Sophie Spliethoff"
                    },
                    {
                        "name": "Sanne Hoeken"
                    },
                    {
                        "name": "Silke Schwandt"
                    },
                    {
                        "name": "Sina Zarrieß"
                    },
                    {
                        "name": "Özge Alaçam"
                    }
                ],
                "author_detail": {
                    "name": "Özge Alaçam"
                },
                "author": "Özge Alaçam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22343v1",
                "updated": "2025-09-26T13:39:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    39,
                    9,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T13:39:09Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    39,
                    9,
                    4,
                    269,
                    0
                ],
                "title": "Transformers Can Learn Connectivity in Some Graphs but Not Others",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers Can Learn Connectivity in Some Graphs but Not Others"
                },
                "summary": "Reasoning capability is essential to ensure the factual correctness of the\nresponses of transformer-based Large Language Models (LLMs), and robust\nreasoning about transitive relations is instrumental in many settings, such as\ncausal inference. Hence, it is essential to investigate the capability of\ntransformers in the task of inferring transitive relations (e.g., knowing A\ncauses B and B causes C, then A causes C). The task of inferring transitive\nrelations is equivalent to the task of connectivity in directed graphs (e.g.,\nknowing there is a path from A to B, and there is a path from B to C, then\nthere is a path from A to C). Past research focused on whether transformers can\nlearn to infer transitivity from in-context examples provided in the input\nprompt. However, transformers' capability to infer transitive relations from\ntraining examples and how scaling affects the ability is unexplored. In this\nstudy, we seek to answer this question by generating directed graphs to train\ntransformer models of varying sizes and evaluate their ability to infer\ntransitive relations for various graph sizes. Our findings suggest that\ntransformers are capable of learning connectivity on \"grid-like'' directed\ngraphs where each node can be embedded in a low-dimensional subspace, and\nconnectivity is easily inferable from the embeddings of the nodes. We find that\nthe dimensionality of the underlying grid graph is a strong predictor of\ntransformers' ability to learn the connectivity task, where higher-dimensional\ngrid graphs pose a greater challenge than low-dimensional grid graphs. In\naddition, we observe that increasing the model scale leads to increasingly\nbetter generalization to infer connectivity over grid graphs. However, if the\ngraph is not a grid graph and contains many disconnected components,\ntransformers struggle to learn the connectivity task, especially when the\nnumber of components is large.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning capability is essential to ensure the factual correctness of the\nresponses of transformer-based Large Language Models (LLMs), and robust\nreasoning about transitive relations is instrumental in many settings, such as\ncausal inference. Hence, it is essential to investigate the capability of\ntransformers in the task of inferring transitive relations (e.g., knowing A\ncauses B and B causes C, then A causes C). The task of inferring transitive\nrelations is equivalent to the task of connectivity in directed graphs (e.g.,\nknowing there is a path from A to B, and there is a path from B to C, then\nthere is a path from A to C). Past research focused on whether transformers can\nlearn to infer transitivity from in-context examples provided in the input\nprompt. However, transformers' capability to infer transitive relations from\ntraining examples and how scaling affects the ability is unexplored. In this\nstudy, we seek to answer this question by generating directed graphs to train\ntransformer models of varying sizes and evaluate their ability to infer\ntransitive relations for various graph sizes. Our findings suggest that\ntransformers are capable of learning connectivity on \"grid-like'' directed\ngraphs where each node can be embedded in a low-dimensional subspace, and\nconnectivity is easily inferable from the embeddings of the nodes. We find that\nthe dimensionality of the underlying grid graph is a strong predictor of\ntransformers' ability to learn the connectivity task, where higher-dimensional\ngrid graphs pose a greater challenge than low-dimensional grid graphs. In\naddition, we observe that increasing the model scale leads to increasingly\nbetter generalization to infer connectivity over grid graphs. However, if the\ngraph is not a grid graph and contains many disconnected components,\ntransformers struggle to learn the connectivity task, especially when the\nnumber of components is large."
                },
                "authors": [
                    {
                        "name": "Amit Roy"
                    },
                    {
                        "name": "Abulhair Saparov"
                    }
                ],
                "author_detail": {
                    "name": "Abulhair Saparov"
                },
                "author": "Abulhair Saparov",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01522v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01522v2",
                "updated": "2025-09-26T13:39:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    39,
                    0,
                    4,
                    269,
                    0
                ],
                "published": "2025-08-02T23:52:33Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    23,
                    52,
                    33,
                    5,
                    214,
                    0
                ],
                "title": "Decentralized Aerial Manipulation of a Cable-Suspended Load using\n  Multi-Agent Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Aerial Manipulation of a Cable-Suspended Load using\n  Multi-Agent Reinforcement Learning"
                },
                "summary": "This paper presents the first decentralized method to enable real-world 6-DoF\nmanipulation of a cable-suspended load using a team of Micro-Aerial Vehicles\n(MAVs). Our method leverages multi-agent reinforcement learning (MARL) to train\nan outer-loop control policy for each MAV. Unlike state-of-the-art controllers\nthat utilize a centralized scheme, our policy does not require global states,\ninter-MAV communications, nor neighboring MAV information. Instead, agents\ncommunicate implicitly through load pose observations alone, which enables high\nscalability and flexibility. It also significantly reduces computing costs\nduring inference time, enabling onboard deployment of the policy. In addition,\nwe introduce a new action space design for the MAVs using linear acceleration\nand body rates. This choice, combined with a robust low-level controller,\nenables reliable sim-to-real transfer despite significant uncertainties caused\nby cable tension during dynamic 3D motion. We validate our method in various\nreal-world experiments, including full-pose control under load model\nuncertainties, showing setpoint tracking performance comparable to the\nstate-of-the-art centralized method. We also demonstrate cooperation amongst\nagents with heterogeneous control policies, and robustness to the complete\nin-flight loss of one MAV. Videos of experiments:\nhttps://autonomousrobots.nl/paper_websites/aerial-manipulation-marl",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the first decentralized method to enable real-world 6-DoF\nmanipulation of a cable-suspended load using a team of Micro-Aerial Vehicles\n(MAVs). Our method leverages multi-agent reinforcement learning (MARL) to train\nan outer-loop control policy for each MAV. Unlike state-of-the-art controllers\nthat utilize a centralized scheme, our policy does not require global states,\ninter-MAV communications, nor neighboring MAV information. Instead, agents\ncommunicate implicitly through load pose observations alone, which enables high\nscalability and flexibility. It also significantly reduces computing costs\nduring inference time, enabling onboard deployment of the policy. In addition,\nwe introduce a new action space design for the MAVs using linear acceleration\nand body rates. This choice, combined with a robust low-level controller,\nenables reliable sim-to-real transfer despite significant uncertainties caused\nby cable tension during dynamic 3D motion. We validate our method in various\nreal-world experiments, including full-pose control under load model\nuncertainties, showing setpoint tracking performance comparable to the\nstate-of-the-art centralized method. We also demonstrate cooperation amongst\nagents with heterogeneous control policies, and robustness to the complete\nin-flight loss of one MAV. Videos of experiments:\nhttps://autonomousrobots.nl/paper_websites/aerial-manipulation-marl"
                },
                "authors": [
                    {
                        "name": "Jack Zeng"
                    },
                    {
                        "name": "Andreu Matoses Gimenez"
                    },
                    {
                        "name": "Eugene Vinitsky"
                    },
                    {
                        "name": "Javier Alonso-Mora"
                    },
                    {
                        "name": "Sihao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Sihao Sun"
                },
                "author": "Sihao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01522v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01522v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.9; I.2.11; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16975v2",
                "updated": "2025-09-26T13:37:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    37,
                    33,
                    4,
                    269,
                    0
                ],
                "published": "2025-06-20T13:08:12Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    8,
                    12,
                    4,
                    171,
                    0
                ],
                "title": "Latent Concept Disentanglement in Transformer-based Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Concept Disentanglement in Transformer-based Language Models"
                },
                "summary": "When large language models (LLMs) use in-context learning (ICL) to solve a\nnew task, they must infer latent concepts from demonstration examples. This\nraises the question of whether and how transformers represent latent structures\nas part of their computation. Our work experiments with several controlled\ntasks, studying this question using mechanistic interpretability. First, we\nshow that in transitive reasoning tasks with a latent, discrete concept, the\nmodel successfully identifies the latent concept and does step-by-step concept\ncomposition. This builds upon prior work that analyzes single-step reasoning.\nThen, we consider tasks parameterized by a latent numerical concept. We\ndiscover low-dimensional subspaces in the model's representation space, where\nthe geometry cleanly reflects the underlying parameterization. Overall, we show\nthat small and large models can indeed disentangle and utilize latent concepts\nthat they learn in-context from a handful of abbreviated demonstrations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When large language models (LLMs) use in-context learning (ICL) to solve a\nnew task, they must infer latent concepts from demonstration examples. This\nraises the question of whether and how transformers represent latent structures\nas part of their computation. Our work experiments with several controlled\ntasks, studying this question using mechanistic interpretability. First, we\nshow that in transitive reasoning tasks with a latent, discrete concept, the\nmodel successfully identifies the latent concept and does step-by-step concept\ncomposition. This builds upon prior work that analyzes single-step reasoning.\nThen, we consider tasks parameterized by a latent numerical concept. We\ndiscover low-dimensional subspaces in the model's representation space, where\nthe geometry cleanly reflects the underlying parameterization. Overall, we show\nthat small and large models can indeed disentangle and utilize latent concepts\nthat they learn in-context from a handful of abbreviated demonstrations."
                },
                "authors": [
                    {
                        "name": "Guan Zhe Hong"
                    },
                    {
                        "name": "Bhavya Vasudeva"
                    },
                    {
                        "name": "Vatsal Sharan"
                    },
                    {
                        "name": "Cyrus Rashtchian"
                    },
                    {
                        "name": "Prabhakar Raghavan"
                    },
                    {
                        "name": "Rina Panigrahy"
                    }
                ],
                "author_detail": {
                    "name": "Rina Panigrahy"
                },
                "author": "Rina Panigrahy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22338v1",
                "updated": "2025-09-26T13:30:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    30,
                    50,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T13:30:50Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    30,
                    50,
                    4,
                    269,
                    0
                ],
                "title": "Advancing Natural Language Formalization to First Order Logic with\n  Fine-tuned LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Natural Language Formalization to First Order Logic with\n  Fine-tuned LLMs"
                },
                "summary": "Automating the translation of natural language to first-order logic (FOL) is\ncrucial for knowledge representation and formal methods, yet remains\nchallenging. We present a systematic evaluation of fine-tuned LLMs for this\ntask, comparing architectures (encoder-decoder vs. decoder-only) and training\nstrategies. Using the MALLS and Willow datasets, we explore techniques like\nvocabulary extension, predicate conditioning, and multilingual training,\nintroducing metrics for exact match, logical equivalence, and predicate\nalignment. Our fine-tuned Flan-T5-XXL achieves 70% accuracy with predicate\nlists, outperforming GPT-4o and even the DeepSeek-R1-0528 model with CoT\nreasoning ability as well as symbolic systems like ccg2lambda. Key findings\nshow: (1) predicate availability boosts performance by 15-20%, (2) T5 models\nsurpass larger decoder-only LLMs, and (3) models generalize to unseen logical\narguments (FOLIO dataset) without specific training. While structural logic\ntranslation proves robust, predicate extraction emerges as the main bottleneck.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating the translation of natural language to first-order logic (FOL) is\ncrucial for knowledge representation and formal methods, yet remains\nchallenging. We present a systematic evaluation of fine-tuned LLMs for this\ntask, comparing architectures (encoder-decoder vs. decoder-only) and training\nstrategies. Using the MALLS and Willow datasets, we explore techniques like\nvocabulary extension, predicate conditioning, and multilingual training,\nintroducing metrics for exact match, logical equivalence, and predicate\nalignment. Our fine-tuned Flan-T5-XXL achieves 70% accuracy with predicate\nlists, outperforming GPT-4o and even the DeepSeek-R1-0528 model with CoT\nreasoning ability as well as symbolic systems like ccg2lambda. Key findings\nshow: (1) predicate availability boosts performance by 15-20%, (2) T5 models\nsurpass larger decoder-only LLMs, and (3) models generalize to unseen logical\narguments (FOLIO dataset) without specific training. While structural logic\ntranslation proves robust, predicate extraction emerges as the main bottleneck."
                },
                "authors": [
                    {
                        "name": "Felix Vossel"
                    },
                    {
                        "name": "Till Mossakowski"
                    },
                    {
                        "name": "Björn Gehrke"
                    }
                ],
                "author_detail": {
                    "name": "Björn Gehrke"
                },
                "author": "Björn Gehrke",
                "arxiv_comment": "15 pages, 7 tables, accepted at the International Joint Conference on\n  Learning & Reasoning (IJCLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "03B10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04020v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04020v3",
                "updated": "2025-09-26T13:30:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    30,
                    48,
                    4,
                    269,
                    0
                ],
                "published": "2024-07-04T15:55:13Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    15,
                    55,
                    13,
                    3,
                    186,
                    0
                ],
                "title": "LLMAEL: Large Language Models are Good Context Augmenters for Entity\n  Linking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMAEL: Large Language Models are Good Context Augmenters for Entity\n  Linking"
                },
                "summary": "Specialized entity linking (EL) models are well-trained at mapping mentions\nto unique knowledge base (KB) entities according to a given context. However,\nspecialized EL models struggle to disambiguate long-tail entities due to their\nlimited training data. Meanwhile, extensively pre-trained large language models\n(LLMs) possess broader knowledge of uncommon entities. Yet, with a lack of\nspecialized EL training, LLMs frequently fail to generate accurate KB entity\nnames, limiting their standalone effectiveness in EL. With the observation that\nLLMs are more adept at context generation instead of EL execution, we introduce\nLLM-Augmented Entity Linking (LLMAEL), the first framework to enhance\nspecialized EL models with LLM data augmentation. LLMAEL leverages\noff-the-shelf, tuning-free LLMs as context augmenters, generating entity\ndescriptions to serve as additional input for specialized EL models.\nExperiments show that LLMAEL sets new state-of-the-art results across 6 widely\nadopted EL benchmarks: compared to prior methods that integrate tuning-free\nLLMs into EL, LLMAEL achieves an absolute 8.9% gain in EL accuracy. We release\nour code and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Specialized entity linking (EL) models are well-trained at mapping mentions\nto unique knowledge base (KB) entities according to a given context. However,\nspecialized EL models struggle to disambiguate long-tail entities due to their\nlimited training data. Meanwhile, extensively pre-trained large language models\n(LLMs) possess broader knowledge of uncommon entities. Yet, with a lack of\nspecialized EL training, LLMs frequently fail to generate accurate KB entity\nnames, limiting their standalone effectiveness in EL. With the observation that\nLLMs are more adept at context generation instead of EL execution, we introduce\nLLM-Augmented Entity Linking (LLMAEL), the first framework to enhance\nspecialized EL models with LLM data augmentation. LLMAEL leverages\noff-the-shelf, tuning-free LLMs as context augmenters, generating entity\ndescriptions to serve as additional input for specialized EL models.\nExperiments show that LLMAEL sets new state-of-the-art results across 6 widely\nadopted EL benchmarks: compared to prior methods that integrate tuning-free\nLLMs into EL, LLMAEL achieves an absolute 8.9% gain in EL accuracy. We release\nour code and datasets."
                },
                "authors": [
                    {
                        "name": "Amy Xin"
                    },
                    {
                        "name": "Yunjia Qi"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Fangwei Zhu"
                    },
                    {
                        "name": "Kaisheng Zeng"
                    },
                    {
                        "name": "Xu Bin"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04020v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04020v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11023v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11023v3",
                "updated": "2025-09-26T13:30:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    30,
                    21,
                    4,
                    269,
                    0
                ],
                "published": "2025-03-14T02:43:29Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    2,
                    43,
                    29,
                    4,
                    73,
                    0
                ],
                "title": "Beyond A Single AI Cluster: A Survey of Decentralized LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond A Single AI Cluster: A Survey of Decentralized LLM Training"
                },
                "summary": "The emergence of large language models (LLMs) has revolutionized AI\ndevelopment, yet the resource demands beyond a single cluster or even\ndatacenter, limiting accessibility to well-resourced organizations.\nDecentralized training has emerged as a promising paradigm to leverage\ndispersed resources across clusters, datacenters and regions, offering the\npotential to democratize LLM development for broader communities. As the first\ncomprehensive exploration of this emerging field, we present decentralized LLM\ntraining as a resource-driven paradigm and categorize existing efforts into\ncommunity-driven and organizational approaches. We further clarify this\nthrough: (1) a comparison with related paradigms, (2) a characterization of\ndecentralized resources, and (3) a taxonomy of recent advancements. We also\nprovide up-to-date case studies and outline future directions to advance\nresearch in decentralized LLM training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) has revolutionized AI\ndevelopment, yet the resource demands beyond a single cluster or even\ndatacenter, limiting accessibility to well-resourced organizations.\nDecentralized training has emerged as a promising paradigm to leverage\ndispersed resources across clusters, datacenters and regions, offering the\npotential to democratize LLM development for broader communities. As the first\ncomprehensive exploration of this emerging field, we present decentralized LLM\ntraining as a resource-driven paradigm and categorize existing efforts into\ncommunity-driven and organizational approaches. We further clarify this\nthrough: (1) a comparison with related paradigms, (2) a characterization of\ndecentralized resources, and (3) a taxonomy of recent advancements. We also\nprovide up-to-date case studies and outline future directions to advance\nresearch in decentralized LLM training."
                },
                "authors": [
                    {
                        "name": "Haotian Dong"
                    },
                    {
                        "name": "Jingyan Jiang"
                    },
                    {
                        "name": "Rongwei Lu"
                    },
                    {
                        "name": "Jiajun Luo"
                    },
                    {
                        "name": "Jiajun Song"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Ying Shen"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11023v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11023v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22327v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22327v1",
                "updated": "2025-09-26T13:23:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    23,
                    16,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T13:23:16Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    23,
                    16,
                    4,
                    269,
                    0
                ],
                "title": "Stacked Intelligent Metasurface-Enhanced Wideband Multiuser MIMO OFDM-IM\n  Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stacked Intelligent Metasurface-Enhanced Wideband Multiuser MIMO OFDM-IM\n  Communications"
                },
                "summary": "Leveraging the multilayer realization of programmable metasurfaces, stacked\nintelligent metasurfaces (SIM) enable fine-grained wave-domain control.\nHowever, their wideband deployment is impeded by two structural factors: (i) a\nsingle, quasi-static SIM phase tensor must adapt to all subcarriers, and (ii)\nmultiuser scheduling changes the subcarrier activation pattern frame by frame,\nrequiring rapid reconfiguration. To address both challenges, we develop a\nSIM-enhanced wideband multiuser transceiver built on orthogonal\nfrequency-division multiplexing with index modulation (OFDM-IM). The sparse\nactivation of OFDM-IM confines high-fidelity equalization to the active tones,\neffectively widening the usable bandwidth. To make the design\nreliability-aware, we directly target the worst-link bit-error rate (BER) and\nadopt a max-min per-tone signal-to-interference-plus-noise ratio (SINR) as a\nprincipled surrogate, turning the reliability optimization tractable. For\nframe-rate inference and interpretability, we propose an unfolded\nprojected-gradient-descent network (UPGD-Net) that double-unrolls across the\nSIM's layers and algorithmic iterations: each cell computes the analytic\ngradient from the cascaded precoder with a learnable per-iteration step size.\nSimulations on wideband multiuser downlinks show fast, monotone convergence, an\nevident layer-depth sweet spot, and consistent gains in worst-link BER and sum\nrate. By combining structural sparsity with a BER-driven, deep-unfolded\noptimization backbone, the proposed framework directly addresses the key\nwideband deficiencies of SIM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging the multilayer realization of programmable metasurfaces, stacked\nintelligent metasurfaces (SIM) enable fine-grained wave-domain control.\nHowever, their wideband deployment is impeded by two structural factors: (i) a\nsingle, quasi-static SIM phase tensor must adapt to all subcarriers, and (ii)\nmultiuser scheduling changes the subcarrier activation pattern frame by frame,\nrequiring rapid reconfiguration. To address both challenges, we develop a\nSIM-enhanced wideband multiuser transceiver built on orthogonal\nfrequency-division multiplexing with index modulation (OFDM-IM). The sparse\nactivation of OFDM-IM confines high-fidelity equalization to the active tones,\neffectively widening the usable bandwidth. To make the design\nreliability-aware, we directly target the worst-link bit-error rate (BER) and\nadopt a max-min per-tone signal-to-interference-plus-noise ratio (SINR) as a\nprincipled surrogate, turning the reliability optimization tractable. For\nframe-rate inference and interpretability, we propose an unfolded\nprojected-gradient-descent network (UPGD-Net) that double-unrolls across the\nSIM's layers and algorithmic iterations: each cell computes the analytic\ngradient from the cascaded precoder with a learnable per-iteration step size.\nSimulations on wideband multiuser downlinks show fast, monotone convergence, an\nevident layer-depth sweet spot, and consistent gains in worst-link BER and sum\nrate. By combining structural sparsity with a BER-driven, deep-unfolded\noptimization backbone, the proposed framework directly addresses the key\nwideband deficiencies of SIM."
                },
                "authors": [
                    {
                        "name": "Zheao Li"
                    },
                    {
                        "name": "Jiancheng An"
                    },
                    {
                        "name": "Chau Yuen"
                    }
                ],
                "author_detail": {
                    "name": "Chau Yuen"
                },
                "author": "Chau Yuen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22327v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22327v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22319v1",
                "updated": "2025-09-26T13:19:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    19,
                    32,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T13:19:32Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    19,
                    32,
                    4,
                    269,
                    0
                ],
                "title": "Progressive Weight Loading: Accelerating Initial Inference and Gradually\n  Boosting Performance on Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progressive Weight Loading: Accelerating Initial Inference and Gradually\n  Boosting Performance on Resource-Constrained Environments"
                },
                "summary": "Deep learning models have become increasingly large and complex, resulting in\nhigher memory consumption and computational demands. Consequently, model\nloading times and initial inference latency have increased, posing significant\nchallenges in mobile and latency-sensitive environments where frequent model\nloading and unloading are required, which directly impacts user experience.\nWhile Knowledge Distillation (KD) offers a solution by compressing large\nteacher models into smaller student ones, it often comes at the cost of reduced\nperformance. To address this trade-off, we propose Progressive Weight Loading\n(PWL), a novel technique that enables fast initial inference by first deploying\na lightweight student model, then incrementally replacing its layers with those\nof a pre-trained teacher model. To support seamless layer substitution, we\nintroduce a training method that not only aligns intermediate feature\nrepresentations between student and teacher layers, but also improves the\noverall output performance of the student model. Our experiments on VGG,\nResNet, and ViT architectures demonstrate that models trained with PWL maintain\ncompetitive distillation performance and gradually improve accuracy as teacher\nlayers are loaded-matching the final accuracy of the full teacher model without\ncompromising initial inference speed. This makes PWL particularly suited for\ndynamic, resource-constrained deployments where both responsiveness and\nperformance are critical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have become increasingly large and complex, resulting in\nhigher memory consumption and computational demands. Consequently, model\nloading times and initial inference latency have increased, posing significant\nchallenges in mobile and latency-sensitive environments where frequent model\nloading and unloading are required, which directly impacts user experience.\nWhile Knowledge Distillation (KD) offers a solution by compressing large\nteacher models into smaller student ones, it often comes at the cost of reduced\nperformance. To address this trade-off, we propose Progressive Weight Loading\n(PWL), a novel technique that enables fast initial inference by first deploying\na lightweight student model, then incrementally replacing its layers with those\nof a pre-trained teacher model. To support seamless layer substitution, we\nintroduce a training method that not only aligns intermediate feature\nrepresentations between student and teacher layers, but also improves the\noverall output performance of the student model. Our experiments on VGG,\nResNet, and ViT architectures demonstrate that models trained with PWL maintain\ncompetitive distillation performance and gradually improve accuracy as teacher\nlayers are loaded-matching the final accuracy of the full teacher model without\ncompromising initial inference speed. This makes PWL particularly suited for\ndynamic, resource-constrained deployments where both responsiveness and\nperformance are critical."
                },
                "authors": [
                    {
                        "name": "Hyunwoo Kim"
                    },
                    {
                        "name": "Junha Lee"
                    },
                    {
                        "name": "Mincheol Choi"
                    },
                    {
                        "name": "Jeonghwan Lee"
                    },
                    {
                        "name": "Jaeshin Cho"
                    }
                ],
                "author_detail": {
                    "name": "Jaeshin Cho"
                },
                "author": "Jaeshin Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22317v1",
                "updated": "2025-09-26T13:18:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    18,
                    13,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T13:18:13Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    18,
                    13,
                    4,
                    269,
                    0
                ],
                "title": "Cross-Dialect Bird Species Recognition with Dialect-Calibrated\n  Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Dialect Bird Species Recognition with Dialect-Calibrated\n  Augmentation"
                },
                "summary": "Dialect variation hampers automatic recognition of bird calls collected by\npassive acoustic monitoring. We address the problem on DB3V, a three-region,\nten-species corpus of 8-s clips, and propose a deployable framework built on\nTime-Delay Neural Networks (TDNNs). Frequency-sensitive normalisation (Instance\nFrequency Normalisation and a gated Relaxed-IFN) is paired with\ngradient-reversal adversarial training to learn region-invariant embeddings. A\nmulti-level augmentation scheme combines waveform perturbations, Mixup for rare\nclasses, and CycleGAN transfer that synthesises Region 2 (Interior\nPlains)-style audio, , with Dialect-Calibrated Augmentation (DCA) softly\ndown-weighting synthetic samples to limit artifacts. The complete system lifts\ncross-dialect accuracy by up to twenty percentage points over baseline TDNNs\nwhile preserving in-region performance. Grad-CAM and LIME analyses show that\nrobust models concentrate on stable harmonic bands, providing ecologically\nmeaningful explanations. The study demonstrates that lightweight, transparent,\nand dialect-resilient bird-sound recognition is attainable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialect variation hampers automatic recognition of bird calls collected by\npassive acoustic monitoring. We address the problem on DB3V, a three-region,\nten-species corpus of 8-s clips, and propose a deployable framework built on\nTime-Delay Neural Networks (TDNNs). Frequency-sensitive normalisation (Instance\nFrequency Normalisation and a gated Relaxed-IFN) is paired with\ngradient-reversal adversarial training to learn region-invariant embeddings. A\nmulti-level augmentation scheme combines waveform perturbations, Mixup for rare\nclasses, and CycleGAN transfer that synthesises Region 2 (Interior\nPlains)-style audio, , with Dialect-Calibrated Augmentation (DCA) softly\ndown-weighting synthetic samples to limit artifacts. The complete system lifts\ncross-dialect accuracy by up to twenty percentage points over baseline TDNNs\nwhile preserving in-region performance. Grad-CAM and LIME analyses show that\nrobust models concentrate on stable harmonic bands, providing ecologically\nmeaningful explanations. The study demonstrates that lightweight, transparent,\nand dialect-resilient bird-sound recognition is attainable."
                },
                "authors": [
                    {
                        "name": "Jiani Ding"
                    },
                    {
                        "name": "Qiyang Sun"
                    },
                    {
                        "name": "Alican Akman"
                    },
                    {
                        "name": "Björn W. Schuller"
                    }
                ],
                "author_detail": {
                    "name": "Björn W. Schuller"
                },
                "author": "Björn W. Schuller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22315v1",
                "updated": "2025-09-26T13:16:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    16,
                    36,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T13:16:36Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    16,
                    36,
                    4,
                    269,
                    0
                ],
                "title": "PRIME: Planning and Retrieval-Integrated Memory for Enhanced Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRIME: Planning and Retrieval-Integrated Memory for Enhanced Reasoning"
                },
                "summary": "Inspired by the dual-process theory of human cognition from \\textit{Thinking,\nFast and Slow}, we introduce \\textbf{PRIME} (Planning and Retrieval-Integrated\nMemory for Enhanced Reasoning), a multi-agent reasoning framework that\ndynamically integrates \\textbf{System 1} (fast, intuitive thinking) and\n\\textbf{System 2} (slow, deliberate thinking). PRIME first employs a Quick\nThinking Agent (System 1) to generate a rapid answer; if uncertainty is\ndetected, it then triggers a structured System 2 reasoning pipeline composed of\nspecialized agents for \\textit{planning}, \\textit{hypothesis generation},\n\\textit{retrieval}, \\textit{information integration}, and\n\\textit{decision-making}. This multi-agent design faithfully mimics human\ncognitive processes and enhances both efficiency and accuracy. Experimental\nresults with LLaMA 3 models demonstrate that PRIME enables open-source LLMs to\nperform competitively with state-of-the-art closed-source models like GPT-4 and\nGPT-4o on benchmarks requiring multi-hop and knowledge-grounded reasoning. This\nresearch establishes PRIME as a scalable solution for improving LLMs in domains\nrequiring complex, knowledge-intensive reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspired by the dual-process theory of human cognition from \\textit{Thinking,\nFast and Slow}, we introduce \\textbf{PRIME} (Planning and Retrieval-Integrated\nMemory for Enhanced Reasoning), a multi-agent reasoning framework that\ndynamically integrates \\textbf{System 1} (fast, intuitive thinking) and\n\\textbf{System 2} (slow, deliberate thinking). PRIME first employs a Quick\nThinking Agent (System 1) to generate a rapid answer; if uncertainty is\ndetected, it then triggers a structured System 2 reasoning pipeline composed of\nspecialized agents for \\textit{planning}, \\textit{hypothesis generation},\n\\textit{retrieval}, \\textit{information integration}, and\n\\textit{decision-making}. This multi-agent design faithfully mimics human\ncognitive processes and enhances both efficiency and accuracy. Experimental\nresults with LLaMA 3 models demonstrate that PRIME enables open-source LLMs to\nperform competitively with state-of-the-art closed-source models like GPT-4 and\nGPT-4o on benchmarks requiring multi-hop and knowledge-grounded reasoning. This\nresearch establishes PRIME as a scalable solution for improving LLMs in domains\nrequiring complex, knowledge-intensive reasoning."
                },
                "authors": [
                    {
                        "name": "Hieu Tran"
                    },
                    {
                        "name": "Zonghai Yao"
                    },
                    {
                        "name": "Nguyen Luong Tran"
                    },
                    {
                        "name": "Zhichao Yang"
                    },
                    {
                        "name": "Feiyun Ouyang"
                    },
                    {
                        "name": "Shuo Han"
                    },
                    {
                        "name": "Razieh Rahimi"
                    },
                    {
                        "name": "Hong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Yu"
                },
                "author": "Hong Yu",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16495v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16495v4",
                "updated": "2025-09-26T13:16:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    16,
                    28,
                    4,
                    269,
                    0
                ],
                "published": "2024-11-25T15:35:51Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    35,
                    51,
                    0,
                    330,
                    0
                ],
                "title": "AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous\n  Knowledge Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous\n  Knowledge Reasoning"
                },
                "summary": "Despite the outstanding capabilities of large language models (LLMs),\nknowledge-intensive reasoning still remains a challenging task due to LLMs'\nlimitations in compositional reasoning and the hallucination problem. A\nprevalent solution is to employ chain-of-thought (CoT) with retrieval-augmented\ngeneration (RAG), which first formulates a reasoning plan by decomposing\ncomplex questions into simpler sub-questions, and then applies iterative RAG at\neach sub-question. However, prior works exhibit two crucial problems:\ninadequate reasoning planning and poor incorporation of heterogeneous\nknowledge. In this paper, we introduce AtomR, a framework for LLMs to conduct\naccurate heterogeneous knowledge reasoning at the atomic level. Inspired by how\nknowledge graph query languages model compositional reasoning through combining\npredefined operations, we propose three atomic knowledge operators, a unified\nset of operators for LLMs to retrieve and manipulate knowledge from\nheterogeneous sources. First, in the reasoning planning stage, AtomR decomposes\na complex question into a reasoning tree where each leaf node corresponds to an\natomic knowledge operator, achieving question decomposition that is highly\nfine-grained and orthogonal. Subsequently, in the reasoning execution stage,\nAtomR executes each atomic knowledge operator, which flexibly selects,\nretrieves, and operates atomic level knowledge from heterogeneous sources. We\nalso introduce BlendQA, a challenging benchmark specially tailored for\nheterogeneous knowledge reasoning. Experiments on three single-source and two\nmulti-source datasets show that AtomR outperforms state-of-the-art baselines by\na large margin, with F1 score improvements of 9.4% on 2WikiMultihop and 9.5% on\nBlendQA. We release our code and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the outstanding capabilities of large language models (LLMs),\nknowledge-intensive reasoning still remains a challenging task due to LLMs'\nlimitations in compositional reasoning and the hallucination problem. A\nprevalent solution is to employ chain-of-thought (CoT) with retrieval-augmented\ngeneration (RAG), which first formulates a reasoning plan by decomposing\ncomplex questions into simpler sub-questions, and then applies iterative RAG at\neach sub-question. However, prior works exhibit two crucial problems:\ninadequate reasoning planning and poor incorporation of heterogeneous\nknowledge. In this paper, we introduce AtomR, a framework for LLMs to conduct\naccurate heterogeneous knowledge reasoning at the atomic level. Inspired by how\nknowledge graph query languages model compositional reasoning through combining\npredefined operations, we propose three atomic knowledge operators, a unified\nset of operators for LLMs to retrieve and manipulate knowledge from\nheterogeneous sources. First, in the reasoning planning stage, AtomR decomposes\na complex question into a reasoning tree where each leaf node corresponds to an\natomic knowledge operator, achieving question decomposition that is highly\nfine-grained and orthogonal. Subsequently, in the reasoning execution stage,\nAtomR executes each atomic knowledge operator, which flexibly selects,\nretrieves, and operates atomic level knowledge from heterogeneous sources. We\nalso introduce BlendQA, a challenging benchmark specially tailored for\nheterogeneous knowledge reasoning. Experiments on three single-source and two\nmulti-source datasets show that AtomR outperforms state-of-the-art baselines by\na large margin, with F1 score improvements of 9.4% on 2WikiMultihop and 9.5% on\nBlendQA. We release our code and datasets."
                },
                "authors": [
                    {
                        "name": "Amy Xin"
                    },
                    {
                        "name": "Jinxin Liu"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Zhicheng Lee"
                    },
                    {
                        "name": "Shulin Cao"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16495v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16495v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22310v1",
                "updated": "2025-09-26T13:14:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    14,
                    3,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T13:14:03Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    14,
                    3,
                    4,
                    269,
                    0
                ],
                "title": "Adaptive Policy Backbone via Shared Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Policy Backbone via Shared Network"
                },
                "summary": "Reinforcement learning (RL) has achieved impressive results across domains,\nyet learning an optimal policy typically requires extensive interaction data,\nlimiting practical deployment. A common remedy is to leverage priors, such as\npre-collected datasets or reference policies, but their utility degrades under\ntask mismatch between training and deployment. While prior work has sought to\naddress this mismatch, it has largely been restricted to in-distribution\nsettings. To address this challenge, we propose Adaptive Policy Backbone (APB),\na meta-transfer RL method that inserts lightweight linear layers before and\nafter a shared backbone, thereby enabling parameter-efficient fine-tuning\n(PEFT) while preserving prior knowledge during adaptation. Our results show\nthat APB improves sample efficiency over standard RL and adapts to\nout-of-distribution (OOD) tasks where existing meta-RL baselines typically\nfail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has achieved impressive results across domains,\nyet learning an optimal policy typically requires extensive interaction data,\nlimiting practical deployment. A common remedy is to leverage priors, such as\npre-collected datasets or reference policies, but their utility degrades under\ntask mismatch between training and deployment. While prior work has sought to\naddress this mismatch, it has largely been restricted to in-distribution\nsettings. To address this challenge, we propose Adaptive Policy Backbone (APB),\na meta-transfer RL method that inserts lightweight linear layers before and\nafter a shared backbone, thereby enabling parameter-efficient fine-tuning\n(PEFT) while preserving prior knowledge during adaptation. Our results show\nthat APB improves sample efficiency over standard RL and adapts to\nout-of-distribution (OOD) tasks where existing meta-RL baselines typically\nfail."
                },
                "authors": [
                    {
                        "name": "Bumgeun Park"
                    },
                    {
                        "name": "Donghwan Lee"
                    }
                ],
                "author_detail": {
                    "name": "Donghwan Lee"
                },
                "author": "Donghwan Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22307v1",
                "updated": "2025-09-26T13:12:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    12,
                    43,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T13:12:43Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    12,
                    43,
                    4,
                    269,
                    0
                ],
                "title": "Johnson-Lindenstrauss Lemma Guided Network for Efficient 3D Medical\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Johnson-Lindenstrauss Lemma Guided Network for Efficient 3D Medical\n  Segmentation"
                },
                "summary": "Lightweight 3D medical image segmentation remains constrained by a\nfundamental \"efficiency / robustness conflict\", particularly when processing\ncomplex anatomical structures and heterogeneous modalities. In this paper, we\nstudy how to redesign the framework based on the characteristics of\nhigh-dimensional 3D images, and explore data synergy to overcome the fragile\nrepresentation of lightweight methods. Our approach, VeloxSeg, begins with a\ndeployable and extensible dual-stream CNN-Transformer architecture composed of\nPaired Window Attention (PWA) and Johnson-Lindenstrauss lemma-guided\nconvolution (JLC). For each 3D image, we invoke a \"glance-and-focus\" principle,\nwhere PWA rapidly retrieves multi-scale information, and JLC ensures robust\nlocal feature extraction with minimal parameters, significantly enhancing the\nmodel's ability to operate with low computational budget. Followed by an\nextension of the dual-stream architecture that incorporates modal interaction\ninto the multi-scale image-retrieval process, VeloxSeg efficiently models\nheterogeneous modalities. Finally, Spatially Decoupled Knowledge Transfer\n(SDKT) via Gram matrices injects the texture prior extracted by a\nself-supervised network into the segmentation network, yielding stronger\nrepresentations than baselines at no extra inference cost. Experimental results\non multimodal benchmarks show that VeloxSeg achieves a 26% Dice improvement,\nalongside increasing GPU throughput by 11x and CPU by 48x. Codes are available\nat https://github.com/JinPLu/VeloxSeg.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight 3D medical image segmentation remains constrained by a\nfundamental \"efficiency / robustness conflict\", particularly when processing\ncomplex anatomical structures and heterogeneous modalities. In this paper, we\nstudy how to redesign the framework based on the characteristics of\nhigh-dimensional 3D images, and explore data synergy to overcome the fragile\nrepresentation of lightweight methods. Our approach, VeloxSeg, begins with a\ndeployable and extensible dual-stream CNN-Transformer architecture composed of\nPaired Window Attention (PWA) and Johnson-Lindenstrauss lemma-guided\nconvolution (JLC). For each 3D image, we invoke a \"glance-and-focus\" principle,\nwhere PWA rapidly retrieves multi-scale information, and JLC ensures robust\nlocal feature extraction with minimal parameters, significantly enhancing the\nmodel's ability to operate with low computational budget. Followed by an\nextension of the dual-stream architecture that incorporates modal interaction\ninto the multi-scale image-retrieval process, VeloxSeg efficiently models\nheterogeneous modalities. Finally, Spatially Decoupled Knowledge Transfer\n(SDKT) via Gram matrices injects the texture prior extracted by a\nself-supervised network into the segmentation network, yielding stronger\nrepresentations than baselines at no extra inference cost. Experimental results\non multimodal benchmarks show that VeloxSeg achieves a 26% Dice improvement,\nalongside increasing GPU throughput by 11x and CPU by 48x. Codes are available\nat https://github.com/JinPLu/VeloxSeg."
                },
                "authors": [
                    {
                        "name": "Jinpeng Lu"
                    },
                    {
                        "name": "Linghan Cai"
                    },
                    {
                        "name": "Yinda Chen"
                    },
                    {
                        "name": "Guo Tang"
                    },
                    {
                        "name": "Songhan Jiang"
                    },
                    {
                        "name": "Haoyuan Shi"
                    },
                    {
                        "name": "Zhiwei Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwei Xiong"
                },
                "author": "Zhiwei Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22299v1",
                "updated": "2025-09-26T13:00:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    0,
                    46,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T13:00:46Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    0,
                    46,
                    4,
                    269,
                    0
                ],
                "title": "HEAPr: Hessian-based Efficient Atomic Expert Pruning in Output Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEAPr: Hessian-based Efficient Atomic Expert Pruning in Output Space"
                },
                "summary": "Mixture-of-Experts (MoE) architectures in large language models (LLMs)\ndeliver exceptional performance and reduced inference costs compared to dense\nLLMs. However, their large parameter counts result in prohibitive memory\nrequirements, limiting practical deployment. While existing pruning methods\nprimarily focus on expert-level pruning, this coarse granularity often leads to\nsubstantial accuracy degradation. In this work, we introduce HEAPr, a novel\npruning algorithm that decomposes experts into smaller, indivisible atomic\nexperts, enabling more precise and flexible atomic expert pruning. To measure\nthe importance of each atomic expert, we leverage second-order information\nbased on principles similar to Optimal Brain Surgeon (OBS) theory. To address\nthe computational and storage challenges posed by second-order information,\nHEAPr exploits the inherent properties of atomic experts to transform the\nsecond-order information from expert parameters into that of atomic expert\nparameters, and further simplifies it to the second-order information of atomic\nexpert outputs. This approach reduces the space complexity from $O(d^4)$, where\nd is the model's dimensionality, to $O(d^2)$. HEAPr requires only two forward\npasses and one backward pass on a small calibration set to compute the\nimportance of atomic experts. Extensive experiments on MoE models, including\nDeepSeek MoE and Qwen MoE family, demonstrate that HEAPr outperforms existing\nexpert-level pruning methods across a wide range of compression ratios and\nbenchmarks. Specifically, HEAPr achieves nearly lossless compression at\ncompression ratios of 20% ~ 25% in most models, while also reducing FLOPs\nnearly by 20%. The code can be found at\n\\href{https://github.com/LLIKKE/HEAPr}{https://github.com/LLIKKE/HEAPr}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) architectures in large language models (LLMs)\ndeliver exceptional performance and reduced inference costs compared to dense\nLLMs. However, their large parameter counts result in prohibitive memory\nrequirements, limiting practical deployment. While existing pruning methods\nprimarily focus on expert-level pruning, this coarse granularity often leads to\nsubstantial accuracy degradation. In this work, we introduce HEAPr, a novel\npruning algorithm that decomposes experts into smaller, indivisible atomic\nexperts, enabling more precise and flexible atomic expert pruning. To measure\nthe importance of each atomic expert, we leverage second-order information\nbased on principles similar to Optimal Brain Surgeon (OBS) theory. To address\nthe computational and storage challenges posed by second-order information,\nHEAPr exploits the inherent properties of atomic experts to transform the\nsecond-order information from expert parameters into that of atomic expert\nparameters, and further simplifies it to the second-order information of atomic\nexpert outputs. This approach reduces the space complexity from $O(d^4)$, where\nd is the model's dimensionality, to $O(d^2)$. HEAPr requires only two forward\npasses and one backward pass on a small calibration set to compute the\nimportance of atomic experts. Extensive experiments on MoE models, including\nDeepSeek MoE and Qwen MoE family, demonstrate that HEAPr outperforms existing\nexpert-level pruning methods across a wide range of compression ratios and\nbenchmarks. Specifically, HEAPr achieves nearly lossless compression at\ncompression ratios of 20% ~ 25% in most models, while also reducing FLOPs\nnearly by 20%. The code can be found at\n\\href{https://github.com/LLIKKE/HEAPr}{https://github.com/LLIKKE/HEAPr}."
                },
                "authors": [
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Zheng Yang"
                    },
                    {
                        "name": "Zhongbin Zhou"
                    },
                    {
                        "name": "Feng Xue"
                    },
                    {
                        "name": "Zhonglin Jiang"
                    },
                    {
                        "name": "Wenxiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenxiao Wang"
                },
                "author": "Wenxiao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22297v1",
                "updated": "2025-09-26T12:59:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    12,
                    59,
                    41,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T12:59:41Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    12,
                    59,
                    41,
                    4,
                    269,
                    0
                ],
                "title": "Large Language Models as Nondeterministic Causal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Nondeterministic Causal Models"
                },
                "summary": "Recent work by Chatzi et al. and Ravfogel et al. has developed, for the first\ntime, a method for generating counterfactuals of probabilistic Large Language\nModels. Such counterfactuals tell us what would - or might - have been the\noutput of an LLM if some factual prompt ${\\bf x}$ had been ${\\bf x}^*$ instead.\nThe ability to generate such counterfactuals is an important necessary step\ntowards explaining, evaluating, and comparing, the behavior of LLMs. I argue,\nhowever, that the existing method rests on an ambiguous interpretation of LLMs:\nit does not interpret LLMs literally, for the method involves the assumption\nthat one can change the implementation of an LLM's sampling process without\nchanging the LLM itself, nor does it interpret LLMs as intended, for the method\ninvolves explicitly representing a nondeterministic LLM as a deterministic\ncausal model. I here present a much simpler method for generating\ncounterfactuals that is based on an LLM's intended interpretation by\nrepresenting it as a nondeterministic causal model instead. The advantage of my\nsimpler method is that it is directly applicable to any black-box LLM without\nmodification, as it is agnostic to any implementation details. The advantage of\nthe existing method, on the other hand, is that it directly implements the\ngeneration of a specific type of counterfactuals that is useful for certain\npurposes, but not for others. I clarify how both methods relate by offering a\ntheoretical foundation for reasoning about counterfactuals in LLMs based on\ntheir intended semantics, thereby laying the groundwork for novel\napplication-specific methods for generating counterfactuals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work by Chatzi et al. and Ravfogel et al. has developed, for the first\ntime, a method for generating counterfactuals of probabilistic Large Language\nModels. Such counterfactuals tell us what would - or might - have been the\noutput of an LLM if some factual prompt ${\\bf x}$ had been ${\\bf x}^*$ instead.\nThe ability to generate such counterfactuals is an important necessary step\ntowards explaining, evaluating, and comparing, the behavior of LLMs. I argue,\nhowever, that the existing method rests on an ambiguous interpretation of LLMs:\nit does not interpret LLMs literally, for the method involves the assumption\nthat one can change the implementation of an LLM's sampling process without\nchanging the LLM itself, nor does it interpret LLMs as intended, for the method\ninvolves explicitly representing a nondeterministic LLM as a deterministic\ncausal model. I here present a much simpler method for generating\ncounterfactuals that is based on an LLM's intended interpretation by\nrepresenting it as a nondeterministic causal model instead. The advantage of my\nsimpler method is that it is directly applicable to any black-box LLM without\nmodification, as it is agnostic to any implementation details. The advantage of\nthe existing method, on the other hand, is that it directly implements the\ngeneration of a specific type of counterfactuals that is useful for certain\npurposes, but not for others. I clarify how both methods relate by offering a\ntheoretical foundation for reasoning about counterfactuals in LLMs based on\ntheir intended semantics, thereby laying the groundwork for novel\napplication-specific methods for generating counterfactuals."
                },
                "authors": [
                    {
                        "name": "Sander Beckers"
                    }
                ],
                "author_detail": {
                    "name": "Sander Beckers"
                },
                "author": "Sander Beckers",
                "arxiv_comment": "Preprint: under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22292v1",
                "updated": "2025-09-26T12:54:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    12,
                    54,
                    23,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T12:54:23Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    12,
                    54,
                    23,
                    4,
                    269,
                    0
                ],
                "title": "Jailbreaking on Text-to-Video Models via Scene Splitting Strategy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking on Text-to-Video Models via Scene Splitting Strategy"
                },
                "summary": "Along with the rapid advancement of numerous Text-to-Video (T2V) models,\ngrowing concerns have emerged regarding their safety risks. While recent\nstudies have explored vulnerabilities in models like LLMs, VLMs, and\nText-to-Image (T2I) models through jailbreak attacks, T2V models remain largely\nunexplored, leaving a significant safety gap. To address this gap, we introduce\nSceneSplit, a novel black-box jailbreak method that works by fragmenting a\nharmful narrative into multiple scenes, each individually benign. This approach\nmanipulates the generative output space, the abstract set of all potential\nvideo outputs for a given prompt, using the combination of scenes as a powerful\nconstraint to guide the final outcome. While each scene individually\ncorresponds to a wide and safe space where most outcomes are benign, their\nsequential combination collectively restricts this space, narrowing it to an\nunsafe region and significantly increasing the likelihood of generating a\nharmful video. This core mechanism is further enhanced through iterative scene\nmanipulation, which bypasses the safety filter within this constrained unsafe\nregion. Additionally, a strategy library that reuses successful attack patterns\nfurther improves the attack's overall effectiveness and robustness. To validate\nour method, we evaluate SceneSplit across 11 safety categories on T2V models.\nOur results show that it achieves a high average Attack Success Rate (ASR) of\n77.2% on Luma Ray2, 84.1% on Hailuo, and 78.2% on Veo2, significantly\noutperforming the existing baseline. Through this work, we demonstrate that\ncurrent T2V safety mechanisms are vulnerable to attacks that exploit narrative\nstructure, providing new insights for understanding and improving the safety of\nT2V models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Along with the rapid advancement of numerous Text-to-Video (T2V) models,\ngrowing concerns have emerged regarding their safety risks. While recent\nstudies have explored vulnerabilities in models like LLMs, VLMs, and\nText-to-Image (T2I) models through jailbreak attacks, T2V models remain largely\nunexplored, leaving a significant safety gap. To address this gap, we introduce\nSceneSplit, a novel black-box jailbreak method that works by fragmenting a\nharmful narrative into multiple scenes, each individually benign. This approach\nmanipulates the generative output space, the abstract set of all potential\nvideo outputs for a given prompt, using the combination of scenes as a powerful\nconstraint to guide the final outcome. While each scene individually\ncorresponds to a wide and safe space where most outcomes are benign, their\nsequential combination collectively restricts this space, narrowing it to an\nunsafe region and significantly increasing the likelihood of generating a\nharmful video. This core mechanism is further enhanced through iterative scene\nmanipulation, which bypasses the safety filter within this constrained unsafe\nregion. Additionally, a strategy library that reuses successful attack patterns\nfurther improves the attack's overall effectiveness and robustness. To validate\nour method, we evaluate SceneSplit across 11 safety categories on T2V models.\nOur results show that it achieves a high average Attack Success Rate (ASR) of\n77.2% on Luma Ray2, 84.1% on Hailuo, and 78.2% on Veo2, significantly\noutperforming the existing baseline. Through this work, we demonstrate that\ncurrent T2V safety mechanisms are vulnerable to attacks that exploit narrative\nstructure, providing new insights for understanding and improving the safety of\nT2V models."
                },
                "authors": [
                    {
                        "name": "Wonjun Lee"
                    },
                    {
                        "name": "Haon Park"
                    },
                    {
                        "name": "Doehyeon Lee"
                    },
                    {
                        "name": "Bumsub Ham"
                    },
                    {
                        "name": "Suhyun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Suhyun Kim"
                },
                "author": "Suhyun Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22287v1",
                "updated": "2025-09-26T12:48:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    12,
                    48,
                    51,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T12:48:51Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    12,
                    48,
                    51,
                    4,
                    269,
                    0
                ],
                "title": "Leveraging Large Language Models for Robot-Assisted Learning of\n  Morphological Structures in Preschool Children with Language Vulnerabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Robot-Assisted Learning of\n  Morphological Structures in Preschool Children with Language Vulnerabilities"
                },
                "summary": "Preschool children with language vulnerabilities -- such as developmental\nlanguage disorders or immigration related language challenges -- often require\nsupport to strengthen their expressive language skills. Based on the principle\nof implicit learning, speech-language therapists (SLTs) typically embed target\nmorphological structures (e.g., third person -s) into everyday interactions or\ngame-based learning activities. Educators are recommended by SLTs to do the\nsame. This approach demands precise linguistic knowledge and real-time\nproduction of various morphological forms (e.g., \"Daddy wears these when he\ndrives to work\"). The task becomes even more demanding when educators or parent\nalso must keep children engaged and manage turn-taking in a game-based\nactivity. In the TalBot project our multiprofessional team have developed an\napplication in which the Furhat conversational robot plays the word retrieval\ngame \"Alias\" with children to improve language skills. Our application\ncurrently employs a large language model (LLM) to manage gameplay, dialogue,\naffective responses, and turn-taking. Our next step is to further leverage the\ncapacity of LLMs so the robot can generate and deliver specific morphological\ntargets during the game. We hypothesize that a robot could outperform humans at\nthis task. Novel aspects of this approach are that the robot could ultimately\nserve as a model and tutor for both children and professionals and that using\nLLM capabilities in this context would support basic communication needs for\nchildren with language vulnerabilities. Our long-term goal is to create a\nrobust LLM-based Robot-Assisted Language Learning intervention capable of\nteaching a variety of morphological structures across different languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preschool children with language vulnerabilities -- such as developmental\nlanguage disorders or immigration related language challenges -- often require\nsupport to strengthen their expressive language skills. Based on the principle\nof implicit learning, speech-language therapists (SLTs) typically embed target\nmorphological structures (e.g., third person -s) into everyday interactions or\ngame-based learning activities. Educators are recommended by SLTs to do the\nsame. This approach demands precise linguistic knowledge and real-time\nproduction of various morphological forms (e.g., \"Daddy wears these when he\ndrives to work\"). The task becomes even more demanding when educators or parent\nalso must keep children engaged and manage turn-taking in a game-based\nactivity. In the TalBot project our multiprofessional team have developed an\napplication in which the Furhat conversational robot plays the word retrieval\ngame \"Alias\" with children to improve language skills. Our application\ncurrently employs a large language model (LLM) to manage gameplay, dialogue,\naffective responses, and turn-taking. Our next step is to further leverage the\ncapacity of LLMs so the robot can generate and deliver specific morphological\ntargets during the game. We hypothesize that a robot could outperform humans at\nthis task. Novel aspects of this approach are that the robot could ultimately\nserve as a model and tutor for both children and professionals and that using\nLLM capabilities in this context would support basic communication needs for\nchildren with language vulnerabilities. Our long-term goal is to create a\nrobust LLM-based Robot-Assisted Language Learning intervention capable of\nteaching a variety of morphological structures across different languages."
                },
                "authors": [
                    {
                        "name": "Stina Sundstedt"
                    },
                    {
                        "name": "Mattias Wingren"
                    },
                    {
                        "name": "Susanne Hägglund"
                    },
                    {
                        "name": "Daniel Ventus"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Ventus"
                },
                "author": "Daniel Ventus",
                "arxiv_doi": "10.1007/978-3-031-94153-5_41",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-94153-5_41",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.22287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 2 figures, Preprint of: Sundstedt, S., Wingren, M.,\n  H\\\"agglund, S. & Ventus, D. (2025). Leveraging Large Language Models for\n  Robot-Assisted Learning of Morphological Structures in Preschool Children\n  with Language Vulnerabilities. In: Stephanidis, C., Antona, M., Ntoa, S. &\n  Salvendy, G. (eds.), Communications in Computer and Information Science, vol.\n  2523, pp. 415-425. Springer",
                "arxiv_journal_ref": "Communications in Computer and Information Science(2025).\n  Stephanidis, C., Antona, M., Ntoa, S. & Salvendy, G. (eds.). p. 415-425 11 p.\n  ( Communications in Computer and Information Science; vol. 2523)",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.5.2; K.3.1; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21054v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21054v3",
                "updated": "2025-09-26T12:48:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    12,
                    48,
                    22,
                    4,
                    269,
                    0
                ],
                "published": "2024-10-28T14:09:52Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    9,
                    52,
                    0,
                    302,
                    0
                ],
                "title": "Semantic Component Analysis: Introducing Multi-Topic Distributions to\n  Clustering-Based Topic Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Component Analysis: Introducing Multi-Topic Distributions to\n  Clustering-Based Topic Modeling"
                },
                "summary": "Topic modeling is a key method in text analysis, but existing approaches fail\nto efficiently scale to large datasets or are limited by assuming one topic per\ndocument. Overcoming these limitations, we introduce Semantic Component\nAnalysis (SCA), a topic modeling technique that discovers multiple topics per\nsample by introducing a decomposition step to the clustering-based topic\nmodeling framework. We evaluate SCA on Twitter datasets in English, Hausa and\nChinese. There, it achieves competitive coherence and diversity compared to\nBERTopic, while uncovering at least double the topics and maintaining a noise\nrate close to zero. We also find that SCA outperforms the LLM-based TopicGPT in\nscenarios with similar compute budgets. SCA thus provides an effective and\nefficient approach for topic modeling of large datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topic modeling is a key method in text analysis, but existing approaches fail\nto efficiently scale to large datasets or are limited by assuming one topic per\ndocument. Overcoming these limitations, we introduce Semantic Component\nAnalysis (SCA), a topic modeling technique that discovers multiple topics per\nsample by introducing a decomposition step to the clustering-based topic\nmodeling framework. We evaluate SCA on Twitter datasets in English, Hausa and\nChinese. There, it achieves competitive coherence and diversity compared to\nBERTopic, while uncovering at least double the topics and maintaining a noise\nrate close to zero. We also find that SCA outperforms the LLM-based TopicGPT in\nscenarios with similar compute budgets. SCA thus provides an effective and\nefficient approach for topic modeling of large datasets."
                },
                "authors": [
                    {
                        "name": "Florian Eichin"
                    },
                    {
                        "name": "Carolin M. Schuster"
                    },
                    {
                        "name": "Georg Groh"
                    },
                    {
                        "name": "Michael A. Hedderich"
                    }
                ],
                "author_detail": {
                    "name": "Michael A. Hedderich"
                },
                "author": "Michael A. Hedderich",
                "arxiv_comment": "5 pages, 3 figures, code:\n  https://github.com/mainlp/semantic_components",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21054v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21054v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22281v1",
                "updated": "2025-09-26T12:46:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    12,
                    46,
                    0,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T12:46:00Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    12,
                    46,
                    0,
                    4,
                    269,
                    0
                ],
                "title": "MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial\n  Reasoning"
                },
                "summary": "The ability of robots to interpret human instructions and execute\nmanipulation tasks necessitates the availability of task-relevant tabletop\nscenes for training. However, traditional methods for creating these scenes\nrely on time-consuming manual layout design or purely randomized layouts, which\nare limited in terms of plausibility or alignment with the tasks. In this\npaper, we formulate a novel task, namely task-oriented tabletop scene\ngeneration, which poses significant challenges due to the substantial gap\nbetween high-level task instructions and the tabletop scenes. To support\nresearch on such a challenging task, we introduce MesaTask-10K, a large-scale\ndataset comprising approximately 10,700 synthetic tabletop scenes with manually\ncrafted layouts that ensure realistic layouts and intricate inter-object\nrelations. To bridge the gap between tasks and scenes, we propose a Spatial\nReasoning Chain that decomposes the generation process into object inference,\nspatial interrelation reasoning, and scene graph construction for the final 3D\nlayout. We present MesaTask, an LLM-based framework that utilizes this\nreasoning chain and is further enhanced with DPO algorithms to generate\nphysically plausible tabletop scenes that align well with given task\ndescriptions. Exhaustive experiments demonstrate the superior performance of\nMesaTask compared to baselines in generating task-conforming tabletop scenes\nwith realistic layouts. Project page is at https://mesatask.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability of robots to interpret human instructions and execute\nmanipulation tasks necessitates the availability of task-relevant tabletop\nscenes for training. However, traditional methods for creating these scenes\nrely on time-consuming manual layout design or purely randomized layouts, which\nare limited in terms of plausibility or alignment with the tasks. In this\npaper, we formulate a novel task, namely task-oriented tabletop scene\ngeneration, which poses significant challenges due to the substantial gap\nbetween high-level task instructions and the tabletop scenes. To support\nresearch on such a challenging task, we introduce MesaTask-10K, a large-scale\ndataset comprising approximately 10,700 synthetic tabletop scenes with manually\ncrafted layouts that ensure realistic layouts and intricate inter-object\nrelations. To bridge the gap between tasks and scenes, we propose a Spatial\nReasoning Chain that decomposes the generation process into object inference,\nspatial interrelation reasoning, and scene graph construction for the final 3D\nlayout. We present MesaTask, an LLM-based framework that utilizes this\nreasoning chain and is further enhanced with DPO algorithms to generate\nphysically plausible tabletop scenes that align well with given task\ndescriptions. Exhaustive experiments demonstrate the superior performance of\nMesaTask compared to baselines in generating task-conforming tabletop scenes\nwith realistic layouts. Project page is at https://mesatask.github.io/"
                },
                "authors": [
                    {
                        "name": "Jinkun Hao"
                    },
                    {
                        "name": "Naifu Liang"
                    },
                    {
                        "name": "Zhen Luo"
                    },
                    {
                        "name": "Xudong Xu"
                    },
                    {
                        "name": "Weipeng Zhong"
                    },
                    {
                        "name": "Ran Yi"
                    },
                    {
                        "name": "Yichen Jin"
                    },
                    {
                        "name": "Zhaoyang Lyu"
                    },
                    {
                        "name": "Feng Zheng"
                    },
                    {
                        "name": "Lizhuang Ma"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "Accepted by NeurIPS 2025; Project page: https://mesatask.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]