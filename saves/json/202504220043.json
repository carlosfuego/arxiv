[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.09146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09146v2",
                "updated": "2025-04-18T05:13:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    13,
                    52,
                    4,
                    108,
                    0
                ],
                "published": "2025-01-15T20:55:13Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "title": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs"
                },
                "summary": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "32 pages, 11 figures, 1 table, 4 algorithms, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13385v1",
                "updated": "2025-04-18T00:21:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    0,
                    21,
                    0,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T00:21:00Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    0,
                    21,
                    0,
                    4,
                    108,
                    0
                ],
                "title": "EXAM: Exploiting Exclusive System-Level Cache in Apple M-Series SoCs for\n  Enhanced Cache Occupancy Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EXAM: Exploiting Exclusive System-Level Cache in Apple M-Series SoCs for\n  Enhanced Cache Occupancy Attacks"
                },
                "summary": "Cache occupancy attacks exploit the shared nature of cache hierarchies to\ninfer a victim's activities by monitoring overall cache usage, unlike\naccess-driven cache attacks that focus on specific cache lines or sets. There\nexists some prior work that target the last-level cache (LLC) of Intel\nprocessors, which is inclusive of higher-level caches, and L2 caches of ARM\nsystems. In this paper, we target the System-Level Cache (SLC) of Apple\nM-series SoCs, which is exclusive to higher-level CPU caches. We address the\nchallenges of the exclusiveness and propose a suite of SLC-cache occupancy\nattacks, the first of its kind, where an adversary can monitor GPU and other\nCPU cluster activities from their own CPU cluster. We first discover the\nstructure of SLC in Apple M1 SOC and various policies pertaining to access and\nsharing through reverse engineering. We propose two attacks against websites.\nOne is a coarse-grained fingerprinting attack, recognizing which website is\naccessed based on their different GPU memory access patterns monitored through\nthe SLC occupancy channel. The other attack is a fine-grained pixel stealing\nattack, which precisely monitors the GPU memory usage for rendering different\npixels, through the SLC occupancy channel. Third, we introduce a novel screen\ncapturing attack which works beyond webpages, with the monitoring granularity\nof 57 rows of pixels (there are 1600 rows for the screen). This significantly\nexpands the attack surface, allowing the adversary to retrieve any screen\ndisplay, posing a substantial new threat to system security. Our findings\nreveal critical vulnerabilities in Apple's M-series SoCs and emphasize the\nurgent need for effective countermeasures against cache occupancy attacks in\nheterogeneous computing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache occupancy attacks exploit the shared nature of cache hierarchies to\ninfer a victim's activities by monitoring overall cache usage, unlike\naccess-driven cache attacks that focus on specific cache lines or sets. There\nexists some prior work that target the last-level cache (LLC) of Intel\nprocessors, which is inclusive of higher-level caches, and L2 caches of ARM\nsystems. In this paper, we target the System-Level Cache (SLC) of Apple\nM-series SoCs, which is exclusive to higher-level CPU caches. We address the\nchallenges of the exclusiveness and propose a suite of SLC-cache occupancy\nattacks, the first of its kind, where an adversary can monitor GPU and other\nCPU cluster activities from their own CPU cluster. We first discover the\nstructure of SLC in Apple M1 SOC and various policies pertaining to access and\nsharing through reverse engineering. We propose two attacks against websites.\nOne is a coarse-grained fingerprinting attack, recognizing which website is\naccessed based on their different GPU memory access patterns monitored through\nthe SLC occupancy channel. The other attack is a fine-grained pixel stealing\nattack, which precisely monitors the GPU memory usage for rendering different\npixels, through the SLC occupancy channel. Third, we introduce a novel screen\ncapturing attack which works beyond webpages, with the monitoring granularity\nof 57 rows of pixels (there are 1600 rows for the screen). This significantly\nexpands the attack surface, allowing the adversary to retrieve any screen\ndisplay, posing a substantial new threat to system security. Our findings\nreveal critical vulnerabilities in Apple's M-series SoCs and emphasize the\nurgent need for effective countermeasures against cache occupancy attacks in\nheterogeneous computing environments."
                },
                "authors": [
                    {
                        "name": "Tianhong Xu"
                    },
                    {
                        "name": "Aidong Adam Ding"
                    },
                    {
                        "name": "Yunsi Fei"
                    }
                ],
                "author_detail": {
                    "name": "Yunsi Fei"
                },
                "author": "Yunsi Fei",
                "arxiv_comment": "Accepted to ACM ASIA CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01291v2",
                "updated": "2025-04-17T23:45:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    23,
                    45,
                    51,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-02T01:49:58Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "title": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures"
                },
                "summary": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs."
                },
                "authors": [
                    {
                        "name": "Seungheon Shin"
                    },
                    {
                        "name": "Kyle Liddy"
                    },
                    {
                        "name": "Yinxuan Zhu"
                    },
                    {
                        "name": "Chandan Joishi"
                    },
                    {
                        "name": "Brianna A. Klein"
                    },
                    {
                        "name": "Andrew Armstrong"
                    },
                    {
                        "name": "Andrew A. Allerman"
                    },
                    {
                        "name": "Siddharth Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Rajan"
                },
                "author": "Siddharth Rajan",
                "arxiv_comment": "12 pages, 7 figures, and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v2",
                "updated": "2025-04-17T21:19:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    21,
                    19,
                    19,
                    3,
                    107,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19325v2",
                "updated": "2025-04-17T15:26:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    26,
                    4,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-25T03:38:06Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    3,
                    38,
                    6,
                    1,
                    84,
                    0
                ],
                "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction"
                },
                "summary": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context video modeling\nfaces challenges due to visual redundancy. Training on long videos is\ncomputationally expensive, as vision tokens grow much faster than language\ntokens. To tackle this issue, we propose balancing locality and long-range\ndependency through long short-term context modeling. A high-resolution\nshort-term context window ensures fine-grained temporal consistency, while an\nunlimited long-term context window encodes long-range information using fewer\ntokens. With this approach, we can train on long video sequences with a\nmanageable token context length, thereby significantly reducing training time\nand memory usage. Furthermore, we propose a multi-level KV cache designed to\nsupport the long short-term context modeling, which accelerating inference on\nlong video sequences. We demonstrate that FAR achieves state-of-the-art\nperformance in both short- and long-video generation, providing a simple yet\neffective baseline for video autoregressive modeling. The code is released at\nhttps://github.com/showlab/FAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context video modeling\nfaces challenges due to visual redundancy. Training on long videos is\ncomputationally expensive, as vision tokens grow much faster than language\ntokens. To tackle this issue, we propose balancing locality and long-range\ndependency through long short-term context modeling. A high-resolution\nshort-term context window ensures fine-grained temporal consistency, while an\nunlimited long-term context window encodes long-range information using fewer\ntokens. With this approach, we can train on long video sequences with a\nmanageable token context length, thereby significantly reducing training time\nand memory usage. Furthermore, we propose a multi-level KV cache designed to\nsupport the long short-term context modeling, which accelerating inference on\nlong video sequences. We demonstrate that FAR achieves state-of-the-art\nperformance in both short- and long-video generation, providing a simple yet\neffective baseline for video autoregressive modeling. The code is released at\nhttps://github.com/showlab/FAR."
                },
                "authors": [
                    {
                        "name": "Yuchao Gu"
                    },
                    {
                        "name": "Weijia Mao"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "arxiv_comment": "Project page at https://farlongctx.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12876v3",
                "updated": "2025-04-17T03:51:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    3,
                    51,
                    6,
                    3,
                    107,
                    0
                ],
                "published": "2024-10-15T05:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    1,
                    19,
                    1,
                    289,
                    0
                ],
                "title": "In-context KV-Cache Eviction for LLMs via Attention-Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context KV-Cache Eviction for LLMs via Attention-Gate"
                },
                "summary": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance."
                },
                "authors": [
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10074v2",
                "updated": "2025-04-17T00:38:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    0,
                    38,
                    24,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-13T05:43:14Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "title": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension"
                },
                "summary": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions."
                },
                "authors": [
                    {
                        "name": "Taehun Kim"
                    },
                    {
                        "name": "Hyerean Jang"
                    },
                    {
                        "name": "Youngjoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Youngjoo Shin"
                },
                "author": "Youngjoo Shin",
                "arxiv_comment": "The modified version of this preprint has been submitted to ESORICS\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12526v1",
                "updated": "2025-04-16T23:15:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    23,
                    15,
                    9,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T23:15:09Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    23,
                    15,
                    9,
                    2,
                    106,
                    0
                ],
                "title": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context\n  Language Models"
                },
                "summary": "Long-context language models exhibit impressive performance but remain\nchallenging to deploy due to high GPU memory demands during inference. We\npropose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that\npartitions critical layers into smaller \"mini-sequences\" and integrates\nseamlessly with KV cache offloading. Experiments on various Llama, Qwen, and\nMistral models demonstrate that MOM reduces peak memory usage by over 50\\% on\naverage. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k\nto 455k tokens on a single A100 80GB GPU, while keeping outputs identical and\nnot compromising accuracy. MOM also maintains highly competitive throughput due\nto minimal computational overhead and efficient last-layer processing. Compared\nto traditional chunked prefill methods, MOM achieves a 35\\% greater context\nlength extension. More importantly, our method drastically reduces prefill\nmemory consumption, eliminating it as the longstanding dominant memory\nbottleneck during inference. This breakthrough fundamentally changes research\npriorities, redirecting future efforts from prefill-stage optimizations to\nimproving decode-stage residual KV cache efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context language models exhibit impressive performance but remain\nchallenging to deploy due to high GPU memory demands during inference. We\npropose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that\npartitions critical layers into smaller \"mini-sequences\" and integrates\nseamlessly with KV cache offloading. Experiments on various Llama, Qwen, and\nMistral models demonstrate that MOM reduces peak memory usage by over 50\\% on\naverage. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k\nto 455k tokens on a single A100 80GB GPU, while keeping outputs identical and\nnot compromising accuracy. MOM also maintains highly competitive throughput due\nto minimal computational overhead and efficient last-layer processing. Compared\nto traditional chunked prefill methods, MOM achieves a 35\\% greater context\nlength extension. More importantly, our method drastically reduces prefill\nmemory consumption, eliminating it as the longstanding dominant memory\nbottleneck during inference. This breakthrough fundamentally changes research\npriorities, redirecting future efforts from prefill-stage optimizations to\nimproving decode-stage residual KV cache efficiency."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Tianyi Zhu"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "arxiv_comment": "Submitted to COLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v1",
                "updated": "2025-04-16T18:03:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2504.11704",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09775v2",
                "updated": "2025-04-16T17:34:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    17,
                    34,
                    4,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-14T00:29:49Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    0,
                    29,
                    49,
                    0,
                    104,
                    0
                ],
                "title": "Understanding and Optimizing Multi-Stage AI Inference Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Optimizing Multi-Stage AI Inference Pipelines"
                },
                "summary": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads."
                },
                "authors": [
                    {
                        "name": "Abhimanyu Rajeshkumar Bambhaniya"
                    },
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "name": "Sudarshan Srinivasan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Amir Yazdanbakhsh"
                    },
                    {
                        "name": "Midhilesh Elavazhagan"
                    },
                    {
                        "name": "Madhu Kumar"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "arxiv_comment": "Inference System Design for Multi-Stage AI Inference Pipelines. 13\n  Pages, 15 Figues, 3 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12240v1",
                "updated": "2025-04-16T16:45:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T16:45:19Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cobra: Efficient Line Art COlorization with BRoAder References"
                },
                "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/."
                },
                "authors": [
                    {
                        "name": "Junhao Zhuang"
                    },
                    {
                        "name": "Lingen Li"
                    },
                    {
                        "name": "Xuan Ju"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project page with code: https://zhuang2002.github.io/Cobra/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11816v1",
                "updated": "2025-04-16T07:02:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    2,
                    38,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T07:02:38Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    2,
                    38,
                    2,
                    106,
                    0
                ],
                "title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\n  Offloading"
                },
                "summary": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads."
                },
                "authors": [
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Jinwoo Kim"
                    },
                    {
                        "name": "Hyunsun Chung"
                    },
                    {
                        "name": "Myung-Hoon Cha"
                    },
                    {
                        "name": "Hong-Yeon Kim"
                    },
                    {
                        "name": "Youngjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Kim"
                },
                "author": "Youngjae Kim",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08334v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08334v3",
                "updated": "2025-04-16T05:57:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    5,
                    57,
                    8,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-11T07:59:06Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    59,
                    6,
                    4,
                    101,
                    0
                ],
                "title": "Efficient Architecture for RISC-V Vector Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Architecture for RISC-V Vector Memory Access"
                },
                "summary": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors."
                },
                "authors": [
                    {
                        "name": "Hongyi Guan"
                    },
                    {
                        "name": "Yichuan Gao"
                    },
                    {
                        "name": "Chenlu Miao"
                    },
                    {
                        "name": "Haoyang Wu"
                    },
                    {
                        "name": "Hang Zhu"
                    },
                    {
                        "name": "Mingfeng Lin"
                    },
                    {
                        "name": "Huayue Liang"
                    }
                ],
                "author_detail": {
                    "name": "Huayue Liang"
                },
                "author": "Huayue Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08334v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08334v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11765v1",
                "updated": "2025-04-16T04:59:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    59,
                    18,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T04:59:18Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    59,
                    18,
                    2,
                    106,
                    0
                ],
                "title": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference\n  in RAG-Powered LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference\n  in RAG-Powered LLMs"
                },
                "summary": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration."
                },
                "authors": [
                    {
                        "name": "Hyungwoo Lee"
                    },
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Jinwoo Kim"
                    },
                    {
                        "name": "Jungmin So"
                    },
                    {
                        "name": "Myung-Hoon Cha"
                    },
                    {
                        "name": "Hong-Yeon Kim"
                    },
                    {
                        "name": "James J. Kim"
                    },
                    {
                        "name": "Youngjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Kim"
                },
                "arxiv_affiliation": "Dept. of Computer Science and Engineering, Sogang University, Seoul, Republic of Korea",
                "author": "Youngjae Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11729v1",
                "updated": "2025-04-16T03:07:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    7,
                    7,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T03:07:07Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    7,
                    7,
                    2,
                    106,
                    0
                ],
                "title": "EdgePrompt: A Distributed Key-Value Inference Framework for LLMs in 6G\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgePrompt: A Distributed Key-Value Inference Framework for LLMs in 6G\n  Networks"
                },
                "summary": "As sixth-generation (6G) networks advance, large language models (LLMs) are\nincreasingly integrated into 6G infrastructure to enhance network management\nand intelligence. However, traditional LLMs architecture struggle to meet the\nstringent latency and security requirements of 6G, especially as the increasing\nin sequence length leads to greater task complexity. This paper proposes\nEdge-Prompt, a cloud-edge collaborative framework based on a hierarchical\nattention splicing mechanism. EdgePrompt employs distributed key-value (KV)\npair optimization techniques to accelerate inference and adapt to network\nconditions. Additionally, to reduce the risk of data leakage, EdgePrompt\nincorporates a privacy preserving strategy by isolating sensitive information\nduring processing. Experiments on public dataset show that EdgePrompt\neffectively improves the inference throughput and reduces the latency, which\nprovides a reliable solution for LLMs deployment in 6G environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As sixth-generation (6G) networks advance, large language models (LLMs) are\nincreasingly integrated into 6G infrastructure to enhance network management\nand intelligence. However, traditional LLMs architecture struggle to meet the\nstringent latency and security requirements of 6G, especially as the increasing\nin sequence length leads to greater task complexity. This paper proposes\nEdge-Prompt, a cloud-edge collaborative framework based on a hierarchical\nattention splicing mechanism. EdgePrompt employs distributed key-value (KV)\npair optimization techniques to accelerate inference and adapt to network\nconditions. Additionally, to reduce the risk of data leakage, EdgePrompt\nincorporates a privacy preserving strategy by isolating sensitive information\nduring processing. Experiments on public dataset show that EdgePrompt\neffectively improves the inference throughput and reduces the latency, which\nprovides a reliable solution for LLMs deployment in 6G environments."
                },
                "authors": [
                    {
                        "name": "Jiahong Ning"
                    },
                    {
                        "name": "Pengyan Zhu"
                    },
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Gary Lee"
                    },
                    {
                        "name": "Sumei Sun"
                    },
                    {
                        "name": "Tingting Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Yang"
                },
                "author": "Tingting Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11652v1",
                "updated": "2025-04-15T22:38:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    22,
                    38,
                    54,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T22:38:54Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    22,
                    38,
                    54,
                    1,
                    105,
                    0
                ],
                "title": "Engineering MultiQueues: Fast Relaxed Concurrent Priority Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engineering MultiQueues: Fast Relaxed Concurrent Priority Queues"
                },
                "summary": "Priority queues are used in a wide range of applications, including\nprioritized online scheduling, discrete event simulation, and greedy\nalgorithms. In parallel settings, classical priority queues often become a\nsevere bottleneck, resulting in low throughput. Consequently, there has been\nsignificant interest in concurrent priority queues with relaxed semantics. In\nthis article, we present the MultiQueue, a flexible approach to relaxed\npriority queues that uses multiple internal sequential priority queues. The\nscalability of the MultiQueue is enhanced by buffering elements, batching\noperations on the internal queues, and optimizing access patterns for high\ncache locality. We investigate the complementary quality criteria of rank\nerror, which measures how close deleted elements are to the global minimum, and\ndelay, which quantifies how many smaller elements were deleted before a given\nelement. Extensive experimental evaluation shows that the MultiQueue\noutperforms competing approaches across several benchmarks. This includes\nshortest-path and branch-and-bound benchmarks that resemble real applications.\nMoreover, the MultiQueue can be configured easily to balance throughput and\nquality according to the application's requirements. We employ a seemingly\nparadoxical technique of wait-free locking that might be of broader interest\nfor converting sequential data structures into relaxed concurrent data\nstructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Priority queues are used in a wide range of applications, including\nprioritized online scheduling, discrete event simulation, and greedy\nalgorithms. In parallel settings, classical priority queues often become a\nsevere bottleneck, resulting in low throughput. Consequently, there has been\nsignificant interest in concurrent priority queues with relaxed semantics. In\nthis article, we present the MultiQueue, a flexible approach to relaxed\npriority queues that uses multiple internal sequential priority queues. The\nscalability of the MultiQueue is enhanced by buffering elements, batching\noperations on the internal queues, and optimizing access patterns for high\ncache locality. We investigate the complementary quality criteria of rank\nerror, which measures how close deleted elements are to the global minimum, and\ndelay, which quantifies how many smaller elements were deleted before a given\nelement. Extensive experimental evaluation shows that the MultiQueue\noutperforms competing approaches across several benchmarks. This includes\nshortest-path and branch-and-bound benchmarks that resemble real applications.\nMoreover, the MultiQueue can be configured easily to balance throughput and\nquality according to the application's requirements. We employ a seemingly\nparadoxical technique of wait-free locking that might be of broader interest\nfor converting sequential data structures into relaxed concurrent data\nstructures."
                },
                "authors": [
                    {
                        "name": "Marvin Williams"
                    },
                    {
                        "name": "Peter Sanders"
                    }
                ],
                "author_detail": {
                    "name": "Peter Sanders"
                },
                "author": "Peter Sanders",
                "arxiv_comment": "40 pages, extended journal version of arXiv:2107.01350",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11320v1",
                "updated": "2025-04-15T16:00:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:00:21Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "title": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints"
                },
                "summary": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints."
                },
                "authors": [
                    {
                        "name": "Ruicheng Ao"
                    },
                    {
                        "name": "Gan Luo"
                    },
                    {
                        "name": "David Simchi-Levi"
                    },
                    {
                        "name": "Xinshang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinshang Wang"
                },
                "author": "Xinshang Wang",
                "arxiv_comment": "42 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v3",
                "updated": "2025-04-15T15:40:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    40,
                    25,
                    1,
                    105,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13195v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13195v5",
                "updated": "2025-04-15T15:37:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    37,
                    58,
                    1,
                    105,
                    0
                ],
                "published": "2024-04-19T22:06:14Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    22,
                    6,
                    14,
                    4,
                    110,
                    0
                ],
                "title": "Automatic BLAS Offloading on Unified Memory Architecture: A Study on\n  NVIDIA Grace-Hopper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic BLAS Offloading on Unified Memory Architecture: A Study on\n  NVIDIA Grace-Hopper"
                },
                "summary": "Porting codes to GPU often requires major efforts. While several tools exist\nfor automatically offload numerical libraries such as BLAS and LAPACK, they\noften prove impractical due to the high cost of mandatory data transfer. The\nnew unified memory architecture in NVIDIA Grace-Hopper allows high bandwidth\ncache-coherent memory access of all memory from both CPU and GPU, potentially\neliminating bottleneck faced in conventional architecture. This breakthrough\nopens up new avenues for application development and porting strategies. In\nthis study, we introduce a new tool for automatic BLAS offload, the tool\nleverages the high speed cache coherent NVLink C2C interconnect in\nGrace-Hopper, and enables performant GPU offload for BLAS heavy applications\nwith no code changes or recompilation. The tool was tested on two quantum\nchemistry or physics codes, great performance benefits were observed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Porting codes to GPU often requires major efforts. While several tools exist\nfor automatically offload numerical libraries such as BLAS and LAPACK, they\noften prove impractical due to the high cost of mandatory data transfer. The\nnew unified memory architecture in NVIDIA Grace-Hopper allows high bandwidth\ncache-coherent memory access of all memory from both CPU and GPU, potentially\neliminating bottleneck faced in conventional architecture. This breakthrough\nopens up new avenues for application development and porting strategies. In\nthis study, we introduce a new tool for automatic BLAS offload, the tool\nleverages the high speed cache coherent NVLink C2C interconnect in\nGrace-Hopper, and enables performant GPU offload for BLAS heavy applications\nwith no code changes or recompilation. The tool was tested on two quantum\nchemistry or physics codes, great performance benefits were observed."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Yinzhi Wang"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Hang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Liu"
                },
                "author": "Hang Liu",
                "arxiv_doi": "10.1145/3626203.3670561",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3626203.3670561",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.13195v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13195v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11208v1",
                "updated": "2025-04-15T14:11:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    11,
                    38,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T14:11:38Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    11,
                    38,
                    1,
                    105,
                    0
                ],
                "title": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye"
                },
                "summary": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively."
                },
                "authors": [
                    {
                        "name": "Bradley Morgan"
                    },
                    {
                        "name": "Gal Horowitz"
                    },
                    {
                        "name": "Sioli O'Connell"
                    },
                    {
                        "name": "Stephan van Schaik"
                    },
                    {
                        "name": "Chitchanok Chuengsatiansup"
                    },
                    {
                        "name": "Daniel Genkin"
                    },
                    {
                        "name": "Olaf Maennel"
                    },
                    {
                        "name": "Paul Montague"
                    },
                    {
                        "name": "Eyal Ronen"
                    },
                    {
                        "name": "Yuval Yarom"
                    }
                ],
                "author_detail": {
                    "name": "Yuval Yarom"
                },
                "author": "Yuval Yarom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11067v1",
                "updated": "2025-04-15T11:02:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    2,
                    34,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T11:02:34Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    2,
                    34,
                    1,
                    105,
                    0
                ],
                "title": "Morphing-based Compression for Data-centric ML Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Morphing-based Compression for Data-centric ML Pipelines"
                },
                "summary": "Data-centric ML pipelines extend traditional machine learning (ML) pipelines\n-- of feature transformations and ML model training -- by outer loops for data\ncleaning, augmentation, and feature engineering to create high-quality input\ndata. Existing lossless matrix compression applies lightweight compression\nschemes to numeric matrices and performs linear algebra operations such as\nmatrix-vector multiplications directly on the compressed representation but\nstruggles to efficiently rediscover structural data redundancy. Compressed\noperations are effective at fitting data in available memory, reducing I/O\nacross the storage-memory-cache hierarchy, and improving instruction\nparallelism. The applied data cleaning, augmentation, and feature\ntransformations provide a rich source of information about data characteristics\nsuch as distinct items, column sparsity, and column correlations. In this\npaper, we introduce BWARE -- an extension of AWARE for workload-aware lossless\nmatrix compression -- that pushes compression through feature transformations\nand engineering to leverage information about structural transformations.\nBesides compressed feature transformations, we introduce a novel technique for\nlightweight morphing of a compressed representation into workload-optimized\ncompressed representations without decompression. BWARE shows substantial\nend-to-end runtime improvements, reducing the execution time for training\ndata-centric ML pipelines from days to hours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-centric ML pipelines extend traditional machine learning (ML) pipelines\n-- of feature transformations and ML model training -- by outer loops for data\ncleaning, augmentation, and feature engineering to create high-quality input\ndata. Existing lossless matrix compression applies lightweight compression\nschemes to numeric matrices and performs linear algebra operations such as\nmatrix-vector multiplications directly on the compressed representation but\nstruggles to efficiently rediscover structural data redundancy. Compressed\noperations are effective at fitting data in available memory, reducing I/O\nacross the storage-memory-cache hierarchy, and improving instruction\nparallelism. The applied data cleaning, augmentation, and feature\ntransformations provide a rich source of information about data characteristics\nsuch as distinct items, column sparsity, and column correlations. In this\npaper, we introduce BWARE -- an extension of AWARE for workload-aware lossless\nmatrix compression -- that pushes compression through feature transformations\nand engineering to leverage information about structural transformations.\nBesides compressed feature transformations, we introduce a novel technique for\nlightweight morphing of a compressed representation into workload-optimized\ncompressed representations without decompression. BWARE shows substantial\nend-to-end runtime improvements, reducing the execution time for training\ndata-centric ML pipelines from days to hours."
                },
                "authors": [
                    {
                        "name": "Sebastian Baunsgaard"
                    },
                    {
                        "name": "Matthias Boehm"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Boehm"
                },
                "author": "Matthias Boehm",
                "arxiv_comment": "20 pages, 28 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10326v1",
                "updated": "2025-04-14T15:34:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:34:26Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference"
                },
                "summary": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks."
                },
                "authors": [
                    {
                        "name": "Yangshen Deng"
                    },
                    {
                        "name": "Zhengxin You"
                    },
                    {
                        "name": "Long Xiang"
                    },
                    {
                        "name": "Qilong Li"
                    },
                    {
                        "name": "Peiqi Yuan"
                    },
                    {
                        "name": "Zhaoyang Hong"
                    },
                    {
                        "name": "Yitao Zheng"
                    },
                    {
                        "name": "Wanting Li"
                    },
                    {
                        "name": "Runzhong Li"
                    },
                    {
                        "name": "Haotian Liu"
                    },
                    {
                        "name": "Kyriakos Mouratidis"
                    },
                    {
                        "name": "Man Lung Yiu"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Qiaomu Shen"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Bo Tang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Tang"
                },
                "author": "Bo Tang",
                "arxiv_comment": "14 pages, 12 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.1; H.3.2; H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10318v1",
                "updated": "2025-04-14T15:27:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    27,
                    32,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:27:32Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    27,
                    32,
                    0,
                    104,
                    0
                ],
                "title": "Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing\n  Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing\n  Obfuscation"
                },
                "summary": "Microarchitectural attacks are a significant concern, leading to many\nhardware-based defense proposals. However, different defenses target different\nclasses of attacks, and their impact on each other has not been fully\nconsidered. To raise awareness of this problem, we study an interaction between\ntwo state-of-the art defenses in this paper, timing obfuscations of remote\ncache lines (TORC) and delaying speculative changes to remote cache lines\n(DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative\ncoherence state change attacks.\n  We observe that DSRC enables coherence information to be retrieved into the\nprocessor core, where it is out of the reach of timing obfuscations to protect.\nThis creates an unforeseen consequence that redo operations can be triggered\nwithin the core to detect the presence or absence of remote cache lines, which\nconstitutes a security vulnerability. We demonstrate that a new covert channel\nattack is possible using this vulnerability. We propose two ways to mitigate\nthe attack, whose performance varies depending on an application's cache usage.\nOne way is to never send remote exclusive coherence state (E) information to\nthe core even if it is created. The other way is to never create a remote E\nstate, which is responsible for triggering redos.\n  We demonstrate the timing difference caused by this microarchitectural\ndefense assumption violation using GEM5 simulations. Performance evaluation on\nSPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\\%\naverage overhead across both sets of benchmarks. The repair which prevented the\ncreation of remote E state had less than 2.8% average overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microarchitectural attacks are a significant concern, leading to many\nhardware-based defense proposals. However, different defenses target different\nclasses of attacks, and their impact on each other has not been fully\nconsidered. To raise awareness of this problem, we study an interaction between\ntwo state-of-the art defenses in this paper, timing obfuscations of remote\ncache lines (TORC) and delaying speculative changes to remote cache lines\n(DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative\ncoherence state change attacks.\n  We observe that DSRC enables coherence information to be retrieved into the\nprocessor core, where it is out of the reach of timing obfuscations to protect.\nThis creates an unforeseen consequence that redo operations can be triggered\nwithin the core to detect the presence or absence of remote cache lines, which\nconstitutes a security vulnerability. We demonstrate that a new covert channel\nattack is possible using this vulnerability. We propose two ways to mitigate\nthe attack, whose performance varies depending on an application's cache usage.\nOne way is to never send remote exclusive coherence state (E) information to\nthe core even if it is created. The other way is to never create a remote E\nstate, which is responsible for triggering redos.\n  We demonstrate the timing difference caused by this microarchitectural\ndefense assumption violation using GEM5 simulations. Performance evaluation on\nSPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\\%\naverage overhead across both sets of benchmarks. The repair which prevented the\ncreation of remote E state had less than 2.8% average overhead."
                },
                "authors": [
                    {
                        "name": "Kartik Ramkrishnan"
                    },
                    {
                        "name": "Antonia Zhai"
                    },
                    {
                        "name": "Stephen McCamant"
                    },
                    {
                        "name": "Pen Chung Yew"
                    }
                ],
                "author_detail": {
                    "name": "Pen Chung Yew"
                },
                "author": "Pen Chung Yew",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10181v1",
                "updated": "2025-04-14T12:34:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    34,
                    20,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:34:20Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    34,
                    20,
                    0,
                    104,
                    0
                ],
                "title": "A New Paradigm in IBR Modeling for Power Flow and Short Circuit Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Paradigm in IBR Modeling for Power Flow and Short Circuit Analysis"
                },
                "summary": "The fault characteristics of inverter-based resources (IBRs) are different\nfrom conventional synchronous generators. The fault response of IBRs is\nnon-linear due to saturation states and mainly determined by fault ride through\n(FRT) strategies of the associated voltage source converter (VSC). This results\nin prohibitively large solution times for power flows considering these short\ncircuit characteristics, especially when the power system states change fast\ndue to uncertainty in IBR generations. To overcome this, a phasor-domain steady\nstate (SS) short circuit (SC) solver for IBR dominated power systems is\nproposed in this paper, and subsequently the developed IBR models are\nincorporated with a novel Jacobian-based Power Flow (PF) solver. In this\nmultiphase PF solver, any power system components can be modeled by considering\ntheir original non-linear or linear mathematical representations. Moreover, two\nnovel FRT strategies are proposed to fully utilize the converter capacity and\nto comply with IEEE-2800 2022 std and German grid code. The results are\ncompared with the Electromagnetic Transient (EMT) simulation on the IEEE 34\ntest network and the 120 kV EPRI benchmark system. The developed IBR sequence\ndomain PF model demonstrates more accurate behavior compared to the classical\nIBR generator model. The error in calculating the short circuit current with\nthe proposed SC solver is less than 3%, while achieving significant speed\nimprovements of three order of magnitudes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fault characteristics of inverter-based resources (IBRs) are different\nfrom conventional synchronous generators. The fault response of IBRs is\nnon-linear due to saturation states and mainly determined by fault ride through\n(FRT) strategies of the associated voltage source converter (VSC). This results\nin prohibitively large solution times for power flows considering these short\ncircuit characteristics, especially when the power system states change fast\ndue to uncertainty in IBR generations. To overcome this, a phasor-domain steady\nstate (SS) short circuit (SC) solver for IBR dominated power systems is\nproposed in this paper, and subsequently the developed IBR models are\nincorporated with a novel Jacobian-based Power Flow (PF) solver. In this\nmultiphase PF solver, any power system components can be modeled by considering\ntheir original non-linear or linear mathematical representations. Moreover, two\nnovel FRT strategies are proposed to fully utilize the converter capacity and\nto comply with IEEE-2800 2022 std and German grid code. The results are\ncompared with the Electromagnetic Transient (EMT) simulation on the IEEE 34\ntest network and the 120 kV EPRI benchmark system. The developed IBR sequence\ndomain PF model demonstrates more accurate behavior compared to the classical\nIBR generator model. The error in calculating the short circuit current with\nthe proposed SC solver is less than 3%, while achieving significant speed\nimprovements of three order of magnitudes."
                },
                "authors": [
                    {
                        "name": "Zahid Javid"
                    },
                    {
                        "name": "Firdous Ul Nazir"
                    },
                    {
                        "name": "Wentao Zhu"
                    },
                    {
                        "name": "Diptargha Chakravorty"
                    },
                    {
                        "name": "Ahmed Aboushady"
                    },
                    {
                        "name": "Mohamed Galeela"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Galeela"
                },
                "author": "Mohamed Galeela",
                "arxiv_comment": "12 Pages, First Revision Submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00601v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00601v3",
                "updated": "2025-04-14T11:20:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    20,
                    56,
                    0,
                    104,
                    0
                ],
                "published": "2024-11-01T14:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    3,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Diversity in Network-Friendly Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in Network-Friendly Recommendations"
                },
                "summary": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms."
                },
                "authors": [
                    {
                        "name": "Evangelia Tzimpimpaki"
                    },
                    {
                        "name": "Thrasyvoulos Spyropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Thrasyvoulos Spyropoulos"
                },
                "author": "Thrasyvoulos Spyropoulos",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00601v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00601v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09984v1",
                "updated": "2025-04-14T08:51:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    8,
                    51,
                    35,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T08:51:35Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    8,
                    51,
                    35,
                    0,
                    104,
                    0
                ],
                "title": "On Precomputation and Caching in Information Retrieval Experiments with\n  Pipeline Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Precomputation and Caching in Information Retrieval Experiments with\n  Pipeline Architectures"
                },
                "summary": "Modern information retrieval systems often rely on multiple components\nexecuted in a pipeline. In a research setting, this can lead to substantial\nredundant computations (e.g., retrieving the same query multiple times for\nevaluating different downstream rerankers). To overcome this, researchers take\ncached \"result\" files as inputs, which represent the output of another\npipeline. However, these result files can be brittle and can cause a disconnect\nbetween the conceptual design of the pipeline and its logical implementation.\nTo overcome both the redundancy problem (when executing complete pipelines) and\nthe disconnect problem (when relying on intermediate result files), we describe\nour recent efforts to improve the caching capabilities in the open-source\nPyTerrier IR platform. We focus on two main directions: (1) automatic implicit\ncaching of common pipeline prefixes when comparing systems and (2) explicit\ncaching of operations through a new extension package, pyterrier-caching. These\napproaches allow for the best of both worlds: pipelines can be fully expressed\nend-to-end, while also avoiding redundant computations between pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern information retrieval systems often rely on multiple components\nexecuted in a pipeline. In a research setting, this can lead to substantial\nredundant computations (e.g., retrieving the same query multiple times for\nevaluating different downstream rerankers). To overcome this, researchers take\ncached \"result\" files as inputs, which represent the output of another\npipeline. However, these result files can be brittle and can cause a disconnect\nbetween the conceptual design of the pipeline and its logical implementation.\nTo overcome both the redundancy problem (when executing complete pipelines) and\nthe disconnect problem (when relying on intermediate result files), we describe\nour recent efforts to improve the caching capabilities in the open-source\nPyTerrier IR platform. We focus on two main directions: (1) automatic implicit\ncaching of common pipeline prefixes when comparing systems and (2) explicit\ncaching of operations through a new extension package, pyterrier-caching. These\napproaches allow for the best of both worlds: pipelines can be fully expressed\nend-to-end, while also avoiding redundant computations between pipelines."
                },
                "authors": [
                    {
                        "name": "Sean MacAvaney"
                    },
                    {
                        "name": "Craig Macdonald"
                    }
                ],
                "author_detail": {
                    "name": "Craig Macdonald"
                },
                "author": "Craig Macdonald",
                "arxiv_comment": "WOWS @ ECIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09952v1",
                "updated": "2025-04-14T07:30:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    7,
                    30,
                    3,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T07:30:03Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    7,
                    30,
                    3,
                    0,
                    104,
                    0
                ],
                "title": "Secrecy and Privacy in Multi-Access Combinatorial Topology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secrecy and Privacy in Multi-Access Combinatorial Topology"
                },
                "summary": "In this work, we consider the multi-access combinatorial topology with $C$\ncaches where each user accesses a unique set of $r$ caches. For this setup, we\nconsider secrecy, where each user should not know anything about the files it\ndid not request, and demand privacy, where each user's demand must be kept\nprivate from other non-colluding users. We propose a scheme satisfying both\nconditions and derive a lower bound based on cut-set arguments. Also, we prove\nthat our scheme is optimal when $r\\geq C-1$, and it is order-optimal when the\ncache memory size $M$ is greater than or equal to a certain threshold for\n$r<C-1$. When $r=1$, in most of the memory region, our scheme achieves the same\nrate as the one given by the secretive scheme for the dedicated cache setup by\nRavindrakumar et al. ( 'Private Coded Caching,' in \\textit{IEEE Transactions on\nInformation Forensics and Security}, 2018), while satisfying both secrecy and\ndemand privacy conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we consider the multi-access combinatorial topology with $C$\ncaches where each user accesses a unique set of $r$ caches. For this setup, we\nconsider secrecy, where each user should not know anything about the files it\ndid not request, and demand privacy, where each user's demand must be kept\nprivate from other non-colluding users. We propose a scheme satisfying both\nconditions and derive a lower bound based on cut-set arguments. Also, we prove\nthat our scheme is optimal when $r\\geq C-1$, and it is order-optimal when the\ncache memory size $M$ is greater than or equal to a certain threshold for\n$r<C-1$. When $r=1$, in most of the memory region, our scheme achieves the same\nrate as the one given by the secretive scheme for the dedicated cache setup by\nRavindrakumar et al. ( 'Private Coded Caching,' in \\textit{IEEE Transactions on\nInformation Forensics and Security}, 2018), while satisfying both secrecy and\ndemand privacy conditions."
                },
                "authors": [
                    {
                        "name": "Mallikharjuna Chinnapadamala"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "11 pages and 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09936v1",
                "updated": "2025-04-14T06:58:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    6,
                    58,
                    0,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T06:58:00Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    6,
                    58,
                    0,
                    0,
                    104,
                    0
                ],
                "title": "KeepKV: Eliminating Output Perturbation in KV Cache Compression for\n  Efficient LLMs Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeepKV: Eliminating Output Perturbation in KV Cache Compression for\n  Efficient LLMs Inference"
                },
                "summary": "Efficient inference of large language models (LLMs) is hindered by an\never-growing key-value (KV) cache, making KV cache compression a critical\nresearch direction. Traditional methods selectively evict less important KV\ncache entries based on attention scores or position heuristics, which leads to\ninformation loss and hallucinations. Recently, merging-based strategies have\nbeen explored to retain more information by merging KV pairs that would be\ndiscarded; however, these existing approaches inevitably introduce\ninconsistencies in attention distributions before and after merging, causing\noutput perturbation and degraded generation quality. To overcome this\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\nto eliminate output perturbation while preserving performance under strict\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\nrecords merging history and adaptively adjusts attention scores. Moreover, it\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\nattention consistency and compensating for attention loss resulting from cache\nmerging. KeepKV successfully retains essential context information within a\nsignificantly compressed cache. Extensive experiments on various benchmarks and\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\nenhances inference throughput by more than 2x and keeps superior generation\nquality even with 10% KV cache budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient inference of large language models (LLMs) is hindered by an\never-growing key-value (KV) cache, making KV cache compression a critical\nresearch direction. Traditional methods selectively evict less important KV\ncache entries based on attention scores or position heuristics, which leads to\ninformation loss and hallucinations. Recently, merging-based strategies have\nbeen explored to retain more information by merging KV pairs that would be\ndiscarded; however, these existing approaches inevitably introduce\ninconsistencies in attention distributions before and after merging, causing\noutput perturbation and degraded generation quality. To overcome this\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\nto eliminate output perturbation while preserving performance under strict\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\nrecords merging history and adaptively adjusts attention scores. Moreover, it\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\nattention consistency and compensating for attention loss resulting from cache\nmerging. KeepKV successfully retains essential context information within a\nsignificantly compressed cache. Extensive experiments on various benchmarks and\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\nenhances inference throughput by more than 2x and keeps superior generation\nquality even with 10% KV cache budgets."
                },
                "authors": [
                    {
                        "name": "Yuxuan Tian"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Yebo Peng"
                    },
                    {
                        "name": "Aomufei Yuan"
                    },
                    {
                        "name": "Zhiming Wang"
                    },
                    {
                        "name": "Bairen Yi"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12280v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12280v2",
                "updated": "2025-04-13T14:17:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    17,
                    57,
                    6,
                    103,
                    0
                ],
                "published": "2024-02-19T16:47:04Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    16,
                    47,
                    4,
                    0,
                    50,
                    0
                ],
                "title": "Plato: Plan to Efficiently Decode for Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plato: Plan to Efficiently Decode for Large Language Model Inference"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage tasks, but their inference incurs substantial computational and memory\noverhead. To improve efficiency, parallel decoding methods like\nSkeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent\nprocessing. However, these methods significantly compromise answer quality by\ntreating semantically linked sub-problems as independent. We propose Plato, a\nnovel approach that co-designs algorithms and systems for semantic-aware\nparallel decoding. Plato leverages LLMs to organize sub-problems into a\ndependency graph based on logical and causal relationships, enabling concurrent\ndecoding of non-dependent nodes while preserving answer coherence and quality.\nTo further enhance efficiency, Plato pipelines planning and node decoding\nstages, implements a global context cache, and carefully structures node\ninference prompts to maximize key-value cache reuse and minimize overhead. Our\nevaluations show that Plato improves throughput by 68% over autoregressive\ndecoding while achieving a 40% net win rate in answer quality. Compared to SoT,\nPlato demonstrates a remarkable 90% quality net-win rate. Ablation studies\nreveal that our pipeline design improves speedup by 29%, while our KV cache\nreuse optimization reduces overhead by 75%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage tasks, but their inference incurs substantial computational and memory\noverhead. To improve efficiency, parallel decoding methods like\nSkeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent\nprocessing. However, these methods significantly compromise answer quality by\ntreating semantically linked sub-problems as independent. We propose Plato, a\nnovel approach that co-designs algorithms and systems for semantic-aware\nparallel decoding. Plato leverages LLMs to organize sub-problems into a\ndependency graph based on logical and causal relationships, enabling concurrent\ndecoding of non-dependent nodes while preserving answer coherence and quality.\nTo further enhance efficiency, Plato pipelines planning and node decoding\nstages, implements a global context cache, and carefully structures node\ninference prompts to maximize key-value cache reuse and minimize overhead. Our\nevaluations show that Plato improves throughput by 68% over autoregressive\ndecoding while achieving a 40% net win rate in answer quality. Compared to SoT,\nPlato demonstrates a remarkable 90% quality net-win rate. Ablation studies\nreveal that our pipeline design improves speedup by 29%, while our KV cache\nreuse optimization reduces overhead by 75%."
                },
                "authors": [
                    {
                        "name": "Shuowei Jin"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Haizhong Zheng"
                    },
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Atul Prakash"
                    },
                    {
                        "name": "Matthew Lentz"
                    },
                    {
                        "name": "Danyang Zhuo"
                    },
                    {
                        "name": "Feng Qian"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12280v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12280v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09590v1",
                "updated": "2025-04-13T14:16:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    16,
                    57,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T14:16:57Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    16,
                    57,
                    6,
                    103,
                    0
                ],
                "title": "Efficient LLM Serving on Hybrid Real-time and Best-effort Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Serving on Hybrid Real-time and Best-effort Requests"
                },
                "summary": "Recent breakthroughs in large Language Models (LLMs) have enabled various\ngenerative tasks on a single model. Real-world services (e.g., OpenAI's ChatGPT\n[27]) powered by an LLM often concurrently support latency-critical requests\nfor interactive applications (e.g., question-answering systems, referred to as\nreal-time or RT requests) and throughput-oriented requests for back-of-house\nprocessing (e.g., documents batch processing [28], referred to best-effort or\nBE requests), with complex hybrid inference workloads to the underlying model.\nState-of-the-art (SOTA) LLM serving systems dedicate machines to each type of\nrequest, towards either low inference latency or high serving throughput,\nrespectively. This practice simplifies request scheduling and management but\nsuffers from poor resource utilization. We propose BROS, a hybrid LLM serving\nsystem that aims to collocate RT/BE requests, meeting RT requests' latency\nrequirements while maintaining BE requests' throughput. BROS formulates the\nproblem of hybrid RT/BE request scheduling and solves it with a dynamic\npriority-based algorithm. BROS designs a bidirectional KV cache management\nmechanism, allowing RT requests to share KV memory with BE requests to remove\nthe scheduling restrictions caused by insufficient KV memory and improve\nutilization. Extensive experiments validate that BROS achieves a good trade-off\nwhen serving hybrid RT and BE requests. It significantly reduces the latency of\nRT requests (up to 74.20%), improving their fine-grained service level\nobjectives (SLOs) attainments (up to 36.38x), with negligible throughput\nreduction for BE requests, showing significant advantages over SOTA systems\nlike vLLM and TGI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in large Language Models (LLMs) have enabled various\ngenerative tasks on a single model. Real-world services (e.g., OpenAI's ChatGPT\n[27]) powered by an LLM often concurrently support latency-critical requests\nfor interactive applications (e.g., question-answering systems, referred to as\nreal-time or RT requests) and throughput-oriented requests for back-of-house\nprocessing (e.g., documents batch processing [28], referred to best-effort or\nBE requests), with complex hybrid inference workloads to the underlying model.\nState-of-the-art (SOTA) LLM serving systems dedicate machines to each type of\nrequest, towards either low inference latency or high serving throughput,\nrespectively. This practice simplifies request scheduling and management but\nsuffers from poor resource utilization. We propose BROS, a hybrid LLM serving\nsystem that aims to collocate RT/BE requests, meeting RT requests' latency\nrequirements while maintaining BE requests' throughput. BROS formulates the\nproblem of hybrid RT/BE request scheduling and solves it with a dynamic\npriority-based algorithm. BROS designs a bidirectional KV cache management\nmechanism, allowing RT requests to share KV memory with BE requests to remove\nthe scheduling restrictions caused by insufficient KV memory and improve\nutilization. Extensive experiments validate that BROS achieves a good trade-off\nwhen serving hybrid RT and BE requests. It significantly reduces the latency of\nRT requests (up to 74.20%), improving their fine-grained service level\nobjectives (SLOs) attainments (up to 36.38x), with negligible throughput\nreduction for BE requests, showing significant advantages over SOTA systems\nlike vLLM and TGI."
                },
                "authors": [
                    {
                        "name": "Wan Borui"
                    },
                    {
                        "name": "Zhao Juntao"
                    },
                    {
                        "name": "Jiang Chenyu"
                    },
                    {
                        "name": "Guo Chuanxiong"
                    },
                    {
                        "name": "Wu Chuan"
                    }
                ],
                "author_detail": {
                    "name": "Wu Chuan"
                },
                "author": "Wu Chuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v5",
                "updated": "2025-04-13T14:02:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    2,
                    47,
                    6,
                    103,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient Prefilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient Prefilling"
                },
                "summary": "We introduce Block-attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context in an\nauto-regressive manner. Instead, Block-attention divides retrieved documents\ninto discrete blocks, with each block independently calculating key-value (KV)\nstates except for the final block. In RAG scenarios, by defining each passage\nas a block, Block-attention enables us to reuse the KV states of passages that\nhave been seen before, thereby significantly reducing the latency and the\ncomputation overhead during inference. The implementation of Block-attention\ninvolves block segmentation, position re-encoding, and fine-tuning the LLM to\nadapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks,\nincluding RAG, ICL, and general domains, demonstrate that after block\nfine-tuning, the Block-attention model not only achieves performance comparable\nto that of full-attention models, but can also seamlessly switch between the\nblock and full attention modes without any performance loss. Notably,\nBlock-attention significantly reduces the time to first token (TTFT) and\nfloating point operations (FLOPs) to a very low level. It only takes 45 ms to\noutput the first token for an input sequence with a total length of 32K.\nCompared to the full-attention models, the TTFT and corresponding FLOPs are\nreduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we\nelaborate on how Block-attention is applied in Game AI scenario and the\nsubstantial potential benefits it entails. We strongly suggest researchers in\nthe gaming field not to overlook this section.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context in an\nauto-regressive manner. Instead, Block-attention divides retrieved documents\ninto discrete blocks, with each block independently calculating key-value (KV)\nstates except for the final block. In RAG scenarios, by defining each passage\nas a block, Block-attention enables us to reuse the KV states of passages that\nhave been seen before, thereby significantly reducing the latency and the\ncomputation overhead during inference. The implementation of Block-attention\ninvolves block segmentation, position re-encoding, and fine-tuning the LLM to\nadapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks,\nincluding RAG, ICL, and general domains, demonstrate that after block\nfine-tuning, the Block-attention model not only achieves performance comparable\nto that of full-attention models, but can also seamlessly switch between the\nblock and full attention modes without any performance loss. Notably,\nBlock-attention significantly reduces the time to first token (TTFT) and\nfloating point operations (FLOPs) to a very low level. It only takes 45 ms to\noutput the first token for an input sequence with a total length of 32K.\nCompared to the full-attention models, the TTFT and corresponding FLOPs are\nreduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we\nelaborate on how Block-attention is applied in Game AI scenario and the\nsubstantial potential benefits it entails. We strongly suggest researchers in\nthe gaming field not to overlook this section."
                },
                "authors": [
                    {
                        "name": "Dongyang Ma"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10540v1",
                "updated": "2025-04-13T08:29:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    8,
                    29,
                    58,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T08:29:58Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    8,
                    29,
                    58,
                    6,
                    103,
                    0
                ],
                "title": "AB-Cache: Training-Free Acceleration of Diffusion Models via\n  Adams-Bashforth Cached Feature Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AB-Cache: Training-Free Acceleration of Diffusion Models via\n  Adams-Bashforth Cached Feature Reuse"
                },
                "summary": "Diffusion models have demonstrated remarkable success in generative tasks,\nyet their iterative denoising process results in slow inference, limiting their\npracticality. While existing acceleration methods exploit the well-known\nU-shaped similarity pattern between adjacent steps through caching mechanisms,\nthey lack theoretical foundation and rely on simplistic computation reuse,\noften leading to performance degradation. In this work, we provide a\ntheoretical understanding by analyzing the denoising process through the\nsecond-order Adams-Bashforth method, revealing a linear relationship between\nthe outputs of consecutive steps. This analysis explains why the outputs of\nadjacent steps exhibit a U-shaped pattern. Furthermore, extending\nAdams-Bashforth method to higher order, we propose a novel caching-based\nacceleration approach for diffusion models, instead of directly reusing cached\nresults, with a truncation error bound of only \\(O(h^k)\\) where $h$ is the step\nsize. Extensive validation across diverse image and video diffusion models\n(including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates\nour method's effectiveness in achieving nearly $3\\times$ speedup while\nmaintaining original performance levels, offering a practical real-time\nsolution without compromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated remarkable success in generative tasks,\nyet their iterative denoising process results in slow inference, limiting their\npracticality. While existing acceleration methods exploit the well-known\nU-shaped similarity pattern between adjacent steps through caching mechanisms,\nthey lack theoretical foundation and rely on simplistic computation reuse,\noften leading to performance degradation. In this work, we provide a\ntheoretical understanding by analyzing the denoising process through the\nsecond-order Adams-Bashforth method, revealing a linear relationship between\nthe outputs of consecutive steps. This analysis explains why the outputs of\nadjacent steps exhibit a U-shaped pattern. Furthermore, extending\nAdams-Bashforth method to higher order, we propose a novel caching-based\nacceleration approach for diffusion models, instead of directly reusing cached\nresults, with a truncation error bound of only \\(O(h^k)\\) where $h$ is the step\nsize. Extensive validation across diverse image and video diffusion models\n(including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates\nour method's effectiveness in achieving nearly $3\\times$ speedup while\nmaintaining original performance levels, offering a practical real-time\nsolution without compromising generation quality."
                },
                "authors": [
                    {
                        "name": "Zichao Yu"
                    },
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Guojiang Shao"
                    },
                    {
                        "name": "Chengwei Zhang"
                    },
                    {
                        "name": "Shengze Xu"
                    },
                    {
                        "name": "Jie Huang"
                    },
                    {
                        "name": "Feng Zhao"
                    },
                    {
                        "name": "Xiaodong Cun"
                    },
                    {
                        "name": "Wenyi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenyi Zhang"
                },
                "author": "Wenyi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09431v1",
                "updated": "2025-04-13T04:46:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    4,
                    46,
                    2,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T04:46:02Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    4,
                    46,
                    2,
                    6,
                    103,
                    0
                ],
                "title": "Sub-nanosecond in-plane magnetization switching induced by field-like\n  spin-orbit torques from ferromagnets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-nanosecond in-plane magnetization switching induced by field-like\n  spin-orbit torques from ferromagnets"
                },
                "summary": "Spin-orbit torques (SOTs) generated in SOT-material/ferromagnet structures\nare classified as damping-like (DL) and field-like (FL) torques for\ncurrent-driven magnetization switching. It is well known that both DL- and\nFL-SOTs originate from the SOT-material and DL-SOT dominates the current-driven\nswitching process while FL-SOT contributes limitedly, resulting in an\nincubation time (several nanoseconds) during collinear magnetization switching\nwith the spin polarization because of the DL attributes. Here we report a\nFL-SOT originated from the ferromagnet, different from the origin of DL-SOT,\nand demonstrate that it dominates the collinear magnetization switching. We\nshow that the FL-SOT and resultant collinear switching can be modulated, one\norder of magnitude and sign reversal, by controlling the ferromagnet. Because\nof no incubation time and higher charge-to-spin efficiencies in the FL\nswitching, we further show that the switching time can be down to 200 ps with\none order lower critical switching current density compared to DL switching.\nThese results indicate that the FL switching may provide a practical solution\nfor magnetic memory in speed-priority cache applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spin-orbit torques (SOTs) generated in SOT-material/ferromagnet structures\nare classified as damping-like (DL) and field-like (FL) torques for\ncurrent-driven magnetization switching. It is well known that both DL- and\nFL-SOTs originate from the SOT-material and DL-SOT dominates the current-driven\nswitching process while FL-SOT contributes limitedly, resulting in an\nincubation time (several nanoseconds) during collinear magnetization switching\nwith the spin polarization because of the DL attributes. Here we report a\nFL-SOT originated from the ferromagnet, different from the origin of DL-SOT,\nand demonstrate that it dominates the collinear magnetization switching. We\nshow that the FL-SOT and resultant collinear switching can be modulated, one\norder of magnitude and sign reversal, by controlling the ferromagnet. Because\nof no incubation time and higher charge-to-spin efficiencies in the FL\nswitching, we further show that the switching time can be down to 200 ps with\none order lower critical switching current density compared to DL switching.\nThese results indicate that the FL switching may provide a practical solution\nfor magnetic memory in speed-priority cache applications."
                },
                "authors": [
                    {
                        "name": "Hanying Zhang"
                    },
                    {
                        "name": "Ziqian Cui"
                    },
                    {
                        "name": "Baiqing Jiang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "C. Bi"
                    }
                ],
                "author_detail": {
                    "name": "C. Bi"
                },
                "author": "C. Bi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09261v1",
                "updated": "2025-04-12T15:42:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    12,
                    15,
                    42,
                    17,
                    5,
                    102,
                    0
                ],
                "published": "2025-04-12T15:42:17Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    15,
                    42,
                    17,
                    5,
                    102,
                    0
                ],
                "title": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive\n  Modeling"
                },
                "summary": "Visual Autoregressive (VAR) models have emerged as a powerful approach for\nmulti-modal content creation, offering high efficiency and quality across\ndiverse multimedia applications. However, they face significant memory\nbottlenecks due to extensive KV cache accumulation during inference. Existing\nKV cache compression techniques for large language models are suboptimal for\nVAR models due to, as we identify in this paper, two distinct categories of\nattention heads in VAR models: Structural Heads, which preserve spatial\ncoherence through diagonal attention patterns, and Contextual Heads, which\nmaintain semantic consistency through vertical attention patterns. These\ndifferences render single-strategy KV compression techniques ineffective for\nVAR models. To address this, we propose HACK, a training-free Head-Aware\nCompression method for KV cache. HACK allocates asymmetric cache budgets and\nemploys pattern-specific compression strategies tailored to the essential\ncharacteristics of each head category. Experiments on Infinity-2B, Infinity-8B,\nand VAR-d30 demonstrate its effectiveness in text-to-image and\nclass-conditional generation tasks. HACK can hack down up to 50\\% and 70\\% of\ncache with minimal performance degradation for VAR-d30 and Infinity-8B,\nrespectively. Even with 70\\% and 90\\% KV cache compression in VAR-d30 and\nInfinity-8B, HACK still maintains high-quality generation while reducing memory\nusage by 44.2\\% and 58.9\\%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) models have emerged as a powerful approach for\nmulti-modal content creation, offering high efficiency and quality across\ndiverse multimedia applications. However, they face significant memory\nbottlenecks due to extensive KV cache accumulation during inference. Existing\nKV cache compression techniques for large language models are suboptimal for\nVAR models due to, as we identify in this paper, two distinct categories of\nattention heads in VAR models: Structural Heads, which preserve spatial\ncoherence through diagonal attention patterns, and Contextual Heads, which\nmaintain semantic consistency through vertical attention patterns. These\ndifferences render single-strategy KV compression techniques ineffective for\nVAR models. To address this, we propose HACK, a training-free Head-Aware\nCompression method for KV cache. HACK allocates asymmetric cache budgets and\nemploys pattern-specific compression strategies tailored to the essential\ncharacteristics of each head category. Experiments on Infinity-2B, Infinity-8B,\nand VAR-d30 demonstrate its effectiveness in text-to-image and\nclass-conditional generation tasks. HACK can hack down up to 50\\% and 70\\% of\ncache with minimal performance degradation for VAR-d30 and Infinity-8B,\nrespectively. Even with 70\\% and 90\\% KV cache compression in VAR-d30 and\nInfinity-8B, HACK still maintains high-quality generation while reducing memory\nusage by 44.2\\% and 58.9\\%, respectively."
                },
                "authors": [
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Youru Lv"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Zeren Zhang"
                    },
                    {
                        "name": "Danping Zou"
                    },
                    {
                        "name": "Weiyao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weiyao Lin"
                },
                "author": "Weiyao Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17459v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17459v3",
                "updated": "2025-04-11T12:31:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    12,
                    31,
                    7,
                    4,
                    101,
                    0
                ],
                "published": "2024-11-26T14:23:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model"
                },
                "summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE."
                },
                "authors": [
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Liuhan Chen"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17459v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17459v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08378v1",
                "updated": "2025-04-11T09:26:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T09:26:47Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "title": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash"
                },
                "summary": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods."
                },
                "authors": [
                    {
                        "name": "Fucheng Jia"
                    },
                    {
                        "name": "Zewen Wu"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Ju Ren"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Ting Cao"
                    }
                ],
                "author_detail": {
                    "name": "Ting Cao"
                },
                "author": "Ting Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08204v1",
                "updated": "2025-04-11T02:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    10,
                    2,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T02:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    10,
                    2,
                    4,
                    101,
                    0
                ],
                "title": "II-NVM: Enhancing Map Accuracy and Consistency with Normal\n  Vector-Assisted Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "II-NVM: Enhancing Map Accuracy and Consistency with Normal\n  Vector-Assisted Mapping"
                },
                "summary": "SLAM technology plays a crucial role in indoor mapping and localization. A\ncommon challenge in indoor environments is the \"double-sided mapping issue\",\nwhere closely positioned walls, doors, and other surfaces are mistakenly\nidentified as a single plane, significantly hindering map accuracy and\nconsistency. To address this issue this paper introduces a SLAM approach that\nensures accurate mapping using normal vector consistency. We enhance the voxel\nmap structure to store both point cloud data and normal vector information,\nenabling the system to evaluate consistency during nearest neighbor searches\nand map updates. This process distinguishes between the front and back sides of\nsurfaces, preventing incorrect point-to-plane constraints. Moreover, we\nimplement an adaptive radius KD-tree search method that dynamically adjusts the\nsearch radius based on the local density of the point cloud, thereby enhancing\nthe accuracy of normal vector calculations. To further improve realtime\nperformance and storage efficiency, we incorporate a Least Recently Used (LRU)\ncache strategy, which facilitates efficient incremental updates of the voxel\nmap. The code is released as open-source and validated in both simulated\nenvironments and real indoor scenarios. Experimental results demonstrate that\nthis approach effectively resolves the \"double-sided mapping issue\" and\nsignificantly improves mapping precision. Additionally, we have developed and\nopen-sourced the first simulation and real world dataset specifically tailored\nfor the \"double-sided mapping issue\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLAM technology plays a crucial role in indoor mapping and localization. A\ncommon challenge in indoor environments is the \"double-sided mapping issue\",\nwhere closely positioned walls, doors, and other surfaces are mistakenly\nidentified as a single plane, significantly hindering map accuracy and\nconsistency. To address this issue this paper introduces a SLAM approach that\nensures accurate mapping using normal vector consistency. We enhance the voxel\nmap structure to store both point cloud data and normal vector information,\nenabling the system to evaluate consistency during nearest neighbor searches\nand map updates. This process distinguishes between the front and back sides of\nsurfaces, preventing incorrect point-to-plane constraints. Moreover, we\nimplement an adaptive radius KD-tree search method that dynamically adjusts the\nsearch radius based on the local density of the point cloud, thereby enhancing\nthe accuracy of normal vector calculations. To further improve realtime\nperformance and storage efficiency, we incorporate a Least Recently Used (LRU)\ncache strategy, which facilitates efficient incremental updates of the voxel\nmap. The code is released as open-source and validated in both simulated\nenvironments and real indoor scenarios. Experimental results demonstrate that\nthis approach effectively resolves the \"double-sided mapping issue\" and\nsignificantly improves mapping precision. Additionally, we have developed and\nopen-sourced the first simulation and real world dataset specifically tailored\nfor the \"double-sided mapping issue\"."
                },
                "authors": [
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Yina Jian"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Linji Wang"
                    },
                    {
                        "name": "Yongxin Ma"
                    },
                    {
                        "name": "Xinglai Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xinglai Jin"
                },
                "author": "Xinglai Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07596v2",
                "updated": "2025-04-11T02:05:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    5,
                    1,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-10T09:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    48,
                    56,
                    3,
                    100,
                    0
                ],
                "title": "Boosting Universal LLM Reward Design through Heuristic Reward\n  Observation Space Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Universal LLM Reward Design through Heuristic Reward\n  Observation Space Evolution"
                },
                "summary": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward."
                },
                "authors": [
                    {
                        "name": "Zen Kit Heng"
                    },
                    {
                        "name": "Zimeng Zhao"
                    },
                    {
                        "name": "Tianhao Wu"
                    },
                    {
                        "name": "Yuanfei Wang"
                    },
                    {
                        "name": "Mingdong Wu"
                    },
                    {
                        "name": "Yangang Wang"
                    },
                    {
                        "name": "Hao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Hao Dong"
                },
                "author": "Hao Dong",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07815v1",
                "updated": "2025-04-10T14:52:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    52,
                    3,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T14:52:03Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    52,
                    3,
                    3,
                    100,
                    0
                ],
                "title": "Siren Federate: Bridging document, relational, and graph models for\n  exploratory graph analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Siren Federate: Bridging document, relational, and graph models for\n  exploratory graph analysis"
                },
                "summary": "Investigative workflows require interactive exploratory analysis on large\nheterogeneous knowledge graphs. Current databases show limitations in enabling\nsuch task. This paper discusses the architecture of Siren Federate, a system\nthat efficiently supports exploratory graph analysis by bridging\ndocument-oriented, relational and graph models. Technical contributions include\ndistributed join algorithms, adaptive query planning, query plan folding,\nsemantic caching, and semi-join decomposition for path query. Semi-join\ndecomposition addresses the exponential growth of intermediate results in\npath-based queries. Experiments show that Siren Federate exhibits low latency\nand scales well with the amount of data, the number of users, and the number of\ncomputing nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigative workflows require interactive exploratory analysis on large\nheterogeneous knowledge graphs. Current databases show limitations in enabling\nsuch task. This paper discusses the architecture of Siren Federate, a system\nthat efficiently supports exploratory graph analysis by bridging\ndocument-oriented, relational and graph models. Technical contributions include\ndistributed join algorithms, adaptive query planning, query plan folding,\nsemantic caching, and semi-join decomposition for path query. Semi-join\ndecomposition addresses the exponential growth of intermediate results in\npath-based queries. Experiments show that Siren Federate exhibits low latency\nand scales well with the amount of data, the number of users, and the number of\ncomputing nodes."
                },
                "authors": [
                    {
                        "name": "Georgeta Bordea"
                    },
                    {
                        "name": "Stephane Campinas"
                    },
                    {
                        "name": "Matteo Catena"
                    },
                    {
                        "name": "Renaud Delbru"
                    }
                ],
                "author_detail": {
                    "name": "Renaud Delbru"
                },
                "author": "Renaud Delbru",
                "arxiv_comment": "36 pages, 16 figures, submitted to the ComSIS journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.11; E.1; H.2.4; H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07642v1",
                "updated": "2025-04-10T10:43:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    43,
                    42,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T10:43:42Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    43,
                    42,
                    3,
                    100,
                    0
                ],
                "title": "Cache-a-lot: Pushing the Limits of Unsatisfiable Core Reuse in SMT-Based\n  Program Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-a-lot: Pushing the Limits of Unsatisfiable Core Reuse in SMT-Based\n  Program Analysis"
                },
                "summary": "Satisfiability Modulo Theories (SMT) solvers are integral to program analysis\ntechniques like concolic and symbolic execution, where they help assess the\nsatisfiability of logical formulae to explore execution paths of the program\nunder test. However, frequent solver invocations are still the main performance\nbottleneck of these techniques. One way to mitigate this challenge is through\noptimizations such as caching and reusing solver results. While current methods\ntypically focus on reusing results from fully equivalent or closely related\nformulas, they often miss broader opportunities for reuse. In this paper, we\npropose a novel approach, Cache-a-lot, that extends the reuse of unsatisfiable\n(unsat) results by systematically considering all possible variable\nsubstitutions. This enables more extensive reuse of results, thereby reducing\nthe number of SMT solver invocations and improving the overall efficiency of\nconcolic and symbolic execution. Our evaluation, conducted against the\nstate-of-the-art Utopia solution using two benchmark sets, shows significant\nimprovements, particularly with more complex formulas. Our method achieves up\nto 74% unsat core reuse, compared to Utopia's 41%, and significant increase in\nthe time savings. These results demonstrate that, despite the additional\ncomputational complexity, the broader reuse of unsat results significantly\nenhances performance, offering valuable advancements for formal verification\nand program analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satisfiability Modulo Theories (SMT) solvers are integral to program analysis\ntechniques like concolic and symbolic execution, where they help assess the\nsatisfiability of logical formulae to explore execution paths of the program\nunder test. However, frequent solver invocations are still the main performance\nbottleneck of these techniques. One way to mitigate this challenge is through\noptimizations such as caching and reusing solver results. While current methods\ntypically focus on reusing results from fully equivalent or closely related\nformulas, they often miss broader opportunities for reuse. In this paper, we\npropose a novel approach, Cache-a-lot, that extends the reuse of unsatisfiable\n(unsat) results by systematically considering all possible variable\nsubstitutions. This enables more extensive reuse of results, thereby reducing\nthe number of SMT solver invocations and improving the overall efficiency of\nconcolic and symbolic execution. Our evaluation, conducted against the\nstate-of-the-art Utopia solution using two benchmark sets, shows significant\nimprovements, particularly with more complex formulas. Our method achieves up\nto 74% unsat core reuse, compared to Utopia's 41%, and significant increase in\nthe time savings. These results demonstrate that, despite the additional\ncomputational complexity, the broader reuse of unsat results significantly\nenhances performance, offering valuable advancements for formal verification\nand program analysis."
                },
                "authors": [
                    {
                        "name": "Rustam Sadykov"
                    },
                    {
                        "name": "Azat Abdullin"
                    },
                    {
                        "name": "Marat Akhin"
                    }
                ],
                "author_detail": {
                    "name": "Marat Akhin"
                },
                "author": "Marat Akhin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07494v1",
                "updated": "2025-04-10T06:51:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    51,
                    23,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:51:23Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    51,
                    23,
                    3,
                    100,
                    0
                ],
                "title": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM\n  Inference Serving"
                },
                "summary": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems."
                },
                "authors": [
                    {
                        "name": "Shihong Gao"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Yanyan Shen"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_doi": "10.1145/3725394",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725394",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.07494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07479v1",
                "updated": "2025-04-10T06:13:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    13,
                    30,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:13:30Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    13,
                    30,
                    3,
                    100,
                    0
                ],
                "title": "UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache\n  Pruning for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache\n  Pruning for Efficient Long-Context LLM Inference"
                },
                "summary": "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference."
                },
                "authors": [
                    {
                        "name": "Weikai Xu"
                    },
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Qianqian Huang"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Ru Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ru Huang"
                },
                "author": "Ru Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v3",
                "updated": "2025-04-10T05:06:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    6,
                    29,
                    3,
                    100,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "arxiv_comment": "MLSys 2025 camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07056v2",
                "updated": "2025-04-09T21:47:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    21,
                    47,
                    31,
                    2,
                    99,
                    0
                ],
                "published": "2025-01-13T04:31:04Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "title": "MAGNUS: Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGNUS: Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs"
                },
                "summary": "Sparse general matrix-matrix multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines, including Intel MKL, for a variety of different matrices on three\nIntel architectures. For matrices from the SuiteSparse collection, MAGNUS is\nfaster than all the baselines in most cases and is often an order of magnitude\nfaster than at least one baseline. For massive random matrices, MAGNUS scales\nto the largest matrix sizes, while the baselines do not. Furthermore, MAGNUS is\nclose to the optimal bound for these matrices, regardless of the matrix size,\nstructure, and density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse general matrix-matrix multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines, including Intel MKL, for a variety of different matrices on three\nIntel architectures. For matrices from the SuiteSparse collection, MAGNUS is\nfaster than all the baselines in most cases and is often an order of magnitude\nfaster than at least one baseline. For massive random matrices, MAGNUS scales\nto the largest matrix sizes, while the baselines do not. Furthermore, MAGNUS is\nclose to the optimal bound for these matrices, regardless of the matrix size,\nstructure, and density."
                },
                "authors": [
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Petrini"
                },
                "author": "Fabrizio Petrini",
                "arxiv_comment": "Accepted to ICS25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v3",
                "updated": "2025-04-09T20:51:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    20,
                    51,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "31 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06261v2",
                "updated": "2025-04-09T17:56:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    56,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-08T17:59:41Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    41,
                    1,
                    98,
                    0
                ],
                "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention"
                },
                "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning."
                },
                "authors": [
                    {
                        "name": "Gleb Rodionov"
                    },
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "George Yakushev"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Anton Sinitsin"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04514v2",
                "updated": "2025-04-09T14:36:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    36,
                    19,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-06T15:15:07Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    15,
                    15,
                    7,
                    6,
                    96,
                    0
                ],
                "title": "Saliency-driven Dynamic Token Pruning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saliency-driven Dynamic Token Pruning for Large Language Models"
                },
                "summary": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression."
                },
                "authors": [
                    {
                        "name": "Yao Tao"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Yun Wang"
                    },
                    {
                        "name": "Mingjian Zhu"
                    },
                    {
                        "name": "Hailin Hu"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06813v1",
                "updated": "2025-04-09T12:07:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    7,
                    26,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T12:07:26Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    7,
                    26,
                    2,
                    99,
                    0
                ],
                "title": "Introducing the Arm-membench Throughput Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introducing the Arm-membench Throughput Benchmark"
                },
                "summary": "Application performance of modern day processors is often limited by the\nmemory subsystem rather than actual compute capabilities. Therefore, data\nthroughput specifications play a key role in modeling application performance\nand determining possible bottlenecks. However, while peak instruction\nthroughputs and bandwidths for local caches are often documented, the\nachievable throughput can also depend on the relation between memory access and\ncompute instructions. In this paper, we present an Arm version of the well\nestablished x86-membench throughput benchmark, which we have adapted to support\nall current SIMD extensions of the Armv8 instruction set architecture. We\ndescribe aspects of the Armv8 ISA that need to be considered in the portable\ndesign of this benchmark. We use the benchmark to analyze the memory subsystem\nat a fine spatial granularity and to unveil microarchitectural details of three\nprocessors: Fujitsu A64FX, Ampere Altra and Cavium ThunderX2. Based on the\nresulting performance information, we show that instruction fetch and decoder\nwidths become a potential bottleneck for cache-bandwidth-sensitive workloads\ndue to the load-store concept of the Arm ISA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application performance of modern day processors is often limited by the\nmemory subsystem rather than actual compute capabilities. Therefore, data\nthroughput specifications play a key role in modeling application performance\nand determining possible bottlenecks. However, while peak instruction\nthroughputs and bandwidths for local caches are often documented, the\nachievable throughput can also depend on the relation between memory access and\ncompute instructions. In this paper, we present an Arm version of the well\nestablished x86-membench throughput benchmark, which we have adapted to support\nall current SIMD extensions of the Armv8 instruction set architecture. We\ndescribe aspects of the Armv8 ISA that need to be considered in the portable\ndesign of this benchmark. We use the benchmark to analyze the memory subsystem\nat a fine spatial granularity and to unveil microarchitectural details of three\nprocessors: Fujitsu A64FX, Ampere Altra and Cavium ThunderX2. Based on the\nresulting performance information, we show that instruction fetch and decoder\nwidths become a potential bottleneck for cache-bandwidth-sensitive workloads\ndue to the load-store concept of the Arm ISA."
                },
                "authors": [
                    {
                        "name": "Cyrill Burth"
                    },
                    {
                        "name": "Markus Velten"
                    },
                    {
                        "name": "Robert Schöne"
                    }
                ],
                "author_detail": {
                    "name": "Robert Schöne"
                },
                "author": "Robert Schöne",
                "arxiv_doi": "10.1007/978-3-031-85697-6_7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-85697-6_7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.06813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 6 figures, published in Parallel Processing and Applied\n  Mathematics (PPAM 2024), see https://doi.org/10.1007/978-3-031-85697-6_7",
                "arxiv_journal_ref": "Parallel Processing and Applied Mathematics. PPAM 2024. Lecture\n  Notes in Computer Science, vol 15579. Springer, Cham",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05821v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05821v2",
                "updated": "2025-04-09T10:23:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    23,
                    39,
                    2,
                    99,
                    0
                ],
                "published": "2024-03-09T07:01:44Z",
                "published_parsed": [
                    2024,
                    3,
                    9,
                    7,
                    1,
                    44,
                    5,
                    69,
                    0
                ],
                "title": "Optimizing LLM Queries in Relational Data Analytics Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Queries in Relational Data Analytics Workloads"
                },
                "summary": "Batch data analytics is a growing application for Large Language Models\n(LLMs). LLMs enable users to perform a wide range of natural language tasks,\nsuch as classification, entity extraction, and translation, over large\ndatasets. However, LLM inference is highly costly and slow: for example, an\nNVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second,\ntaking about a day to handle 15 GB of data; processing a similar amount of data\ncosts around $10K on OpenAI's GPT-4o. In this paper, we propose novel\ntechniques that can significantly reduce the cost of LLM calls for relational\ndata analytics workloads. Our key contribution is developing efficient\nalgorithms for reordering the rows and the fields within each row of an input\ntable to maximize key-value (KV) cache reuse when performing LLM serving. As\nsuch, our approach can be easily applied to existing analytics systems and\nserving platforms. Our evaluation shows that our solution can yield up to 3.4x\nimprovement in job completion time on a benchmark of diverse LLM-based queries\nusing Llama 3 models. Our solution also achieves a 32% cost savings under\nOpenAI and Anthropic pricing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch data analytics is a growing application for Large Language Models\n(LLMs). LLMs enable users to perform a wide range of natural language tasks,\nsuch as classification, entity extraction, and translation, over large\ndatasets. However, LLM inference is highly costly and slow: for example, an\nNVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second,\ntaking about a day to handle 15 GB of data; processing a similar amount of data\ncosts around $10K on OpenAI's GPT-4o. In this paper, we propose novel\ntechniques that can significantly reduce the cost of LLM calls for relational\ndata analytics workloads. Our key contribution is developing efficient\nalgorithms for reordering the rows and the fields within each row of an input\ntable to maximize key-value (KV) cache reuse when performing LLM serving. As\nsuch, our approach can be easily applied to existing analytics systems and\nserving platforms. Our evaluation shows that our solution can yield up to 3.4x\nimprovement in job completion time on a benchmark of diverse LLM-based queries\nusing Llama 3 models. Our solution also achieves a 32% cost savings under\nOpenAI and Anthropic pricing models."
                },
                "authors": [
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Asim Biswal"
                    },
                    {
                        "name": "Amog Kamsetty"
                    },
                    {
                        "name": "Audrey Cheng"
                    },
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Liana Patel"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Matei Zaharia"
                    }
                ],
                "author_detail": {
                    "name": "Matei Zaharia"
                },
                "author": "Matei Zaharia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05821v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05821v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05591v3",
                "updated": "2025-04-09T09:09:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    9,
                    37,
                    2,
                    99,
                    0
                ],
                "published": "2024-09-09T13:20:31Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    13,
                    20,
                    31,
                    0,
                    253,
                    0
                ],
                "title": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced\n  Retrieval Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced\n  Retrieval Augmentation"
                },
                "summary": "Processing long contexts presents a significant challenge for large language\nmodels (LLMs). While recent advancements allow LLMs to handle much longer\ncontexts than before (e.g., 32K or 128K tokens), it is computationally\nexpensive and can still be insufficient for many applications.\nRetrieval-Augmented Generation (RAG) is considered a promising strategy to\naddress this problem. However, conventional RAG methods face inherent\nlimitations because of two underlying requirements: 1) explicitly stated\nqueries, and 2) well-structured knowledge. These conditions, however, do not\nhold in general long-context processing tasks.\n  In this work, we propose MemoRAG, a novel RAG framework empowered by global\nmemory-augmented retrieval. MemoRAG features a dual-system architecture. First,\nit employs a light but long-range system to create a global memory of the long\ncontext. Once a task is presented, it generates draft answers, providing useful\nclues for the retrieval tools to locate relevant information within the long\ncontext. Second, it leverages an expensive but expressive system, which\ngenerates the final answer based on the retrieved information. Building upon\nthis fundamental framework, we realize the memory module in the form of KV\ncompression, and reinforce its memorization and cluing capacity from the\nGeneration quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG\nachieves superior performances across a variety of long-context evaluation\ntasks, not only complex scenarios where traditional RAG methods struggle, but\nalso simpler ones where RAG is typically applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts presents a significant challenge for large language\nmodels (LLMs). While recent advancements allow LLMs to handle much longer\ncontexts than before (e.g., 32K or 128K tokens), it is computationally\nexpensive and can still be insufficient for many applications.\nRetrieval-Augmented Generation (RAG) is considered a promising strategy to\naddress this problem. However, conventional RAG methods face inherent\nlimitations because of two underlying requirements: 1) explicitly stated\nqueries, and 2) well-structured knowledge. These conditions, however, do not\nhold in general long-context processing tasks.\n  In this work, we propose MemoRAG, a novel RAG framework empowered by global\nmemory-augmented retrieval. MemoRAG features a dual-system architecture. First,\nit employs a light but long-range system to create a global memory of the long\ncontext. Once a task is presented, it generates draft answers, providing useful\nclues for the retrieval tools to locate relevant information within the long\ncontext. Second, it leverages an expensive but expressive system, which\ngenerates the final answer based on the retrieved information. Building upon\nthis fundamental framework, we realize the memory module in the form of KV\ncompression, and reinforce its memorization and cluing capacity from the\nGeneration quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG\nachieves superior performances across a variety of long-context evaluation\ntasks, not only complex scenarios where traditional RAG methods struggle, but\nalso simpler ones where RAG is typically applied."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Kelong Mao"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Tiejun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Tiejun Huang"
                },
                "author": "Tiejun Huang",
                "arxiv_comment": "theWebConf 2025. Codes and models are in\n  https://github.com/qhjqhj00/MemoRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v5",
                "updated": "2025-04-09T07:55:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    55,
                    43,
                    2,
                    99,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v5",
                "updated": "2025-04-09T03:49:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    3,
                    49,
                    16,
                    2,
                    99,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    },
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Feng Yao"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "This paper was accepted by VLDB2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06419v1",
                "updated": "2025-04-08T20:39:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    39,
                    20,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T20:39:20Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    39,
                    20,
                    1,
                    98,
                    0
                ],
                "title": "SPIRe: Boosting LLM Inference Throughput with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPIRe: Boosting LLM Inference Throughput with Speculative Decoding"
                },
                "summary": "Speculative decoding (SD) has been shown to reduce the latency of\nautoregressive decoding (AD) by 2-3x for small batch sizes. However, increasing\nthroughput and therefore reducing the cost per token requires decoding with\nlarge batch sizes. Recent work shows that SD can accelerate decoding with large\nbatch sizes too if the context is sufficiently long and the draft model's KV\ncache is sparse. We introduce SPIRe, a draft model that combines static sparse\nattention, pruned initialization, and feedback memory to increase the modeled\nthroughput of speculative decoding by over 100% compared to speculation with a\nmuch smaller draft model and by over 35% compared to the strong baseline of\nsparse self-speculation. Our approach is particularly effective when context\nlengths vary significantly across requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD) has been shown to reduce the latency of\nautoregressive decoding (AD) by 2-3x for small batch sizes. However, increasing\nthroughput and therefore reducing the cost per token requires decoding with\nlarge batch sizes. Recent work shows that SD can accelerate decoding with large\nbatch sizes too if the context is sufficiently long and the draft model's KV\ncache is sparse. We introduce SPIRe, a draft model that combines static sparse\nattention, pruned initialization, and feedback memory to increase the modeled\nthroughput of speculative decoding by over 100% compared to speculation with a\nmuch smaller draft model and by over 35% compared to the strong baseline of\nsparse self-speculation. Our approach is particularly effective when context\nlengths vary significantly across requests."
                },
                "authors": [
                    {
                        "name": "Sanjit Neelam"
                    },
                    {
                        "name": "Daniel Heinlein"
                    },
                    {
                        "name": "Vaclav Cvicek"
                    },
                    {
                        "name": "Akshay Mishra"
                    },
                    {
                        "name": "Reiner Pope"
                    }
                ],
                "author_detail": {
                    "name": "Reiner Pope"
                },
                "author": "Reiner Pope",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06416v1",
                "updated": "2025-04-08T20:32:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T20:32:10Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "title": "Unifying Autoregressive and Diffusion-Based Sequence Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Autoregressive and Diffusion-Based Sequence Generation"
                },
                "summary": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation."
                },
                "authors": [
                    {
                        "name": "Nima Fathi"
                    },
                    {
                        "name": "Torsten Scholak"
                    },
                    {
                        "name": "Pierre-André Noël"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-André Noël"
                },
                "author": "Pierre-André Noël",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.17692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.17692v2",
                "updated": "2025-04-08T19:26:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    19,
                    26,
                    41,
                    1,
                    98,
                    0
                ],
                "published": "2024-04-26T20:44:36Z",
                "published_parsed": [
                    2024,
                    4,
                    26,
                    20,
                    44,
                    36,
                    4,
                    117,
                    0
                ],
                "title": "Walking on Spheres and Talking to Neighbors: Variance Reduction for\n  Laplace's Equation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Walking on Spheres and Talking to Neighbors: Variance Reduction for\n  Laplace's Equation"
                },
                "summary": "Walk on Spheres algorithms leverage properties of Brownian Motion to create\nMonte Carlo estimates of solutions to a class of elliptic partial differential\nequations. We propose a new caching strategy which leverages the continuity of\npaths of Brownian Motion. In the case of Laplace's equation with Dirichlet\nboundary conditions, our algorithm has improved asymptotic runtime compared to\nprevious approaches. Until recently, estimates were constructed pointwise and\ndid not use the relationship between solutions at nearby points within a\ndomain. Instead, our results are achieved by passing information from a cache\nof fixed size. We also provide bounds on the performance of our algorithm and\ndemonstrate its performance on example problems of increasing complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Walk on Spheres algorithms leverage properties of Brownian Motion to create\nMonte Carlo estimates of solutions to a class of elliptic partial differential\nequations. We propose a new caching strategy which leverages the continuity of\npaths of Brownian Motion. In the case of Laplace's equation with Dirichlet\nboundary conditions, our algorithm has improved asymptotic runtime compared to\nprevious approaches. Until recently, estimates were constructed pointwise and\ndid not use the relationship between solutions at nearby points within a\ndomain. Instead, our results are achieved by passing information from a cache\nof fixed size. We also provide bounds on the performance of our algorithm and\ndemonstrate its performance on example problems of increasing complexity."
                },
                "authors": [
                    {
                        "name": "Michael Czekanski"
                    },
                    {
                        "name": "Benjamin Faber"
                    },
                    {
                        "name": "Margaret Fairborn"
                    },
                    {
                        "name": "Adelle Wright"
                    },
                    {
                        "name": "David Bindel"
                    }
                ],
                "author_detail": {
                    "name": "David Bindel"
                },
                "author": "David Bindel",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.17692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.17692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06067v1",
                "updated": "2025-04-08T14:09:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    9,
                    23,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T14:09:23Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    9,
                    23,
                    1,
                    98,
                    0
                ],
                "title": "GPU-accelerated Evolutionary Many-objective Optimization Using\n  Tensorized NSGA-III",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU-accelerated Evolutionary Many-objective Optimization Using\n  Tensorized NSGA-III"
                },
                "summary": "NSGA-III is one of the most widely adopted algorithms for tackling\nmany-objective optimization problems. However, its CPU-based design severely\nlimits scalability and computational efficiency. To address the limitations, we\npropose {TensorNSGA-III}, a fully tensorized implementation of NSGA-III that\nleverages GPU parallelism for large-scale many-objective optimization. Unlike\nconventional GPU-accelerated evolutionary algorithms that rely on heuristic\napproximations to improve efficiency, TensorNSGA-III maintains the exact\nselection and variation mechanisms of NSGA-III while achieving significant\nacceleration. By reformulating the selection process with tensorized data\nstructures and an optimized caching strategy, our approach effectively\neliminates computational bottlenecks inherent in traditional CPU-based and\nna\\\"ive GPU implementations. Experimental results on widely used numerical\nbenchmarks show that TensorNSGA-III achieves speedups of up to $3629\\times$\nover the CPU version of NSGA-III. Additionally, we validate its effectiveness\nin multiobjective robotic control tasks, where it discovers diverse and\nhigh-quality behavioral solutions. Furthermore, we investigate the critical\nrole of large population sizes in many-objective optimization and demonstrate\nthe scalability of TensorNSGA-III in such scenarios. The source code is\navailable at https://github.com/EMI-Group/evomo",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NSGA-III is one of the most widely adopted algorithms for tackling\nmany-objective optimization problems. However, its CPU-based design severely\nlimits scalability and computational efficiency. To address the limitations, we\npropose {TensorNSGA-III}, a fully tensorized implementation of NSGA-III that\nleverages GPU parallelism for large-scale many-objective optimization. Unlike\nconventional GPU-accelerated evolutionary algorithms that rely on heuristic\napproximations to improve efficiency, TensorNSGA-III maintains the exact\nselection and variation mechanisms of NSGA-III while achieving significant\nacceleration. By reformulating the selection process with tensorized data\nstructures and an optimized caching strategy, our approach effectively\neliminates computational bottlenecks inherent in traditional CPU-based and\nna\\\"ive GPU implementations. Experimental results on widely used numerical\nbenchmarks show that TensorNSGA-III achieves speedups of up to $3629\\times$\nover the CPU version of NSGA-III. Additionally, we validate its effectiveness\nin multiobjective robotic control tasks, where it discovers diverse and\nhigh-quality behavioral solutions. Furthermore, we investigate the critical\nrole of large population sizes in many-objective optimization and demonstrate\nthe scalability of TensorNSGA-III in such scenarios. The source code is\navailable at https://github.com/EMI-Group/evomo"
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Zhenyu Liang"
                    },
                    {
                        "name": "Ran Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Ran Cheng"
                },
                "author": "Ran Cheng",
                "arxiv_comment": "Accepted by IEEE CEC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v2",
                "updated": "2025-04-08T14:05:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    5,
                    12,
                    1,
                    98,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying KV Cache Compression for Large Language Models with LeanKV"
                },
                "summary": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04760v2",
                "updated": "2025-04-08T12:46:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    12,
                    46,
                    45,
                    1,
                    98,
                    0
                ],
                "published": "2025-02-07T08:48:06Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "title": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing"
                },
                "summary": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05897v1",
                "updated": "2025-04-08T10:47:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    47,
                    37,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T10:47:37Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    47,
                    37,
                    1,
                    98,
                    0
                ],
                "title": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference"
                },
                "summary": "The Mixture of Experts (MoE) architecture has demonstrated significant\nadvantages as it enables to increase the model capacity without a proportional\nincrease in computation. However, the large MoE model size still introduces\nsubstantial memory demands, which usually requires expert offloading on\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\ninference has been proposed to leverage CPU computation to reduce expert\nloading overhead but faces major challenges: on one hand, the expert activation\npatterns of MoE models are highly unstable, rendering the fixed mapping\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\nschedule for MoE is inherently complex due to the diverse expert sizes,\nstructures, uneven workload distribution, etc. To address these challenges, in\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\nimproves resource utilization through a novel CPU-GPU scheduling and cache\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\nmitigate expert activation instability. We implement HybriMoE on top of the\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\nExperimental results demonstrate that HybriMoE achieves an average speedup of\n1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\nhttps://github.com/PKU-SEC-Lab/HybriMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) architecture has demonstrated significant\nadvantages as it enables to increase the model capacity without a proportional\nincrease in computation. However, the large MoE model size still introduces\nsubstantial memory demands, which usually requires expert offloading on\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\ninference has been proposed to leverage CPU computation to reduce expert\nloading overhead but faces major challenges: on one hand, the expert activation\npatterns of MoE models are highly unstable, rendering the fixed mapping\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\nschedule for MoE is inherently complex due to the diverse expert sizes,\nstructures, uneven workload distribution, etc. To address these challenges, in\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\nimproves resource utilization through a novel CPU-GPU scheduling and cache\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\nmitigate expert activation instability. We implement HybriMoE on top of the\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\nExperimental results demonstrate that HybriMoE achieves an average speedup of\n1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\nhttps://github.com/PKU-SEC-Lab/HybriMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Yanfan Sun"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "Accepted by DAC 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06319v1",
                "updated": "2025-04-08T09:17:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    17,
                    35,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T09:17:35Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    17,
                    35,
                    1,
                    98,
                    0
                ],
                "title": "Accelerating LLM Inference Throughput via Asynchronous KV Cache\n  Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference Throughput via Asynchronous KV Cache\n  Prefetching"
                },
                "summary": "Large Language Models (LLMs) exhibit pronounced memory-bound characteristics\nduring inference due to High Bandwidth Memory (HBM) bandwidth constraints. In\nthis paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching\nmethod to break through the memory bandwidth bottleneck in LLM inference\nthrough computation-load overlap. By strategically scheduling idle memory\nbandwidth during active computation windows, our method proactively prefetches\nrequired KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for\nsubsequent accesses and effectively hiding HBM access latency within\ncomputational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that\nthe proposed method achieves 2.15x improvement in attention kernel efficiency\nand up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art\nbaseline FlashAttention-3. Notably, our solution maintains orthogonality to\nexisting optimization techniques and can be integrated with current inference\nframeworks, providing a scalable latency-hiding solution for next-generation\nLLM inference engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit pronounced memory-bound characteristics\nduring inference due to High Bandwidth Memory (HBM) bandwidth constraints. In\nthis paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching\nmethod to break through the memory bandwidth bottleneck in LLM inference\nthrough computation-load overlap. By strategically scheduling idle memory\nbandwidth during active computation windows, our method proactively prefetches\nrequired KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for\nsubsequent accesses and effectively hiding HBM access latency within\ncomputational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that\nthe proposed method achieves 2.15x improvement in attention kernel efficiency\nand up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art\nbaseline FlashAttention-3. Notably, our solution maintains orthogonality to\nexisting optimization techniques and can be integrated with current inference\nframeworks, providing a scalable latency-hiding solution for next-generation\nLLM inference engines."
                },
                "authors": [
                    {
                        "name": "Yanhao Dong"
                    },
                    {
                        "name": "Yubo Miao"
                    },
                    {
                        "name": "Weinan Li"
                    },
                    {
                        "name": "Xiao Zheng"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Feng Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Lyu"
                },
                "author": "Feng Lyu",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05807v1",
                "updated": "2025-04-08T08:40:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    40,
                    36,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T08:40:36Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    40,
                    36,
                    1,
                    98,
                    0
                ],
                "title": "Low-Complexity AoI-Optimal Status Update Control with Partial Battery\n  State Information in Energy Harvesting IoT Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Complexity AoI-Optimal Status Update Control with Partial Battery\n  State Information in Energy Harvesting IoT Networks"
                },
                "summary": "For a two-hop IoT system consisting of multiple energy harvesting sensors, a\ncache-enabled edge node, and multiple monitors, the status update control at\nthe edge node, which has partial battery state information (pBSI) of the\nsensors, is formulated as a pBSI problem. The concept of inferred pBSI is\nintroduced to reduce the noiseless single-sensor pBSI problem to a Markov\ndecision process with a moderate state-space size, enabling the optimal policy\nto be obtained through a value iteration algorithm. A lower bound on the\nexpected time-average on-demand age of information performance is established\nfor the general single-sensor status update problem. For the single-sensor pBSI\nproblem, a semi-closed-form policy called the current-next (CN) policy is\nproposed, along with an efficient post-update value iteration algorithm with a\nper-iteration time complexity proportional to the square of the battery\ncapacity. A weighted-update-gain-competition (WUGC) approach is further\nleveraged to extend the CN policy to the multi-sensor case. Numerical results\nin the single-sensor case demonstrate the near-optimal performance of the CN\npolicy across various energy arrival processes. Simulations for an IoT system\nwith $100$ sensors reveal that the WUGC-CN policy outperforms the\nmaximum-age-first policy and the random-scheduling-based CN policy under\nBernoulli energy arrival processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For a two-hop IoT system consisting of multiple energy harvesting sensors, a\ncache-enabled edge node, and multiple monitors, the status update control at\nthe edge node, which has partial battery state information (pBSI) of the\nsensors, is formulated as a pBSI problem. The concept of inferred pBSI is\nintroduced to reduce the noiseless single-sensor pBSI problem to a Markov\ndecision process with a moderate state-space size, enabling the optimal policy\nto be obtained through a value iteration algorithm. A lower bound on the\nexpected time-average on-demand age of information performance is established\nfor the general single-sensor status update problem. For the single-sensor pBSI\nproblem, a semi-closed-form policy called the current-next (CN) policy is\nproposed, along with an efficient post-update value iteration algorithm with a\nper-iteration time complexity proportional to the square of the battery\ncapacity. A weighted-update-gain-competition (WUGC) approach is further\nleveraged to extend the CN policy to the multi-sensor case. Numerical results\nin the single-sensor case demonstrate the near-optimal performance of the CN\npolicy across various energy arrival processes. Simulations for an IoT system\nwith $100$ sensors reveal that the WUGC-CN policy outperforms the\nmaximum-age-first policy and the random-scheduling-based CN policy under\nBernoulli energy arrival processes."
                },
                "authors": [
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Shengtian Yang"
                    },
                    {
                        "name": "Jun Chen"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Anding Wang"
                    }
                ],
                "author_detail": {
                    "name": "Anding Wang"
                },
                "author": "Anding Wang",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05718v1",
                "updated": "2025-04-08T06:38:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    38,
                    27,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T06:38:27Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    38,
                    27,
                    1,
                    98,
                    0
                ],
                "title": "CVA6-VMRT: A Modular Approach Towards Time-Predictable Virtual Memory in\n  a 64-bit Application Class RISC-V Processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CVA6-VMRT: A Modular Approach Towards Time-Predictable Virtual Memory in\n  a 64-bit Application Class RISC-V Processor"
                },
                "summary": "The increasing complexity of autonomous systems has driven a shift to\nintegrated heterogeneous SoCs with real-time and safety demands. Ensuring\ndeterministic WCETs and low-latency for critical tasks requires minimizing\ninterference on shared resources like virtual memory. Existing techniques, such\nas software coloring and memory replication, introduce significant area and\nperformance overhead, especially with virtualized memory where address\ntranslation adds latency uncertainty. To address these limitations, we propose\nCVA6-VMRT, an extension of the open-source RISC-V CVA6 core, adding hardware\nsupport for predictability in virtual memory access with minimal area overhead.\nCVA6-VMRT features dynamically partitioned Translation Look-aside Buffers\n(TLBs) and hybrid L1 cache/scratchpad memory (SPM) functionality. It allows\nfine-grained per-thread control of resources, enabling the operating system to\nmanage TLB replacements, including static overwrites, to ensure single-cycle\naddress translation for critical memory regions. Additionally, CVA6-VMRT\nenables runtime partitioning of data and instruction caches into cache and SPM\nsections, providing low and predictable access times for critical data without\nimpacting other accesses. In a virtualized setting, CVA6-VMRT enhances\nexecution time determinism for critical guests by 94% during interference from\nnon-critical guests, with minimal impact on their average absolute execution\ntime compared to isolated execution of the critical guests only. This\ninterference-aware behaviour is achieved with just a 4% area overhead and no\ntiming penalty compared to the baseline CVA6 core.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of autonomous systems has driven a shift to\nintegrated heterogeneous SoCs with real-time and safety demands. Ensuring\ndeterministic WCETs and low-latency for critical tasks requires minimizing\ninterference on shared resources like virtual memory. Existing techniques, such\nas software coloring and memory replication, introduce significant area and\nperformance overhead, especially with virtualized memory where address\ntranslation adds latency uncertainty. To address these limitations, we propose\nCVA6-VMRT, an extension of the open-source RISC-V CVA6 core, adding hardware\nsupport for predictability in virtual memory access with minimal area overhead.\nCVA6-VMRT features dynamically partitioned Translation Look-aside Buffers\n(TLBs) and hybrid L1 cache/scratchpad memory (SPM) functionality. It allows\nfine-grained per-thread control of resources, enabling the operating system to\nmanage TLB replacements, including static overwrites, to ensure single-cycle\naddress translation for critical memory regions. Additionally, CVA6-VMRT\nenables runtime partitioning of data and instruction caches into cache and SPM\nsections, providing low and predictable access times for critical data without\nimpacting other accesses. In a virtualized setting, CVA6-VMRT enhances\nexecution time determinism for critical guests by 94% during interference from\nnon-critical guests, with minimal impact on their average absolute execution\ntime compared to isolated execution of the critical guests only. This\ninterference-aware behaviour is achieved with just a 4% area overhead and no\ntiming penalty compared to the baseline CVA6 core."
                },
                "authors": [
                    {
                        "name": "Christopher Reinwardt"
                    },
                    {
                        "name": "Robert Balas"
                    },
                    {
                        "name": "Alessandro Ottaviano"
                    },
                    {
                        "name": "Angelo Garofalo"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "8 pages, 7 figures, accepted at the 22nd ACM International Conference\n  on Computing Frontiers 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22926v2",
                "updated": "2025-04-08T05:27:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    5,
                    27,
                    15,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-29T01:06:54Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    1,
                    6,
                    54,
                    5,
                    88,
                    0
                ],
                "title": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction"
                },
                "summary": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware."
                },
                "authors": [
                    {
                        "name": "Zikang Yuan"
                    },
                    {
                        "name": "Ruiye Ming"
                    },
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yonghao Tan"
                    },
                    {
                        "name": "Pingcheng Dong"
                    },
                    {
                        "name": "Hongcheng Luo"
                    },
                    {
                        "name": "Yuzhong Jiao"
                    },
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Kwang-Ting Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Kwang-Ting Cheng"
                },
                "author": "Kwang-Ting Cheng",
                "arxiv_comment": "10 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03661v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03661v2",
                "updated": "2025-04-08T04:34:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    4,
                    34,
                    44,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-12T13:32:50Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    32,
                    50,
                    2,
                    71,
                    0
                ],
                "title": "MILLION: Mastering Long-Context LLM Inference Via Outlier-Immunized KV\n  Product Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MILLION: Mastering Long-Context LLM Inference Via Outlier-Immunized KV\n  Product Quantization"
                },
                "summary": "Large language models (LLMs) are increasingly utilized for complex tasks\nrequiring longer context lengths, with some models supporting up to 128K or 1M\ntokens. This trend, however, presents significant challenges in inference speed\nand memory management. Quantization emerges as a promising approach to address\nthe widening gap between LLM size and memory capacity. However, traditional\nquantization schemes often yield suboptimal compression results for KV caches\ndue to two key factors: i) On-the-fly quantization and de-quantization, causing\nsignificant performance overhead; ii) Prevalence of outliers in KV values,\nchallenging low-bitwidth uniform quantization. To this end, we propose MILLION,\na novel quantization framework achieving low-bitwidth KV cache through product\nquantization. First, we conduct a thorough analysis of KV cache distribution,\nrevealing the limitations of existing quantization schemes. Second, we\nintroduce a non-uniform quantization algorithm based on product quantization,\nwhich efficiently compresses data while preserving accuracy. Third, we develop\na high-performance GPU inference framework with efficient attention kernel and\npipeline design for MILLION that leverages sparse computation and asynchronous\nquantization, significantly enhancing inference speed. Comprehensive evaluation\nresults demonstrate that MILLION can achieve 4 bits quantization with trivial\nperplexity and accuracy loss, and achieve 2.09x end-to-end performance gains at\n32K context length. Code is released at https://github.com/ZongwuWang/MILLION.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly utilized for complex tasks\nrequiring longer context lengths, with some models supporting up to 128K or 1M\ntokens. This trend, however, presents significant challenges in inference speed\nand memory management. Quantization emerges as a promising approach to address\nthe widening gap between LLM size and memory capacity. However, traditional\nquantization schemes often yield suboptimal compression results for KV caches\ndue to two key factors: i) On-the-fly quantization and de-quantization, causing\nsignificant performance overhead; ii) Prevalence of outliers in KV values,\nchallenging low-bitwidth uniform quantization. To this end, we propose MILLION,\na novel quantization framework achieving low-bitwidth KV cache through product\nquantization. First, we conduct a thorough analysis of KV cache distribution,\nrevealing the limitations of existing quantization schemes. Second, we\nintroduce a non-uniform quantization algorithm based on product quantization,\nwhich efficiently compresses data while preserving accuracy. Third, we develop\na high-performance GPU inference framework with efficient attention kernel and\npipeline design for MILLION that leverages sparse computation and asynchronous\nquantization, significantly enhancing inference speed. Comprehensive evaluation\nresults demonstrate that MILLION can achieve 4 bits quantization with trivial\nperplexity and accuracy loss, and achieve 2.09x end-to-end performance gains at\n32K context length. Code is released at https://github.com/ZongwuWang/MILLION."
                },
                "authors": [
                    {
                        "name": "Zongwu Wang"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Fangxin Liu"
                    },
                    {
                        "name": "Yiwei Hu"
                    },
                    {
                        "name": "Qingxiao Sun"
                    },
                    {
                        "name": "Gezi Li"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Li Jiang"
                    },
                    {
                        "name": "Haibing Guan"
                    }
                ],
                "author_detail": {
                    "name": "Haibing Guan"
                },
                "author": "Haibing Guan",
                "arxiv_comment": "7 pages, 7 figures and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03661v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03661v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05646v1",
                "updated": "2025-04-08T03:48:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    3,
                    48,
                    43,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T03:48:43Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    3,
                    48,
                    43,
                    1,
                    98,
                    0
                ],
                "title": "Lattice: Learning to Efficiently Compress the Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lattice: Learning to Efficiently Compress the Memory"
                },
                "summary": "Attention mechanisms have revolutionized sequence learning but suffer from\nquadratic computational complexity. This paper introduces Lattice, a novel\nrecurrent neural network (RNN) mechanism that leverages the inherent low-rank\nstructure of K-V matrices to efficiently compress the cache into a fixed number\nof memory slots, achieving sub-quadratic complexity. We formulate this\ncompression as an online optimization problem and derive a dynamic memory\nupdate rule based on a single gradient descent step. The resulting recurrence\nfeatures a state- and input-dependent gating mechanism, offering an\ninterpretable memory update process. The core innovation is the orthogonal\nupdate: each memory slot is updated exclusively with information orthogonal to\nits current state hence incorporation of only novel, non-redundant data, which\nminimizes the interference with previously stored information. The experimental\nresults show that Lattice achieves the best perplexity compared to all\nbaselines across diverse context lengths, with performance improvement becoming\nmore pronounced as the context length increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms have revolutionized sequence learning but suffer from\nquadratic computational complexity. This paper introduces Lattice, a novel\nrecurrent neural network (RNN) mechanism that leverages the inherent low-rank\nstructure of K-V matrices to efficiently compress the cache into a fixed number\nof memory slots, achieving sub-quadratic complexity. We formulate this\ncompression as an online optimization problem and derive a dynamic memory\nupdate rule based on a single gradient descent step. The resulting recurrence\nfeatures a state- and input-dependent gating mechanism, offering an\ninterpretable memory update process. The core innovation is the orthogonal\nupdate: each memory slot is updated exclusively with information orthogonal to\nits current state hence incorporation of only novel, non-redundant data, which\nminimizes the interference with previously stored information. The experimental\nresults show that Lattice achieves the best perplexity compared to all\nbaselines across diverse context lengths, with performance improvement becoming\nmore pronounced as the context length increases."
                },
                "authors": [
                    {
                        "name": "Mahdi Karami"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    }
                ],
                "author_detail": {
                    "name": "Vahab Mirrokni"
                },
                "author": "Vahab Mirrokni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02533v3",
                "updated": "2025-04-07T22:48:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    22,
                    48,
                    33,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-03T12:36:01Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    12,
                    36,
                    1,
                    3,
                    93,
                    0
                ],
                "title": "ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions"
                },
                "summary": "Modern data-driven applications expose limitations of von Neumann\narchitectures - extensive data movement, low throughput, and poor energy\nefficiency. Accelerators improve performance but lack flexibility and require\ndata transfers. Existing compute in- and near-memory solutions mitigate these\nissues but face usability challenges due to data placement constraints. We\npropose a novel cache architecture that doubles as a tightly-coupled\ncompute-near-memory coprocessor. Our RISC-V cache controller executes custom\ninstructions from the host CPU using vector operations dispatched to\nnear-memory vector processing units within the cache memory subsystem. This\narchitecture abstracts memory synchronization and data mapping from application\nsoftware while offering software-based Instruction Set Architecture\nextensibility. Our implementation shows $30\\times$ to $84\\times$ performance\nimprovement when operating on 8-bit data over the same system with a\ntraditional cache when executing a worst-case 32-bit CNN workload, with only\n$41.3\\%$ area overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern data-driven applications expose limitations of von Neumann\narchitectures - extensive data movement, low throughput, and poor energy\nefficiency. Accelerators improve performance but lack flexibility and require\ndata transfers. Existing compute in- and near-memory solutions mitigate these\nissues but face usability challenges due to data placement constraints. We\npropose a novel cache architecture that doubles as a tightly-coupled\ncompute-near-memory coprocessor. Our RISC-V cache controller executes custom\ninstructions from the host CPU using vector operations dispatched to\nnear-memory vector processing units within the cache memory subsystem. This\narchitecture abstracts memory synchronization and data mapping from application\nsoftware while offering software-based Instruction Set Architecture\nextensibility. Our implementation shows $30\\times$ to $84\\times$ performance\nimprovement when operating on 8-bit data over the same system with a\ntraditional cache when executing a worst-case 32-bit CNN workload, with only\n$41.3\\%$ area overhead."
                },
                "authors": [
                    {
                        "name": "Vincenzo Petrolo"
                    },
                    {
                        "name": "Flavia Guella"
                    },
                    {
                        "name": "Michele Caon"
                    },
                    {
                        "name": "Pasquale Davide Schiavone"
                    },
                    {
                        "name": "Guido Masera"
                    },
                    {
                        "name": "Maurizio Martina"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Martina"
                },
                "author": "Maurizio Martina",
                "arxiv_comment": "6 pages, 4 figures, accepted at the Design Automation Conference\n  (DAC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07467v2",
                "updated": "2025-04-07T20:52:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    20,
                    52,
                    4,
                    0,
                    97,
                    0
                ],
                "published": "2024-06-11T17:13:18Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    13,
                    18,
                    1,
                    163,
                    0
                ],
                "title": "LLM meets ML: Data-efficient Anomaly Detection on Unseen Unstable Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM meets ML: Data-efficient Anomaly Detection on Unseen Unstable Logs"
                },
                "summary": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for ULAD, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\nin F1 score while using 62.87 percentage points less labeled data. When trained\non the same amount of data as the baselines, FlexLog achieves up to a 13\npercentage points increase in F1 score on ADFA-U across varying training\ndataset sizes. Additionally, FlexLog maintains inference time under one second\nper log sequence, making it suitable for most applications except\nlatency-sensitive systems. Further analysis reveals the positive impact of\nFlexLog's key components: cache, RAG and ensemble learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for ULAD, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\nin F1 score while using 62.87 percentage points less labeled data. When trained\non the same amount of data as the baselines, FlexLog achieves up to a 13\npercentage points increase in F1 score on ADFA-U across varying training\ndataset sizes. Additionally, FlexLog maintains inference time under one second\nper log sequence, making it suitable for most applications except\nlatency-sensitive systems. Further analysis reveals the positive impact of\nFlexLog's key components: cache, RAG and ensemble learning."
                },
                "authors": [
                    {
                        "name": "Fatemeh Hadadi"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Domenico Bianculli"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05097v1",
                "updated": "2025-04-07T14:04:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    4,
                    30,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T14:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    4,
                    30,
                    0,
                    97,
                    0
                ],
                "title": "State Tuning: State-based Test-Time Scaling on RWKV-7",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Tuning: State-based Test-Time Scaling on RWKV-7"
                },
                "summary": "Test-time scaling has emerged as a prominent research direction in machine\nlearning, enabling models to enhance their expressive capabilities during\ninference.Transformers, renowned for striking a delicate balance between\nefficiency and expressiveness, have benefited from test-time scaling techniques\nthat leverage an expanding key-value (KV) cache to significantly improve\nperformance.In this paper, we introduce a novel state-based approach to\ntest-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7\nmodel.By exploiting the unique strengths of RWKV-7, our method achieves\nstate-of-the-art performance on the target task without altering the model's\npre-trained weights. Our approach centers on three key innovations. First, we\ndevelop an observer framework that allows a smaller model to replicate and\nlearn the state dynamics of the RWKV-7 model. Second, we employ a kernel method\nto dynamically upscale the state size, enhancing the model's capacity to\ncapture intricate patterns. Third, we integrate Decorrelated Backpropagation\n(DBP) to optimize the upscaled state matrix, thereby improving convergence and\nexpressivity. By tuning only the state matrix, we demonstrate that a smaller\nmodel can outperform larger models on the given task. This method preserves the\nefficiency of the original RWKV-7 architecture while harnessing the power of\ntest-time scaling to deliver superior results. Our findings underscore the\npotential of state tuning as an effective strategy for advancing model\nperformance in resource-constrained settings. Our code is\nhttps://github.com/TorchRWKV/flash-linear-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling has emerged as a prominent research direction in machine\nlearning, enabling models to enhance their expressive capabilities during\ninference.Transformers, renowned for striking a delicate balance between\nefficiency and expressiveness, have benefited from test-time scaling techniques\nthat leverage an expanding key-value (KV) cache to significantly improve\nperformance.In this paper, we introduce a novel state-based approach to\ntest-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7\nmodel.By exploiting the unique strengths of RWKV-7, our method achieves\nstate-of-the-art performance on the target task without altering the model's\npre-trained weights. Our approach centers on three key innovations. First, we\ndevelop an observer framework that allows a smaller model to replicate and\nlearn the state dynamics of the RWKV-7 model. Second, we employ a kernel method\nto dynamically upscale the state size, enhancing the model's capacity to\ncapture intricate patterns. Third, we integrate Decorrelated Backpropagation\n(DBP) to optimize the upscaled state matrix, thereby improving convergence and\nexpressivity. By tuning only the state matrix, we demonstrate that a smaller\nmodel can outperform larger models on the given task. This method preserves the\nefficiency of the original RWKV-7 architecture while harnessing the power of\ntest-time scaling to deliver superior results. Our findings underscore the\npotential of state tuning as an effective strategy for advancing model\nperformance in resource-constrained settings. Our code is\nhttps://github.com/TorchRWKV/flash-linear-attention."
                },
                "authors": [
                    {
                        "name": "Liu Xiao"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Lin Yueyu"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yueyu"
                },
                "author": "Lin Yueyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04823v1",
                "updated": "2025-04-07T08:22:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T08:22:45Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models"
                },
                "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models."
                },
                "authors": [
                    {
                        "name": "Ruikang Liu"
                    },
                    {
                        "name": "Yuxuan Sun"
                    },
                    {
                        "name": "Manyi Zhang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Tiezheng Yu"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Lu Hou"
                    }
                ],
                "author_detail": {
                    "name": "Lu Hou"
                },
                "author": "Lu Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00414v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00414v3",
                "updated": "2025-04-07T06:27:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    6,
                    27,
                    48,
                    0,
                    97,
                    0
                ],
                "published": "2024-10-01T05:46:22Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    5,
                    46,
                    22,
                    1,
                    275,
                    0
                ],
                "title": "Semantic Parsing with Candidate Expressions for Knowledge Base Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Parsing with Candidate Expressions for Knowledge Base Question\n  Answering"
                },
                "summary": "Semantic parsers convert natural language to logical forms, which can be\nevaluated on knowledge bases (KBs) to produce denotations. Recent semantic\nparsers have been developed with sequence-to-sequence (seq2seq) pre-trained\nlanguage models (PLMs) or large language models, where the models treat logical\nforms as sequences of tokens. For syntactic and semantic validity, the semantic\nparsers use grammars that enable constrained decoding. However, the grammars\nlack the ability to utilize large information of KBs, although logical forms\ncontain representations of KB elements, such as entities or relations. In this\nwork, we propose a grammar augmented with candidate expressions for semantic\nparsing on a large KB with a seq2seq PLM. The grammar defines actions as\nproduction rules, and our semantic parser predicts actions during inference\nunder the constraints by types and candidate expressions. We apply the grammar\nto knowledge base question answering, where the constraints by candidate\nexpressions assist a semantic parser to generate valid KB elements. We also\nintroduce two special rules, sub-type inference and union types, and a mask\ncaching algorithm. In particular, sub-type inference and the mask caching\nalgorithm greatly increase the decoding speed of our semantic parser. We\nexperimented on two benchmarks, KQA Pro and Overnight, where the constraints by\ncandidate expressions increased the accuracy of our semantic parser, whether it\nwas trained with strong supervision or weak supervision. In addition, our\nsemantic parser had a fast decoding speed in the experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic parsers convert natural language to logical forms, which can be\nevaluated on knowledge bases (KBs) to produce denotations. Recent semantic\nparsers have been developed with sequence-to-sequence (seq2seq) pre-trained\nlanguage models (PLMs) or large language models, where the models treat logical\nforms as sequences of tokens. For syntactic and semantic validity, the semantic\nparsers use grammars that enable constrained decoding. However, the grammars\nlack the ability to utilize large information of KBs, although logical forms\ncontain representations of KB elements, such as entities or relations. In this\nwork, we propose a grammar augmented with candidate expressions for semantic\nparsing on a large KB with a seq2seq PLM. The grammar defines actions as\nproduction rules, and our semantic parser predicts actions during inference\nunder the constraints by types and candidate expressions. We apply the grammar\nto knowledge base question answering, where the constraints by candidate\nexpressions assist a semantic parser to generate valid KB elements. We also\nintroduce two special rules, sub-type inference and union types, and a mask\ncaching algorithm. In particular, sub-type inference and the mask caching\nalgorithm greatly increase the decoding speed of our semantic parser. We\nexperimented on two benchmarks, KQA Pro and Overnight, where the constraints by\ncandidate expressions increased the accuracy of our semantic parser, whether it\nwas trained with strong supervision or weak supervision. In addition, our\nsemantic parser had a fast decoding speed in the experiments."
                },
                "authors": [
                    {
                        "name": "Daehwan Nam"
                    },
                    {
                        "name": "Gary Geunbae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Gary Geunbae Lee"
                },
                "author": "Gary Geunbae Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00414v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00414v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04704v1",
                "updated": "2025-04-07T03:22:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T03:22:15Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important"
                },
                "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "JiaMing Zhang"
                    },
                    {
                        "name": "Xiong Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23367v2",
                "updated": "2025-04-07T01:35:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    1,
                    35,
                    39,
                    0,
                    97,
                    0
                ],
                "published": "2025-03-30T08:51:19Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning"
                },
                "summary": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR."
                },
                "authors": [
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Jiangshan Wang"
                    },
                    {
                        "name": "Tao Dai"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10714v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10714v2",
                "updated": "2025-04-06T12:20:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    6,
                    12,
                    20,
                    25,
                    6,
                    96,
                    0
                ],
                "published": "2025-03-13T03:36:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs"
                },
                "summary": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Pei Liu"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10714v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10714v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04005v1",
                "updated": "2025-04-05T00:59:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    5,
                    0,
                    59,
                    52,
                    5,
                    95,
                    0
                ],
                "published": "2025-04-05T00:59:52Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    0,
                    59,
                    52,
                    5,
                    95,
                    0
                ],
                "title": "Learning Cache Coherence Traffic for NoC Routing Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Cache Coherence Traffic for NoC Routing Design"
                },
                "summary": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design."
                },
                "authors": [
                    {
                        "name": "Guochu Xiong"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Weichen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weichen Liu"
                },
                "author": "Weichen Liu",
                "arxiv_comment": "7 pages, 14 figures. Preprint version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03632v1",
                "updated": "2025-04-04T17:56:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    56,
                    44,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T17:56:44Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    56,
                    44,
                    4,
                    94,
                    0
                ],
                "title": "Performance Analysis of HPC applications on the Aurora Supercomputer:\n  Exploring the Impact of HBM-Enabled Intel Xeon Max CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Analysis of HPC applications on the Aurora Supercomputer:\n  Exploring the Impact of HBM-Enabled Intel Xeon Max CPUs"
                },
                "summary": "The Aurora supercomputer is an exascale-class system designed to tackle some\nof the most demanding computational workloads. Equipped with both High\nBandwidth Memory (HBM) and DDR memory, it provides unique trade-offs in\nperformance, latency, and capacity. This paper presents a comprehensive\nanalysis of the memory systems on the Aurora supercomputer, with a focus on\nevaluating the trade-offs between HBM and DDR memory systems. We explore how\ndifferent memory configurations, including memory modes (Flat and Cache) and\nclustering modes (Quad and SNC4), influence key system performance metrics such\nas memory bandwidth, latency, CPU-GPU PCIe bandwidth, and MPI communication\nbandwidth. Additionally, we examine the performance of three representative HPC\napplications -- HACC, QMCPACK, and BFS -- each illustrating the impact of\nmemory configurations on performance. By using microbenchmarks and\napplication-level analysis, we provide insights into how to select the optimal\nmemory system and configuration to maximize performance based on the\napplication characteristics. The findings presented in this paper offer\nguidance for users of the Aurora system and similar exascale systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Aurora supercomputer is an exascale-class system designed to tackle some\nof the most demanding computational workloads. Equipped with both High\nBandwidth Memory (HBM) and DDR memory, it provides unique trade-offs in\nperformance, latency, and capacity. This paper presents a comprehensive\nanalysis of the memory systems on the Aurora supercomputer, with a focus on\nevaluating the trade-offs between HBM and DDR memory systems. We explore how\ndifferent memory configurations, including memory modes (Flat and Cache) and\nclustering modes (Quad and SNC4), influence key system performance metrics such\nas memory bandwidth, latency, CPU-GPU PCIe bandwidth, and MPI communication\nbandwidth. Additionally, we examine the performance of three representative HPC\napplications -- HACC, QMCPACK, and BFS -- each illustrating the impact of\nmemory configurations on performance. By using microbenchmarks and\napplication-level analysis, we provide insights into how to select the optimal\nmemory system and configuration to maximize performance based on the\napplication characteristics. The findings presented in this paper offer\nguidance for users of the Aurora system and similar exascale systems."
                },
                "authors": [
                    {
                        "name": "Huda Ibeid"
                    },
                    {
                        "name": "Vikram Narayana"
                    },
                    {
                        "name": "Jeongnim Kim"
                    },
                    {
                        "name": "Anthony Nguyen"
                    },
                    {
                        "name": "Vitali Morozov"
                    },
                    {
                        "name": "Ye Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ye Luo"
                },
                "author": "Ye Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v2",
                "updated": "2025-04-04T16:51:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    51,
                    15,
                    4,
                    94,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "Adaptive Semantic Prompt Caching with VectorQ",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Semantic Prompt Caching with VectorQ"
                },
                "summary": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different embeddings.\nWe propose VectorQ, an online framework with a threshold convergence guarantee\nto learn embedding-specific threshold regions that adapt to the uncertainty of\nan embedding. Through evaluations on a combination of three diverse datasets,\nwe show that VectorQ consistently outperforms state-of-the-art systems across\nall static thresholds, achieving up to 26x increases in cache hit rate and\nerror rate reductions up to 74%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different embeddings.\nWe propose VectorQ, an online framework with a threshold convergence guarantee\nto learn embedding-specific threshold regions that adapt to the uncertainty of\nan embedding. Through evaluations on a combination of three diverse datasets,\nwe show that VectorQ consistently outperforms state-of-the-art systems across\nall static thresholds, achieving up to 26x increases in cache hit rate and\nerror rate reductions up to 74%."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.02073v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.02073v2",
                "updated": "2025-04-04T15:30:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    30,
                    20,
                    4,
                    94,
                    0
                ],
                "published": "2023-07-05T07:30:53Z",
                "published_parsed": [
                    2023,
                    7,
                    5,
                    7,
                    30,
                    53,
                    2,
                    186,
                    0
                ],
                "title": "Performance Modeling of Data Storage Systems using Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Modeling of Data Storage Systems using Generative Models"
                },
                "summary": "High-precision modeling of systems is one of the main areas of industrial\ndata analysis. Models of systems, their digital twins, are used to predict\ntheir behavior under various conditions. We have developed several models of a\nstorage system using machine learning-based generative models. The system\nconsists of several components: hard disk drive (HDD) and solid-state drive\n(SSD) storage pools with different RAID schemes and cache. Each storage\ncomponent is represented by a probabilistic model that describes the\nprobability distribution of the component performance in terms of IOPS and\nlatency, depending on their configuration and external data load parameters.\nThe results of the experiments demonstrate the errors of 4-10 % for IOPS and\n3-16 % for latency predictions depending on the components and models of the\nsystem. The predictions show up to 0.99 Pearson correlation with Little's law,\nwhich can be used for unsupervised reliability checks of the models. In\naddition, we present novel data sets that can be used for benchmarking\nregression algorithms, conditional generative models, and uncertainty\nestimation methods in machine learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-precision modeling of systems is one of the main areas of industrial\ndata analysis. Models of systems, their digital twins, are used to predict\ntheir behavior under various conditions. We have developed several models of a\nstorage system using machine learning-based generative models. The system\nconsists of several components: hard disk drive (HDD) and solid-state drive\n(SSD) storage pools with different RAID schemes and cache. Each storage\ncomponent is represented by a probabilistic model that describes the\nprobability distribution of the component performance in terms of IOPS and\nlatency, depending on their configuration and external data load parameters.\nThe results of the experiments demonstrate the errors of 4-10 % for IOPS and\n3-16 % for latency predictions depending on the components and models of the\nsystem. The predictions show up to 0.99 Pearson correlation with Little's law,\nwhich can be used for unsupervised reliability checks of the models. In\naddition, we present novel data sets that can be used for benchmarking\nregression algorithms, conditional generative models, and uncertainty\nestimation methods in machine learning."
                },
                "authors": [
                    {
                        "name": "Abdalaziz Rashid Al-Maeeni"
                    },
                    {
                        "name": "Aziz Temirkhanov"
                    },
                    {
                        "name": "Artem Ryzhikov"
                    },
                    {
                        "name": "Mikhail Hushchyn"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Hushchyn"
                },
                "author": "Mikhail Hushchyn",
                "arxiv_doi": "10.1109/ACCESS.2025.3552409",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2025.3552409",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2307.02073v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.02073v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Access 2025 ( Volume: 13) 49643 - 49658",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03499v1",
                "updated": "2025-04-04T14:55:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    55,
                    27,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T14:55:27Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    55,
                    27,
                    4,
                    94,
                    0
                ],
                "title": "Optimistic Learning for Communication Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimistic Learning for Communication Networks"
                },
                "summary": "AI/ML-based tools are at the forefront of resource management solutions for\ncommunication networks. Deep learning, in particular, is highly effective in\nfacilitating fast and high-performing decision-making whenever representative\ntraining data is available to build offline accurate models. Conversely, online\nlearning solutions do not require training and enable adaptive decisions based\non runtime observations, alas are often overly conservative. This extensive\ntutorial proposes the use of optimistic learning (OpL) as a decision engine for\nresource management frameworks in modern communication systems. When properly\ndesigned, such solutions can achieve fast and high-performing decisions --\ncomparable to offline-trained models -- while preserving the robustness and\nperformance guarantees of the respective online learning approaches. We\nintroduce the fundamental concepts, algorithms and results of OpL, discuss the\nroots of this theory and present different approaches to defining and achieving\noptimism. We proceed to showcase how OpL can enhance resource management in\ncommunication networks for several key problems such as caching, edge\ncomputing, network slicing, and workload assignment in decentralized O-RAN\nplatforms. Finally, we discuss the open challenges that must be addressed to\nunlock the full potential of this new resource management approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI/ML-based tools are at the forefront of resource management solutions for\ncommunication networks. Deep learning, in particular, is highly effective in\nfacilitating fast and high-performing decision-making whenever representative\ntraining data is available to build offline accurate models. Conversely, online\nlearning solutions do not require training and enable adaptive decisions based\non runtime observations, alas are often overly conservative. This extensive\ntutorial proposes the use of optimistic learning (OpL) as a decision engine for\nresource management frameworks in modern communication systems. When properly\ndesigned, such solutions can achieve fast and high-performing decisions --\ncomparable to offline-trained models -- while preserving the robustness and\nperformance guarantees of the respective online learning approaches. We\nintroduce the fundamental concepts, algorithms and results of OpL, discuss the\nroots of this theory and present different approaches to defining and achieving\noptimism. We proceed to showcase how OpL can enhance resource management in\ncommunication networks for several key problems such as caching, edge\ncomputing, network slicing, and workload assignment in decentralized O-RAN\nplatforms. Finally, we discuss the open challenges that must be addressed to\nunlock the full potential of this new resource management approach."
                },
                "authors": [
                    {
                        "name": "George Iosifidis"
                    },
                    {
                        "name": "Naram Mhaisen"
                    },
                    {
                        "name": "Douglas J. Leith"
                    }
                ],
                "author_detail": {
                    "name": "Douglas J. Leith"
                },
                "author": "Douglas J. Leith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10153v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10153v3",
                "updated": "2025-04-04T13:27:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    27,
                    49,
                    4,
                    94,
                    0
                ],
                "published": "2024-12-13T14:11:42Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector"
                },
                "summary": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Shuzhao Xie"
                    },
                    {
                        "name": "Chengwei Ren"
                    },
                    {
                        "name": "Siyi Xie"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shijia Ge"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10153v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10153v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03140v1",
                "updated": "2025-04-04T03:30:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T03:30:15Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "title": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models"
                },
                "summary": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation."
                },
                "authors": [
                    {
                        "name": "Xuran Ma"
                    },
                    {
                        "name": "Yexin Liu"
                    },
                    {
                        "name": "Yaofu Liu"
                    },
                    {
                        "name": "Xianfeng Wu"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Ser-Nam Lim"
                    },
                    {
                        "name": "Harry Yang"
                    }
                ],
                "author_detail": {
                    "name": "Harry Yang"
                },
                "author": "Harry Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16444v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16444v3",
                "updated": "2025-04-03T22:49:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    22,
                    49,
                    22,
                    3,
                    93,
                    0
                ],
                "published": "2024-05-26T06:00:17Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    6,
                    0,
                    17,
                    6,
                    147,
                    0
                ],
                "title": "CacheBlend: Fast Large Language Model Serving for RAG with Cached\n  Knowledge Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheBlend: Fast Large Language Model Serving for RAG with Cached\n  Knowledge Fusion"
                },
                "summary": "Large language models (LLMs) often incorporate multiple text chunks in their\ninputs to provide the necessary contexts. To speed up the prefill of the long\nLLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input. However, the\nreused text chunks are not always the input prefix, which makes precomputed KV\ncaches not directly usable since they ignore the text's cross-attention with\nthe preceding texts. Thus, the benefits of reusing KV caches remain largely\nunrealized.\n  This paper tackles just one challenge: when an LLM input contains multiple\ntext chunks, how to quickly combine their precomputed KV caches in order to\nachieve the same generation quality as the expensive full prefill (i.e.,\nwithout reusing KV cache)? This challenge naturally arises in\nretrieval-augmented generation (RAG) where the input is supplemented with\nmultiple retrieved texts as the context. We present CacheBlend, a scheme that\nreuses the precomputed KV caches, regardless prefix or not, and selectively\nrecomputes the KV values of a small subset of tokens to partially update each\nreused KV cache. In the meantime, the small extra delay for recomputing some\ntokens can be pipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices with more storage\ncapacity while retrieving them without increasing the inference delay. By\ncomparing CacheBlend with the state-of-the-art KV cache reusing schemes on\nthree open-source LLMs of various sizes and four popular benchmark datasets of\ndifferent tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by\n2.2-3.3x and increases the inference throughput by 2.8-5x from full KV\nrecompute without compromising generation quality. The code is available at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often incorporate multiple text chunks in their\ninputs to provide the necessary contexts. To speed up the prefill of the long\nLLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input. However, the\nreused text chunks are not always the input prefix, which makes precomputed KV\ncaches not directly usable since they ignore the text's cross-attention with\nthe preceding texts. Thus, the benefits of reusing KV caches remain largely\nunrealized.\n  This paper tackles just one challenge: when an LLM input contains multiple\ntext chunks, how to quickly combine their precomputed KV caches in order to\nachieve the same generation quality as the expensive full prefill (i.e.,\nwithout reusing KV cache)? This challenge naturally arises in\nretrieval-augmented generation (RAG) where the input is supplemented with\nmultiple retrieved texts as the context. We present CacheBlend, a scheme that\nreuses the precomputed KV caches, regardless prefix or not, and selectively\nrecomputes the KV values of a small subset of tokens to partially update each\nreused KV cache. In the meantime, the small extra delay for recomputing some\ntokens can be pipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices with more storage\ncapacity while retrieving them without increasing the inference delay. By\ncomparing CacheBlend with the state-of-the-art KV cache reusing schemes on\nthree open-source LLMs of various sizes and four popular benchmark datasets of\ndifferent tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by\n2.2-3.3x and increases the inference throughput by 2.8-5x from full KV\nrecompute without compromising generation quality. The code is available at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16444v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16444v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03048v1",
                "updated": "2025-04-03T21:53:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    53,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T21:53:51Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    53,
                    51,
                    3,
                    93,
                    0
                ],
                "title": "LLM Library Learning Fails: A LEGO-Prover Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Library Learning Fails: A LEGO-Prover Case Study"
                },
                "summary": "Recent advancements in the coding, reasoning, and tool-using abilities of\nLLMs have spurred interest in library learning (i.e., online learning through\nthe creation, storage, and retrieval of reusable and composable functions,\nknowledge, checklists, or lemmas). Such systems often promise improved task\nperformance through the automatic creation of broadly applicable tools, as well\nas superior computational performance through the caching of reasoning (i.e.,\nthe storage of generated tools). However, we find strong reason to be\nskeptical. We perform a deep dive into one such system, LEGO-Prover, which\npurports to learn reusable lemmas for mathematical reasoning. We find no\nevidence of the direct reuse of learned lemmas, and find evidence against the\nsoft reuse of learned lemmas (i.e., reuse by modifying relevant examples).\nCrucially, we find that LEGO-Prover does not in fact improve over the simple\nbaseline of prompting the model - the improvements in task accuracy vanish once\ncomputational cost is accounted for. Our findings suggest that serious\nmisconceptions exist as to the effectiveness of these techniques, that a\nserious re-examination of the state of LLM-based library learning is required,\nand that we require much stronger standards for evaluation including\nbehavioural analysis and ensuring that an equal computational budget is used\nfor baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in the coding, reasoning, and tool-using abilities of\nLLMs have spurred interest in library learning (i.e., online learning through\nthe creation, storage, and retrieval of reusable and composable functions,\nknowledge, checklists, or lemmas). Such systems often promise improved task\nperformance through the automatic creation of broadly applicable tools, as well\nas superior computational performance through the caching of reasoning (i.e.,\nthe storage of generated tools). However, we find strong reason to be\nskeptical. We perform a deep dive into one such system, LEGO-Prover, which\npurports to learn reusable lemmas for mathematical reasoning. We find no\nevidence of the direct reuse of learned lemmas, and find evidence against the\nsoft reuse of learned lemmas (i.e., reuse by modifying relevant examples).\nCrucially, we find that LEGO-Prover does not in fact improve over the simple\nbaseline of prompting the model - the improvements in task accuracy vanish once\ncomputational cost is accounted for. Our findings suggest that serious\nmisconceptions exist as to the effectiveness of these techniques, that a\nserious re-examination of the state of LLM-based library learning is required,\nand that we require much stronger standards for evaluation including\nbehavioural analysis and ensuring that an equal computational budget is used\nfor baselines."
                },
                "authors": [
                    {
                        "name": "Ian Berlot-Attwell"
                    },
                    {
                        "name": "Frank Rudzicz"
                    },
                    {
                        "name": "Xujie Si"
                    }
                ],
                "author_detail": {
                    "name": "Xujie Si"
                },
                "author": "Xujie Si",
                "arxiv_comment": "24 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02976v1",
                "updated": "2025-04-03T18:54:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    54,
                    50,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T18:54:50Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    54,
                    50,
                    3,
                    93,
                    0
                ],
                "title": "Localized Definitions and Distributed Reasoning: A Proof-of-Concept\n  Mechanistic Interpretability Study via Activation Patching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localized Definitions and Distributed Reasoning: A Proof-of-Concept\n  Mechanistic Interpretability Study via Activation Patching"
                },
                "summary": "This study investigates the localization of knowledge representation in\nfine-tuned GPT-2 models using Causal Layer Attribution via Activation Patching\n(CLAP), a method that identifies critical neural layers responsible for correct\nanswer generation. The model was fine-tuned on 9,958 PubMed abstracts\n(epilepsy: 20,595 mentions, EEG: 11,674 mentions, seizure: 13,921 mentions)\nusing two configurations with validation loss monitoring for early stopping.\nCLAP involved (1) caching clean (correct answer) and corrupted (incorrect\nanswer) activations, (2) computing logit difference to quantify model\npreference, and (3) patching corrupted activations with clean ones to assess\nrecovery. Results revealed three findings: First, patching the first\nfeedforward layer recovered 56% of correct preference, demonstrating that\nassociative knowledge is distributed across multiple layers. Second, patching\nthe final output layer completely restored accuracy (100% recovery), indicating\nthat definitional knowledge is localised. The stronger clean logit difference\nfor definitional questions further supports this localized representation.\nThird, minimal recovery from convolutional layer patching (13.6%) suggests\nlow-level features contribute marginally to high-level reasoning. Statistical\nanalysis confirmed significant layer-specific effects (p<0.01). These findings\ndemonstrate that factual knowledge is more localized and associative knowledge\ndepends on distributed representations. We also showed that editing efficacy\ndepends on task type. Our findings not only reconcile conflicting observations\nabout localization in model editing but also emphasize on using task-adaptive\ntechniques for reliable, interpretable updates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the localization of knowledge representation in\nfine-tuned GPT-2 models using Causal Layer Attribution via Activation Patching\n(CLAP), a method that identifies critical neural layers responsible for correct\nanswer generation. The model was fine-tuned on 9,958 PubMed abstracts\n(epilepsy: 20,595 mentions, EEG: 11,674 mentions, seizure: 13,921 mentions)\nusing two configurations with validation loss monitoring for early stopping.\nCLAP involved (1) caching clean (correct answer) and corrupted (incorrect\nanswer) activations, (2) computing logit difference to quantify model\npreference, and (3) patching corrupted activations with clean ones to assess\nrecovery. Results revealed three findings: First, patching the first\nfeedforward layer recovered 56% of correct preference, demonstrating that\nassociative knowledge is distributed across multiple layers. Second, patching\nthe final output layer completely restored accuracy (100% recovery), indicating\nthat definitional knowledge is localised. The stronger clean logit difference\nfor definitional questions further supports this localized representation.\nThird, minimal recovery from convolutional layer patching (13.6%) suggests\nlow-level features contribute marginally to high-level reasoning. Statistical\nanalysis confirmed significant layer-specific effects (p<0.01). These findings\ndemonstrate that factual knowledge is more localized and associative knowledge\ndepends on distributed representations. We also showed that editing efficacy\ndepends on task type. Our findings not only reconcile conflicting observations\nabout localization in model editing but also emphasize on using task-adaptive\ntechniques for reliable, interpretable updates."
                },
                "authors": [
                    {
                        "name": "Nooshin Bahador"
                    }
                ],
                "author_detail": {
                    "name": "Nooshin Bahador"
                },
                "author": "Nooshin Bahador",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02972v1",
                "updated": "2025-04-03T18:47:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    47,
                    26,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T18:47:26Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    47,
                    26,
                    3,
                    93,
                    0
                ],
                "title": "Improved Compact Genetic Algorithms with Efficient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Compact Genetic Algorithms with Efficient Caching"
                },
                "summary": "Compact Genetic Algorithms (cGAs) are condensed variants of classical Genetic\nAlgorithms (GAs) that use a probability vector representation of the population\ninstead of the complete population. cGAs have been shown to significantly\nreduce the number of function evaluations required while producing outcomes\nsimilar to those of classical GAs. However, cGAs have a tendency to repeatedly\ngenerate the same chromosomes as they approach convergence, resulting in\nunnecessary evaluations of identical chromosomes. This article introduces the\nconcept of caching in cGAs as a means of avoiding redundant evaluations of the\nsame chromosomes. Our proposed approach operates equivalently to cGAs, but\nenhances the algorithm's time efficiency by reducing the number of function\nevaluations. We also present a data structure for efficient cache maintenance\nto ensure low overhead. The proposed caching approach has an asymptotically\nconstant time complexity on average. The proposed method further generalizes\nthe caching mechanism with higher selection pressure for elitism-based cGAs. We\nconduct a rigorous analysis based on experiments on benchmark optimization\nproblems using two well-known cache replacement strategies. The results\ndemonstrate that caching significantly reduces the number of function\nevaluations required while maintaining the same level of performance accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compact Genetic Algorithms (cGAs) are condensed variants of classical Genetic\nAlgorithms (GAs) that use a probability vector representation of the population\ninstead of the complete population. cGAs have been shown to significantly\nreduce the number of function evaluations required while producing outcomes\nsimilar to those of classical GAs. However, cGAs have a tendency to repeatedly\ngenerate the same chromosomes as they approach convergence, resulting in\nunnecessary evaluations of identical chromosomes. This article introduces the\nconcept of caching in cGAs as a means of avoiding redundant evaluations of the\nsame chromosomes. Our proposed approach operates equivalently to cGAs, but\nenhances the algorithm's time efficiency by reducing the number of function\nevaluations. We also present a data structure for efficient cache maintenance\nto ensure low overhead. The proposed caching approach has an asymptotically\nconstant time complexity on average. The proposed method further generalizes\nthe caching mechanism with higher selection pressure for elitism-based cGAs. We\nconduct a rigorous analysis based on experiments on benchmark optimization\nproblems using two well-known cache replacement strategies. The results\ndemonstrate that caching significantly reduces the number of function\nevaluations required while maintaining the same level of performance accuracy."
                },
                "authors": [
                    {
                        "name": "Prasanta Dutta"
                    },
                    {
                        "name": "Anirban Mukhopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Anirban Mukhopadhyay"
                },
                "author": "Anirban Mukhopadhyay",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02921v1",
                "updated": "2025-04-03T17:08:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    8,
                    42,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T17:08:42Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    8,
                    42,
                    3,
                    93,
                    0
                ],
                "title": "HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented\n  Generation with Reranker KV-Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented\n  Generation with Reranker KV-Cache Reuse"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing the performance of large language models (LLMs) by integrating\nexternal knowledge into the generation process. A key component of RAG\npipelines is the reranker, which selects the most relevant documents from a\npool of retrieved candidates and significantly improves the quality of the\ngenerated responses. While rerankers refine the selection of retrieved\ndocuments in RAG pipelines, they introduce computational challenges that hinder\nhigh throughput and low latency. To address this problem, we propose HyperRAG,\na system that optimizes the trade-off between quality and efficiency in RAG\npipelines by leveraging KV-cache reuse for efficient reranker inference. By\nreusing document-side KV-cache, HyperRAG achieves both high-quality generation\nand system-level efficiency. To fully realize the benefits of KV-cache reuse,\nHyperRAG incorporates a range of system-level optimizations designed to enhance\nefficiency and scalability. Experiments show that HyperRAG achieves a 2 - 3\nthroughput improvement with decoder-only rerankers while also delivering higher\ndownstream performance compared with traditional RAG service.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing the performance of large language models (LLMs) by integrating\nexternal knowledge into the generation process. A key component of RAG\npipelines is the reranker, which selects the most relevant documents from a\npool of retrieved candidates and significantly improves the quality of the\ngenerated responses. While rerankers refine the selection of retrieved\ndocuments in RAG pipelines, they introduce computational challenges that hinder\nhigh throughput and low latency. To address this problem, we propose HyperRAG,\na system that optimizes the trade-off between quality and efficiency in RAG\npipelines by leveraging KV-cache reuse for efficient reranker inference. By\nreusing document-side KV-cache, HyperRAG achieves both high-quality generation\nand system-level efficiency. To fully realize the benefits of KV-cache reuse,\nHyperRAG incorporates a range of system-level optimizations designed to enhance\nefficiency and scalability. Experiments show that HyperRAG achieves a 2 - 3\nthroughput improvement with decoder-only rerankers while also delivering higher\ndownstream performance compared with traditional RAG service."
                },
                "authors": [
                    {
                        "name": "Yuwei An"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Seo Jin Park"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01380v2",
                "updated": "2025-04-03T13:28:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    28,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2024-12-02T11:07:51Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "title": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking"
                },
                "summary": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which results in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46\\% reduction in memory and 40\\% increase in throughput with $<$\n0.1 loss in perplexity when compared to streaming the dense model from Flash.\nThe open source code for HW simulator, methods, and experiments in this paper\nis available at https://github.com/Qualcomm-AI-research/dynamic-sparsity .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which results in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46\\% reduction in memory and 40\\% increase in throughput with $<$\n0.1 loss in perplexity when compared to streaming the dense model from Flash.\nThe open source code for HW simulator, methods, and experiments in this paper\nis available at https://github.com/Qualcomm-AI-research/dynamic-sparsity ."
                },
                "authors": [
                    {
                        "name": "Marco Federici"
                    },
                    {
                        "name": "Davide Belli"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Amir Jalalirad"
                    },
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Bence Major"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    }
                ],
                "author_detail": {
                    "name": "Paul Whatmough"
                },
                "author": "Paul Whatmough",
                "arxiv_comment": "Main Text: 10 pages, 11 figures. Appendix: 6 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02441v1",
                "updated": "2025-04-03T09:58:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T09:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "title": "Cognitive Memory in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Memory in Large Language Models"
                },
                "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions."
                },
                "authors": [
                    {
                        "name": "Lianlei Shan"
                    },
                    {
                        "name": "Shixian Luo"
                    },
                    {
                        "name": "Zezhou Zhu"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Yong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wu"
                },
                "author": "Yong Wu",
                "arxiv_comment": "37 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03775v1",
                "updated": "2025-04-03T08:58:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    58,
                    5,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T08:58:05Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    58,
                    5,
                    3,
                    93,
                    0
                ],
                "title": "FlowKV: A Disaggregated Inference Framework with Low-Latency KV Cache\n  Transfer and Load-Aware Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowKV: A Disaggregated Inference Framework with Low-Latency KV Cache\n  Transfer and Load-Aware Scheduling"
                },
                "summary": "Disaggregated inference has become an essential framework that separates the\nprefill (P) and decode (D) stages in large language model inference to improve\nthroughput. However, the KV cache transfer faces significant delays between\nprefill and decode nodes. The block-wise calling method and discontinuous KV\ncache memory allocation increase the number of calls to the transmission\nkernel. Additionally, existing frameworks often fix the roles of P and D nodes,\nleading to computational imbalances. In this paper, we propose FlowKV, a novel\ndisaggregated inference framework, which reduces the average transmission\nlatency of KV cache by 96%, from 0.944s to 0.053s, almost eliminating the\ntransfer time relative to the total request latency by optimizing the KV cache\ntransfer. FlowKV introduces the Load-Aware Scheduler for balanced request\nscheduling and flexible PD node allocation. This design maximizes hardware\nresource utilization, achieving peak system throughput across various\nscenarios, including normal, computational imbalance, and extreme overload\nconditions. Experimental results demonstrate that FlowKV significantly\naccelerates inference by 15.2%-48.9% on LongBench dataset compared to the\nbaseline and supports applications with heterogeneous GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated inference has become an essential framework that separates the\nprefill (P) and decode (D) stages in large language model inference to improve\nthroughput. However, the KV cache transfer faces significant delays between\nprefill and decode nodes. The block-wise calling method and discontinuous KV\ncache memory allocation increase the number of calls to the transmission\nkernel. Additionally, existing frameworks often fix the roles of P and D nodes,\nleading to computational imbalances. In this paper, we propose FlowKV, a novel\ndisaggregated inference framework, which reduces the average transmission\nlatency of KV cache by 96%, from 0.944s to 0.053s, almost eliminating the\ntransfer time relative to the total request latency by optimizing the KV cache\ntransfer. FlowKV introduces the Load-Aware Scheduler for balanced request\nscheduling and flexible PD node allocation. This design maximizes hardware\nresource utilization, achieving peak system throughput across various\nscenarios, including normal, computational imbalance, and extreme overload\nconditions. Experimental results demonstrate that FlowKV significantly\naccelerates inference by 15.2%-48.9% on LongBench dataset compared to the\nbaseline and supports applications with heterogeneous GPUs."
                },
                "authors": [
                    {
                        "name": "Weiqing Li"
                    },
                    {
                        "name": "Guochao Jiang"
                    },
                    {
                        "name": "Xiangyong Ding"
                    },
                    {
                        "name": "Zhangcheng Tao"
                    },
                    {
                        "name": "Chuzhan Hao"
                    },
                    {
                        "name": "Chenfeng Xu"
                    },
                    {
                        "name": "Yuewei Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02268v1",
                "updated": "2025-04-03T04:27:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    27,
                    2,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T04:27:02Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    27,
                    2,
                    3,
                    93,
                    0
                ],
                "title": "Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and\n  Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and\n  Synthetic Data"
                },
                "summary": "This report investigates enhancing semantic caching effectiveness by\nemploying specialized, fine-tuned embedding models. Semantic caching relies on\nembedding similarity rather than exact key matching, presenting unique\nchallenges in balancing precision, query latency, and computational efficiency.\nWe propose leveraging smaller, domain-specific embedding models, fine-tuned\nwith targeted real-world and synthetically generated datasets. Our empirical\nevaluations demonstrate that compact embedding models fine-tuned for just one\nepoch on specialized datasets significantly surpass both state-of-the-art\nopen-source and proprietary alternatives in precision and recall. Moreover, we\nintroduce a novel synthetic data generation pipeline for the semantic cache\nthat mitigates the challenge of limited domain-specific annotated data, further\nboosting embedding performance. Our approach effectively balances computational\noverhead and accuracy, establishing a viable and efficient strategy for\npractical semantic caching implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report investigates enhancing semantic caching effectiveness by\nemploying specialized, fine-tuned embedding models. Semantic caching relies on\nembedding similarity rather than exact key matching, presenting unique\nchallenges in balancing precision, query latency, and computational efficiency.\nWe propose leveraging smaller, domain-specific embedding models, fine-tuned\nwith targeted real-world and synthetically generated datasets. Our empirical\nevaluations demonstrate that compact embedding models fine-tuned for just one\nepoch on specialized datasets significantly surpass both state-of-the-art\nopen-source and proprietary alternatives in precision and recall. Moreover, we\nintroduce a novel synthetic data generation pipeline for the semantic cache\nthat mitigates the challenge of limited domain-specific annotated data, further\nboosting embedding performance. Our approach effectively balances computational\noverhead and accuracy, establishing a viable and efficient strategy for\npractical semantic caching implementations."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Justin Cechmanek"
                    },
                    {
                        "name": "Tyler Hutcherson"
                    },
                    {
                        "name": "Srijith Rajamohan"
                    },
                    {
                        "name": "Jen Agarwal"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    },
                    {
                        "name": "Manvinder Singh"
                    },
                    {
                        "name": "Benoit Dion"
                    }
                ],
                "author_detail": {
                    "name": "Benoit Dion"
                },
                "arxiv_affiliation": "Redis",
                "author": "Benoit Dion",
                "arxiv_comment": "Initial study on embedding fine tuning for semantic cache. It also\n  explores synthetic data. Total pages are 12, including refrences",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02220v1",
                "updated": "2025-04-03T02:24:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    2,
                    24,
                    21,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T02:24:21Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    2,
                    24,
                    21,
                    3,
                    93,
                    0
                ],
                "title": "Comparative Analysis of Distributed Caching Algorithms: Performance\n  Metrics and Implementation Considerations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of Distributed Caching Algorithms: Performance\n  Metrics and Implementation Considerations"
                },
                "summary": "This paper presents a comprehensive comparison of distributed caching\nalgorithms employed in modern distributed systems. We evaluate various caching\nstrategies including Least Recently Used (LRU), Least Frequently Used (LFU),\nAdaptive Replacement Cache (ARC), and Time-Aware Least Recently Used (TLRU)\nagainst metrics such as hit ratio, latency reduction, memory overhead, and\nscalability. Our analysis reveals that while traditional algorithms like LRU\nremain prevalent, hybrid approaches incorporating machine learning techniques\ndemonstrate superior performance in dynamic environments. Additionally, we\nanalyze implementation patterns across different distributed architectures and\nprovide recommendations for algorithm selection based on specific workload\ncharacteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive comparison of distributed caching\nalgorithms employed in modern distributed systems. We evaluate various caching\nstrategies including Least Recently Used (LRU), Least Frequently Used (LFU),\nAdaptive Replacement Cache (ARC), and Time-Aware Least Recently Used (TLRU)\nagainst metrics such as hit ratio, latency reduction, memory overhead, and\nscalability. Our analysis reveals that while traditional algorithms like LRU\nremain prevalent, hybrid approaches incorporating machine learning techniques\ndemonstrate superior performance in dynamic environments. Additionally, we\nanalyze implementation patterns across different distributed architectures and\nprovide recommendations for algorithm selection based on specific workload\ncharacteristics."
                },
                "authors": [
                    {
                        "name": "Helen Mayer"
                    },
                    {
                        "name": "James Richards"
                    }
                ],
                "author_detail": {
                    "name": "James Richards"
                },
                "author": "James Richards",
                "arxiv_comment": "International Conference on Computing Technologies and Artificial\n  Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01281v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01281v2",
                "updated": "2025-04-03T01:23:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    1,
                    23,
                    22,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-02T01:16:10Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    16,
                    10,
                    2,
                    92,
                    0
                ],
                "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding"
                },
                "summary": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01281v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01281v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22875v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22875v2",
                "updated": "2025-04-02T18:51:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    18,
                    51,
                    53,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-28T21:02:32Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    21,
                    2,
                    32,
                    4,
                    87,
                    0
                ],
                "title": "A Pilot Study on Tunable Precision Emulation via Automatic BLAS\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Pilot Study on Tunable Precision Emulation via Automatic BLAS\n  Offloading"
                },
                "summary": "This study explores the use of automatic BLAS offloading and INT8-based\nemulation for accelerating traditional HPC workloads on modern GPU\narchitectures. Through the use of low-bitwidth integer units and cache-coherent\nUnified Memory Architecture, we emulate double-precision matrix multiplications\nin the MuST application without code changes. We find that accuracy depends on\nboth arithmetic precision and the properties of the operator, which can be\ndealt with through tunable precision emulation. Unlike traditional\nmixed-precision approaches, this method preserves original algorithms while\noptimizing hardware utilization. We showcases the potential of improving\naccuracy and performance at the same time. This work highlights the potential\nof AI-driven hardware to transform HPC, advocating for adaptive precision\nstrategies in future scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the use of automatic BLAS offloading and INT8-based\nemulation for accelerating traditional HPC workloads on modern GPU\narchitectures. Through the use of low-bitwidth integer units and cache-coherent\nUnified Memory Architecture, we emulate double-precision matrix multiplications\nin the MuST application without code changes. We find that accuracy depends on\nboth arithmetic precision and the properties of the operator, which can be\ndealt with through tunable precision emulation. Unlike traditional\nmixed-precision approaches, this method preserves original algorithms while\noptimizing hardware utilization. We showcases the potential of improving\naccuracy and performance at the same time. This work highlights the potential\nof AI-driven hardware to transform HPC, advocating for adaptive precision\nstrategies in future scientific computing."
                },
                "authors": [
                    {
                        "name": "Hang Liu"
                    },
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Yinzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yinzhi Wang"
                },
                "author": "Yinzhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22875v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22875v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01582v1",
                "updated": "2025-04-02T10:38:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    38,
                    25,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T10:38:25Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    38,
                    25,
                    2,
                    92,
                    0
                ],
                "title": "MERE: Hardware-Software Co-Design for Masking Cache Miss Latency in\n  Embedded Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MERE: Hardware-Software Co-Design for Masking Cache Miss Latency in\n  Embedded Processors"
                },
                "summary": "Runahead execution is a technique to mask memory latency caused by irregular\nmemory accesses. By pre-executing the application code during occurrences of\nlong-latency operations and prefetching anticipated cache-missed data into the\ncache hierarchy, runahead effectively masks memory latency for subsequent cache\nmisses and achieves high prefetching accuracy; however, this technique has been\nlimited to superscalar out-of-order and superscalar in-order cores. For\nimplementation in scalar in-order cores, the challenges of\narea-/energy-constraint and severe cache contention remain.\n  Here, we build the first full-stack system featuring runahead, MERE, from SoC\nand a dedicated ISA to the OS and programming model. Through this deployment,\nwe show that enabling runahead in scalar in-order cores is possible, with\nminimal area and power overheads, while still achieving high performance. By\nre-constructing the sequential runahead employing a hardware/software co-design\napproach, the system can be implemented on a mature processor and SoC. Building\non this, an adaptive runahead mechanism is proposed to mitigate the severe\ncache contention in scalar in-order cores. Combining this, we provide a\ncomprehensive solution for embedded processors managing irregular workloads.\nOur evaluation demonstrates that the proposed MERE attains 93.5% of a 2-wide\nout-of-order core's performance while constraining area and power overheads\nbelow 5%, with the adaptive runahead mechanism delivering an additional 20.1%\nperformance gain through mitigating the severe cache contention issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Runahead execution is a technique to mask memory latency caused by irregular\nmemory accesses. By pre-executing the application code during occurrences of\nlong-latency operations and prefetching anticipated cache-missed data into the\ncache hierarchy, runahead effectively masks memory latency for subsequent cache\nmisses and achieves high prefetching accuracy; however, this technique has been\nlimited to superscalar out-of-order and superscalar in-order cores. For\nimplementation in scalar in-order cores, the challenges of\narea-/energy-constraint and severe cache contention remain.\n  Here, we build the first full-stack system featuring runahead, MERE, from SoC\nand a dedicated ISA to the OS and programming model. Through this deployment,\nwe show that enabling runahead in scalar in-order cores is possible, with\nminimal area and power overheads, while still achieving high performance. By\nre-constructing the sequential runahead employing a hardware/software co-design\napproach, the system can be implemented on a mature processor and SoC. Building\non this, an adaptive runahead mechanism is proposed to mitigate the severe\ncache contention in scalar in-order cores. Combining this, we provide a\ncomprehensive solution for embedded processors managing irregular workloads.\nOur evaluation demonstrates that the proposed MERE attains 93.5% of a 2-wide\nout-of-order core's performance while constraining area and power overheads\nbelow 5%, with the adaptive runahead mechanism delivering an additional 20.1%\nperformance gain through mitigating the severe cache contention issues."
                },
                "authors": [
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Xiaoxuan Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Zhihang Tan"
                    },
                    {
                        "name": "Wenbo Xu"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Zhenyuan Wang"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v4",
                "updated": "2025-04-02T04:57:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    4,
                    57,
                    15,
                    2,
                    92,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "Hexa-MoE: Efficient and Heterogeneous-aware Training for\n  Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hexa-MoE: Efficient and Heterogeneous-aware Training for\n  Mixture-of-Experts"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Hanrui Wang"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v5",
                "updated": "2025-04-02T01:58:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    58,
                    38,
                    2,
                    92,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks."
                },
                "authors": [
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01157v1",
                "updated": "2025-04-01T19:48:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    48,
                    17,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T19:48:17Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    48,
                    17,
                    1,
                    91,
                    0
                ],
                "title": "Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB"
                },
                "summary": "Knowledge-intensive analytical applications retrieve context from both\nstructured tabular data and unstructured, text-free documents for effective\ndecision-making. Large language models (LLMs) have made it significantly easier\nto prototype such retrieval and reasoning data pipelines. However, implementing\nthese pipelines efficiently still demands significant effort and has several\nchallenges. This often involves orchestrating heterogeneous data systems,\nmanaging data movement, and handling low-level implementation details, e.g.,\nLLM context management.\n  To address these challenges, we introduce FlockMTL: an extension for DBMSs\nthat deeply integrates LLM capabilities and retrieval-augmented generation\n(RAG). FlockMTL includes model-driven scalar and aggregate functions, enabling\nchained predictions through tuple-level mappings and reductions. Drawing\ninspiration from the relational model, FlockMTL incorporates: (i) cost-based\noptimizations, which seamlessly apply techniques such as batching and caching;\nand (ii) resource independence, enabled through novel SQL DDL abstractions:\nPROMPT and MODEL, introduced as first-class schema objects alongside TABLE.\nFlockMTL streamlines the development of knowledge-intensive analytical\napplications, and its optimizations ease the implementation burden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-intensive analytical applications retrieve context from both\nstructured tabular data and unstructured, text-free documents for effective\ndecision-making. Large language models (LLMs) have made it significantly easier\nto prototype such retrieval and reasoning data pipelines. However, implementing\nthese pipelines efficiently still demands significant effort and has several\nchallenges. This often involves orchestrating heterogeneous data systems,\nmanaging data movement, and handling low-level implementation details, e.g.,\nLLM context management.\n  To address these challenges, we introduce FlockMTL: an extension for DBMSs\nthat deeply integrates LLM capabilities and retrieval-augmented generation\n(RAG). FlockMTL includes model-driven scalar and aggregate functions, enabling\nchained predictions through tuple-level mappings and reductions. Drawing\ninspiration from the relational model, FlockMTL incorporates: (i) cost-based\noptimizations, which seamlessly apply techniques such as batching and caching;\nand (ii) resource independence, enabled through novel SQL DDL abstractions:\nPROMPT and MODEL, introduced as first-class schema objects alongside TABLE.\nFlockMTL streamlines the development of knowledge-intensive analytical\napplications, and its optimizations ease the implementation burden."
                },
                "authors": [
                    {
                        "name": "Anas Dorbani"
                    },
                    {
                        "name": "Sunny Yasser"
                    },
                    {
                        "name": "Jimmy Lin"
                    },
                    {
                        "name": "Amine Mhedhbi"
                    }
                ],
                "author_detail": {
                    "name": "Amine Mhedhbi"
                },
                "author": "Amine Mhedhbi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01104v1",
                "updated": "2025-04-01T18:21:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    21,
                    43,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T18:21:43Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    21,
                    43,
                    1,
                    91,
                    0
                ],
                "title": "Fundamentals of Caching Layered Data objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamentals of Caching Layered Data objects"
                },
                "summary": "The effective management of large amounts of data processed or required by\ntoday's cloud or edge computing systems remains a fundamental challenge. This\npaper focuses on cache management for applications where data objects can be\nstored in layered representations. In such representations, each additional\ndata layer enhances the \"quality\" of the object's version but comes with an\nincremental cost of memory space. This layered approach proves beneficial in\nvarious scenarios, including the delivery of zoomable maps, video coding,\nfuture Virtual Reality gaming, and layered neural network models where\nadditional data layers improve inference accuracy. In systems where users or\ndevices demand different versions of a data object, layered representations\noffer flexibility for caching policies to achieve improved hit rates.\n  In this paper, we explore the performance of various traditionally studied\ncaching policies, such as Belady, LRU, and LFU, both with and without layering.\nTo this end, we develop an asymptotically accurate analytical model for Layered\nLRU (LLRU). We study how the performance of LLRU is impacted by factors such as\nthe number of layers, the popularity of different objects and layers, and\noverheads associated with storing layered representations. For instance, we\nshow that, for LLRU, more layers are not always beneficial and indeed\nperformance depends in subtle ways on the popularity and size profiles of\nlayers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effective management of large amounts of data processed or required by\ntoday's cloud or edge computing systems remains a fundamental challenge. This\npaper focuses on cache management for applications where data objects can be\nstored in layered representations. In such representations, each additional\ndata layer enhances the \"quality\" of the object's version but comes with an\nincremental cost of memory space. This layered approach proves beneficial in\nvarious scenarios, including the delivery of zoomable maps, video coding,\nfuture Virtual Reality gaming, and layered neural network models where\nadditional data layers improve inference accuracy. In systems where users or\ndevices demand different versions of a data object, layered representations\noffer flexibility for caching policies to achieve improved hit rates.\n  In this paper, we explore the performance of various traditionally studied\ncaching policies, such as Belady, LRU, and LFU, both with and without layering.\nTo this end, we develop an asymptotically accurate analytical model for Layered\nLRU (LLRU). We study how the performance of LLRU is impacted by factors such as\nthe number of layers, the popularity of different objects and layers, and\noverheads associated with storing layered representations. For instance, we\nshow that, for LLRU, more layers are not always beneficial and indeed\nperformance depends in subtle ways on the popularity and size profiles of\nlayers."
                },
                "authors": [
                    {
                        "name": "Agrim Bari"
                    },
                    {
                        "name": "Gustavo de Veciana"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "arxiv_comment": "An abridged version of this paper has been accepted at the 45th IEEE\n  International Conference on Distributed Computing Systems (ICDCS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01084v1",
                "updated": "2025-04-01T18:00:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    0,
                    48,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T18:00:48Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    0,
                    48,
                    1,
                    91,
                    0
                ],
                "title": "Surfactants Screen Slide Electrification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surfactants Screen Slide Electrification"
                },
                "summary": "Water drops spontaneously accumulate charges when they move on hydrophobic\ndielectric surfaces by slide electrification. On the one hand, slide\nelectrification generates electricity with possible applications on tiny\ndevices. On the other hand, the potential of up to 1 KV generated by slide\nelectrification alters wetting and drop motion. Therefore, it is important to\nknow the factors that affect slide electrification. To find out how surfactants\naffect slide electrification, we measured drop charges of aqueous drops\ncontaining cationic CTAB, anionic SDS and neutral C8E3 sliding on different\nhydrophobic surfaces. The result is: addition of surfactant significantly\nreduces the spontaneous charging of moving water drops. Based on zeta potential\nmeasurements, confocal microscopy of deposited surface-active dyes and drop\nimpact studies, we propose that several factors contribute to this suppression\nof charge separation: (1) Surfactants tend to lower the contact angles, which\nreduces charge separation. (2) Surfactant adsorption at the solid-liquid\ninterface can reduce the density of primary ions, particularly for anionic\nsurfactants. (3) Anionic and neutral surfactants are mostly transferred to the\nliquid-air interface at the rear of the sliding drop, retaining primary ions\nwithin the drop. (4) Deposited cationic surfactant directly reduces the charge\nof the drop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Water drops spontaneously accumulate charges when they move on hydrophobic\ndielectric surfaces by slide electrification. On the one hand, slide\nelectrification generates electricity with possible applications on tiny\ndevices. On the other hand, the potential of up to 1 KV generated by slide\nelectrification alters wetting and drop motion. Therefore, it is important to\nknow the factors that affect slide electrification. To find out how surfactants\naffect slide electrification, we measured drop charges of aqueous drops\ncontaining cationic CTAB, anionic SDS and neutral C8E3 sliding on different\nhydrophobic surfaces. The result is: addition of surfactant significantly\nreduces the spontaneous charging of moving water drops. Based on zeta potential\nmeasurements, confocal microscopy of deposited surface-active dyes and drop\nimpact studies, we propose that several factors contribute to this suppression\nof charge separation: (1) Surfactants tend to lower the contact angles, which\nreduces charge separation. (2) Surfactant adsorption at the solid-liquid\ninterface can reduce the density of primary ions, particularly for anionic\nsurfactants. (3) Anionic and neutral surfactants are mostly transferred to the\nliquid-air interface at the rear of the sliding drop, retaining primary ions\nwithin the drop. (4) Deposited cationic surfactant directly reduces the charge\nof the drop."
                },
                "authors": [
                    {
                        "name": "Xiaomei Li"
                    },
                    {
                        "name": "Zhongyuan Ni"
                    },
                    {
                        "name": "Xiaoteng Zhou"
                    },
                    {
                        "name": "Lisa S. Bauer"
                    },
                    {
                        "name": "Diego Diaz"
                    },
                    {
                        "name": "Gabriele Schäfer"
                    },
                    {
                        "name": "Hans-Jürgen Butt"
                    }
                ],
                "author_detail": {
                    "name": "Hans-Jürgen Butt"
                },
                "author": "Hans-Jürgen Butt",
                "arxiv_comment": "13 pages, 4 figures, 50 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00999v1",
                "updated": "2025-04-01T17:39:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    39,
                    19,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T17:39:19Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    39,
                    19,
                    1,
                    91,
                    0
                ],
                "title": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization"
                },
                "summary": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ."
                },
                "authors": [
                    {
                        "name": "Siyuan Li"
                    },
                    {
                        "name": "Luyuan Zhang"
                    },
                    {
                        "name": "Zedong Wang"
                    },
                    {
                        "name": "Juanxi Tian"
                    },
                    {
                        "name": "Cheng Tan"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Chang Yu"
                    },
                    {
                        "name": "Qingsong Xie"
                    },
                    {
                        "name": "Haonan Lu"
                    },
                    {
                        "name": "Haoqian Wang"
                    },
                    {
                        "name": "Zhen Lei"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Lei"
                },
                "author": "Zhen Lei",
                "arxiv_comment": "CVPR2025 (in process for more analysis and extension)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00970v1",
                "updated": "2025-04-01T17:08:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T17:08:57Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "title": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching"
                },
                "summary": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Ali Falahati"
                    },
                    {
                        "name": "David H. Yang"
                    },
                    {
                        "name": "Mohammad Mohammadi Amiri"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Mohammadi Amiri"
                },
                "author": "Mohammad Mohammadi Amiri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.13837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13837v1",
                "updated": "2025-04-18T17:59:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    59,
                    56,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T17:59:56Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    59,
                    56,
                    4,
                    108,
                    0
                ],
                "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in\n  LLMs Beyond the Base Model?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in\n  LLMs Beyond the Base Model?"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently\ndemonstrated notable success in enhancing the reasoning capabilities of LLMs,\nparticularly in mathematics and programming tasks. It is widely believed that\nRLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning\nabilities that exceed corresponding base models' capacity. In this study,\nhowever, we critically re-examines this assumption by measuring the\npass@\\textit{k} metric with large values of \\textit{k} to explore the reasoning\ncapability boundary of the models across a wide range of model families and\nbenchmarks. Surprisingly, the RL does \\emph{not}, in fact, elicit fundamentally\nnew reasoning patterns. While RL-trained models outperform their base models at\nsmaller values of $k$ (\\eg, $k$=1), base models can achieve a comparable or\neven higher pass@$k$ score compared to their RL counterparts at large $k$\nvalues. The reasoning paths generated by RL-trained models are already included\nin the base models' sampling distribution, suggesting that most reasoning\nabilities manifested in RL-trained models are already obtained by base models.\nFurther analysis shows that RL training boosts the performance by biasing the\nmodel's output distribution toward paths that are more likely to yield rewards,\ntherefore sampling correct responses more efficiently. But this also results in\na narrower reasoning capability boundary compared to base models. Similar\nresults are observed in visual reasoning tasks trained with RLVR. Moreover, we\nfind that distillation can genuinely introduce new knowledge into the model,\ndifferent from RLVR. These findings underscore a critical limitation of RLVR in\nadvancing LLM reasoning abilities which requires us to fundamentally rethink\nthe impact of RL training in reasoning LLMs and the need of a better paradigm.\nProject Page: https://limit-of-RLVR.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently\ndemonstrated notable success in enhancing the reasoning capabilities of LLMs,\nparticularly in mathematics and programming tasks. It is widely believed that\nRLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning\nabilities that exceed corresponding base models' capacity. In this study,\nhowever, we critically re-examines this assumption by measuring the\npass@\\textit{k} metric with large values of \\textit{k} to explore the reasoning\ncapability boundary of the models across a wide range of model families and\nbenchmarks. Surprisingly, the RL does \\emph{not}, in fact, elicit fundamentally\nnew reasoning patterns. While RL-trained models outperform their base models at\nsmaller values of $k$ (\\eg, $k$=1), base models can achieve a comparable or\neven higher pass@$k$ score compared to their RL counterparts at large $k$\nvalues. The reasoning paths generated by RL-trained models are already included\nin the base models' sampling distribution, suggesting that most reasoning\nabilities manifested in RL-trained models are already obtained by base models.\nFurther analysis shows that RL training boosts the performance by biasing the\nmodel's output distribution toward paths that are more likely to yield rewards,\ntherefore sampling correct responses more efficiently. But this also results in\na narrower reasoning capability boundary compared to base models. Similar\nresults are observed in visual reasoning tasks trained with RLVR. Moreover, we\nfind that distillation can genuinely introduce new knowledge into the model,\ndifferent from RLVR. These findings underscore a critical limitation of RLVR in\nadvancing LLM reasoning abilities which requires us to fundamentally rethink\nthe impact of RL training in reasoning LLMs and the need of a better paradigm.\nProject Page: https://limit-of-RLVR.github.io"
                },
                "authors": [
                    {
                        "name": "Yang Yue"
                    },
                    {
                        "name": "Zhiqi Chen"
                    },
                    {
                        "name": "Rui Lu"
                    },
                    {
                        "name": "Andrew Zhao"
                    },
                    {
                        "name": "Zhaokai Wang"
                    },
                    {
                        "name": "Yang Yue"
                    },
                    {
                        "name": "Shiji Song"
                    },
                    {
                        "name": "Gao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Gao Huang"
                },
                "author": "Gao Huang",
                "arxiv_comment": "24 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13834v1",
                "updated": "2025-04-18T17:59:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    59,
                    29,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T17:59:29Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    59,
                    29,
                    4,
                    108,
                    0
                ],
                "title": "Science Hierarchography: Hierarchical Organization of Science Literature",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Science Hierarchography: Hierarchical Organization of Science Literature"
                },
                "summary": "Scientific knowledge is growing rapidly, making it challenging to track\nprogress and high-level conceptual links across broad disciplines. While\nexisting tools like citation networks and search engines make it easy to access\na few related papers, they fundamentally lack the flexible abstraction needed\nto represent the density of activity in various scientific subfields. We\nmotivate SCIENCE HIERARCHOGRAPHY, the goal of organizing scientific literature\ninto a high-quality hierarchical structure that allows for the categorization\nof scientific work across varying levels of abstraction, from very broad fields\nto very specific studies. Such a representation can provide insights into which\nfields are well-explored and which are under-explored. To achieve the goals of\nSCIENCE HIERARCHOGRAPHY, we develop a range of algorithms. Our primary approach\ncombines fast embedding-based clustering with LLM-based prompting to balance\nthe computational efficiency of embedding methods with the semantic precision\noffered by LLM prompting. We demonstrate that this approach offers the best\ntrade-off between quality and speed compared to methods that heavily rely on\nLLM prompting, such as iterative tree construction with LLMs. To better reflect\nthe interdisciplinary and multifaceted nature of research papers, our hierarchy\ncaptures multiple dimensions of categorization beyond simple topic labels. We\nevaluate the utility of our framework by assessing how effectively an LLM-based\nagent can locate target papers using the hierarchy. Results show that this\nstructured approach enhances interpretability, supports trend discovery, and\noffers an alternative pathway for exploring scientific literature beyond\ntraditional search methods. Code, data and demo:\n$\\href{https://github.com/JHU-CLSP/science-hierarchography}{https://github.com/JHU-CLSP/science-hierarchography}$",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific knowledge is growing rapidly, making it challenging to track\nprogress and high-level conceptual links across broad disciplines. While\nexisting tools like citation networks and search engines make it easy to access\na few related papers, they fundamentally lack the flexible abstraction needed\nto represent the density of activity in various scientific subfields. We\nmotivate SCIENCE HIERARCHOGRAPHY, the goal of organizing scientific literature\ninto a high-quality hierarchical structure that allows for the categorization\nof scientific work across varying levels of abstraction, from very broad fields\nto very specific studies. Such a representation can provide insights into which\nfields are well-explored and which are under-explored. To achieve the goals of\nSCIENCE HIERARCHOGRAPHY, we develop a range of algorithms. Our primary approach\ncombines fast embedding-based clustering with LLM-based prompting to balance\nthe computational efficiency of embedding methods with the semantic precision\noffered by LLM prompting. We demonstrate that this approach offers the best\ntrade-off between quality and speed compared to methods that heavily rely on\nLLM prompting, such as iterative tree construction with LLMs. To better reflect\nthe interdisciplinary and multifaceted nature of research papers, our hierarchy\ncaptures multiple dimensions of categorization beyond simple topic labels. We\nevaluate the utility of our framework by assessing how effectively an LLM-based\nagent can locate target papers using the hierarchy. Results show that this\nstructured approach enhances interpretability, supports trend discovery, and\noffers an alternative pathway for exploring scientific literature beyond\ntraditional search methods. Code, data and demo:\n$\\href{https://github.com/JHU-CLSP/science-hierarchography}{https://github.com/JHU-CLSP/science-hierarchography}$"
                },
                "authors": [
                    {
                        "name": "Muhan Gao"
                    },
                    {
                        "name": "Jash Shah"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Daniel Khashabi"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Khashabi"
                },
                "author": "Daniel Khashabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13825v1",
                "updated": "2025-04-18T17:54:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    54,
                    33,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T17:54:33Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    54,
                    33,
                    4,
                    108,
                    0
                ],
                "title": "Feature Alignment and Representation Transfer in Knowledge Distillation\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature Alignment and Representation Transfer in Knowledge Distillation\n  for Large Language Models"
                },
                "summary": "Knowledge distillation (KD) is a technique for transferring knowledge from\ncomplex teacher models to simpler student models, significantly enhancing model\nefficiency and accuracy. It has demonstrated substantial advancements in\nvarious applications including image classification, object detection, language\nmodeling, text classification, and sentiment analysis. Recent innovations in KD\nmethods, such as attention-based approaches, block-wise logit distillation, and\ndecoupling distillation, have notably improved student model performance. These\ntechniques focus on stimulus complexity, attention mechanisms, and global\ninformation capture to optimize knowledge transfer. In addition, KD has proven\neffective in compressing large language models while preserving accuracy,\nreducing computational overhead, and improving inference speed. This survey\nsynthesizes the latest literature, highlighting key findings, contributions,\nand future directions in knowledge distillation to provide insights for\nresearchers and practitioners on its evolving role in artificial intelligence\nand machine learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation (KD) is a technique for transferring knowledge from\ncomplex teacher models to simpler student models, significantly enhancing model\nefficiency and accuracy. It has demonstrated substantial advancements in\nvarious applications including image classification, object detection, language\nmodeling, text classification, and sentiment analysis. Recent innovations in KD\nmethods, such as attention-based approaches, block-wise logit distillation, and\ndecoupling distillation, have notably improved student model performance. These\ntechniques focus on stimulus complexity, attention mechanisms, and global\ninformation capture to optimize knowledge transfer. In addition, KD has proven\neffective in compressing large language models while preserving accuracy,\nreducing computational overhead, and improving inference speed. This survey\nsynthesizes the latest literature, highlighting key findings, contributions,\nand future directions in knowledge distillation to provide insights for\nresearchers and practitioners on its evolving role in artificial intelligence\nand machine learning."
                },
                "authors": [
                    {
                        "name": "Junjie Yang"
                    },
                    {
                        "name": "Junhao Song"
                    },
                    {
                        "name": "Xudong Han"
                    },
                    {
                        "name": "Ziqian Bi"
                    },
                    {
                        "name": "Tianyang Wang"
                    },
                    {
                        "name": "Chia Xin Liang"
                    },
                    {
                        "name": "Xinyuan Song"
                    },
                    {
                        "name": "Yichao Zhang"
                    },
                    {
                        "name": "Qian Niu"
                    },
                    {
                        "name": "Benji Peng"
                    },
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Ming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ming Liu"
                },
                "author": "Ming Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13818v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13818v1",
                "updated": "2025-04-18T17:49:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    49,
                    55,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T17:49:55Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    49,
                    55,
                    4,
                    108,
                    0
                ],
                "title": "Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement\n  Learning"
                },
                "summary": "Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing\nreasoning capabilities in large language models, but faces a fundamental\nasymmetry in computation and memory requirements: inference is embarrassingly\nparallel with a minimal memory footprint, while policy updates require\nextensive synchronization and are memory-intensive. To address this asymmetry,\nwe introduce PODS (Policy Optimization with Down-Sampling), a framework that\nstrategically decouples these phases by generating numerous rollouts in\nparallel but updating only on an informative subset. Within this framework, we\ndevelop max-variance down-sampling, a theoretically motivated method that\nselects rollouts with maximally diverse reward signals. We prove that this\napproach has an efficient algorithmic solution, and empirically demonstrate\nthat GRPO with PODS using max-variance down-sampling achieves superior\nperformance over standard GRPO on the GSM8K benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing\nreasoning capabilities in large language models, but faces a fundamental\nasymmetry in computation and memory requirements: inference is embarrassingly\nparallel with a minimal memory footprint, while policy updates require\nextensive synchronization and are memory-intensive. To address this asymmetry,\nwe introduce PODS (Policy Optimization with Down-Sampling), a framework that\nstrategically decouples these phases by generating numerous rollouts in\nparallel but updating only on an informative subset. Within this framework, we\ndevelop max-variance down-sampling, a theoretically motivated method that\nselects rollouts with maximally diverse reward signals. We prove that this\napproach has an efficient algorithmic solution, and empirically demonstrate\nthat GRPO with PODS using max-variance down-sampling achieves superior\nperformance over standard GRPO on the GSM8K benchmark."
                },
                "authors": [
                    {
                        "name": "Yixuan Even Xu"
                    },
                    {
                        "name": "Yash Savani"
                    },
                    {
                        "name": "Fei Fang"
                    },
                    {
                        "name": "Zico Kolter"
                    }
                ],
                "author_detail": {
                    "name": "Zico Kolter"
                },
                "author": "Zico Kolter",
                "arxiv_comment": "9 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13818v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13816v1",
                "updated": "2025-04-18T17:44:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    44,
                    12,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T17:44:12Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    44,
                    12,
                    4,
                    108,
                    0
                ],
                "title": "Analyzing LLMs' Knowledge Boundary Cognition Across Languages Through\n  the Lens of Internal Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing LLMs' Knowledge Boundary Cognition Across Languages Through\n  the Lens of Internal Representations"
                },
                "summary": "While understanding the knowledge boundaries of LLMs is crucial to prevent\nhallucination, research on knowledge boundaries of LLMs has predominantly\nfocused on English. In this work, we present the first study to analyze how\nLLMs recognize knowledge boundaries across different languages by probing their\ninternal representations when processing known and unknown questions in\nmultiple languages. Our empirical studies reveal three key findings: 1) LLMs'\nperceptions of knowledge boundaries are encoded in the middle to middle-upper\nlayers across different languages. 2) Language differences in knowledge\nboundary perception follow a linear structure, which motivates our proposal of\na training-free alignment method that effectively transfers knowledge boundary\nperception ability across languages, thereby helping reduce hallucination risk\nin low-resource languages; 3) Fine-tuning on bilingual question pair\ntranslation further enhances LLMs' recognition of knowledge boundaries across\nlanguages. Given the absence of standard testbeds for cross-lingual knowledge\nboundary analysis, we construct a multilingual evaluation suite comprising\nthree representative types of knowledge boundary data. Our code and datasets\nare publicly available at\nhttps://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While understanding the knowledge boundaries of LLMs is crucial to prevent\nhallucination, research on knowledge boundaries of LLMs has predominantly\nfocused on English. In this work, we present the first study to analyze how\nLLMs recognize knowledge boundaries across different languages by probing their\ninternal representations when processing known and unknown questions in\nmultiple languages. Our empirical studies reveal three key findings: 1) LLMs'\nperceptions of knowledge boundaries are encoded in the middle to middle-upper\nlayers across different languages. 2) Language differences in knowledge\nboundary perception follow a linear structure, which motivates our proposal of\na training-free alignment method that effectively transfers knowledge boundary\nperception ability across languages, thereby helping reduce hallucination risk\nin low-resource languages; 3) Fine-tuning on bilingual question pair\ntranslation further enhances LLMs' recognition of knowledge boundaries across\nlanguages. Given the absence of standard testbeds for cross-lingual knowledge\nboundary analysis, we construct a multilingual evaluation suite comprising\nthree representative types of knowledge boundary data. Our code and datasets\nare publicly available at\nhttps://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundaries."
                },
                "authors": [
                    {
                        "name": "Chenghao Xiao"
                    },
                    {
                        "name": "Hou Pong Chan"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Mahani Aljunied"
                    },
                    {
                        "name": "Lidong Bing"
                    },
                    {
                        "name": "Noura Al Moubayed"
                    },
                    {
                        "name": "Yu Rong"
                    }
                ],
                "author_detail": {
                    "name": "Yu Rong"
                },
                "author": "Yu Rong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03535v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03535v2",
                "updated": "2025-04-18T17:36:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    36,
                    25,
                    4,
                    108,
                    0
                ],
                "published": "2024-10-04T15:54:02Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    15,
                    54,
                    2,
                    4,
                    278,
                    0
                ],
                "title": "NRGBoost: Energy-Based Generative Boosted Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NRGBoost: Energy-Based Generative Boosted Trees"
                },
                "summary": "Despite the rise to dominance of deep learning in unstructured data domains,\ntree-based methods such as Random Forests (RF) and Gradient Boosted Decision\nTrees (GBDT) are still the workhorses for handling discriminative tasks on\ntabular data. We explore generative extensions of these popular algorithms with\na focus on explicitly modeling the data density (up to a normalization\nconstant), thus enabling other applications besides sampling. As our main\ncontribution we propose an energy-based generative boosting algorithm that is\nanalogous to the second-order boosting implemented in popular libraries like\nXGBoost. We show that, despite producing a generative model capable of handling\ninference tasks over any input variable, our proposed algorithm can achieve\nsimilar discriminative performance to GBDT on a number of real world tabular\ndatasets, outperforming alternative generative approaches. At the same time, we\nshow that it is also competitive with neural-network-based models for sampling.\nCode is available at https://github.com/ajoo/nrgboost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the rise to dominance of deep learning in unstructured data domains,\ntree-based methods such as Random Forests (RF) and Gradient Boosted Decision\nTrees (GBDT) are still the workhorses for handling discriminative tasks on\ntabular data. We explore generative extensions of these popular algorithms with\na focus on explicitly modeling the data density (up to a normalization\nconstant), thus enabling other applications besides sampling. As our main\ncontribution we propose an energy-based generative boosting algorithm that is\nanalogous to the second-order boosting implemented in popular libraries like\nXGBoost. We show that, despite producing a generative model capable of handling\ninference tasks over any input variable, our proposed algorithm can achieve\nsimilar discriminative performance to GBDT on a number of real world tabular\ndatasets, outperforming alternative generative approaches. At the same time, we\nshow that it is also competitive with neural-network-based models for sampling.\nCode is available at https://github.com/ajoo/nrgboost."
                },
                "authors": [
                    {
                        "name": "João Bravo"
                    }
                ],
                "author_detail": {
                    "name": "João Bravo"
                },
                "author": "João Bravo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03535v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03535v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11838v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11838v2",
                "updated": "2025-04-18T17:32:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    32,
                    22,
                    4,
                    108,
                    0
                ],
                "published": "2024-10-15T17:59:04Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    59,
                    4,
                    1,
                    289,
                    0
                ],
                "title": "High-Resolution Frame Interpolation with Patch-based Cascaded Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Resolution Frame Interpolation with Patch-based Cascaded Diffusion"
                },
                "summary": "Despite the recent progress, existing frame interpolation methods still\nstruggle with processing extremely high resolution input and handling\nchallenging cases such as repetitive textures, thin objects, and large motion.\nTo address these issues, we introduce a patch-based cascaded pixel diffusion\nmodel for high resolution frame interpolation, HIFI, that excels in these\nscenarios while achieving competitive performance on standard benchmarks.\nCascades, which generate a series of images from low to high resolution, can\nhelp significantly with large or complex motion that require both global\ncontext for a coarse solution and detailed context for high resolution output.\nHowever, contrary to prior work on cascaded diffusion models which perform\ndiffusion on increasingly large resolutions, we use a single model that always\nperforms diffusion at the same resolution and upsamples by processing patches\nof the inputs and the prior solution. At inference time, this drastically\nreduces memory usage and allows a single model, solving both frame\ninterpolation (base model's task) and spatial up-sampling, saving training cost\nas well. HIFI excels at high-resolution images and complex repeated textures\nthat require global context, achieving comparable or state-of-the-art\nperformance on various benchmarks (Vimeo, Xiph, X-Test, and SEPE-8K). We\nfurther introduce a new dataset, LaMoR, that focuses on particularly\nchallenging cases, and HIFI significantly outperforms other baselines. Please\nvisit our project page for video results: https://hifi-diffusion.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent progress, existing frame interpolation methods still\nstruggle with processing extremely high resolution input and handling\nchallenging cases such as repetitive textures, thin objects, and large motion.\nTo address these issues, we introduce a patch-based cascaded pixel diffusion\nmodel for high resolution frame interpolation, HIFI, that excels in these\nscenarios while achieving competitive performance on standard benchmarks.\nCascades, which generate a series of images from low to high resolution, can\nhelp significantly with large or complex motion that require both global\ncontext for a coarse solution and detailed context for high resolution output.\nHowever, contrary to prior work on cascaded diffusion models which perform\ndiffusion on increasingly large resolutions, we use a single model that always\nperforms diffusion at the same resolution and upsamples by processing patches\nof the inputs and the prior solution. At inference time, this drastically\nreduces memory usage and allows a single model, solving both frame\ninterpolation (base model's task) and spatial up-sampling, saving training cost\nas well. HIFI excels at high-resolution images and complex repeated textures\nthat require global context, achieving comparable or state-of-the-art\nperformance on various benchmarks (Vimeo, Xiph, X-Test, and SEPE-8K). We\nfurther introduce a new dataset, LaMoR, that focuses on particularly\nchallenging cases, and HIFI significantly outperforms other baselines. Please\nvisit our project page for video results: https://hifi-diffusion.github.io"
                },
                "authors": [
                    {
                        "name": "Junhwa Hur"
                    },
                    {
                        "name": "Charles Herrmann"
                    },
                    {
                        "name": "Saurabh Saxena"
                    },
                    {
                        "name": "Janne Kontkanen"
                    },
                    {
                        "name": "Wei-Sheng Lai"
                    },
                    {
                        "name": "Yichang Shih"
                    },
                    {
                        "name": "Michael Rubinstein"
                    },
                    {
                        "name": "David J. Fleet"
                    },
                    {
                        "name": "Deqing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Deqing Sun"
                },
                "author": "Deqing Sun",
                "arxiv_comment": "Project page: https://hifi-diffusion.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11838v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11838v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12110v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12110v5",
                "updated": "2025-04-18T17:26:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    26,
                    57,
                    4,
                    108,
                    0
                ],
                "published": "2025-02-17T18:36:14Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    18,
                    36,
                    14,
                    0,
                    48,
                    0
                ],
                "title": "A-MEM: Agentic Memory for LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A-MEM: Agentic Memory for LLM Agents"
                },
                "summary": "While large language model (LLM) agents can effectively use external tools\nfor complex real-world tasks, they require memory systems to leverage\nhistorical experiences. Current memory systems enable basic storage and\nretrieval but lack sophisticated memory organization, despite recent attempts\nto incorporate graph databases. Moreover, these systems' fixed operations and\nstructures limit their adaptability across diverse tasks. To address this\nlimitation, this paper proposes a novel agentic memory system for LLM agents\nthat can dynamically organize memories in an agentic way. Following the basic\nprinciples of the Zettelkasten method, we designed our memory system to create\ninterconnected knowledge networks through dynamic indexing and linking. When a\nnew memory is added, we generate a comprehensive note containing multiple\nstructured attributes, including contextual descriptions, keywords, and tags.\nThe system then analyzes historical memories to identify relevant connections,\nestablishing links where meaningful similarities exist. Additionally, this\nprocess enables memory evolution - as new memories are integrated, they can\ntrigger updates to the contextual representations and attributes of existing\nhistorical memories, allowing the memory network to continuously refine its\nunderstanding. Our approach combines the structured organization principles of\nZettelkasten with the flexibility of agent-driven decision making, allowing for\nmore adaptive and context-aware memory management. Empirical experiments on six\nfoundation models show superior improvement against existing SOTA baselines.\nThe source code for evaluating performance is available at\nhttps://github.com/WujiangXu/AgenticMemory, while the source code of agentic\nmemory system is available at https://github.com/agiresearch/A-mem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language model (LLM) agents can effectively use external tools\nfor complex real-world tasks, they require memory systems to leverage\nhistorical experiences. Current memory systems enable basic storage and\nretrieval but lack sophisticated memory organization, despite recent attempts\nto incorporate graph databases. Moreover, these systems' fixed operations and\nstructures limit their adaptability across diverse tasks. To address this\nlimitation, this paper proposes a novel agentic memory system for LLM agents\nthat can dynamically organize memories in an agentic way. Following the basic\nprinciples of the Zettelkasten method, we designed our memory system to create\ninterconnected knowledge networks through dynamic indexing and linking. When a\nnew memory is added, we generate a comprehensive note containing multiple\nstructured attributes, including contextual descriptions, keywords, and tags.\nThe system then analyzes historical memories to identify relevant connections,\nestablishing links where meaningful similarities exist. Additionally, this\nprocess enables memory evolution - as new memories are integrated, they can\ntrigger updates to the contextual representations and attributes of existing\nhistorical memories, allowing the memory network to continuously refine its\nunderstanding. Our approach combines the structured organization principles of\nZettelkasten with the flexibility of agent-driven decision making, allowing for\nmore adaptive and context-aware memory management. Empirical experiments on six\nfoundation models show superior improvement against existing SOTA baselines.\nThe source code for evaluating performance is available at\nhttps://github.com/WujiangXu/AgenticMemory, while the source code of agentic\nmemory system is available at https://github.com/agiresearch/A-mem."
                },
                "authors": [
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Hang Gao"
                    },
                    {
                        "name": "Juntao Tan"
                    },
                    {
                        "name": "Zujie Liang"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12110v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12110v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13803v1",
                "updated": "2025-04-18T17:12:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    12,
                    0,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T17:12:00Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    12,
                    0,
                    4,
                    108,
                    0
                ],
                "title": "Imitation Learning with Precisely Labeled Human Demonstrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imitation Learning with Precisely Labeled Human Demonstrations"
                },
                "summary": "Within the imitation learning paradigm, training generalist robots requires\nlarge-scale datasets obtainable only through diverse curation. Due to the\nrelative ease to collect, human demonstrations constitute a valuable addition\nwhen incorporated appropriately. However, existing methods utilizing human\ndemonstrations face challenges in inferring precise actions, ameliorating\nembodiment gaps, and fusing with frontier generalist robot training pipelines.\nIn this work, building on prior studies that demonstrate the viability of using\nhand-held grippers for efficient data collection, we leverage the user's\ncontrol over the gripper's appearance--specifically by assigning it a unique,\neasily segmentable color--to enable simple and reliable application of the\nRANSAC and ICP registration method for precise end-effector pose estimation. We\nshow in simulation that precisely labeled human demonstrations on their own\nallow policies to reach on average 88.1% of the performance of using robot\ndemonstrations, and boost policy performance when combined with robot\ndemonstrations, despite the inherent embodiment gap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Within the imitation learning paradigm, training generalist robots requires\nlarge-scale datasets obtainable only through diverse curation. Due to the\nrelative ease to collect, human demonstrations constitute a valuable addition\nwhen incorporated appropriately. However, existing methods utilizing human\ndemonstrations face challenges in inferring precise actions, ameliorating\nembodiment gaps, and fusing with frontier generalist robot training pipelines.\nIn this work, building on prior studies that demonstrate the viability of using\nhand-held grippers for efficient data collection, we leverage the user's\ncontrol over the gripper's appearance--specifically by assigning it a unique,\neasily segmentable color--to enable simple and reliable application of the\nRANSAC and ICP registration method for precise end-effector pose estimation. We\nshow in simulation that precisely labeled human demonstrations on their own\nallow policies to reach on average 88.1% of the performance of using robot\ndemonstrations, and boost policy performance when combined with robot\ndemonstrations, despite the inherent embodiment gap."
                },
                "authors": [
                    {
                        "name": "Yilong Song"
                    }
                ],
                "author_detail": {
                    "name": "Yilong Song"
                },
                "author": "Yilong Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07433v2",
                "updated": "2025-04-18T17:03:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    3,
                    1,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-10T04:03:25Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    4,
                    3,
                    25,
                    3,
                    100,
                    0
                ],
                "title": "From Token to Line: Enhancing Code Generation with a Long-Term\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Token to Line: Enhancing Code Generation with a Long-Term\n  Perspective"
                },
                "summary": "The emergence of large language models (LLMs) has significantly promoted the\ndevelopment of code generation task, sparking a surge in pertinent literature.\nCurrent research is hindered by redundant generation results and a tendency to\noverfit local patterns in the short term. Although existing studies attempt to\nalleviate the issue by adopting a multi-token prediction strategy, there\nremains limited focus on choosing the appropriate processing length for\ngenerations. By analyzing the attention between tokens during the generation\nprocess of LLMs, it can be observed that the high spikes of the attention\nscores typically appear at the end of lines. This insight suggests that it is\nreasonable to treat each line of code as a fundamental processing unit and\ngenerate them sequentially. Inspired by this, we propose the \\textbf{LSR-MCTS}\nalgorithm, which leverages MCTS to determine the code line-by-line and select\nthe optimal path. Further, we integrate a self-refine mechanism at each node to\nenhance diversity and generate higher-quality programs through error\ncorrection. Extensive experiments and comprehensive analyses on three public\ncoding benchmarks demonstrate that our method outperforms the state-of-the-art\nperformance approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) has significantly promoted the\ndevelopment of code generation task, sparking a surge in pertinent literature.\nCurrent research is hindered by redundant generation results and a tendency to\noverfit local patterns in the short term. Although existing studies attempt to\nalleviate the issue by adopting a multi-token prediction strategy, there\nremains limited focus on choosing the appropriate processing length for\ngenerations. By analyzing the attention between tokens during the generation\nprocess of LLMs, it can be observed that the high spikes of the attention\nscores typically appear at the end of lines. This insight suggests that it is\nreasonable to treat each line of code as a fundamental processing unit and\ngenerate them sequentially. Inspired by this, we propose the \\textbf{LSR-MCTS}\nalgorithm, which leverages MCTS to determine the code line-by-line and select\nthe optimal path. Further, we integrate a self-refine mechanism at each node to\nenhance diversity and generate higher-quality programs through error\ncorrection. Extensive experiments and comprehensive analyses on three public\ncoding benchmarks demonstrate that our method outperforms the state-of-the-art\nperformance approaches."
                },
                "authors": [
                    {
                        "name": "Tingwei Lu"
                    },
                    {
                        "name": "Yangning Li"
                    },
                    {
                        "name": "Liyuan Wang"
                    },
                    {
                        "name": "Binghuai Lin"
                    },
                    {
                        "name": "Jiwei Tang"
                    },
                    {
                        "name": "Wanshi Xu"
                    },
                    {
                        "name": "Hai-Tao Zheng"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Bingxu An"
                    },
                    {
                        "name": "Zhao Wei"
                    },
                    {
                        "name": "Yong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Xu"
                },
                "author": "Yong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19653v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19653v4",
                "updated": "2025-04-18T16:49:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    16,
                    49,
                    44,
                    4,
                    108,
                    0
                ],
                "published": "2024-05-30T03:12:04Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    3,
                    12,
                    4,
                    3,
                    151,
                    0
                ],
                "title": "SysCaps: Language Interfaces for Simulation Surrogates of Complex\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SysCaps: Language Interfaces for Simulation Surrogates of Complex\n  Systems"
                },
                "summary": "Surrogate models are used to predict the behavior of complex energy systems\nthat are too expensive to simulate with traditional numerical methods. Our work\nintroduces the use of language descriptions, which we call ``system captions''\nor SysCaps, to interface with such surrogates. We argue that interacting with\nsurrogates through text, particularly natural language, makes these models more\naccessible for both experts and non-experts. We introduce a lightweight\nmultimodal text and timeseries regression model and a training pipeline that\nuses large language models (LLMs) to synthesize high-quality captions from\nsimulation metadata. Our experiments on two real-world simulators of buildings\nand wind farms show that our SysCaps-augmented surrogates have better accuracy\non held-out systems than traditional methods while enjoying new generalization\nabilities, such as handling semantically related descriptions of the same test\nsystem. Additional experiments also highlight the potential of SysCaps to\nunlock language-driven design space exploration and to regularize training\nthrough prompt augmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models are used to predict the behavior of complex energy systems\nthat are too expensive to simulate with traditional numerical methods. Our work\nintroduces the use of language descriptions, which we call ``system captions''\nor SysCaps, to interface with such surrogates. We argue that interacting with\nsurrogates through text, particularly natural language, makes these models more\naccessible for both experts and non-experts. We introduce a lightweight\nmultimodal text and timeseries regression model and a training pipeline that\nuses large language models (LLMs) to synthesize high-quality captions from\nsimulation metadata. Our experiments on two real-world simulators of buildings\nand wind farms show that our SysCaps-augmented surrogates have better accuracy\non held-out systems than traditional methods while enjoying new generalization\nabilities, such as handling semantically related descriptions of the same test\nsystem. Additional experiments also highlight the potential of SysCaps to\nunlock language-driven design space exploration and to regularize training\nthrough prompt augmentation."
                },
                "authors": [
                    {
                        "name": "Patrick Emami"
                    },
                    {
                        "name": "Zhaonan Li"
                    },
                    {
                        "name": "Saumya Sinha"
                    },
                    {
                        "name": "Truc Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Truc Nguyen"
                },
                "author": "Truc Nguyen",
                "arxiv_comment": "Accepted at ICLR 2025. 23 pages. Updated with final camera ready\n  version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19653v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19653v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13785v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13785v1",
                "updated": "2025-04-18T16:35:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    16,
                    35,
                    12,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T16:35:12Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    16,
                    35,
                    12,
                    4,
                    108,
                    0
                ],
                "title": "Learning Through Retrospection: Improving Trajectory Prediction for\n  Automated Driving with Error Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Through Retrospection: Improving Trajectory Prediction for\n  Automated Driving with Error Feedback"
                },
                "summary": "In automated driving, predicting trajectories of surrounding vehicles\nsupports reasoning about scene dynamics and enables safe planning for the ego\nvehicle. However, existing models handle predictions as an instantaneous task\nof forecasting future trajectories based on observed information. As time\nproceeds, the next prediction is made independently of the previous one, which\nmeans that the model cannot correct its errors during inference and will repeat\nthem. To alleviate this problem and better leverage temporal data, we propose a\nnovel retrospection technique. Through training on closed-loop rollouts the\nmodel learns to use aggregated feedback. Given new observations it reflects on\nprevious predictions and analyzes its errors to improve the quality of\nsubsequent predictions. Thus, the model can learn to correct systematic errors\nduring inference. Comprehensive experiments on nuScenes and Argoverse\ndemonstrate a considerable decrease in minimum Average Displacement Error of up\nto 31.9% compared to the state-of-the-art baseline without retrospection. We\nfurther showcase the robustness of our technique by demonstrating a better\nhandling of out-of-distribution scenarios with undetected road-users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In automated driving, predicting trajectories of surrounding vehicles\nsupports reasoning about scene dynamics and enables safe planning for the ego\nvehicle. However, existing models handle predictions as an instantaneous task\nof forecasting future trajectories based on observed information. As time\nproceeds, the next prediction is made independently of the previous one, which\nmeans that the model cannot correct its errors during inference and will repeat\nthem. To alleviate this problem and better leverage temporal data, we propose a\nnovel retrospection technique. Through training on closed-loop rollouts the\nmodel learns to use aggregated feedback. Given new observations it reflects on\nprevious predictions and analyzes its errors to improve the quality of\nsubsequent predictions. Thus, the model can learn to correct systematic errors\nduring inference. Comprehensive experiments on nuScenes and Argoverse\ndemonstrate a considerable decrease in minimum Average Displacement Error of up\nto 31.9% compared to the state-of-the-art baseline without retrospection. We\nfurther showcase the robustness of our technique by demonstrating a better\nhandling of out-of-distribution scenarios with undetected road-users."
                },
                "authors": [
                    {
                        "name": "Steffen Hagedorn"
                    },
                    {
                        "name": "Aron Distelzweig"
                    },
                    {
                        "name": "Marcel Hallgarten"
                    },
                    {
                        "name": "Alexandru P. Condurache"
                    }
                ],
                "author_detail": {
                    "name": "Alexandru P. Condurache"
                },
                "author": "Alexandru P. Condurache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13785v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13775v1",
                "updated": "2025-04-18T16:22:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    16,
                    22,
                    41,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T16:22:41Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    16,
                    22,
                    41,
                    4,
                    108,
                    0
                ],
                "title": "BadApex: Backdoor Attack Based on Adaptive Optimization Mechanism of\n  Black-box Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BadApex: Backdoor Attack Based on Adaptive Optimization Mechanism of\n  Black-box Large Language Models"
                },
                "summary": "Previous insertion-based and paraphrase-based backdoors have achieved great\nsuccess in attack efficacy, but they ignore the text quality and semantic\nconsistency between poisoned and clean texts. Although recent studies introduce\nLLMs to generate poisoned texts and improve the stealthiness, semantic\nconsistency, and text quality, their hand-crafted prompts rely on expert\nexperiences, facing significant challenges in prompt adaptability and attack\nperformance after defenses. In this paper, we propose a novel backdoor attack\nbased on adaptive optimization mechanism of black-box large language models\n(BadApex), which leverages a black-box LLM to generate poisoned text through a\nrefined prompt. Specifically, an Adaptive Optimization Mechanism is designed to\nrefine an initial prompt iteratively using the generation and modification\nagents. The generation agent generates the poisoned text based on the initial\nprompt. Then the modification agent evaluates the quality of the poisoned text\nand refines a new prompt. After several iterations of the above process, the\nrefined prompt is used to generate poisoned texts through LLMs. We conduct\nextensive experiments on three dataset with six backdoor attacks and two\ndefenses. Extensive experimental results demonstrate that BadApex significantly\noutperforms state-of-the-art attacks. It improves prompt adaptability, semantic\nconsistency, and text quality. Furthermore, when two defense methods are\napplied, the average attack success rate (ASR) still up to 96.75%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous insertion-based and paraphrase-based backdoors have achieved great\nsuccess in attack efficacy, but they ignore the text quality and semantic\nconsistency between poisoned and clean texts. Although recent studies introduce\nLLMs to generate poisoned texts and improve the stealthiness, semantic\nconsistency, and text quality, their hand-crafted prompts rely on expert\nexperiences, facing significant challenges in prompt adaptability and attack\nperformance after defenses. In this paper, we propose a novel backdoor attack\nbased on adaptive optimization mechanism of black-box large language models\n(BadApex), which leverages a black-box LLM to generate poisoned text through a\nrefined prompt. Specifically, an Adaptive Optimization Mechanism is designed to\nrefine an initial prompt iteratively using the generation and modification\nagents. The generation agent generates the poisoned text based on the initial\nprompt. Then the modification agent evaluates the quality of the poisoned text\nand refines a new prompt. After several iterations of the above process, the\nrefined prompt is used to generate poisoned texts through LLMs. We conduct\nextensive experiments on three dataset with six backdoor attacks and two\ndefenses. Extensive experimental results demonstrate that BadApex significantly\noutperforms state-of-the-art attacks. It improves prompt adaptability, semantic\nconsistency, and text quality. Furthermore, when two defense methods are\napplied, the average attack success rate (ASR) still up to 96.75%."
                },
                "authors": [
                    {
                        "name": "Zhengxian Wu"
                    },
                    {
                        "name": "Juan Wen"
                    },
                    {
                        "name": "Wanli Peng"
                    },
                    {
                        "name": "Ziwei Zhang"
                    },
                    {
                        "name": "Yinghan Zhou"
                    },
                    {
                        "name": "Yiming Xue"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Xue"
                },
                "author": "Yiming Xue",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13774v1",
                "updated": "2025-04-18T16:22:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    16,
                    22,
                    20,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T16:22:20Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    16,
                    22,
                    20,
                    4,
                    108,
                    0
                ],
                "title": "DP2Unlearning: An Efficient and Guaranteed Unlearning Framework for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DP2Unlearning: An Efficient and Guaranteed Unlearning Framework for LLMs"
                },
                "summary": "Large language models (LLMs) have recently revolutionized language processing\ntasks but have also brought ethical and legal issues. LLMs have a tendency to\nmemorize potentially private or copyrighted information present in the training\ndata, which might then be delivered to end users at inference time. When this\nhappens, a naive solution is to retrain the model from scratch after excluding\nthe undesired data. Although this guarantees that the target data have been\nforgotten, it is also prohibitively expensive for LLMs. Approximate unlearning\noffers a more efficient alternative, as it consists of ex post modifications of\nthe trained model itself to prevent undesirable results, but it lacks\nforgetting guarantees because it relies solely on empirical evidence. In this\nwork, we present DP2Unlearning, a novel LLM unlearning framework that offers\nformal forgetting guarantees at a significantly lower cost than retraining from\nscratch on the data to be retained. DP2Unlearning involves training LLMs on\ntextual data protected using {\\epsilon}-differential privacy (DP), which later\nenables efficient unlearning with the guarantees against disclosure associated\nwith the chosen {\\epsilon}. Our experiments demonstrate that DP2Unlearning\nachieves similar model performance post-unlearning, compared to an LLM\nretraining from scratch on retained data -- the gold standard exact unlearning\n-- but at approximately half the unlearning cost. In addition, with a\nreasonable computational cost, it outperforms approximate unlearning methods at\nboth preserving the utility of the model post-unlearning and effectively\nforgetting the targeted information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently revolutionized language processing\ntasks but have also brought ethical and legal issues. LLMs have a tendency to\nmemorize potentially private or copyrighted information present in the training\ndata, which might then be delivered to end users at inference time. When this\nhappens, a naive solution is to retrain the model from scratch after excluding\nthe undesired data. Although this guarantees that the target data have been\nforgotten, it is also prohibitively expensive for LLMs. Approximate unlearning\noffers a more efficient alternative, as it consists of ex post modifications of\nthe trained model itself to prevent undesirable results, but it lacks\nforgetting guarantees because it relies solely on empirical evidence. In this\nwork, we present DP2Unlearning, a novel LLM unlearning framework that offers\nformal forgetting guarantees at a significantly lower cost than retraining from\nscratch on the data to be retained. DP2Unlearning involves training LLMs on\ntextual data protected using {\\epsilon}-differential privacy (DP), which later\nenables efficient unlearning with the guarantees against disclosure associated\nwith the chosen {\\epsilon}. Our experiments demonstrate that DP2Unlearning\nachieves similar model performance post-unlearning, compared to an LLM\nretraining from scratch on retained data -- the gold standard exact unlearning\n-- but at approximately half the unlearning cost. In addition, with a\nreasonable computational cost, it outperforms approximate unlearning methods at\nboth preserving the utility of the model post-unlearning and effectively\nforgetting the targeted information."
                },
                "authors": [
                    {
                        "name": "Tamim Al Mahmud"
                    },
                    {
                        "name": "Najeeb Jebreel"
                    },
                    {
                        "name": "Josep Domingo-Ferrer"
                    },
                    {
                        "name": "David Sanchez"
                    }
                ],
                "author_detail": {
                    "name": "David Sanchez"
                },
                "author": "David Sanchez",
                "arxiv_doi": "10.2139/ssrn.5217160",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.2139/ssrn.5217160",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.13774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "49 pages",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13769v1",
                "updated": "2025-04-18T16:11:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    16,
                    11,
                    59,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T16:11:59Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    16,
                    11,
                    59,
                    4,
                    108,
                    0
                ],
                "title": "Detecting Malicious Source Code in PyPI Packages with LLMs: Does RAG\n  Come in Handy?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Malicious Source Code in PyPI Packages with LLMs: Does RAG\n  Come in Handy?"
                },
                "summary": "Malicious software packages in open-source ecosystems, such as PyPI, pose\ngrowing security risks. Unlike traditional vulnerabilities, these packages are\nintentionally designed to deceive users, making detection challenging due to\nevolving attack methods and the lack of structured datasets. In this work, we\nempirically evaluate the effectiveness of Large Language Models (LLMs),\nRetrieval-Augmented Generation (RAG), and few-shot learning for detecting\nmalicious source code. We fine-tune LLMs on curated datasets and integrate YARA\nrules, GitHub Security Advisories, and malicious code snippets with the aim of\nenhancing classification accuracy. We came across a counterintuitive outcome:\nWhile RAG is expected to boost up the prediction performance, it fails in the\nperformed evaluation, obtaining a mediocre accuracy. In contrast, few-shot\nlearning is more effective as it significantly improves the detection of\nmalicious code, achieving 97% accuracy and 95% balanced accuracy, outperforming\ntraditional RAG approaches. Thus, future work should expand structured\nknowledge bases, refine retrieval models, and explore hybrid AI-driven\ncybersecurity solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Malicious software packages in open-source ecosystems, such as PyPI, pose\ngrowing security risks. Unlike traditional vulnerabilities, these packages are\nintentionally designed to deceive users, making detection challenging due to\nevolving attack methods and the lack of structured datasets. In this work, we\nempirically evaluate the effectiveness of Large Language Models (LLMs),\nRetrieval-Augmented Generation (RAG), and few-shot learning for detecting\nmalicious source code. We fine-tune LLMs on curated datasets and integrate YARA\nrules, GitHub Security Advisories, and malicious code snippets with the aim of\nenhancing classification accuracy. We came across a counterintuitive outcome:\nWhile RAG is expected to boost up the prediction performance, it fails in the\nperformed evaluation, obtaining a mediocre accuracy. In contrast, few-shot\nlearning is more effective as it significantly improves the detection of\nmalicious code, achieving 97% accuracy and 95% balanced accuracy, outperforming\ntraditional RAG approaches. Thus, future work should expand structured\nknowledge bases, refine retrieval models, and explore hybrid AI-driven\ncybersecurity solutions."
                },
                "authors": [
                    {
                        "name": "Motunrayo Ibiyo"
                    },
                    {
                        "name": "Thinakone Louangdy"
                    },
                    {
                        "name": "Phuong T. Nguyen"
                    },
                    {
                        "name": "Claudio Di Sipio"
                    },
                    {
                        "name": "Davide Di Ruscio"
                    }
                ],
                "author_detail": {
                    "name": "Davide Di Ruscio"
                },
                "author": "Davide Di Ruscio",
                "arxiv_comment": "The paper has been peer-reviewed and accepted for publication to the\n  29th International Conference on Evaluation and Assessment in Software\n  Engineering (EASE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13765v1",
                "updated": "2025-04-18T16:04:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    16,
                    4,
                    22,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T16:04:22Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    16,
                    4,
                    22,
                    4,
                    108,
                    0
                ],
                "title": "Modeling L1 Influence on L2 Pronunciation: An MFCC-Based Framework for\n  Explainable Machine Learning and Pedagogical Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling L1 Influence on L2 Pronunciation: An MFCC-Based Framework for\n  Explainable Machine Learning and Pedagogical Feedback"
                },
                "summary": "This study investigates the extent to which Mel-Frequency Cepstral\nCoefficients (MFCCs) capture first language (L1) transfer in extended second\nlanguage (L2) English speech. Speech samples from Mandarin and American English\nL1 speakers were extracted from the GMU Speech Accent Archive, converted to WAV\nformat, and processed to obtain thirteen MFCCs per speaker. A multi-method\nanalytic framework combining inferential statistics (t-tests, MANOVA, Canonical\nDiscriminant Analysis) and machine learning (Random Forest classification)\nidentified MFCC-1 (broadband energy), MFCC-2 (first formant region), and MFCC-5\n(voicing and fricative energy) as the most discriminative features for\ndistinguishing L1 backgrounds. A reduced-feature model using these MFCCs\nsignificantly outperformed the full-feature model, as confirmed by McNemar's\ntest and non-overlapping confidence intervals. The findings empirically support\nthe Perceptual Assimilation Model for L2 (PAM-L2) and the Speech Learning Model\n(SLM), demonstrating that L1-conditioned variation in L2 speech is both\nperceptually grounded and acoustically quantifiable. Methodologically, the\nstudy contributes to applied linguistics and explainable AI by proposing a\ntransparent, data-efficient pipeline for L2 pronunciation modeling. The results\nalso offer pedagogical implications for ESL/EFL instruction by highlighting\nL1-specific features that can inform intelligibility-oriented instruction,\ncurriculum design, and speech assessment tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the extent to which Mel-Frequency Cepstral\nCoefficients (MFCCs) capture first language (L1) transfer in extended second\nlanguage (L2) English speech. Speech samples from Mandarin and American English\nL1 speakers were extracted from the GMU Speech Accent Archive, converted to WAV\nformat, and processed to obtain thirteen MFCCs per speaker. A multi-method\nanalytic framework combining inferential statistics (t-tests, MANOVA, Canonical\nDiscriminant Analysis) and machine learning (Random Forest classification)\nidentified MFCC-1 (broadband energy), MFCC-2 (first formant region), and MFCC-5\n(voicing and fricative energy) as the most discriminative features for\ndistinguishing L1 backgrounds. A reduced-feature model using these MFCCs\nsignificantly outperformed the full-feature model, as confirmed by McNemar's\ntest and non-overlapping confidence intervals. The findings empirically support\nthe Perceptual Assimilation Model for L2 (PAM-L2) and the Speech Learning Model\n(SLM), demonstrating that L1-conditioned variation in L2 speech is both\nperceptually grounded and acoustically quantifiable. Methodologically, the\nstudy contributes to applied linguistics and explainable AI by proposing a\ntransparent, data-efficient pipeline for L2 pronunciation modeling. The results\nalso offer pedagogical implications for ESL/EFL instruction by highlighting\nL1-specific features that can inform intelligibility-oriented instruction,\ncurriculum design, and speech assessment tools."
                },
                "authors": [
                    {
                        "name": "Peyman Jahanbin"
                    }
                ],
                "author_detail": {
                    "name": "Peyman Jahanbin"
                },
                "author": "Peyman Jahanbin",
                "arxiv_comment": "27 pages (including references), 4 figures, 1 table. Combines\n  statistical inference and explainable machine learning to model L1 influence\n  in L2 pronunciation using MFCC features. Methodology and code are openly\n  available via Zenodo and OSF: Zenodo: https://doi.org/10.5281/zenodo.15186197\n  OSF: https://doi.org/10.17605/OSF.IO/4UXGM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.5.4; I.2.6; H.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.02849v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.02849v3",
                "updated": "2025-04-18T15:55:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    15,
                    55,
                    11,
                    4,
                    108,
                    0
                ],
                "published": "2023-12-05T16:02:04Z",
                "published_parsed": [
                    2023,
                    12,
                    5,
                    16,
                    2,
                    4,
                    1,
                    339,
                    0
                ],
                "title": "Algorithms for mean-field variational inference via polyhedral\n  optimization in the Wasserstein space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithms for mean-field variational inference via polyhedral\n  optimization in the Wasserstein space"
                },
                "summary": "We develop a theory of finite-dimensional polyhedral subsets over the\nWasserstein space and optimization of functionals over them via first-order\nmethods. Our main application is to the problem of mean-field variational\ninference, which seeks to approximate a distribution $\\pi$ over $\\mathbb{R}^d$\nby a product measure $\\pi^\\star$. When $\\pi$ is strongly log-concave and\nlog-smooth, we provide (1) approximation rates certifying that $\\pi^\\star$ is\nclose to the minimizer $\\pi^\\star_\\diamond$ of the KL divergence over a\n\\emph{polyhedral} set $\\mathcal{P}_\\diamond$, and (2) an algorithm for\nminimizing $\\text{KL}(\\cdot\\|\\pi)$ over $\\mathcal{P}_\\diamond$ based on\naccelerated gradient descent over $\\R^d$. As a byproduct of our analysis, we\nobtain the first end-to-end analysis for gradient-based algorithms for MFVI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a theory of finite-dimensional polyhedral subsets over the\nWasserstein space and optimization of functionals over them via first-order\nmethods. Our main application is to the problem of mean-field variational\ninference, which seeks to approximate a distribution $\\pi$ over $\\mathbb{R}^d$\nby a product measure $\\pi^\\star$. When $\\pi$ is strongly log-concave and\nlog-smooth, we provide (1) approximation rates certifying that $\\pi^\\star$ is\nclose to the minimizer $\\pi^\\star_\\diamond$ of the KL divergence over a\n\\emph{polyhedral} set $\\mathcal{P}_\\diamond$, and (2) an algorithm for\nminimizing $\\text{KL}(\\cdot\\|\\pi)$ over $\\mathcal{P}_\\diamond$ based on\naccelerated gradient descent over $\\R^d$. As a byproduct of our analysis, we\nobtain the first end-to-end analysis for gradient-based algorithms for MFVI."
                },
                "authors": [
                    {
                        "name": "Yiheng Jiang"
                    },
                    {
                        "name": "Sinho Chewi"
                    },
                    {
                        "name": "Aram-Alexandre Pooladian"
                    }
                ],
                "author_detail": {
                    "name": "Aram-Alexandre Pooladian"
                },
                "author": "Aram-Alexandre Pooladian",
                "arxiv_comment": "49 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.02849v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.02849v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19413v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19413v2",
                "updated": "2025-04-18T15:48:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    15,
                    48,
                    1,
                    4,
                    108,
                    0
                ],
                "published": "2025-02-26T18:56:52Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    56,
                    52,
                    2,
                    57,
                    0
                ],
                "title": "Project Alexandria: Towards Freeing Scientific Knowledge from Copyright\n  Burdens via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Project Alexandria: Towards Freeing Scientific Knowledge from Copyright\n  Burdens via LLMs"
                },
                "summary": "Paywalls, licenses and copyright rules often restrict the broad dissemination\nand reuse of scientific knowledge. We take the position that it is both legally\nand technically feasible to extract the scientific knowledge in scholarly\ntexts. Current methods, like text embeddings, fail to reliably preserve factual\ncontent, and simple paraphrasing may not be legally sound. We propose a new\nidea for the community to adopt: convert scholarly documents into knowledge\npreserving, but style agnostic representations we term Knowledge Units using\nLLMs. These units use structured data capturing entities, attributes and\nrelationships without stylistic content. We provide evidence that Knowledge\nUnits (1) form a legally defensible framework for sharing knowledge from\ncopyrighted research texts, based on legal analyses of German copyright law and\nU.S. Fair Use doctrine, and (2) preserve most (~95\\%) factual knowledge from\noriginal text, measured by MCQ performance on facts from the original\ncopyrighted text across four research domains. Freeing scientific knowledge\nfrom copyright promises transformative benefits for scientific research and\neducation by allowing language models to reuse important facts from copyrighted\ntext. To support this, we share open-source tools for converting research\ndocuments into Knowledge Units. Overall, our work posits the feasibility of\ndemocratizing access to scientific knowledge while respecting copyright.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Paywalls, licenses and copyright rules often restrict the broad dissemination\nand reuse of scientific knowledge. We take the position that it is both legally\nand technically feasible to extract the scientific knowledge in scholarly\ntexts. Current methods, like text embeddings, fail to reliably preserve factual\ncontent, and simple paraphrasing may not be legally sound. We propose a new\nidea for the community to adopt: convert scholarly documents into knowledge\npreserving, but style agnostic representations we term Knowledge Units using\nLLMs. These units use structured data capturing entities, attributes and\nrelationships without stylistic content. We provide evidence that Knowledge\nUnits (1) form a legally defensible framework for sharing knowledge from\ncopyrighted research texts, based on legal analyses of German copyright law and\nU.S. Fair Use doctrine, and (2) preserve most (~95\\%) factual knowledge from\noriginal text, measured by MCQ performance on facts from the original\ncopyrighted text across four research domains. Freeing scientific knowledge\nfrom copyright promises transformative benefits for scientific research and\neducation by allowing language models to reuse important facts from copyrighted\ntext. To support this, we share open-source tools for converting research\ndocuments into Knowledge Units. Overall, our work posits the feasibility of\ndemocratizing access to scientific knowledge while respecting copyright."
                },
                "authors": [
                    {
                        "name": "Christoph Schuhmann"
                    },
                    {
                        "name": "Gollam Rabby"
                    },
                    {
                        "name": "Ameya Prabhu"
                    },
                    {
                        "name": "Tawsif Ahmed"
                    },
                    {
                        "name": "Andreas Hochlehnert"
                    },
                    {
                        "name": "Huu Nguyen"
                    },
                    {
                        "name": "Nick Akinci"
                    },
                    {
                        "name": "Ludwig Schmidt"
                    },
                    {
                        "name": "Robert Kaczmarczyk"
                    },
                    {
                        "name": "Sören Auer"
                    },
                    {
                        "name": "Jenia Jitsev"
                    },
                    {
                        "name": "Matthias Bethge"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Bethge"
                },
                "author": "Matthias Bethge",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19413v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19413v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12022v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12022v2",
                "updated": "2025-04-18T15:31:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    15,
                    31,
                    32,
                    4,
                    108,
                    0
                ],
                "published": "2024-08-21T22:29:56Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    22,
                    29,
                    56,
                    2,
                    234,
                    0
                ],
                "title": "Understanding Epistemic Language with a Language-augmented Bayesian\n  Theory of Mind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Epistemic Language with a Language-augmented Bayesian\n  Theory of Mind"
                },
                "summary": "How do people understand and evaluate claims about others' beliefs, even\nthough these beliefs cannot be directly observed? In this paper, we introduce a\ncognitive model of epistemic language interpretation, grounded in Bayesian\ninferences about other agents' goals, beliefs, and intentions: a\nlanguage-augmented Bayesian theory-of-mind (LaBToM). By translating natural\nlanguage into an epistemic ``language-of-thought'' with grammar-constrained LLM\ndecoding, then evaluating these translations against the inferences produced by\ninverting a generative model of rational action and perception, LaBToM captures\ngraded plausibility judgments of epistemic claims. We validate our model in an\nexperiment where participants watch an agent navigate a maze to find keys\nhidden in boxes needed to reach their goal, then rate sentences about the\nagent's beliefs. In contrast with multimodal LLMs (GPT-4o, Gemini Pro) and\nablated models, our model correlates highly with human judgments for a wide\nrange of expressions, including modal language, uncertainty expressions,\nknowledge claims, likelihood comparisons, and attributions of false belief.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How do people understand and evaluate claims about others' beliefs, even\nthough these beliefs cannot be directly observed? In this paper, we introduce a\ncognitive model of epistemic language interpretation, grounded in Bayesian\ninferences about other agents' goals, beliefs, and intentions: a\nlanguage-augmented Bayesian theory-of-mind (LaBToM). By translating natural\nlanguage into an epistemic ``language-of-thought'' with grammar-constrained LLM\ndecoding, then evaluating these translations against the inferences produced by\ninverting a generative model of rational action and perception, LaBToM captures\ngraded plausibility judgments of epistemic claims. We validate our model in an\nexperiment where participants watch an agent navigate a maze to find keys\nhidden in boxes needed to reach their goal, then rate sentences about the\nagent's beliefs. In contrast with multimodal LLMs (GPT-4o, Gemini Pro) and\nablated models, our model correlates highly with human judgments for a wide\nrange of expressions, including modal language, uncertainty expressions,\nknowledge claims, likelihood comparisons, and attributions of false belief."
                },
                "authors": [
                    {
                        "name": "Lance Ying"
                    },
                    {
                        "name": "Tan Zhi-Xuan"
                    },
                    {
                        "name": "Lionel Wong"
                    },
                    {
                        "name": "Vikash Mansinghka"
                    },
                    {
                        "name": "Joshua B. Tenenbaum"
                    }
                ],
                "author_detail": {
                    "name": "Joshua B. Tenenbaum"
                },
                "author": "Joshua B. Tenenbaum",
                "arxiv_comment": "23 pages; Published at the Transactions of the Association for\n  Computational Linguistics (TACL); Presented at NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12022v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12022v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03489v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03489v2",
                "updated": "2025-04-18T15:30:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    15,
                    30,
                    4,
                    4,
                    108,
                    0
                ],
                "published": "2024-11-05T20:13:17Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    20,
                    13,
                    17,
                    1,
                    310,
                    0
                ],
                "title": "A Bayesian nonparametric approach to mediation and spillover effects\n  with multiple mediators in cluster-randomized trials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bayesian nonparametric approach to mediation and spillover effects\n  with multiple mediators in cluster-randomized trials"
                },
                "summary": "Cluster randomized trials (CRTs) with multiple unstructured mediators present\nsignificant methodological challenges for causal inference due to\nwithin-cluster correlation, interference among units, and the complexity\nintroduced by multiple mediators. Existing causal mediation methods often fall\nshort in simultaneously addressing these complexities, particularly in\ndisentangling mediator-specific effects under interference that are central to\nstudying complex mechanisms. To address this gap, we propose new causal\nestimands for spillover mediation effects that differentiate the roles of each\nindividual's own mediator and the spillover effects resulting from interactions\namong individuals within the same cluster. We establish identification results\nfor each estimand and, to flexibly model the complex data structures inherent\nin CRTs, we develop a new Bayesian nonparametric prior -- the Nested Dependent\nDirichlet Process Mixture -- designed for flexibly capture the outcome and\nmediator surfaces at different levels. We conduct extensive simulations across\nvarious scenarios to evaluate the frequentist performance of our methods,\ncompare them with a Bayesian parametric counterpart and illustrate our new\nmethods in an analysis of a completed CRT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cluster randomized trials (CRTs) with multiple unstructured mediators present\nsignificant methodological challenges for causal inference due to\nwithin-cluster correlation, interference among units, and the complexity\nintroduced by multiple mediators. Existing causal mediation methods often fall\nshort in simultaneously addressing these complexities, particularly in\ndisentangling mediator-specific effects under interference that are central to\nstudying complex mechanisms. To address this gap, we propose new causal\nestimands for spillover mediation effects that differentiate the roles of each\nindividual's own mediator and the spillover effects resulting from interactions\namong individuals within the same cluster. We establish identification results\nfor each estimand and, to flexibly model the complex data structures inherent\nin CRTs, we develop a new Bayesian nonparametric prior -- the Nested Dependent\nDirichlet Process Mixture -- designed for flexibly capture the outcome and\nmediator surfaces at different levels. We conduct extensive simulations across\nvarious scenarios to evaluate the frequentist performance of our methods,\ncompare them with a Bayesian parametric counterpart and illustrate our new\nmethods in an analysis of a completed CRT."
                },
                "authors": [
                    {
                        "name": "Yuki Ohnishi"
                    },
                    {
                        "name": "Fan Li"
                    }
                ],
                "author_detail": {
                    "name": "Fan Li"
                },
                "author": "Fan Li",
                "arxiv_comment": "94 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03489v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03489v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13744v1",
                "updated": "2025-04-18T15:18:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    15,
                    18,
                    8,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T15:18:08Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    15,
                    18,
                    8,
                    4,
                    108,
                    0
                ],
                "title": "Observation of gyroscopic coupling in a non-spinning levitated\n  ferromagnet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observation of gyroscopic coupling in a non-spinning levitated\n  ferromagnet"
                },
                "summary": "A non-spinning permanent ferromagnet is predicted to behave as a gyroscope at\nsufficiently low frequencies, which can be seen as a manifestation of the\nEinstein-de Haas effect. This yet unexplored regime has been recently proposed\nfor ultrasensitive precession-based magnetometry and for atomic-like quantum\nstabilization of a levitated nanomagnet in a static field. Here, we observe\nsignatures of gyroscopic effects in the rotational dynamics of a non-spinning\npermanent ferromagnet levitated in a superconducting trap. Specifically, we\ndetect spin-rotation coupling between different librational modes, in good\nagreement with theoretical predictions. From our measurements, we can infer\nboth the intrinsic angular momentum of the levitated magnet and its\ngyromagnetic $g$-factor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A non-spinning permanent ferromagnet is predicted to behave as a gyroscope at\nsufficiently low frequencies, which can be seen as a manifestation of the\nEinstein-de Haas effect. This yet unexplored regime has been recently proposed\nfor ultrasensitive precession-based magnetometry and for atomic-like quantum\nstabilization of a levitated nanomagnet in a static field. Here, we observe\nsignatures of gyroscopic effects in the rotational dynamics of a non-spinning\npermanent ferromagnet levitated in a superconducting trap. Specifically, we\ndetect spin-rotation coupling between different librational modes, in good\nagreement with theoretical predictions. From our measurements, we can infer\nboth the intrinsic angular momentum of the levitated magnet and its\ngyromagnetic $g$-factor."
                },
                "authors": [
                    {
                        "name": "F. Ahrens"
                    },
                    {
                        "name": "A. Vinante"
                    }
                ],
                "author_detail": {
                    "name": "A. Vinante"
                },
                "author": "A. Vinante",
                "arxiv_comment": "5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10957v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10957v2",
                "updated": "2025-04-18T15:14:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    15,
                    14,
                    13,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-15T08:04:39Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    8,
                    4,
                    39,
                    1,
                    105,
                    0
                ],
                "title": "When is Task Vector Provably Effective for Model Editing? A\n  Generalization Analysis of Nonlinear Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When is Task Vector Provably Effective for Model Editing? A\n  Generalization Analysis of Nonlinear Transformers"
                },
                "summary": "Task arithmetic refers to editing the pre-trained model by adding a weighted\nsum of task vectors, each of which is the weight update from the pre-trained\nmodel to fine-tuned models for certain tasks. This approach recently gained\nattention as a computationally efficient inference method for model editing,\ne.g., multi-task learning, forgetting, and out-of-domain generalization\ncapabilities. However, the theoretical understanding of why task vectors can\nexecute various conceptual operations remains limited, due to the highly\nnon-convexity of training Transformer-based models. To the best of our\nknowledge, this paper provides the first theoretical characterization of the\ngeneralization guarantees of task vector methods on nonlinear Transformers. We\nconsider a conceptual learning setting, where each task is a binary\nclassification problem based on a discriminative pattern. We theoretically\nprove the effectiveness of task addition in simultaneously learning a set of\nirrelevant or aligned tasks, as well as the success of task negation in\nunlearning one task from irrelevant or contradictory tasks. Moreover, we prove\nthe proper selection of linear coefficients for task arithmetic to achieve\nguaranteed generalization to out-of-domain tasks. All of our theoretical\nresults hold for both dense-weight parameters and their low-rank\napproximations. Although established in a conceptual setting, our theoretical\nfindings were validated on a practical machine unlearning task using the large\nlanguage model Phi-1.5 (1.3B).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task arithmetic refers to editing the pre-trained model by adding a weighted\nsum of task vectors, each of which is the weight update from the pre-trained\nmodel to fine-tuned models for certain tasks. This approach recently gained\nattention as a computationally efficient inference method for model editing,\ne.g., multi-task learning, forgetting, and out-of-domain generalization\ncapabilities. However, the theoretical understanding of why task vectors can\nexecute various conceptual operations remains limited, due to the highly\nnon-convexity of training Transformer-based models. To the best of our\nknowledge, this paper provides the first theoretical characterization of the\ngeneralization guarantees of task vector methods on nonlinear Transformers. We\nconsider a conceptual learning setting, where each task is a binary\nclassification problem based on a discriminative pattern. We theoretically\nprove the effectiveness of task addition in simultaneously learning a set of\nirrelevant or aligned tasks, as well as the success of task negation in\nunlearning one task from irrelevant or contradictory tasks. Moreover, we prove\nthe proper selection of linear coefficients for task arithmetic to achieve\nguaranteed generalization to out-of-domain tasks. All of our theoretical\nresults hold for both dense-weight parameters and their low-rank\napproximations. Although established in a conceptual setting, our theoretical\nfindings were validated on a practical machine unlearning task using the large\nlanguage model Phi-1.5 (1.3B)."
                },
                "authors": [
                    {
                        "name": "Hongkang Li"
                    },
                    {
                        "name": "Yihua Zhang"
                    },
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Sijia Liu"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Pin-Yu Chen"
                },
                "author": "Pin-Yu Chen",
                "arxiv_comment": "Published at ICLR 2025 as an oral paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10957v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10957v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13736v1",
                "updated": "2025-04-18T15:04:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    15,
                    4,
                    53,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T15:04:53Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    15,
                    4,
                    53,
                    4,
                    108,
                    0
                ],
                "title": "LimitNet: Progressive, Content-Aware Image Offloading for Extremely Weak\n  Devices & Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LimitNet: Progressive, Content-Aware Image Offloading for Extremely Weak\n  Devices & Networks"
                },
                "summary": "IoT devices have limited hardware capabilities and are often deployed in\nremote areas. Consequently, advanced vision models surpass such devices'\nprocessing and storage capabilities, requiring offloading of such tasks to the\ncloud. However, remote areas often rely on LPWANs technology with limited\nbandwidth, high packet loss rates, and extremely low duty cycles, which makes\nfast offloading for time-sensitive inference challenging. Today's approaches,\nwhich are deployable on weak devices, generate a non-progressive bit stream,\nand therefore, their decoding quality suffers strongly when data is only\npartially available on the cloud at a deadline due to limited bandwidth or\npacket losses.\n  In this paper, we introduce LimitNet, a progressive, content-aware image\ncompression model designed for extremely weak devices and networks. LimitNet's\nlightweight progressive encoder prioritizes critical data during transmission\nbased on the content of the image, which gives the cloud the opportunity to run\ninference even with partial data availability.\n  Experimental results demonstrate that LimitNet, on average, compared to SOTA,\nachieves 14.01 p.p. (percentage point) higher accuracy on ImageNet1000, 18.01\npp on CIFAR100, and 0.1 higher mAP@0.5 on COCO. Also, on average, LimitNet\nsaves 61.24% bandwidth on ImageNet1000, 83.68% on CIFAR100, and 42.25% on the\nCOCO dataset compared to SOTA, while it only has 4% more encoding time compared\nto JPEG (with a fixed quality) on STM32F7 (Cortex-M7).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IoT devices have limited hardware capabilities and are often deployed in\nremote areas. Consequently, advanced vision models surpass such devices'\nprocessing and storage capabilities, requiring offloading of such tasks to the\ncloud. However, remote areas often rely on LPWANs technology with limited\nbandwidth, high packet loss rates, and extremely low duty cycles, which makes\nfast offloading for time-sensitive inference challenging. Today's approaches,\nwhich are deployable on weak devices, generate a non-progressive bit stream,\nand therefore, their decoding quality suffers strongly when data is only\npartially available on the cloud at a deadline due to limited bandwidth or\npacket losses.\n  In this paper, we introduce LimitNet, a progressive, content-aware image\ncompression model designed for extremely weak devices and networks. LimitNet's\nlightweight progressive encoder prioritizes critical data during transmission\nbased on the content of the image, which gives the cloud the opportunity to run\ninference even with partial data availability.\n  Experimental results demonstrate that LimitNet, on average, compared to SOTA,\nachieves 14.01 p.p. (percentage point) higher accuracy on ImageNet1000, 18.01\npp on CIFAR100, and 0.1 higher mAP@0.5 on COCO. Also, on average, LimitNet\nsaves 61.24% bandwidth on ImageNet1000, 83.68% on CIFAR100, and 42.25% on the\nCOCO dataset compared to SOTA, while it only has 4% more encoding time compared\nto JPEG (with a fixed quality) on STM32F7 (Cortex-M7)."
                },
                "authors": [
                    {
                        "name": "Ali Hojjat"
                    },
                    {
                        "name": "Janek Haberer"
                    },
                    {
                        "name": "Tayyaba Zainab"
                    },
                    {
                        "name": "Olaf Landsiedel"
                    }
                ],
                "author_detail": {
                    "name": "Olaf Landsiedel"
                },
                "author": "Olaf Landsiedel",
                "arxiv_doi": "10.1145/3643832.3661856",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3643832.3661856",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.13736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This is the author's accepted manuscript. The Version of Record is\n  available at: https://doi.org/10.1145/3643832.3661856",
                "arxiv_journal_ref": "In Proceedings of the 22nd ACM International Conference on Mobile\n  Systems, Applications, and Services (MobiSys '24), June 3-7, 2024, Minato-ku,\n  Tokyo, Japan. ACM, New York, NY, USA",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13733v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13733v1",
                "updated": "2025-04-18T15:02:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    15,
                    2,
                    6,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T15:02:06Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    15,
                    2,
                    6,
                    4,
                    108,
                    0
                ],
                "title": "Dynamic Regularized CBDT: Variance-Calibrated Causal Boosting for\n  Interpretable Heterogeneous Treatment Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Regularized CBDT: Variance-Calibrated Causal Boosting for\n  Interpretable Heterogeneous Treatment Effects"
                },
                "summary": "Heterogeneous treatment effect estimation in high-stakes applications demands\nmodels that simultaneously optimize precision, interpretability, and\ncalibration. Many existing tree-based causal inference techniques, however,\nexhibit high estimation errors when applied to observational data because they\nstruggle to capture complex interactions among factors and rely on static\nregularization schemes. In this work, we propose Dynamic Regularized Causal\nBoosted Decision Trees (CBDT), a novel framework that integrates variance\nregularization and average treatment effect calibration into the loss function\nof gradient boosted decision trees. Our approach dynamically updates the\nregularization parameters using gradient statistics to better balance the\nbias-variance tradeoff. Extensive experiments on standard benchmark datasets\nand real-world clinical data demonstrate that the proposed method significantly\nimproves estimation accuracy while maintaining reliable coverage of true\ntreatment effects. In an intensive care unit patient triage study, the method\nsuccessfully identified clinically actionable rules and achieved high accuracy\nin treatment effect estimation. The results validate that dynamic\nregularization can effectively tighten error bounds and enhance both predictive\nperformance and model interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous treatment effect estimation in high-stakes applications demands\nmodels that simultaneously optimize precision, interpretability, and\ncalibration. Many existing tree-based causal inference techniques, however,\nexhibit high estimation errors when applied to observational data because they\nstruggle to capture complex interactions among factors and rely on static\nregularization schemes. In this work, we propose Dynamic Regularized Causal\nBoosted Decision Trees (CBDT), a novel framework that integrates variance\nregularization and average treatment effect calibration into the loss function\nof gradient boosted decision trees. Our approach dynamically updates the\nregularization parameters using gradient statistics to better balance the\nbias-variance tradeoff. Extensive experiments on standard benchmark datasets\nand real-world clinical data demonstrate that the proposed method significantly\nimproves estimation accuracy while maintaining reliable coverage of true\ntreatment effects. In an intensive care unit patient triage study, the method\nsuccessfully identified clinically actionable rules and achieved high accuracy\nin treatment effect estimation. The results validate that dynamic\nregularization can effectively tighten error bounds and enhance both predictive\nperformance and model interpretability."
                },
                "authors": [
                    {
                        "name": "Yichen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yichen Liu"
                },
                "author": "Yichen Liu",
                "arxiv_comment": "Preprint version. 13 pages, 4 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13733v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13733v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05, 62H12, 90C30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; G.3; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13730v1",
                "updated": "2025-04-18T14:57:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    14,
                    57,
                    7,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T14:57:07Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    14,
                    57,
                    7,
                    4,
                    108,
                    0
                ],
                "title": "Controlled Territory and Conflict Tracking (CONTACT): (Geo-)Mapping\n  Occupied Territory from Open Source Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlled Territory and Conflict Tracking (CONTACT): (Geo-)Mapping\n  Occupied Territory from Open Source Intelligence"
                },
                "summary": "Open-source intelligence provides a stream of unstructured textual data that\ncan inform assessments of territorial control. We present CONTACT, a framework\nfor territorial control prediction using large language models (LLMs) and\nminimal supervision. We evaluate two approaches: SetFit, an embedding-based\nfew-shot classifier, and a prompt tuning method applied to BLOOMZ-560m, a\nmultilingual generative LLM. Our model is trained on a small hand-labeled\ndataset of news articles covering ISIS activity in Syria and Iraq, using\nprompt-conditioned extraction of control-relevant signals such as military\noperations, casualties, and location references. We show that the BLOOMZ-based\nmodel outperforms the SetFit baseline, and that prompt-based supervision\nimproves generalization in low-resource settings. CONTACT demonstrates that\nLLMs fine-tuned using few-shot methods can reduce annotation burdens and\nsupport structured inference from open-ended OSINT streams. Our code is\navailable at https://github.com/PaulKMandal/CONTACT/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source intelligence provides a stream of unstructured textual data that\ncan inform assessments of territorial control. We present CONTACT, a framework\nfor territorial control prediction using large language models (LLMs) and\nminimal supervision. We evaluate two approaches: SetFit, an embedding-based\nfew-shot classifier, and a prompt tuning method applied to BLOOMZ-560m, a\nmultilingual generative LLM. Our model is trained on a small hand-labeled\ndataset of news articles covering ISIS activity in Syria and Iraq, using\nprompt-conditioned extraction of control-relevant signals such as military\noperations, casualties, and location references. We show that the BLOOMZ-based\nmodel outperforms the SetFit baseline, and that prompt-based supervision\nimproves generalization in low-resource settings. CONTACT demonstrates that\nLLMs fine-tuned using few-shot methods can reduce annotation burdens and\nsupport structured inference from open-ended OSINT streams. Our code is\navailable at https://github.com/PaulKMandal/CONTACT/."
                },
                "authors": [
                    {
                        "name": "Paul K. Mandal"
                    },
                    {
                        "name": "Cole Leo"
                    },
                    {
                        "name": "Connor Hurley"
                    }
                ],
                "author_detail": {
                    "name": "Connor Hurley"
                },
                "author": "Connor Hurley",
                "arxiv_comment": "7 pages, 1 figure, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; I.2.8; H.3.1; K.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09024v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09024v3",
                "updated": "2025-04-18T14:30:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    14,
                    30,
                    31,
                    4,
                    108,
                    0
                ],
                "published": "2024-10-11T17:39:22Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    39,
                    22,
                    4,
                    285,
                    0
                ],
                "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents"
                },
                "summary": "The robustness of LLMs to jailbreak attacks, where users design prompts to\ncircumvent safety measures and misuse model capabilities, has been studied\nprimarily for LLMs acting as simple chatbots. Meanwhile, LLM agents -- which\nuse external tools and can execute multi-stage tasks -- may pose a greater risk\nif misused, but their robustness remains underexplored. To facilitate research\non LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark\nincludes a diverse set of 110 explicitly malicious agent tasks (440 with\naugmentations), covering 11 harm categories including fraud, cybercrime, and\nharassment. In addition to measuring whether models refuse harmful agentic\nrequests, scoring well on AgentHarm requires jailbroken agents to maintain\ntheir capabilities following an attack to complete a multi-step task. We\nevaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly\ncompliant with malicious agent requests without jailbreaking, (2) simple\nuniversal jailbreak templates can be adapted to effectively jailbreak agents,\nand (3) these jailbreaks enable coherent and malicious multi-step agent\nbehavior and retain model capabilities. To enable simple and reliable\nevaluation of attacks and defenses for LLM-based agents, we publicly release\nAgentHarm at https://huggingface.co/datasets/ai-safety-institute/AgentHarm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The robustness of LLMs to jailbreak attacks, where users design prompts to\ncircumvent safety measures and misuse model capabilities, has been studied\nprimarily for LLMs acting as simple chatbots. Meanwhile, LLM agents -- which\nuse external tools and can execute multi-stage tasks -- may pose a greater risk\nif misused, but their robustness remains underexplored. To facilitate research\non LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark\nincludes a diverse set of 110 explicitly malicious agent tasks (440 with\naugmentations), covering 11 harm categories including fraud, cybercrime, and\nharassment. In addition to measuring whether models refuse harmful agentic\nrequests, scoring well on AgentHarm requires jailbroken agents to maintain\ntheir capabilities following an attack to complete a multi-step task. We\nevaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly\ncompliant with malicious agent requests without jailbreaking, (2) simple\nuniversal jailbreak templates can be adapted to effectively jailbreak agents,\nand (3) these jailbreaks enable coherent and malicious multi-step agent\nbehavior and retain model capabilities. To enable simple and reliable\nevaluation of attacks and defenses for LLM-based agents, we publicly release\nAgentHarm at https://huggingface.co/datasets/ai-safety-institute/AgentHarm."
                },
                "authors": [
                    {
                        "name": "Maksym Andriushchenko"
                    },
                    {
                        "name": "Alexandra Souly"
                    },
                    {
                        "name": "Mateusz Dziemian"
                    },
                    {
                        "name": "Derek Duenas"
                    },
                    {
                        "name": "Maxwell Lin"
                    },
                    {
                        "name": "Justin Wang"
                    },
                    {
                        "name": "Dan Hendrycks"
                    },
                    {
                        "name": "Andy Zou"
                    },
                    {
                        "name": "Zico Kolter"
                    },
                    {
                        "name": "Matt Fredrikson"
                    },
                    {
                        "name": "Eric Winsor"
                    },
                    {
                        "name": "Jerome Wynne"
                    },
                    {
                        "name": "Yarin Gal"
                    },
                    {
                        "name": "Xander Davies"
                    }
                ],
                "author_detail": {
                    "name": "Xander Davies"
                },
                "author": "Xander Davies",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09024v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09024v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13707v1",
                "updated": "2025-04-18T14:11:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    14,
                    11,
                    27,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T14:11:27Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    14,
                    11,
                    27,
                    4,
                    108,
                    0
                ],
                "title": "OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via\n  Open-ended Interaction Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via\n  Open-ended Interaction Simulation"
                },
                "summary": "As the general capabilities of large language models (LLMs) improve and agent\napplications become more widespread, the underlying deception risks urgently\nrequire systematic evaluation and effective oversight. Unlike existing\nevaluation which uses simulated games or presents limited choices, we introduce\nOpenDeception, a novel deception evaluation framework with an open-ended\nscenario dataset. OpenDeception jointly evaluates both the deception intention\nand capabilities of LLM-based agents by inspecting their internal reasoning\nprocess. Specifically, we construct five types of common use cases where LLMs\nintensively interact with the user, each consisting of ten diverse, concrete\nscenarios from the real world. To avoid ethical concerns and costs of high-risk\ndeceptive interactions with human testers, we propose to simulate the\nmulti-turn dialogue via agent simulation. Extensive evaluation of eleven\nmainstream LLMs on OpenDeception highlights the urgent need to address\ndeception risks and security concerns in LLM-based agents: the deception\nintention ratio across the models exceeds 80%, while the deception success rate\nsurpasses 50%. Furthermore, we observe that LLMs with stronger capabilities do\nexhibit a higher risk of deception, which calls for more alignment efforts on\ninhibiting deceptive behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the general capabilities of large language models (LLMs) improve and agent\napplications become more widespread, the underlying deception risks urgently\nrequire systematic evaluation and effective oversight. Unlike existing\nevaluation which uses simulated games or presents limited choices, we introduce\nOpenDeception, a novel deception evaluation framework with an open-ended\nscenario dataset. OpenDeception jointly evaluates both the deception intention\nand capabilities of LLM-based agents by inspecting their internal reasoning\nprocess. Specifically, we construct five types of common use cases where LLMs\nintensively interact with the user, each consisting of ten diverse, concrete\nscenarios from the real world. To avoid ethical concerns and costs of high-risk\ndeceptive interactions with human testers, we propose to simulate the\nmulti-turn dialogue via agent simulation. Extensive evaluation of eleven\nmainstream LLMs on OpenDeception highlights the urgent need to address\ndeception risks and security concerns in LLM-based agents: the deception\nintention ratio across the models exceeds 80%, while the deception success rate\nsurpasses 50%. Furthermore, we observe that LLMs with stronger capabilities do\nexhibit a higher risk of deception, which calls for more alignment efforts on\ninhibiting deceptive behaviors."
                },
                "authors": [
                    {
                        "name": "Yichen Wu"
                    },
                    {
                        "name": "Xudong Pan"
                    },
                    {
                        "name": "Geng Hong"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13701v1",
                "updated": "2025-04-18T14:02:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    14,
                    2,
                    29,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T14:02:29Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    14,
                    2,
                    29,
                    4,
                    108,
                    0
                ],
                "title": "Inverse Inference on Cooperative Control of Networked Dynamical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse Inference on Cooperative Control of Networked Dynamical Systems"
                },
                "summary": "Recent years have witnessed the rapid advancement of understanding the\ncontrol mechanism of networked dynamical systems (NDSs), which are governed by\ncomponents such as nodal dynamics and topology. This paper reveals that the\ncritical components in continuous-time state feedback cooperative control of\nNDSs can be inferred merely from discrete observations. In particular, we\nadvocate a bi-level inference framework to estimate the global closed-loop\nsystem and extract the components, respectively. The novelty lies in bridging\nthe gap from discrete observations to the continuous-time model and effectively\ndecoupling the concerned components. Specifically, in the first level, we\ndesign a causality-based estimator for the discrete-time closed-loop system\nmatrix, which can achieve asymptotically unbiased performance when the NDS is\nstable. In the second level, we introduce a matrix logarithm based method to\nrecover the continuous-time counterpart matrix, providing new sampling period\nguarantees and establishing the recovery error bound. By utilizing graph\nproperties of the NDS, we develop least square based procedures to decouple the\nconcerned components with up to a scalar ambiguity. Furthermore, we employ\ninverse optimal control techniques to reconstruct the objective function\ndriving the control process, deriving necessary conditions for the solutions.\nNumerical simulations demonstrate the effectiveness of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed the rapid advancement of understanding the\ncontrol mechanism of networked dynamical systems (NDSs), which are governed by\ncomponents such as nodal dynamics and topology. This paper reveals that the\ncritical components in continuous-time state feedback cooperative control of\nNDSs can be inferred merely from discrete observations. In particular, we\nadvocate a bi-level inference framework to estimate the global closed-loop\nsystem and extract the components, respectively. The novelty lies in bridging\nthe gap from discrete observations to the continuous-time model and effectively\ndecoupling the concerned components. Specifically, in the first level, we\ndesign a causality-based estimator for the discrete-time closed-loop system\nmatrix, which can achieve asymptotically unbiased performance when the NDS is\nstable. In the second level, we introduce a matrix logarithm based method to\nrecover the continuous-time counterpart matrix, providing new sampling period\nguarantees and establishing the recovery error bound. By utilizing graph\nproperties of the NDS, we develop least square based procedures to decouple the\nconcerned components with up to a scalar ambiguity. Furthermore, we employ\ninverse optimal control techniques to reconstruct the objective function\ndriving the control process, deriving necessary conditions for the solutions.\nNumerical simulations demonstrate the effectiveness of the proposed method."
                },
                "authors": [
                    {
                        "name": "Yushan Li"
                    },
                    {
                        "name": "Jianping He"
                    },
                    {
                        "name": "Dimos V. Dimarogonas"
                    }
                ],
                "author_detail": {
                    "name": "Dimos V. Dimarogonas"
                },
                "author": "Dimos V. Dimarogonas",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11005v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11005v3",
                "updated": "2025-04-18T14:01:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    14,
                    1,
                    42,
                    4,
                    108,
                    0
                ],
                "published": "2024-02-16T18:28:43Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    18,
                    28,
                    43,
                    4,
                    47,
                    0
                ],
                "title": "A Theory of LLM Sampling: Part Descriptive and Part Prescriptive",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Theory of LLM Sampling: Part Descriptive and Part Prescriptive"
                },
                "summary": "Large Language Models (LLMs) are increasingly utilized in autonomous\ndecision-making, where they sample options from vast action spaces. However,\nthe heuristics that guide this sampling process remain under-explored. We study\nthis sampling behavior and show that this underlying heuristics resembles that\nof human decision-making: comprising a descriptive component (reflecting\nstatistical norm) and a prescriptive component (implicit ideal encoded in the\nLLM) of a concept. We show that this deviation of a sample from the statistical\nnorm towards a prescriptive component consistently appears in concepts across\ndiverse real-world domains like public health, and economic trends. To further\nillustrate the theory, we demonstrate that concept prototypes in LLMs are\naffected by prescriptive norms, similar to the concept of normality in humans.\nThrough case studies and comparison with human studies, we illustrate that in\nreal-world applications, the shift of samples toward an ideal value in LLMs'\noutputs can result in significantly biased decision-making, raising ethical\nconcerns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly utilized in autonomous\ndecision-making, where they sample options from vast action spaces. However,\nthe heuristics that guide this sampling process remain under-explored. We study\nthis sampling behavior and show that this underlying heuristics resembles that\nof human decision-making: comprising a descriptive component (reflecting\nstatistical norm) and a prescriptive component (implicit ideal encoded in the\nLLM) of a concept. We show that this deviation of a sample from the statistical\nnorm towards a prescriptive component consistently appears in concepts across\ndiverse real-world domains like public health, and economic trends. To further\nillustrate the theory, we demonstrate that concept prototypes in LLMs are\naffected by prescriptive norms, similar to the concept of normality in humans.\nThrough case studies and comparison with human studies, we illustrate that in\nreal-world applications, the shift of samples toward an ideal value in LLMs'\noutputs can result in significantly biased decision-making, raising ethical\nconcerns."
                },
                "authors": [
                    {
                        "name": "Sarath Sivaprasad"
                    },
                    {
                        "name": "Pramod Kaushik"
                    },
                    {
                        "name": "Sahar Abdelnabi"
                    },
                    {
                        "name": "Mario Fritz"
                    }
                ],
                "author_detail": {
                    "name": "Mario Fritz"
                },
                "author": "Mario Fritz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11005v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11005v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13700v1",
                "updated": "2025-04-18T14:00:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    14,
                    0,
                    55,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T14:00:55Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    14,
                    0,
                    55,
                    4,
                    108,
                    0
                ],
                "title": "Exploring Multimodal Prompt for Visualization Authoring with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Multimodal Prompt for Visualization Authoring with Large\n  Language Models"
                },
                "summary": "Recent advances in large language models (LLMs) have shown great potential in\nautomating the process of visualization authoring through simple natural\nlanguage utterances. However, instructing LLMs using natural language is\nlimited in precision and expressiveness for conveying visualization intent,\nleading to misinterpretation and time-consuming iterations. To address these\nlimitations, we conduct an empirical study to understand how LLMs interpret\nambiguous or incomplete text prompts in the context of visualization authoring,\nand the conditions making LLMs misinterpret user intent. Informed by the\nfindings, we introduce visual prompts as a complementary input modality to text\nprompts, which help clarify user intent and improve LLMs' interpretation\nabilities. To explore the potential of multimodal prompting in visualization\nauthoring, we design VisPilot, which enables users to easily create\nvisualizations using multimodal prompts, including text, sketches, and direct\nmanipulations on existing visualizations. Through two case studies and a\ncontrolled user study, we demonstrate that VisPilot provides a more intuitive\nway to create visualizations without affecting the overall task efficiency\ncompared to text-only prompting approaches. Furthermore, we analyze the impact\nof text and visual prompts in different visualization tasks. Our findings\nhighlight the importance of multimodal prompting in improving the usability of\nLLMs for visualization authoring. We discuss design implications for future\nvisualization systems and provide insights into how multimodal prompts can\nenhance human-AI collaboration in creative visualization tasks. All materials\nare available at https://OSF.IO/2QRAK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have shown great potential in\nautomating the process of visualization authoring through simple natural\nlanguage utterances. However, instructing LLMs using natural language is\nlimited in precision and expressiveness for conveying visualization intent,\nleading to misinterpretation and time-consuming iterations. To address these\nlimitations, we conduct an empirical study to understand how LLMs interpret\nambiguous or incomplete text prompts in the context of visualization authoring,\nand the conditions making LLMs misinterpret user intent. Informed by the\nfindings, we introduce visual prompts as a complementary input modality to text\nprompts, which help clarify user intent and improve LLMs' interpretation\nabilities. To explore the potential of multimodal prompting in visualization\nauthoring, we design VisPilot, which enables users to easily create\nvisualizations using multimodal prompts, including text, sketches, and direct\nmanipulations on existing visualizations. Through two case studies and a\ncontrolled user study, we demonstrate that VisPilot provides a more intuitive\nway to create visualizations without affecting the overall task efficiency\ncompared to text-only prompting approaches. Furthermore, we analyze the impact\nof text and visual prompts in different visualization tasks. Our findings\nhighlight the importance of multimodal prompting in improving the usability of\nLLMs for visualization authoring. We discuss design implications for future\nvisualization systems and provide insights into how multimodal prompts can\nenhance human-AI collaboration in creative visualization tasks. All materials\nare available at https://OSF.IO/2QRAK."
                },
                "authors": [
                    {
                        "name": "Zhen Wen"
                    },
                    {
                        "name": "Luoxuan Weng"
                    },
                    {
                        "name": "Yinghao Tang"
                    },
                    {
                        "name": "Runjin Zhang"
                    },
                    {
                        "name": "Yuxin Liu"
                    },
                    {
                        "name": "Bo Pan"
                    },
                    {
                        "name": "Minfeng Zhu"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "arxiv_comment": "11 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13684v1",
                "updated": "2025-04-18T13:35:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    35,
                    21,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T13:35:21Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    35,
                    21,
                    4,
                    108,
                    0
                ],
                "title": "Intelligent Interaction Strategies for Context-Aware Cognitive\n  Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Interaction Strategies for Context-Aware Cognitive\n  Augmentation"
                },
                "summary": "Human cognition is constrained by processing limitations, leading to\ncognitive overload and inefficiencies in knowledge synthesis and\ndecision-making. Large Language Models (LLMs) present an opportunity for\ncognitive augmentation, but their current reactive nature limits their\nreal-world applicability. This position paper explores the potential of\ncontext-aware cognitive augmentation, where LLMs dynamically adapt to users'\ncognitive states and task environments to provide appropriate support. Through\na think-aloud study in an exhibition setting, we examine how individuals\ninteract with multi-modal information and identify key cognitive challenges in\nstructuring, retrieving, and applying knowledge. Our findings highlight the\nneed for AI-driven cognitive support systems that integrate real-time\ncontextual awareness, personalized reasoning assistance, and socially adaptive\ninteractions. We propose a framework for AI augmentation that seamlessly\ntransitions between real-time cognitive support and post-experience knowledge\norganization, contributing to the design of more effective human-centered AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human cognition is constrained by processing limitations, leading to\ncognitive overload and inefficiencies in knowledge synthesis and\ndecision-making. Large Language Models (LLMs) present an opportunity for\ncognitive augmentation, but their current reactive nature limits their\nreal-world applicability. This position paper explores the potential of\ncontext-aware cognitive augmentation, where LLMs dynamically adapt to users'\ncognitive states and task environments to provide appropriate support. Through\na think-aloud study in an exhibition setting, we examine how individuals\ninteract with multi-modal information and identify key cognitive challenges in\nstructuring, retrieving, and applying knowledge. Our findings highlight the\nneed for AI-driven cognitive support systems that integrate real-time\ncontextual awareness, personalized reasoning assistance, and socially adaptive\ninteractions. We propose a framework for AI augmentation that seamlessly\ntransitions between real-time cognitive support and post-experience knowledge\norganization, contributing to the design of more effective human-centered AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Xiangrong"
                    },
                    {
                        "name": "Zhu"
                    },
                    {
                        "name": "Yuan Xu"
                    },
                    {
                        "name": "Tianjian Liu"
                    },
                    {
                        "name": "Jingwei Sun"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Xin Tong"
                    }
                ],
                "author_detail": {
                    "name": "Xin Tong"
                },
                "arxiv_affiliation": "Daniel",
                "author": "Xin Tong",
                "arxiv_comment": "Presented at the 2025 ACM Workshop on Human-AI Interaction for\n  Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING",
                "arxiv_journal_ref": "Proceedings of the 2025 ACM CHI Workshop on Human-AI Interaction\n  for Augmented Reasoning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.13410v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.13410v3",
                "updated": "2025-04-18T13:18:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    18,
                    34,
                    4,
                    108,
                    0
                ],
                "published": "2023-11-22T14:13:31Z",
                "published_parsed": [
                    2023,
                    11,
                    22,
                    14,
                    13,
                    31,
                    2,
                    326,
                    0
                ],
                "title": "Navigating Unmeasured Confounding in Quantitative Sociology: A\n  Sensitivity Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating Unmeasured Confounding in Quantitative Sociology: A\n  Sensitivity Framework"
                },
                "summary": "Unmeasured confounding remains a critical challenge in causal inference for\nthe social sciences. This paper proposes a sensitivity analysis framework to\nsystematically evaluate how unmeasured confounders influence statistical\ninference in sociology. Given these sensitivity analysis methods, we introduce\na five-step workflow that integrates sensitivity analysis into research design\nrather than treating it as a post-hoc robustness check. Using the Blau and\nDuncan (1967) study as an empirical example, we demonstrate how different\nsensitivity methods provide complementary insights. By extending existing\nframeworks, we show how sensitivity analysis enhances causal transparency,\noffering a practical tool for assessing uncertainty in observational research.\nOur approach contributes to a more rigorous application of causal inference in\nsociology, bridging gaps between theory, identification strategies, and\nstatistical modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmeasured confounding remains a critical challenge in causal inference for\nthe social sciences. This paper proposes a sensitivity analysis framework to\nsystematically evaluate how unmeasured confounders influence statistical\ninference in sociology. Given these sensitivity analysis methods, we introduce\na five-step workflow that integrates sensitivity analysis into research design\nrather than treating it as a post-hoc robustness check. Using the Blau and\nDuncan (1967) study as an empirical example, we demonstrate how different\nsensitivity methods provide complementary insights. By extending existing\nframeworks, we show how sensitivity analysis enhances causal transparency,\noffering a practical tool for assessing uncertainty in observational research.\nOur approach contributes to a more rigorous application of causal inference in\nsociology, bridging gaps between theory, identification strategies, and\nstatistical modeling."
                },
                "authors": [
                    {
                        "name": "Cheng Lin"
                    },
                    {
                        "name": "Jose M. Pena"
                    },
                    {
                        "name": "Adel Daoud"
                    }
                ],
                "author_detail": {
                    "name": "Adel Daoud"
                },
                "author": "Adel Daoud",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.13410v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.13410v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13677v1",
                "updated": "2025-04-18T13:13:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    13,
                    42,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T13:13:42Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    13,
                    42,
                    4,
                    108,
                    0
                ],
                "title": "Revisiting Uncertainty Quantification Evaluation in Language Models:\n  Spurious Interactions with Response Length Bias Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Uncertainty Quantification Evaluation in Language Models:\n  Spurious Interactions with Response Length Bias Results"
                },
                "summary": "Uncertainty Quantification (UQ) in Language Models (LMs) is crucial for\nimproving their safety and reliability. Evaluations often use performance\nmetrics like AUROC to assess how well UQ methods (e.g., negative sequence\nprobabilities) correlate with task correctness functions (e.g., ROUGE-L). In\nthis paper, we show that commonly used correctness functions bias UQ\nevaluations by inflating the performance of certain UQ methods. We evaluate 7\ncorrectness functions -- from lexical-based and embedding-based metrics to\nLLM-as-a-judge approaches -- across 4 datasets x 4 models x 6 UQ methods. Our\nanalysis reveals that length biases in the errors of these correctness\nfunctions distort UQ assessments by interacting with length biases in UQ\nmethods. We identify LLM-as-a-judge approaches as among the least length-biased\nchoices and hence a potential solution to mitigate these biases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty Quantification (UQ) in Language Models (LMs) is crucial for\nimproving their safety and reliability. Evaluations often use performance\nmetrics like AUROC to assess how well UQ methods (e.g., negative sequence\nprobabilities) correlate with task correctness functions (e.g., ROUGE-L). In\nthis paper, we show that commonly used correctness functions bias UQ\nevaluations by inflating the performance of certain UQ methods. We evaluate 7\ncorrectness functions -- from lexical-based and embedding-based metrics to\nLLM-as-a-judge approaches -- across 4 datasets x 4 models x 6 UQ methods. Our\nanalysis reveals that length biases in the errors of these correctness\nfunctions distort UQ assessments by interacting with length biases in UQ\nmethods. We identify LLM-as-a-judge approaches as among the least length-biased\nchoices and hence a potential solution to mitigate these biases."
                },
                "authors": [
                    {
                        "name": "Andrea Santilli"
                    },
                    {
                        "name": "Adam Golinski"
                    },
                    {
                        "name": "Michael Kirchhof"
                    },
                    {
                        "name": "Federico Danieli"
                    },
                    {
                        "name": "Arno Blaas"
                    },
                    {
                        "name": "Miao Xiong"
                    },
                    {
                        "name": "Luca Zappella"
                    },
                    {
                        "name": "Sinead Williamson"
                    }
                ],
                "author_detail": {
                    "name": "Sinead Williamson"
                },
                "author": "Sinead Williamson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12319v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12319v4",
                "updated": "2025-04-18T13:12:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    12,
                    42,
                    4,
                    108,
                    0
                ],
                "published": "2024-06-18T06:43:04Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    6,
                    43,
                    4,
                    1,
                    170,
                    0
                ],
                "title": "The Comparative Trap: Pairwise Comparisons Amplifies Biased Preferences\n  of LLM Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Comparative Trap: Pairwise Comparisons Amplifies Biased Preferences\n  of LLM Evaluators"
                },
                "summary": "As large language models (LLMs) are increasingly used as evaluators for\nnatural language generation tasks, ensuring unbiased assessments is essential.\nHowever, LLM evaluators often display biased preferences, such as favoring\nverbosity and authoritative tones. Our empirical analysis reveals that these\nbiases are exacerbated in pairwise evaluation, where LLMs directly compare two\noutputs and easily prioritize superficial attributes. In contrast, pointwise\nevaluation, which assesses outputs independently, is less susceptible to such\nbias because each output is judged in isolation. To address the limitations of\nthe pairwise evaluation, we introduce a novel evaluation method, PRePair, which\nintegrates pointwise reasoning within a pairwise framework. PRePair effectively\nalleviates biased preference, improving performance on the adversarial\nbenchmark (LLMBar) while outperforming pointwise evaluation on the standard\nbenchmark (MT-Bench).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly used as evaluators for\nnatural language generation tasks, ensuring unbiased assessments is essential.\nHowever, LLM evaluators often display biased preferences, such as favoring\nverbosity and authoritative tones. Our empirical analysis reveals that these\nbiases are exacerbated in pairwise evaluation, where LLMs directly compare two\noutputs and easily prioritize superficial attributes. In contrast, pointwise\nevaluation, which assesses outputs independently, is less susceptible to such\nbias because each output is judged in isolation. To address the limitations of\nthe pairwise evaluation, we introduce a novel evaluation method, PRePair, which\nintegrates pointwise reasoning within a pairwise framework. PRePair effectively\nalleviates biased preference, improving performance on the adversarial\nbenchmark (LLMBar) while outperforming pointwise evaluation on the standard\nbenchmark (MT-Bench)."
                },
                "authors": [
                    {
                        "name": "Hawon Jeong"
                    },
                    {
                        "name": "ChaeHun Park"
                    },
                    {
                        "name": "Jimin Hong"
                    },
                    {
                        "name": "Hojoon Lee"
                    },
                    {
                        "name": "Jaegul Choo"
                    }
                ],
                "author_detail": {
                    "name": "Jaegul Choo"
                },
                "author": "Jaegul Choo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12319v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12319v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12307v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12307v4",
                "updated": "2025-04-18T13:07:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    7,
                    8,
                    4,
                    108,
                    0
                ],
                "published": "2024-06-18T06:28:06Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    6,
                    28,
                    6,
                    1,
                    170,
                    0
                ],
                "title": "Can Tool-augmented Large Language Models be Aware of Incomplete\n  Conditions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Tool-augmented Large Language Models be Aware of Incomplete\n  Conditions?"
                },
                "summary": "Recent advancements in integrating large language models (LLMs) with tools\nhave allowed the models to interact with real-world environments. However,\nthese tool-augmented LLMs often encounter incomplete scenarios when users\nprovide partial information or the necessary tools are unavailable. Recognizing\nand managing such scenarios is crucial for LLMs to ensure their reliability,\nbut this exploration remains understudied. This study examines whether LLMs can\nidentify incomplete conditions and appropriately determine when to refrain from\nusing tools. To this end, we address a dataset by manipulating instances from\ntwo datasets by removing necessary tools or essential information for tool\ninvocation. Our experiments show that LLMs often struggle to identify the\nabsence of information required to utilize specific tools and recognize the\nabsence of appropriate tools. We further analyze model behaviors in different\nenvironments and compare their performance against humans. Our research can\ncontribute to advancing reliable LLMs by addressing common scenarios during\ninteractions between humans and LLMs. Our code and dataset will be publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in integrating large language models (LLMs) with tools\nhave allowed the models to interact with real-world environments. However,\nthese tool-augmented LLMs often encounter incomplete scenarios when users\nprovide partial information or the necessary tools are unavailable. Recognizing\nand managing such scenarios is crucial for LLMs to ensure their reliability,\nbut this exploration remains understudied. This study examines whether LLMs can\nidentify incomplete conditions and appropriately determine when to refrain from\nusing tools. To this end, we address a dataset by manipulating instances from\ntwo datasets by removing necessary tools or essential information for tool\ninvocation. Our experiments show that LLMs often struggle to identify the\nabsence of information required to utilize specific tools and recognize the\nabsence of appropriate tools. We further analyze model behaviors in different\nenvironments and compare their performance against humans. Our research can\ncontribute to advancing reliable LLMs by addressing common scenarios during\ninteractions between humans and LLMs. Our code and dataset will be publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Seungbin Yang"
                    },
                    {
                        "name": "ChaeHun Park"
                    },
                    {
                        "name": "Taehee Kim"
                    },
                    {
                        "name": "Jaegul Choo"
                    }
                ],
                "author_detail": {
                    "name": "Jaegul Choo"
                },
                "author": "Jaegul Choo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12307v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12307v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13667v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13667v1",
                "updated": "2025-04-18T13:01:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    1,
                    27,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T13:01:27Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    1,
                    27,
                    4,
                    108,
                    0
                ],
                "title": "Large Language Models Will Change The Way Children Think About\n  Technology And Impact Every Interaction Paradigm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Will Change The Way Children Think About\n  Technology And Impact Every Interaction Paradigm"
                },
                "summary": "This paper presents a hopeful perspective on the potentially dramatic impacts\nof Large Language Models on how we children learn and how they will expect to\ninteract with technology. We review the effects of LLMs on education so far,\nand make the case that these effects are minor compared to the upcoming changes\nthat are occurring. We present a small scenario and self-ethnographic study\ndemonstrating the effects of these changes, and define five significant\nconsiderations that interactive systems designers will have to accommodate in\nthe future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a hopeful perspective on the potentially dramatic impacts\nof Large Language Models on how we children learn and how they will expect to\ninteract with technology. We review the effects of LLMs on education so far,\nand make the case that these effects are minor compared to the upcoming changes\nthat are occurring. We present a small scenario and self-ethnographic study\ndemonstrating the effects of these changes, and define five significant\nconsiderations that interactive systems designers will have to accommodate in\nthe future."
                },
                "authors": [
                    {
                        "name": "Russell Beale"
                    }
                ],
                "author_detail": {
                    "name": "Russell Beale"
                },
                "author": "Russell Beale",
                "arxiv_comment": "Accepted for IDC 2025. Citation: Russell Beale. 2025. Large Language\n  Models Will Change The Way Children Think About Technology And Impact Every\n  Interaction Paradigm. In Proceedings of Interaction Design and Children\n  Conference (IDC2025). ACM, New York, NY, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13667v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13667v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13665v1",
                "updated": "2025-04-18T12:52:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    12,
                    52,
                    50,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T12:52:50Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    12,
                    52,
                    50,
                    4,
                    108,
                    0
                ],
                "title": "Modeling Bounded Count Environmental Data Using a Contaminated\n  Beta-Binomial Regression Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Bounded Count Environmental Data Using a Contaminated\n  Beta-Binomial Regression Model"
                },
                "summary": "This paper investigates two environmental applications related to climate\nchange, where observations consist of bounded counts. The binomial and\nbeta-binomial (BB) models are commonly used for bounded count data, with the BB\nmodel offering the advantage of accounting for potential overdispersion.\nHowever, extreme observations in real-world applications may hinder the\nperformance of the BB model and lead to misleading inferences. To address this\nissue, we propose the contaminated beta-binomial (cBB) distribution (cBB-D),\nwhich provides the necessary flexibility to accommodate extreme observations.\nThe cBB model accounts for overdispersion and extreme values while maintaining\nthe mean and variance properties of the BB distribution. The availability of\ncovariates that improve inference about the mean of the bounded count variable\nmotivates the further proposal of the cBB regression model (cBB-RM). Different\nversions of the cBB-RM model - where none, some, or all of the cBB parameters\nare regressed on available covariates - are fitted to the datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates two environmental applications related to climate\nchange, where observations consist of bounded counts. The binomial and\nbeta-binomial (BB) models are commonly used for bounded count data, with the BB\nmodel offering the advantage of accounting for potential overdispersion.\nHowever, extreme observations in real-world applications may hinder the\nperformance of the BB model and lead to misleading inferences. To address this\nissue, we propose the contaminated beta-binomial (cBB) distribution (cBB-D),\nwhich provides the necessary flexibility to accommodate extreme observations.\nThe cBB model accounts for overdispersion and extreme values while maintaining\nthe mean and variance properties of the BB distribution. The availability of\ncovariates that improve inference about the mean of the bounded count variable\nmotivates the further proposal of the cBB regression model (cBB-RM). Different\nversions of the cBB-RM model - where none, some, or all of the cBB parameters\nare regressed on available covariates - are fitted to the datasets."
                },
                "authors": [
                    {
                        "name": "Arnoldus F. Otto"
                    },
                    {
                        "name": "Antonio Punzo"
                    },
                    {
                        "name": "Johannes T. Ferreira"
                    },
                    {
                        "name": "Andriëtte Bekker"
                    },
                    {
                        "name": "Salvatorie D. Tomarchio"
                    },
                    {
                        "name": "Cristina Tortora"
                    }
                ],
                "author_detail": {
                    "name": "Cristina Tortora"
                },
                "arxiv_affiliation": "Department of Mathematics and Statistics, San José State University, California, United States of America",
                "author": "Cristina Tortora",
                "arxiv_comment": "25 pages, 7 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03503v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03503v2",
                "updated": "2025-04-18T12:50:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    12,
                    50,
                    30,
                    4,
                    108,
                    0
                ],
                "published": "2024-12-04T17:45:21Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    17,
                    45,
                    21,
                    2,
                    339,
                    0
                ],
                "title": "Reducing nuisance prior sensitivity via non-linear reparameterization,\n  with application to EFT analyses of large-scale structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing nuisance prior sensitivity via non-linear reparameterization,\n  with application to EFT analyses of large-scale structure"
                },
                "summary": "Many physical models contain nuisance parameters that quantify unknown\nproperties of an experiment that are not of primary relevance. Typically, these\ncannot be measured except by fitting the models to the data from the\nexperiment, requiring simultaneous measurement of interesting parameters that\nare our target of inference and nuisance terms that are not directly of\ninterest. A recent example of this is fitting Effective Field Theory (EFT)\nmodels to large-scale structure (LSS) data to make cosmological inferences.\nThese models have a large number of nuisance parameters that are typically\ncorrelated with cosmological parameters in the posterior, leading to strong\ndependence on the nuisance parameter priors. We introduce a reparametrization\nmethod that leverages Generalized Additive Models (GAMs) to decorrelate\nnuisance parameters from the parameters of interest in the likelihood, even in\nthe presence of non-linear relationships. This reparametrization forms a\nnatural basis within which to define priors that are independent between\nnuisance and target parameters: the separation means that the marginal\nposterior for cosmological parameters does not depend on simple priors placed\non nuisance terms. In application to EFT models using LSS data, we demonstrate\nthat the proposed approach leads to robust cosmological inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many physical models contain nuisance parameters that quantify unknown\nproperties of an experiment that are not of primary relevance. Typically, these\ncannot be measured except by fitting the models to the data from the\nexperiment, requiring simultaneous measurement of interesting parameters that\nare our target of inference and nuisance terms that are not directly of\ninterest. A recent example of this is fitting Effective Field Theory (EFT)\nmodels to large-scale structure (LSS) data to make cosmological inferences.\nThese models have a large number of nuisance parameters that are typically\ncorrelated with cosmological parameters in the posterior, leading to strong\ndependence on the nuisance parameter priors. We introduce a reparametrization\nmethod that leverages Generalized Additive Models (GAMs) to decorrelate\nnuisance parameters from the parameters of interest in the likelihood, even in\nthe presence of non-linear relationships. This reparametrization forms a\nnatural basis within which to define priors that are independent between\nnuisance and target parameters: the separation means that the marginal\nposterior for cosmological parameters does not depend on simple priors placed\non nuisance terms. In application to EFT models using LSS data, we demonstrate\nthat the proposed approach leads to robust cosmological inference."
                },
                "authors": [
                    {
                        "name": "S. Paradiso"
                    },
                    {
                        "name": "M. Bonici"
                    },
                    {
                        "name": "M. Chen"
                    },
                    {
                        "name": "W. J. Percival"
                    },
                    {
                        "name": "G. D'Amico"
                    },
                    {
                        "name": "H. Zhang"
                    },
                    {
                        "name": "G. McGee"
                    }
                ],
                "author_detail": {
                    "name": "G. McGee"
                },
                "author": "G. McGee",
                "arxiv_comment": "24 pages, 9 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03503v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03503v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13656v1",
                "updated": "2025-04-18T12:37:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    12,
                    37,
                    2,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T12:37:02Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    12,
                    37,
                    2,
                    4,
                    108,
                    0
                ],
                "title": "Do Prompt Patterns Affect Code Quality? A First Empirical Assessment of\n  ChatGPT-Generated Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Prompt Patterns Affect Code Quality? A First Empirical Assessment of\n  ChatGPT-Generated Code"
                },
                "summary": "Large Language Models (LLMs) have rapidly transformed software development,\nespecially in code generation. However, their inconsistent performance, prone\nto hallucinations and quality issues, complicates program comprehension and\nhinders maintainability. Research indicates that prompt engineering-the\npractice of designing inputs to direct LLMs toward generating relevant\noutputs-may help address these challenges. In this regard, researchers have\nintroduced prompt patterns, structured templates intended to guide users in\nformulating their requests. However, the influence of prompt patterns on code\nquality has yet to be thoroughly investigated. An improved understanding of\nthis relationship would be essential to advancing our collective knowledge on\nhow to effectively use LLMs for code generation, thereby enhancing their\nunderstandability in contemporary software development. This paper empirically\ninvestigates the impact of prompt patterns on code quality, specifically\nmaintainability, security, and reliability, using the Dev-GPT dataset. Results\nshow that Zero-Shot prompting is most common, followed by Zero-Shot with\nChain-of-Thought and Few-Shot. Analysis of 7583 code files across quality\nmetrics revealed minimal issues, with Kruskal-Wallis tests indicating no\nsignificant differences among patterns, suggesting that prompt structure may\nnot substantially impact these quality metrics in ChatGPT-assisted code\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have rapidly transformed software development,\nespecially in code generation. However, their inconsistent performance, prone\nto hallucinations and quality issues, complicates program comprehension and\nhinders maintainability. Research indicates that prompt engineering-the\npractice of designing inputs to direct LLMs toward generating relevant\noutputs-may help address these challenges. In this regard, researchers have\nintroduced prompt patterns, structured templates intended to guide users in\nformulating their requests. However, the influence of prompt patterns on code\nquality has yet to be thoroughly investigated. An improved understanding of\nthis relationship would be essential to advancing our collective knowledge on\nhow to effectively use LLMs for code generation, thereby enhancing their\nunderstandability in contemporary software development. This paper empirically\ninvestigates the impact of prompt patterns on code quality, specifically\nmaintainability, security, and reliability, using the Dev-GPT dataset. Results\nshow that Zero-Shot prompting is most common, followed by Zero-Shot with\nChain-of-Thought and Few-Shot. Analysis of 7583 code files across quality\nmetrics revealed minimal issues, with Kruskal-Wallis tests indicating no\nsignificant differences among patterns, suggesting that prompt structure may\nnot substantially impact these quality metrics in ChatGPT-assisted code\ngeneration."
                },
                "authors": [
                    {
                        "name": "Antonio Della Porta"
                    },
                    {
                        "name": "Stefano Lambiase"
                    },
                    {
                        "name": "Fabio Palomba"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Palomba"
                },
                "author": "Fabio Palomba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19874v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19874v3",
                "updated": "2025-04-18T12:31:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    12,
                    31,
                    18,
                    4,
                    108,
                    0
                ],
                "published": "2024-05-30T09:28:56Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    9,
                    28,
                    56,
                    3,
                    151,
                    0
                ],
                "title": "Is In-Context Learning Sufficient for Instruction Following in LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is In-Context Learning Sufficient for Instruction Following in LLMs?"
                },
                "summary": "In-context learning (ICL) allows LLMs to learn from examples without changing\ntheir weights: this is a particularly promising capability for long-context\nLLMs that can potentially learn from many examples. Recently, Lin et al. (2024)\nproposed URIAL, a method using only three in-context examples to align base\nLLMs, achieving non-trivial instruction following performance. In this work, we\nshow that, while effective, ICL alignment with URIAL still underperforms\ncompared to instruction fine-tuning on the established benchmark MT-Bench,\nespecially with more capable base LLMs. We then uncover the most relevant\nelements for successful in-context alignment, finding the crucial role of the\ndecoding parameters. Based on these insights, we show that the approach of\nURIAL can indeed be improved by adding high-quality, potentially carefully\nselected via greedy search, demonstrations in context, getting closer to the\nperformance of instruct models. Finally, we provide the first, to our\nknowledge, systematic comparison of ICL and instruction fine-tuning (IFT) for\ninstruction following in the low data regime, where ICL can be a viable\nalternative to IFT. Overall, our work advances the understanding of ICL as an\nalignment technique and its relationship to IFT. We provide our code at\nhttps://github.com/tml-epfl/icl-alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) allows LLMs to learn from examples without changing\ntheir weights: this is a particularly promising capability for long-context\nLLMs that can potentially learn from many examples. Recently, Lin et al. (2024)\nproposed URIAL, a method using only three in-context examples to align base\nLLMs, achieving non-trivial instruction following performance. In this work, we\nshow that, while effective, ICL alignment with URIAL still underperforms\ncompared to instruction fine-tuning on the established benchmark MT-Bench,\nespecially with more capable base LLMs. We then uncover the most relevant\nelements for successful in-context alignment, finding the crucial role of the\ndecoding parameters. Based on these insights, we show that the approach of\nURIAL can indeed be improved by adding high-quality, potentially carefully\nselected via greedy search, demonstrations in context, getting closer to the\nperformance of instruct models. Finally, we provide the first, to our\nknowledge, systematic comparison of ICL and instruction fine-tuning (IFT) for\ninstruction following in the low data regime, where ICL can be a viable\nalternative to IFT. Overall, our work advances the understanding of ICL as an\nalignment technique and its relationship to IFT. We provide our code at\nhttps://github.com/tml-epfl/icl-alignment."
                },
                "authors": [
                    {
                        "name": "Hao Zhao"
                    },
                    {
                        "name": "Maksym Andriushchenko"
                    },
                    {
                        "name": "Francesco Croce"
                    },
                    {
                        "name": "Nicolas Flammarion"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Flammarion"
                },
                "author": "Nicolas Flammarion",
                "arxiv_comment": "Accepted at ICLR 2025. This camera-ready version v3 adds multi-turn\n  alignment via ICL, revisiting main results on instruct models, and simple\n  mechanistic study. Updates in the v2: experiment with decoding schemes,\n  scaling in-context alignment, ICL vs IFT for instruction following. Code at\n  https://github.com/tml-epfl/icl-alignment",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19874v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19874v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13647v1",
                "updated": "2025-04-18T11:59:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    11,
                    59,
                    34,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T11:59:34Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    11,
                    59,
                    34,
                    4,
                    108,
                    0
                ],
                "title": "Lightweight LiDAR-Camera 3D Dynamic Object Detection and Multi-Class\n  Trajectory Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight LiDAR-Camera 3D Dynamic Object Detection and Multi-Class\n  Trajectory Prediction"
                },
                "summary": "Service mobile robots are often required to avoid dynamic objects while\nperforming their tasks, but they usually have only limited computational\nresources. So we present a lightweight multi-modal framework for 3D object\ndetection and trajectory prediction. Our system synergistically integrates\nLiDAR and camera inputs to achieve real-time perception of pedestrians,\nvehicles, and riders in 3D space. The framework proposes two novel modules: 1)\na Cross-Modal Deformable Transformer (CMDT) for object detection with high\naccuracy and acceptable amount of computation, and 2) a Reference\nTrajectory-based Multi-Class Transformer (RTMCT) for efficient and diverse\ntrajectory prediction of mult-class objects with flexible trajectory lengths.\nEvaluations on the CODa benchmark demonstrate superior performance over\nexisting methods across detection (+2.03% in mAP) and trajectory prediction\n(-0.408m in minADE5 of pedestrians) metrics. Remarkably, the system exhibits\nexceptional deployability - when implemented on a wheelchair robot with an\nentry-level NVIDIA 3060 GPU, it achieves real-time inference at 13.2 fps. To\nfacilitate reproducibility and practical deployment, we release the related\ncode of the method at https://github.com/TossherO/3D_Perception and its ROS\ninference version at https://github.com/TossherO/ros_packages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Service mobile robots are often required to avoid dynamic objects while\nperforming their tasks, but they usually have only limited computational\nresources. So we present a lightweight multi-modal framework for 3D object\ndetection and trajectory prediction. Our system synergistically integrates\nLiDAR and camera inputs to achieve real-time perception of pedestrians,\nvehicles, and riders in 3D space. The framework proposes two novel modules: 1)\na Cross-Modal Deformable Transformer (CMDT) for object detection with high\naccuracy and acceptable amount of computation, and 2) a Reference\nTrajectory-based Multi-Class Transformer (RTMCT) for efficient and diverse\ntrajectory prediction of mult-class objects with flexible trajectory lengths.\nEvaluations on the CODa benchmark demonstrate superior performance over\nexisting methods across detection (+2.03% in mAP) and trajectory prediction\n(-0.408m in minADE5 of pedestrians) metrics. Remarkably, the system exhibits\nexceptional deployability - when implemented on a wheelchair robot with an\nentry-level NVIDIA 3060 GPU, it achieves real-time inference at 13.2 fps. To\nfacilitate reproducibility and practical deployment, we release the related\ncode of the method at https://github.com/TossherO/3D_Perception and its ROS\ninference version at https://github.com/TossherO/ros_packages."
                },
                "authors": [
                    {
                        "name": "Yushen He"
                    },
                    {
                        "name": "Lei Zhao"
                    },
                    {
                        "name": "Tianchen Deng"
                    },
                    {
                        "name": "Zipeng Fang"
                    },
                    {
                        "name": "Weidong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weidong Chen"
                },
                "author": "Weidong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13645v1",
                "updated": "2025-04-18T11:52:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    11,
                    52,
                    21,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T11:52:21Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    11,
                    52,
                    21,
                    4,
                    108,
                    0
                ],
                "title": "Efficient Parameter Adaptation for Multi-Modal Medical Image\n  Segmentation and Prognosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Parameter Adaptation for Multi-Modal Medical Image\n  Segmentation and Prognosis"
                },
                "summary": "Cancer detection and prognosis relies heavily on medical imaging,\nparticularly CT and PET scans. Deep Neural Networks (DNNs) have shown promise\nin tumor segmentation by fusing information from these modalities. However, a\ncritical bottleneck exists: the dependency on CT-PET data concurrently for\ntraining and inference, posing a challenge due to the limited availability of\nPET scans. Hence, there is a clear need for a flexible and efficient framework\nthat can be trained with the widely available CT scans and can be still adapted\nfor PET scans when they become available. In this work, we propose a\nparameter-efficient multi-modal adaptation (PEMMA) framework for lightweight\nupgrading of a transformer-based segmentation model trained only on CT scans\nsuch that it can be efficiently adapted for use with PET scans when they become\navailable. This framework is further extended to perform prognosis task\nmaintaining the same efficient cross-modal fine-tuning approach. The proposed\napproach is tested with two well-known segementation backbones, namely UNETR\nand Swin UNETR. Our approach offers two main advantages. Firstly, we leverage\nthe inherent modularity of the transformer architecture and perform low-rank\nadaptation (LoRA) as well as decomposed low-rank adaptation (DoRA) of the\nattention weights to achieve parameter-efficient adaptation. Secondly, by\nminimizing cross-modal entanglement, PEMMA allows updates using only one\nmodality without causing catastrophic forgetting in the other. Our method\nachieves comparable performance to early fusion, but with only 8% of the\ntrainable parameters, and demonstrates a significant +28% Dice score\nimprovement on PET scans when trained with a single modality. Furthermore, in\nprognosis, our method improves the concordance index by +10% when adapting a\nCT-pretrained model to include PET scans, and by +23% when adapting for both\nPET and EHR data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cancer detection and prognosis relies heavily on medical imaging,\nparticularly CT and PET scans. Deep Neural Networks (DNNs) have shown promise\nin tumor segmentation by fusing information from these modalities. However, a\ncritical bottleneck exists: the dependency on CT-PET data concurrently for\ntraining and inference, posing a challenge due to the limited availability of\nPET scans. Hence, there is a clear need for a flexible and efficient framework\nthat can be trained with the widely available CT scans and can be still adapted\nfor PET scans when they become available. In this work, we propose a\nparameter-efficient multi-modal adaptation (PEMMA) framework for lightweight\nupgrading of a transformer-based segmentation model trained only on CT scans\nsuch that it can be efficiently adapted for use with PET scans when they become\navailable. This framework is further extended to perform prognosis task\nmaintaining the same efficient cross-modal fine-tuning approach. The proposed\napproach is tested with two well-known segementation backbones, namely UNETR\nand Swin UNETR. Our approach offers two main advantages. Firstly, we leverage\nthe inherent modularity of the transformer architecture and perform low-rank\nadaptation (LoRA) as well as decomposed low-rank adaptation (DoRA) of the\nattention weights to achieve parameter-efficient adaptation. Secondly, by\nminimizing cross-modal entanglement, PEMMA allows updates using only one\nmodality without causing catastrophic forgetting in the other. Our method\nachieves comparable performance to early fusion, but with only 8% of the\ntrainable parameters, and demonstrates a significant +28% Dice score\nimprovement on PET scans when trained with a single modality. Furthermore, in\nprognosis, our method improves the concordance index by +10% when adapting a\nCT-pretrained model to include PET scans, and by +23% when adapting for both\nPET and EHR data."
                },
                "authors": [
                    {
                        "name": "Numan Saeed"
                    },
                    {
                        "name": "Shahad Hardan"
                    },
                    {
                        "name": "Muhammad Ridzuan"
                    },
                    {
                        "name": "Nada Saadi"
                    },
                    {
                        "name": "Karthik Nandakumar"
                    },
                    {
                        "name": "Mohammad Yaqub"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Yaqub"
                },
                "author": "Mohammad Yaqub",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13644v1",
                "updated": "2025-04-18T11:50:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    11,
                    50,
                    30,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T11:50:30Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    11,
                    50,
                    30,
                    4,
                    108,
                    0
                ],
                "title": "Exploring the Potential for Large Language Models to Demonstrate\n  Rational Probabilistic Beliefs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Potential for Large Language Models to Demonstrate\n  Rational Probabilistic Beliefs"
                },
                "summary": "Advances in the general capabilities of large language models (LLMs) have led\nto their use for information retrieval, and as components in automated decision\nsystems. A faithful representation of probabilistic reasoning in these models\nmay be essential to ensure trustworthy, explainable and effective performance\nin these tasks. Despite previous work suggesting that LLMs can perform complex\nreasoning and well-calibrated uncertainty quantification, we find that current\nversions of this class of model lack the ability to provide rational and\ncoherent representations of probabilistic beliefs. To demonstrate this, we\nintroduce a novel dataset of claims with indeterminate truth values and apply a\nnumber of well-established techniques for uncertainty quantification to measure\nthe ability of LLM's to adhere to fundamental properties of probabilistic\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in the general capabilities of large language models (LLMs) have led\nto their use for information retrieval, and as components in automated decision\nsystems. A faithful representation of probabilistic reasoning in these models\nmay be essential to ensure trustworthy, explainable and effective performance\nin these tasks. Despite previous work suggesting that LLMs can perform complex\nreasoning and well-calibrated uncertainty quantification, we find that current\nversions of this class of model lack the ability to provide rational and\ncoherent representations of probabilistic beliefs. To demonstrate this, we\nintroduce a novel dataset of claims with indeterminate truth values and apply a\nnumber of well-established techniques for uncertainty quantification to measure\nthe ability of LLM's to adhere to fundamental properties of probabilistic\nreasoning."
                },
                "authors": [
                    {
                        "name": "Gabriel Freedman"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13643v1",
                "updated": "2025-04-18T11:48:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    11,
                    48,
                    55,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T11:48:55Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    11,
                    48,
                    55,
                    4,
                    108,
                    0
                ],
                "title": "Simulating Before Planning: Constructing Intrinsic User World Model for\n  User-Tailored Dialogue Policy Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating Before Planning: Constructing Intrinsic User World Model for\n  User-Tailored Dialogue Policy Planning"
                },
                "summary": "Recent advancements in dialogue policy planning have emphasized optimizing\nsystem agent policies to achieve predefined goals, focusing on strategy design,\ntrajectory acquisition, and efficient training paradigms. However, these\napproaches often overlook the critical role of user characteristics, which are\nessential in real-world scenarios like conversational search and\nrecommendation, where interactions must adapt to individual user traits such as\npersonality, preferences, and goals. To address this gap, we first conduct a\ncomprehensive study utilizing task-specific user personas to systematically\nassess dialogue policy planning under diverse user behaviors. By leveraging\nrealistic user profiles for different tasks, our study reveals significant\nlimitations in existing approaches, highlighting the need for user-tailored\ndialogue policy planning. Building on this foundation, we present the\nUser-Tailored Dialogue Policy Planning (UDP) framework, which incorporates an\nIntrinsic User World Model to model user traits and feedback. UDP operates in\nthree stages: (1) User Persona Portraying, using a diffusion model to\ndynamically infer user profiles; (2) User Feedback Anticipating, leveraging a\nBrownian Bridge-inspired anticipator to predict user reactions; and (3)\nUser-Tailored Policy Planning, integrating these insights to optimize response\nstrategies. To ensure robust performance, we further propose an active learning\napproach that prioritizes challenging user personas during training.\nComprehensive experiments on benchmarks, including collaborative and\nnon-collaborative settings, demonstrate the effectiveness of UDP in learning\nuser-specific dialogue strategies. Results validate the protocol's utility and\nhighlight UDP's robustness, adaptability, and potential to advance user-centric\ndialogue systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in dialogue policy planning have emphasized optimizing\nsystem agent policies to achieve predefined goals, focusing on strategy design,\ntrajectory acquisition, and efficient training paradigms. However, these\napproaches often overlook the critical role of user characteristics, which are\nessential in real-world scenarios like conversational search and\nrecommendation, where interactions must adapt to individual user traits such as\npersonality, preferences, and goals. To address this gap, we first conduct a\ncomprehensive study utilizing task-specific user personas to systematically\nassess dialogue policy planning under diverse user behaviors. By leveraging\nrealistic user profiles for different tasks, our study reveals significant\nlimitations in existing approaches, highlighting the need for user-tailored\ndialogue policy planning. Building on this foundation, we present the\nUser-Tailored Dialogue Policy Planning (UDP) framework, which incorporates an\nIntrinsic User World Model to model user traits and feedback. UDP operates in\nthree stages: (1) User Persona Portraying, using a diffusion model to\ndynamically infer user profiles; (2) User Feedback Anticipating, leveraging a\nBrownian Bridge-inspired anticipator to predict user reactions; and (3)\nUser-Tailored Policy Planning, integrating these insights to optimize response\nstrategies. To ensure robust performance, we further propose an active learning\napproach that prioritizes challenging user personas during training.\nComprehensive experiments on benchmarks, including collaborative and\nnon-collaborative settings, demonstrate the effectiveness of UDP in learning\nuser-specific dialogue strategies. Results validate the protocol's utility and\nhighlight UDP's robustness, adaptability, and potential to advance user-centric\ndialogue systems."
                },
                "authors": [
                    {
                        "name": "Tao He"
                    },
                    {
                        "name": "Lizi Liao"
                    },
                    {
                        "name": "Ming Liu"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "arxiv_comment": "11 pages, 6 figures, SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02079v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02079v3",
                "updated": "2025-04-18T11:20:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    11,
                    20,
                    24,
                    4,
                    108,
                    0
                ],
                "published": "2024-05-03T13:12:28Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    13,
                    12,
                    28,
                    4,
                    124,
                    0
                ],
                "title": "Argumentative Large Language Models for Explainable and Contestable\n  Claim Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Argumentative Large Language Models for Explainable and Contestable\n  Claim Verification"
                },
                "summary": "The profusion of knowledge encoded in large language models (LLMs) and their\nability to apply this knowledge zero-shot in a range of settings makes them\npromising candidates for use in decision-making. However, they are currently\nlimited by their inability to provide outputs which can be faithfully explained\nand effectively contested to correct mistakes. In this paper, we attempt to\nreconcile these strengths and weaknesses by introducing \\emph{argumentative\nLLMs (ArgLLMs)}, a method for augmenting LLMs with argumentative reasoning.\nConcretely, ArgLLMs construct argumentation frameworks, which then serve as the\nbasis for formal reasoning in support of decision-making. The interpretable\nnature of these argumentation frameworks and formal reasoning means that any\ndecision made by ArgLLMs may be explained and contested. We evaluate ArgLLMs'\nperformance experimentally in comparison with state-of-the-art techniques, in\nthe context of the decision-making task of claim verification. We also define\nnovel properties to characterise contestability and assess ArgLLMs formally in\nterms of these properties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The profusion of knowledge encoded in large language models (LLMs) and their\nability to apply this knowledge zero-shot in a range of settings makes them\npromising candidates for use in decision-making. However, they are currently\nlimited by their inability to provide outputs which can be faithfully explained\nand effectively contested to correct mistakes. In this paper, we attempt to\nreconcile these strengths and weaknesses by introducing \\emph{argumentative\nLLMs (ArgLLMs)}, a method for augmenting LLMs with argumentative reasoning.\nConcretely, ArgLLMs construct argumentation frameworks, which then serve as the\nbasis for formal reasoning in support of decision-making. The interpretable\nnature of these argumentation frameworks and formal reasoning means that any\ndecision made by ArgLLMs may be explained and contested. We evaluate ArgLLMs'\nperformance experimentally in comparison with state-of-the-art techniques, in\nthe context of the decision-making task of claim verification. We also define\nnovel properties to characterise contestability and assess ArgLLMs formally in\nterms of these properties."
                },
                "authors": [
                    {
                        "name": "Gabriel Freedman"
                    },
                    {
                        "name": "Adam Dejl"
                    },
                    {
                        "name": "Deniz Gorur"
                    },
                    {
                        "name": "Xiang Yin"
                    },
                    {
                        "name": "Antonio Rago"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "arxiv_comment": "18 pages, 18 figures. Accepted as an oral presentation at AAAI 2025",
                "arxiv_journal_ref": "Proceedings of the AAAI Conference on Artificial Intelligence,\n  39(14), 14930-14939. 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02079v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02079v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13630v1",
                "updated": "2025-04-18T11:11:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    11,
                    11,
                    14,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T11:11:14Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    11,
                    11,
                    14,
                    4,
                    108,
                    0
                ],
                "title": "Remedy: Learning Machine Translation Evaluation from Human Preferences\n  with Reward Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remedy: Learning Machine Translation Evaluation from Human Preferences\n  with Reward Modeling"
                },
                "summary": "A key challenge in MT evaluation is the inherent noise and inconsistency of\nhuman ratings. Regression-based neural metrics struggle with this noise, while\nprompting LLMs shows promise at system-level evaluation but performs poorly at\nsegment level. In this work, we propose ReMedy, a novel MT metric framework\nthat reformulates translation evaluation as a reward modeling task. Instead of\nregressing on imperfect human ratings directly, ReMedy learns relative\ntranslation quality using pairwise preference data, resulting in a more\nreliable evaluation. In extensive experiments across WMT22-24 shared tasks (39\nlanguage pairs, 111 MT systems), ReMedy achieves state-of-the-art performance\nat both segment- and system-level evaluation. Specifically, ReMedy-9B surpasses\nlarger WMT winners and massive closed LLMs such as MetricX-13B,\nXCOMET-Ensemble, GEMBA-GPT-4, PaLM-540B, and finetuned PaLM2. Further analyses\ndemonstrate that ReMedy delivers superior capability in detecting translation\nerrors and evaluating low-quality translations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key challenge in MT evaluation is the inherent noise and inconsistency of\nhuman ratings. Regression-based neural metrics struggle with this noise, while\nprompting LLMs shows promise at system-level evaluation but performs poorly at\nsegment level. In this work, we propose ReMedy, a novel MT metric framework\nthat reformulates translation evaluation as a reward modeling task. Instead of\nregressing on imperfect human ratings directly, ReMedy learns relative\ntranslation quality using pairwise preference data, resulting in a more\nreliable evaluation. In extensive experiments across WMT22-24 shared tasks (39\nlanguage pairs, 111 MT systems), ReMedy achieves state-of-the-art performance\nat both segment- and system-level evaluation. Specifically, ReMedy-9B surpasses\nlarger WMT winners and massive closed LLMs such as MetricX-13B,\nXCOMET-Ensemble, GEMBA-GPT-4, PaLM-540B, and finetuned PaLM2. Further analyses\ndemonstrate that ReMedy delivers superior capability in detecting translation\nerrors and evaluating low-quality translations."
                },
                "authors": [
                    {
                        "name": "Shaomu Tan"
                    },
                    {
                        "name": "Christof Monz"
                    }
                ],
                "author_detail": {
                    "name": "Christof Monz"
                },
                "author": "Christof Monz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13629v1",
                "updated": "2025-04-18T11:09:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    11,
                    9,
                    16,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T11:09:16Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    11,
                    9,
                    16,
                    4,
                    108,
                    0
                ],
                "title": "Divergent LLM Adoption and Heterogeneous Convergence Paths in Research\n  Writing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Divergent LLM Adoption and Heterogeneous Convergence Paths in Research\n  Writing"
                },
                "summary": "Large Language Models (LLMs), such as ChatGPT, are reshaping content creation\nand academic writing. This study investigates the impact of AI-assisted\ngenerative revisions on research manuscripts, focusing on heterogeneous\nadoption patterns and their influence on writing convergence. Leveraging a\ndataset of over 627,000 academic papers from arXiv, we develop a novel\nclassification framework by fine-tuning prompt- and discipline-specific large\nlanguage models to detect the style of ChatGPT-revised texts. Our findings\nreveal substantial disparities in LLM adoption across academic disciplines,\ngender, native language status, and career stage, alongside a rapid evolution\nin scholarly writing styles. Moreover, LLM usage enhances clarity, conciseness,\nand adherence to formal writing conventions, with improvements varying by\nrevision type. Finally, a difference-in-differences analysis shows that while\nLLMs drive convergence in academic writing, early adopters, male researchers,\nnon-native speakers, and junior scholars exhibit the most pronounced stylistic\nshifts, aligning their writing more closely with that of established\nresearchers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as ChatGPT, are reshaping content creation\nand academic writing. This study investigates the impact of AI-assisted\ngenerative revisions on research manuscripts, focusing on heterogeneous\nadoption patterns and their influence on writing convergence. Leveraging a\ndataset of over 627,000 academic papers from arXiv, we develop a novel\nclassification framework by fine-tuning prompt- and discipline-specific large\nlanguage models to detect the style of ChatGPT-revised texts. Our findings\nreveal substantial disparities in LLM adoption across academic disciplines,\ngender, native language status, and career stage, alongside a rapid evolution\nin scholarly writing styles. Moreover, LLM usage enhances clarity, conciseness,\nand adherence to formal writing conventions, with improvements varying by\nrevision type. Finally, a difference-in-differences analysis shows that while\nLLMs drive convergence in academic writing, early adopters, male researchers,\nnon-native speakers, and junior scholars exhibit the most pronounced stylistic\nshifts, aligning their writing more closely with that of established\nresearchers."
                },
                "authors": [
                    {
                        "name": "Cong William Lin"
                    },
                    {
                        "name": "Wu Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Wu Zhu"
                },
                "author": "Wu Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13622v1",
                "updated": "2025-04-18T10:55:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    55,
                    24,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T10:55:24Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    55,
                    24,
                    4,
                    108,
                    0
                ],
                "title": "SupResDiffGAN a new approach for the Super-Resolution task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SupResDiffGAN a new approach for the Super-Resolution task"
                },
                "summary": "In this work, we present SupResDiffGAN, a novel hybrid architecture that\ncombines the strengths of Generative Adversarial Networks (GANs) and diffusion\nmodels for super-resolution tasks. By leveraging latent space representations\nand reducing the number of diffusion steps, SupResDiffGAN achieves\nsignificantly faster inference times than other diffusion-based\nsuper-resolution models while maintaining competitive perceptual quality. To\nprevent discriminator overfitting, we propose adaptive noise corruption,\nensuring a stable balance between the generator and the discriminator during\ntraining. Extensive experiments on benchmark datasets show that our approach\noutperforms traditional diffusion models such as SR3 and I$^2$SB in efficiency\nand image quality. This work bridges the performance gap between diffusion- and\nGAN-based methods, laying the foundation for real-time applications of\ndiffusion models in high-resolution image generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present SupResDiffGAN, a novel hybrid architecture that\ncombines the strengths of Generative Adversarial Networks (GANs) and diffusion\nmodels for super-resolution tasks. By leveraging latent space representations\nand reducing the number of diffusion steps, SupResDiffGAN achieves\nsignificantly faster inference times than other diffusion-based\nsuper-resolution models while maintaining competitive perceptual quality. To\nprevent discriminator overfitting, we propose adaptive noise corruption,\nensuring a stable balance between the generator and the discriminator during\ntraining. Extensive experiments on benchmark datasets show that our approach\noutperforms traditional diffusion models such as SR3 and I$^2$SB in efficiency\nand image quality. This work bridges the performance gap between diffusion- and\nGAN-based methods, laying the foundation for real-time applications of\ndiffusion models in high-resolution image generation."
                },
                "authors": [
                    {
                        "name": "Dawid Kopeć"
                    },
                    {
                        "name": "Wojciech Kozłowski"
                    },
                    {
                        "name": "Maciej Wizerkaniuk"
                    },
                    {
                        "name": "Dawid Krutul"
                    },
                    {
                        "name": "Jan Kocoń"
                    },
                    {
                        "name": "Maciej Zięba"
                    }
                ],
                "author_detail": {
                    "name": "Maciej Zięba"
                },
                "author": "Maciej Zięba",
                "arxiv_comment": "25th International Conference on Computational Science",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13621v1",
                "updated": "2025-04-18T10:54:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    54,
                    52,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T10:54:52Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    54,
                    52,
                    4,
                    108,
                    0
                ],
                "title": "Visual Intention Grounding for Egocentric Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Intention Grounding for Egocentric Assistants"
                },
                "summary": "Visual grounding associates textual descriptions with objects in an image.\nConventional methods target third-person image inputs and named object queries.\nIn applications such as AI assistants, the perspective shifts -- inputs are\negocentric, and objects may be referred to implicitly through needs and\nintentions. To bridge this gap, we introduce EgoIntention, the first dataset\nfor egocentric visual intention grounding. EgoIntention challenges multimodal\nLLMs to 1) understand and ignore unintended contextual objects and 2) reason\nabout uncommon object functionalities. Benchmark results show that current\nmodels misidentify context objects and lack affordance understanding in\negocentric views. We also propose Reason-to-Ground (RoG) instruction tuning; it\nenables hybrid training with normal descriptions and egocentric intentions with\na chained intention reasoning and object grounding mechanism. RoG significantly\noutperforms naive finetuning and hybrid training on EgoIntention, while\nmaintaining or slightly improving naive description grounding. This advancement\nenables unified visual grounding for egocentric and exocentric visual inputs\nwhile handling explicit object queries and implicit human intentions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual grounding associates textual descriptions with objects in an image.\nConventional methods target third-person image inputs and named object queries.\nIn applications such as AI assistants, the perspective shifts -- inputs are\negocentric, and objects may be referred to implicitly through needs and\nintentions. To bridge this gap, we introduce EgoIntention, the first dataset\nfor egocentric visual intention grounding. EgoIntention challenges multimodal\nLLMs to 1) understand and ignore unintended contextual objects and 2) reason\nabout uncommon object functionalities. Benchmark results show that current\nmodels misidentify context objects and lack affordance understanding in\negocentric views. We also propose Reason-to-Ground (RoG) instruction tuning; it\nenables hybrid training with normal descriptions and egocentric intentions with\na chained intention reasoning and object grounding mechanism. RoG significantly\noutperforms naive finetuning and hybrid training on EgoIntention, while\nmaintaining or slightly improving naive description grounding. This advancement\nenables unified visual grounding for egocentric and exocentric visual inputs\nwhile handling explicit object queries and implicit human intentions."
                },
                "authors": [
                    {
                        "name": "Pengzhan Sun"
                    },
                    {
                        "name": "Junbin Xiao"
                    },
                    {
                        "name": "Tze Ho Elden Tse"
                    },
                    {
                        "name": "Yicong Li"
                    },
                    {
                        "name": "Arjun Akula"
                    },
                    {
                        "name": "Angela Yao"
                    }
                ],
                "author_detail": {
                    "name": "Angela Yao"
                },
                "author": "Angela Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13617v1",
                "updated": "2025-04-18T10:46:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    46,
                    22,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T10:46:22Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    46,
                    22,
                    4,
                    108,
                    0
                ],
                "title": "Compile Scene Graphs with Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compile Scene Graphs with Reinforcement Learning"
                },
                "summary": "Next token prediction is the fundamental principle for training large\nlanguage models (LLMs), and reinforcement learning (RL) further enhances their\nreasoning performance. As an effective way to model language, image, video, and\nother modalities, the use of LLMs for end-to-end extraction of structured\nvisual representations, such as scene graphs, remains underexplored. It\nrequires the model to accurately produce a set of objects and relationship\ntriplets, rather than generating text token by token. To achieve this, we\nintroduce R1-SGG, a multimodal LLM (M-LLM) initially trained via supervised\nfine-tuning (SFT) on the scene graph dataset and subsequently refined using\nreinforcement learning to enhance its ability to generate scene graphs in an\nend-to-end manner. The SFT follows a conventional prompt-response paradigm,\nwhile RL requires the design of effective reward signals. Given the structured\nnature of scene graphs, we design a graph-centric reward function that\nintegrates node-level rewards, edge-level rewards, and a format consistency\nreward. Our experiments demonstrate that rule-based RL substantially enhances\nmodel performance in the SGG task, achieving a zero failure rate--unlike\nsupervised fine-tuning (SFT), which struggles to generalize effectively. Our\ncode is available at https://github.com/gpt4vision/R1-SGG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next token prediction is the fundamental principle for training large\nlanguage models (LLMs), and reinforcement learning (RL) further enhances their\nreasoning performance. As an effective way to model language, image, video, and\nother modalities, the use of LLMs for end-to-end extraction of structured\nvisual representations, such as scene graphs, remains underexplored. It\nrequires the model to accurately produce a set of objects and relationship\ntriplets, rather than generating text token by token. To achieve this, we\nintroduce R1-SGG, a multimodal LLM (M-LLM) initially trained via supervised\nfine-tuning (SFT) on the scene graph dataset and subsequently refined using\nreinforcement learning to enhance its ability to generate scene graphs in an\nend-to-end manner. The SFT follows a conventional prompt-response paradigm,\nwhile RL requires the design of effective reward signals. Given the structured\nnature of scene graphs, we design a graph-centric reward function that\nintegrates node-level rewards, edge-level rewards, and a format consistency\nreward. Our experiments demonstrate that rule-based RL substantially enhances\nmodel performance in the SGG task, achieving a zero failure rate--unlike\nsupervised fine-tuning (SFT), which struggles to generalize effectively. Our\ncode is available at https://github.com/gpt4vision/R1-SGG."
                },
                "authors": [
                    {
                        "name": "Zuyao Chen"
                    },
                    {
                        "name": "Jinlin Wu"
                    },
                    {
                        "name": "Zhen Lei"
                    },
                    {
                        "name": "Marc Pollefeys"
                    },
                    {
                        "name": "Chang Wen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chang Wen Chen"
                },
                "author": "Chang Wen Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13615v1",
                "updated": "2025-04-18T10:43:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    43,
                    21,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T10:43:21Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    43,
                    21,
                    4,
                    108,
                    0
                ],
                "title": "Long-context Non-factoid Question Answering in Indic Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Non-factoid Question Answering in Indic Languages"
                },
                "summary": "Question Answering (QA) tasks, which involve extracting answers from a given\ncontext, are relatively straightforward for modern Large Language Models (LLMs)\nwhen the context is short. However, long contexts pose challenges due to the\nquadratic complexity of the self-attention mechanism. This challenge is\ncompounded in Indic languages, which are often low-resource. This study\nexplores context-shortening techniques, including Open Information Extraction\n(OIE), coreference resolution, Answer Paragraph Selection (APS), and their\ncombinations, to improve QA performance. Compared to the baseline of\nunshortened (long) contexts, our experiments on four Indic languages (Hindi,\nTamil, Telugu, and Urdu) demonstrate that context-shortening techniques yield\nan average improvement of 4\\% in semantic scores and 47\\% in token-level scores\nwhen evaluated on three popular LLMs without fine-tuning. Furthermore, with\nfine-tuning, we achieve an average increase of 2\\% in both semantic and\ntoken-level scores. Additionally, context-shortening reduces computational\noverhead. Explainability techniques like LIME and SHAP reveal that when the APS\nmodel confidently identifies the paragraph containing the answer, nearly all\ntokens within the selected text receive high relevance scores. However, the\nstudy also highlights the limitations of LLM-based QA systems in addressing\nnon-factoid questions, particularly those requiring reasoning or debate.\nMoreover, verbalizing OIE-generated triples does not enhance system\nperformance. These findings emphasize the potential of context-shortening\ntechniques to improve the efficiency and effectiveness of LLM-based QA systems,\nespecially for low-resource languages. The source code and resources are\navailable at https://github.com/ritwikmishra/IndicGenQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question Answering (QA) tasks, which involve extracting answers from a given\ncontext, are relatively straightforward for modern Large Language Models (LLMs)\nwhen the context is short. However, long contexts pose challenges due to the\nquadratic complexity of the self-attention mechanism. This challenge is\ncompounded in Indic languages, which are often low-resource. This study\nexplores context-shortening techniques, including Open Information Extraction\n(OIE), coreference resolution, Answer Paragraph Selection (APS), and their\ncombinations, to improve QA performance. Compared to the baseline of\nunshortened (long) contexts, our experiments on four Indic languages (Hindi,\nTamil, Telugu, and Urdu) demonstrate that context-shortening techniques yield\nan average improvement of 4\\% in semantic scores and 47\\% in token-level scores\nwhen evaluated on three popular LLMs without fine-tuning. Furthermore, with\nfine-tuning, we achieve an average increase of 2\\% in both semantic and\ntoken-level scores. Additionally, context-shortening reduces computational\noverhead. Explainability techniques like LIME and SHAP reveal that when the APS\nmodel confidently identifies the paragraph containing the answer, nearly all\ntokens within the selected text receive high relevance scores. However, the\nstudy also highlights the limitations of LLM-based QA systems in addressing\nnon-factoid questions, particularly those requiring reasoning or debate.\nMoreover, verbalizing OIE-generated triples does not enhance system\nperformance. These findings emphasize the potential of context-shortening\ntechniques to improve the efficiency and effectiveness of LLM-based QA systems,\nespecially for low-resource languages. The source code and resources are\navailable at https://github.com/ritwikmishra/IndicGenQA."
                },
                "authors": [
                    {
                        "name": "Ritwik Mishra"
                    },
                    {
                        "name": "Rajiv Ratn Shah"
                    },
                    {
                        "name": "Ponnurangam Kumaraguru"
                    }
                ],
                "author_detail": {
                    "name": "Ponnurangam Kumaraguru"
                },
                "author": "Ponnurangam Kumaraguru",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13613v1",
                "updated": "2025-04-18T10:39:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    39,
                    43,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T10:39:43Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    39,
                    43,
                    4,
                    108,
                    0
                ],
                "title": "Speedup Chip Yield Analysis by Improved Quantum Bayesian Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speedup Chip Yield Analysis by Improved Quantum Bayesian Inference"
                },
                "summary": "The semiconductor chip manufacturing process is complex and lengthy, and\npotential errors arise at every stage. Each wafer contains numerous chips, and\nwafer bin maps can be generated after chip testing. By analyzing the defect\npatterns on these wafer bin maps, the steps in the manufacturing process where\nerrors occurred can be inferred. In this letter, we propose an improved quantum\nBayesian inference to accelerate the identification of error patterns on wafer\nbin maps, thereby assisting in chip yield analysis. We outline the algorithm\nfor error identification and detail the implementation of improved quantum\nBayesian inference. Our results demonstrate the speed advantage of quantum\ncomputation over classical algorithms with a real-world problem, highlighting\nthe practical significance of quantum computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The semiconductor chip manufacturing process is complex and lengthy, and\npotential errors arise at every stage. Each wafer contains numerous chips, and\nwafer bin maps can be generated after chip testing. By analyzing the defect\npatterns on these wafer bin maps, the steps in the manufacturing process where\nerrors occurred can be inferred. In this letter, we propose an improved quantum\nBayesian inference to accelerate the identification of error patterns on wafer\nbin maps, thereby assisting in chip yield analysis. We outline the algorithm\nfor error identification and detail the implementation of improved quantum\nBayesian inference. Our results demonstrate the speed advantage of quantum\ncomputation over classical algorithms with a real-world problem, highlighting\nthe practical significance of quantum computation."
                },
                "authors": [
                    {
                        "name": "Zi-Ming Li"
                    },
                    {
                        "name": "Zeji Li"
                    },
                    {
                        "name": "Tie-Fu Li"
                    },
                    {
                        "name": "Yu-xi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yu-xi Liu"
                },
                "author": "Yu-xi Liu",
                "arxiv_comment": "18 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11094v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11094v2",
                "updated": "2025-04-18T10:39:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    39,
                    23,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-15T11:40:12Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    40,
                    12,
                    1,
                    105,
                    0
                ],
                "title": "Evaluation Report on MCP Servers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation Report on MCP Servers"
                },
                "summary": "With the rise of LLMs, a large number of Model Context Protocol (MCP)\nservices have emerged since the end of 2024. However, the effectiveness and\nefficiency of MCP servers have not been well studied. To study these questions,\nwe propose an evaluation framework, called MCPBench. We selected several widely\nused MCP server and conducted an experimental evaluation on their accuracy,\ntime, and token usage. Our experiments showed that the most effective MCP, Bing\nWeb Search, achieved an accuracy of 64%. Importantly, we found that the\naccuracy of MCP servers can be substantially enhanced by involving declarative\ninterface. This research paves the way for further investigations into\noptimized MCP implementations, ultimately leading to better AI-driven\napplications and data retrieval solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of LLMs, a large number of Model Context Protocol (MCP)\nservices have emerged since the end of 2024. However, the effectiveness and\nefficiency of MCP servers have not been well studied. To study these questions,\nwe propose an evaluation framework, called MCPBench. We selected several widely\nused MCP server and conducted an experimental evaluation on their accuracy,\ntime, and token usage. Our experiments showed that the most effective MCP, Bing\nWeb Search, achieved an accuracy of 64%. Importantly, we found that the\naccuracy of MCP servers can be substantially enhanced by involving declarative\ninterface. This research paves the way for further investigations into\noptimized MCP implementations, ultimately leading to better AI-driven\napplications and data retrieval solutions."
                },
                "authors": [
                    {
                        "name": "Zhiling Luo"
                    },
                    {
                        "name": "Xiaorong Shi"
                    },
                    {
                        "name": "Xuanrui Lin"
                    },
                    {
                        "name": "Jinyang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jinyang Gao"
                },
                "author": "Jinyang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11094v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11094v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17176v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17176v2",
                "updated": "2025-04-18T10:35:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    35,
                    50,
                    4,
                    108,
                    0
                ],
                "published": "2025-01-24T08:15:05Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    15,
                    5,
                    4,
                    24,
                    0
                ],
                "title": "Prompt-Based Cost-Effective Evaluation and Operation of ChatGPT as a\n  Computer Programming Teaching Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-Based Cost-Effective Evaluation and Operation of ChatGPT as a\n  Computer Programming Teaching Assistant"
                },
                "summary": "The dream of achieving a student-teacher ratio of 1:1 is closer than ever\nthanks to the emergence of large language models (LLMs). One potential\napplication of these models in the educational field would be to provide\nfeedback to students in university introductory programming courses, so that a\nstudent struggling to solve a basic implementation problem could seek help from\nan LLM available 24/7. This article focuses on studying three aspects related\nto such an application. First, the performance of two well-known models,\nGPT-3.5T and GPT-4T, in providing feedback to students is evaluated. The\nempirical results showed that GPT-4T performs much better than GPT-3.5T,\nhowever, it is not yet ready for use in a real-world scenario. This is due to\nthe possibility of generating incorrect information that potential users may\nnot always be able to detect. Second, the article proposes a carefully designed\nprompt using in-context learning techniques that allows automating important\nparts of the evaluation process, as well as providing a lower bound for the\nfraction of feedbacks containing incorrect information, saving time and effort.\nThis was possible because the resulting feedback has a programmatically\nanalyzable structure that incorporates diagnostic information about the LLM's\nperformance in solving the requested task. Third, the article also suggests a\npossible strategy for implementing a practical learning tool based on LLMs,\nwhich is rooted on the proposed prompting techniques. This strategy opens up a\nwhole range of interesting possibilities from a pedagogical perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dream of achieving a student-teacher ratio of 1:1 is closer than ever\nthanks to the emergence of large language models (LLMs). One potential\napplication of these models in the educational field would be to provide\nfeedback to students in university introductory programming courses, so that a\nstudent struggling to solve a basic implementation problem could seek help from\nan LLM available 24/7. This article focuses on studying three aspects related\nto such an application. First, the performance of two well-known models,\nGPT-3.5T and GPT-4T, in providing feedback to students is evaluated. The\nempirical results showed that GPT-4T performs much better than GPT-3.5T,\nhowever, it is not yet ready for use in a real-world scenario. This is due to\nthe possibility of generating incorrect information that potential users may\nnot always be able to detect. Second, the article proposes a carefully designed\nprompt using in-context learning techniques that allows automating important\nparts of the evaluation process, as well as providing a lower bound for the\nfraction of feedbacks containing incorrect information, saving time and effort.\nThis was possible because the resulting feedback has a programmatically\nanalyzable structure that incorporates diagnostic information about the LLM's\nperformance in solving the requested task. Third, the article also suggests a\npossible strategy for implementing a practical learning tool based on LLMs,\nwhich is rooted on the proposed prompting techniques. This strategy opens up a\nwhole range of interesting possibilities from a pedagogical perspective."
                },
                "authors": [
                    {
                        "name": "Marc Ballestero-Ribó"
                    },
                    {
                        "name": "Daniel Ortiz-Martínez"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Ortiz-Martínez"
                },
                "author": "Daniel Ortiz-Martínez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17176v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17176v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13612v1",
                "updated": "2025-04-18T10:35:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    35,
                    19,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T10:35:19Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    35,
                    19,
                    4,
                    108,
                    0
                ],
                "title": "Entropic Time Schedulers for Generative Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entropic Time Schedulers for Generative Diffusion Models"
                },
                "summary": "The practical performance of generative diffusion models depends on the\nappropriate choice of the noise scheduling function, which can also be\nequivalently expressed as a time reparameterization. In this paper, we present\na time scheduler that selects sampling points based on entropy rather than\nuniform time spacing, ensuring that each point contributes an equal amount of\ninformation to the final generation. We prove that this time reparameterization\ndoes not depend on the initial choice of time. Furthermore, we provide a\ntractable exact formula to estimate this \\emph{entropic time} for a trained\nmodel using the training loss without substantial overhead. Alongside the\nentropic time, inspired by the optimality results, we introduce a rescaled\nentropic time. In our experiments with mixtures of Gaussian distributions and\nImageNet, we show that using the (rescaled) entropic times greatly improves the\ninference performance of trained models. In particular, we found that the image\nquality in pretrained EDM2 models, as evaluated by FID and FD-DINO scores, can\nbe substantially increased by the rescaled entropic time reparameterization\nwithout increasing the number of function evaluations, with greater\nimprovements in the few NFEs regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The practical performance of generative diffusion models depends on the\nappropriate choice of the noise scheduling function, which can also be\nequivalently expressed as a time reparameterization. In this paper, we present\na time scheduler that selects sampling points based on entropy rather than\nuniform time spacing, ensuring that each point contributes an equal amount of\ninformation to the final generation. We prove that this time reparameterization\ndoes not depend on the initial choice of time. Furthermore, we provide a\ntractable exact formula to estimate this \\emph{entropic time} for a trained\nmodel using the training loss without substantial overhead. Alongside the\nentropic time, inspired by the optimality results, we introduce a rescaled\nentropic time. In our experiments with mixtures of Gaussian distributions and\nImageNet, we show that using the (rescaled) entropic times greatly improves the\ninference performance of trained models. In particular, we found that the image\nquality in pretrained EDM2 models, as evaluated by FID and FD-DINO scores, can\nbe substantially increased by the rescaled entropic time reparameterization\nwithout increasing the number of function evaluations, with greater\nimprovements in the few NFEs regime."
                },
                "authors": [
                    {
                        "name": "Dejan Stancevic"
                    },
                    {
                        "name": "Luca Ambrogioni"
                    }
                ],
                "author_detail": {
                    "name": "Luca Ambrogioni"
                },
                "author": "Luca Ambrogioni",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06549v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06549v2",
                "updated": "2025-04-18T10:32:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    32,
                    24,
                    4,
                    108,
                    0
                ],
                "published": "2024-04-09T18:02:01Z",
                "published_parsed": [
                    2024,
                    4,
                    9,
                    18,
                    2,
                    1,
                    1,
                    100,
                    0
                ],
                "title": "Variational Stochastic Gradient Descent for Deep Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Stochastic Gradient Descent for Deep Neural Networks"
                },
                "summary": "Current state-of-the-art optimizers are adaptive gradient-based optimization\nmethods such as Adam. Recently, there has been an increasing interest in\nformulating gradient-based optimizers in a probabilistic framework for better\nmodeling the uncertainty of the gradients. Here, we propose to combine both\napproaches, resulting in the Variational Stochastic Gradient Descent (VSGD)\noptimizer. We model gradient updates as a probabilistic model and utilize\nstochastic variational inference (SVI) to derive an efficient and effective\nupdate rule. Further, we show how our VSGD method relates to other adaptive\ngradient-based optimizers like Adam. Lastly, we carry out experiments on two\nimage classification datasets and four deep neural network architectures, where\nwe show that VSGD outperforms Adam and SGD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current state-of-the-art optimizers are adaptive gradient-based optimization\nmethods such as Adam. Recently, there has been an increasing interest in\nformulating gradient-based optimizers in a probabilistic framework for better\nmodeling the uncertainty of the gradients. Here, we propose to combine both\napproaches, resulting in the Variational Stochastic Gradient Descent (VSGD)\noptimizer. We model gradient updates as a probabilistic model and utilize\nstochastic variational inference (SVI) to derive an efficient and effective\nupdate rule. Further, we show how our VSGD method relates to other adaptive\ngradient-based optimizers like Adam. Lastly, we carry out experiments on two\nimage classification datasets and four deep neural network architectures, where\nwe show that VSGD outperforms Adam and SGD."
                },
                "authors": [
                    {
                        "name": "Haotian Chen"
                    },
                    {
                        "name": "Anna Kuzina"
                    },
                    {
                        "name": "Babak Esmaeili"
                    },
                    {
                        "name": "Jakub M Tomczak"
                    }
                ],
                "author_detail": {
                    "name": "Jakub M Tomczak"
                },
                "author": "Jakub M Tomczak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06549v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06549v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18337v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18337v2",
                "updated": "2025-04-18T10:26:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    26,
                    12,
                    4,
                    108,
                    0
                ],
                "published": "2024-11-27T13:35:32Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    13,
                    35,
                    32,
                    2,
                    332,
                    0
                ],
                "title": "Can LLMs assist with Ambiguity? A Quantitative Evaluation of various\n  Large Language Models on Word Sense Disambiguation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs assist with Ambiguity? A Quantitative Evaluation of various\n  Large Language Models on Word Sense Disambiguation"
                },
                "summary": "Ambiguous words are often found in modern digital communications. Lexical\nambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due\nto limited data. Consequently, the efficiency of translation, information\nretrieval, and question-answering systems is hindered by these limitations.\nThis study investigates the use of Large Language Models (LLMs) to improve WSD\nusing a novel approach combining a systematic prompt augmentation mechanism\nwith a knowledge base (KB) consisting of different sense interpretations. The\nproposed method incorporates a human-in-loop approach for prompt augmentation\nwhere prompt is supported by Part-of-Speech (POS) tagging, synonyms of\nambiguous words, aspect-based sense filtering and few-shot prompting to guide\nthe LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based\napproach, this work demonstrates a substantial improvement in performance. The\nevaluation was conducted using FEWS test data and sense tags. This research\nadvances accurate word interpretation in social media and digital\ncommunication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ambiguous words are often found in modern digital communications. Lexical\nambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due\nto limited data. Consequently, the efficiency of translation, information\nretrieval, and question-answering systems is hindered by these limitations.\nThis study investigates the use of Large Language Models (LLMs) to improve WSD\nusing a novel approach combining a systematic prompt augmentation mechanism\nwith a knowledge base (KB) consisting of different sense interpretations. The\nproposed method incorporates a human-in-loop approach for prompt augmentation\nwhere prompt is supported by Part-of-Speech (POS) tagging, synonyms of\nambiguous words, aspect-based sense filtering and few-shot prompting to guide\nthe LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based\napproach, this work demonstrates a substantial improvement in performance. The\nevaluation was conducted using FEWS test data and sense tags. This research\nadvances accurate word interpretation in social media and digital\ncommunication."
                },
                "authors": [
                    {
                        "name": "T. G. D. K. Sumanathilaka"
                    },
                    {
                        "name": "Nicholas Micallef"
                    },
                    {
                        "name": "Julian Hough"
                    }
                ],
                "author_detail": {
                    "name": "Julian Hough"
                },
                "author": "Julian Hough",
                "arxiv_comment": "12 pages,6 tables, 1 figure, Proceedings of the 1st International\n  Conference on NLP & AI for Cyber Security",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18337v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18337v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13603v1",
                "updated": "2025-04-18T10:14:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    14,
                    51,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T10:14:51Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    14,
                    51,
                    4,
                    108,
                    0
                ],
                "title": "Continual Pre-Training is (not) What You Need in Domain Adaption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Pre-Training is (not) What You Need in Domain Adaption"
                },
                "summary": "The recent advances in Legal Large Language Models (LLMs) have transformed\nthe landscape of legal research and practice by automating tasks, enhancing\nresearch precision, and supporting complex decision-making processes. However,\neffectively adapting LLMs to the legal domain remains challenging due to the\ncomplexity of legal reasoning, the need for precise interpretation of\nspecialized language, and the potential for hallucinations. This paper examines\nthe efficacy of Domain-Adaptive Continual Pre-Training (DACP) in improving the\nlegal reasoning capabilities of LLMs. Through a series of experiments on legal\nreasoning tasks within the Taiwanese legal framework, we demonstrate that while\nDACP enhances domain-specific knowledge, it does not uniformly improve\nperformance across all legal tasks. We discuss the trade-offs involved in DACP,\nparticularly its impact on model generalization and performance in prompt-based\ntasks, and propose directions for future research to optimize domain adaptation\nstrategies in legal AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advances in Legal Large Language Models (LLMs) have transformed\nthe landscape of legal research and practice by automating tasks, enhancing\nresearch precision, and supporting complex decision-making processes. However,\neffectively adapting LLMs to the legal domain remains challenging due to the\ncomplexity of legal reasoning, the need for precise interpretation of\nspecialized language, and the potential for hallucinations. This paper examines\nthe efficacy of Domain-Adaptive Continual Pre-Training (DACP) in improving the\nlegal reasoning capabilities of LLMs. Through a series of experiments on legal\nreasoning tasks within the Taiwanese legal framework, we demonstrate that while\nDACP enhances domain-specific knowledge, it does not uniformly improve\nperformance across all legal tasks. We discuss the trade-offs involved in DACP,\nparticularly its impact on model generalization and performance in prompt-based\ntasks, and propose directions for future research to optimize domain adaptation\nstrategies in legal AI."
                },
                "authors": [
                    {
                        "name": "Pin-Er Chen"
                    },
                    {
                        "name": "Da-Chen Lian"
                    },
                    {
                        "name": "Shu-Kai Hsieh"
                    },
                    {
                        "name": "Sieh-Chuen Huang"
                    },
                    {
                        "name": "Hsuan-Lei Shao"
                    },
                    {
                        "name": "Jun-Wei Chiu"
                    },
                    {
                        "name": "Yang-Hsien Lin"
                    },
                    {
                        "name": "Zih-Ching Chen"
                    },
                    {
                        "name": "Cheng-Kuang"
                    },
                    {
                        "name": "Eddie TC Huang"
                    },
                    {
                        "name": "Simon See"
                    }
                ],
                "author_detail": {
                    "name": "Simon See"
                },
                "author": "Simon See",
                "arxiv_comment": "11 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13592v1",
                "updated": "2025-04-18T09:52:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    52,
                    12,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T09:52:12Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    52,
                    12,
                    4,
                    108,
                    0
                ],
                "title": "Improving Generalization in Intent Detection: GRPO with Reward-Based\n  Curriculum Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Generalization in Intent Detection: GRPO with Reward-Based\n  Curriculum Sampling"
                },
                "summary": "Intent detection, a critical component in task-oriented dialogue (TOD)\nsystems, faces significant challenges in adapting to the rapid influx of\nintegrable tools with complex interrelationships. Existing approaches, such as\nzero-shot reformulations and LLM-based dynamic recognition, struggle with\nperformance degradation when encountering unseen intents, leading to erroneous\ntask routing. To enhance the model's generalization performance on unseen\ntasks, we employ Reinforcement Learning (RL) combined with a Reward-based\nCurriculum Sampling (RCS) during Group Relative Policy Optimization (GRPO)\ntraining in intent detection tasks. Experiments demonstrate that RL-trained\nmodels substantially outperform supervised fine-tuning (SFT) baselines in\ngeneralization. Besides, the introduction of the RCS, significantly bolsters\nthe effectiveness of RL in intent detection by focusing the model on\nchallenging cases during training. Moreover, incorporating Chain-of-Thought\n(COT) processes in RL notably improves generalization in complex intent\ndetection tasks, underscoring the importance of thought in challenging\nscenarios. This work advances the generalization of intent detection tasks,\noffering practical insights for deploying adaptable dialogue systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent detection, a critical component in task-oriented dialogue (TOD)\nsystems, faces significant challenges in adapting to the rapid influx of\nintegrable tools with complex interrelationships. Existing approaches, such as\nzero-shot reformulations and LLM-based dynamic recognition, struggle with\nperformance degradation when encountering unseen intents, leading to erroneous\ntask routing. To enhance the model's generalization performance on unseen\ntasks, we employ Reinforcement Learning (RL) combined with a Reward-based\nCurriculum Sampling (RCS) during Group Relative Policy Optimization (GRPO)\ntraining in intent detection tasks. Experiments demonstrate that RL-trained\nmodels substantially outperform supervised fine-tuning (SFT) baselines in\ngeneralization. Besides, the introduction of the RCS, significantly bolsters\nthe effectiveness of RL in intent detection by focusing the model on\nchallenging cases during training. Moreover, incorporating Chain-of-Thought\n(COT) processes in RL notably improves generalization in complex intent\ndetection tasks, underscoring the importance of thought in challenging\nscenarios. This work advances the generalization of intent detection tasks,\noffering practical insights for deploying adaptable dialogue systems."
                },
                "authors": [
                    {
                        "name": "Zihao Feng"
                    },
                    {
                        "name": "Xiaoxue Wang"
                    },
                    {
                        "name": "Ziwei Bai"
                    },
                    {
                        "name": "Donghang Su"
                    },
                    {
                        "name": "Bowen Wu"
                    },
                    {
                        "name": "Qun Yu"
                    },
                    {
                        "name": "Baoxun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Baoxun Wang"
                },
                "author": "Baoxun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13074v2",
                "updated": "2025-04-18T09:46:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    46,
                    32,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-17T16:37:27Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    37,
                    27,
                    3,
                    107,
                    0
                ],
                "title": "SkyReels-V2: Infinite-length Film Generative Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyReels-V2: Infinite-length Film Generative Model"
                },
                "summary": "Recent advances in video generation have been driven by diffusion models and\nautoregressive frameworks, yet critical challenges persist in harmonizing\nprompt adherence, visual quality, motion dynamics, and duration: compromises in\nmotion dynamics to enhance temporal visual quality, constrained video duration\n(5-10 seconds) to prioritize resolution, and inadequate shot-aware generation\nstemming from general-purpose MLLMs' inability to interpret cinematic grammar,\nsuch as shot composition, actor expressions, and camera motions. These\nintertwined limitations hinder realistic long-form synthesis and professional\nfilm-style generation. To address these limitations, we propose SkyReels-V2, an\nInfinite-length Film Generative Model, that synergizes Multi-modal Large\nLanguage Model (MLLM), Multi-stage Pretraining, Reinforcement Learning, and\nDiffusion Forcing Framework. Firstly, we design a comprehensive structural\nrepresentation of video that combines the general descriptions by the\nMulti-modal LLM and the detailed shot language by sub-expert models. Aided with\nhuman annotation, we then train a unified Video Captioner, named\nSkyCaptioner-V1, to efficiently label the video data. Secondly, we establish\nprogressive-resolution pretraining for the fundamental video generation,\nfollowed by a four-stage post-training enhancement: Initial concept-balanced\nSupervised Fine-Tuning (SFT) improves baseline quality; Motion-specific\nReinforcement Learning (RL) training with human-annotated and synthetic\ndistortion data addresses dynamic artifacts; Our diffusion forcing framework\nwith non-decreasing noise schedules enables long-video synthesis in an\nefficient search space; Final high-quality SFT refines visual fidelity. All the\ncode and models are available at https://github.com/SkyworkAI/SkyReels-V2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in video generation have been driven by diffusion models and\nautoregressive frameworks, yet critical challenges persist in harmonizing\nprompt adherence, visual quality, motion dynamics, and duration: compromises in\nmotion dynamics to enhance temporal visual quality, constrained video duration\n(5-10 seconds) to prioritize resolution, and inadequate shot-aware generation\nstemming from general-purpose MLLMs' inability to interpret cinematic grammar,\nsuch as shot composition, actor expressions, and camera motions. These\nintertwined limitations hinder realistic long-form synthesis and professional\nfilm-style generation. To address these limitations, we propose SkyReels-V2, an\nInfinite-length Film Generative Model, that synergizes Multi-modal Large\nLanguage Model (MLLM), Multi-stage Pretraining, Reinforcement Learning, and\nDiffusion Forcing Framework. Firstly, we design a comprehensive structural\nrepresentation of video that combines the general descriptions by the\nMulti-modal LLM and the detailed shot language by sub-expert models. Aided with\nhuman annotation, we then train a unified Video Captioner, named\nSkyCaptioner-V1, to efficiently label the video data. Secondly, we establish\nprogressive-resolution pretraining for the fundamental video generation,\nfollowed by a four-stage post-training enhancement: Initial concept-balanced\nSupervised Fine-Tuning (SFT) improves baseline quality; Motion-specific\nReinforcement Learning (RL) training with human-annotated and synthetic\ndistortion data addresses dynamic artifacts; Our diffusion forcing framework\nwith non-decreasing noise schedules enables long-video synthesis in an\nefficient search space; Final high-quality SFT refines visual fidelity. All the\ncode and models are available at https://github.com/SkyworkAI/SkyReels-V2."
                },
                "authors": [
                    {
                        "name": "Guibin Chen"
                    },
                    {
                        "name": "Dixuan Lin"
                    },
                    {
                        "name": "Jiangping Yang"
                    },
                    {
                        "name": "Chunze Lin"
                    },
                    {
                        "name": "Juncheng Zhu"
                    },
                    {
                        "name": "Mingyuan Fan"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Sheng Chen"
                    },
                    {
                        "name": "Zheng Chen"
                    },
                    {
                        "name": "Chengchen Ma"
                    },
                    {
                        "name": "Weiming Xiong"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Nuo Pang"
                    },
                    {
                        "name": "Kang Kang"
                    },
                    {
                        "name": "Zhiheng Xu"
                    },
                    {
                        "name": "Yuzhe Jin"
                    },
                    {
                        "name": "Yupeng Liang"
                    },
                    {
                        "name": "Yubing Song"
                    },
                    {
                        "name": "Peng Zhao"
                    },
                    {
                        "name": "Boyuan Xu"
                    },
                    {
                        "name": "Di Qiu"
                    },
                    {
                        "name": "Debang Li"
                    },
                    {
                        "name": "Zhengcong Fei"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yahui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yahui Zhou"
                },
                "author": "Yahui Zhou",
                "arxiv_comment": "31 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13589v1",
                "updated": "2025-04-18T09:41:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    41,
                    35,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T09:41:35Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    41,
                    35,
                    4,
                    108,
                    0
                ],
                "title": "Towards End-to-End Network Intent Management with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards End-to-End Network Intent Management with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are likely to play a key role in Intent-Based\nNetworking (IBN) as they show remarkable performance in interpreting human\nlanguage as well as code generation, enabling the translation of high-level\nintents expressed by humans into low-level network configurations. In this\npaper, we leverage closed-source language models (i.e., Google Gemini 1.5 pro,\nChatGPT-4) and open-source models (i.e., LLama, Mistral) to investigate their\ncapacity to generate E2E network configurations for radio access networks\n(RANs) and core networks in 5G/6G mobile networks. We introduce a novel\nperformance metrics, known as FEACI, to quantitatively assess the format (F),\nexplainability (E), accuracy (A), cost (C), and inference time (I) of the\ngenerated answer; existing general metrics are unable to capture these\nfeatures. The results of our study demonstrate that open-source models can\nachieve comparable or even superior translation performance compared with the\nclosed-source models requiring costly hardware setup and not accessible to all\nusers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are likely to play a key role in Intent-Based\nNetworking (IBN) as they show remarkable performance in interpreting human\nlanguage as well as code generation, enabling the translation of high-level\nintents expressed by humans into low-level network configurations. In this\npaper, we leverage closed-source language models (i.e., Google Gemini 1.5 pro,\nChatGPT-4) and open-source models (i.e., LLama, Mistral) to investigate their\ncapacity to generate E2E network configurations for radio access networks\n(RANs) and core networks in 5G/6G mobile networks. We introduce a novel\nperformance metrics, known as FEACI, to quantitatively assess the format (F),\nexplainability (E), accuracy (A), cost (C), and inference time (I) of the\ngenerated answer; existing general metrics are unable to capture these\nfeatures. The results of our study demonstrate that open-source models can\nachieve comparable or even superior translation performance compared with the\nclosed-source models requiring costly hardware setup and not accessible to all\nusers."
                },
                "authors": [
                    {
                        "name": "Lam Dinh"
                    },
                    {
                        "name": "Sihem Cherrared"
                    },
                    {
                        "name": "Xiaofeng Huang"
                    },
                    {
                        "name": "Fabrice Guillemin"
                    }
                ],
                "author_detail": {
                    "name": "Fabrice Guillemin"
                },
                "author": "Fabrice Guillemin",
                "arxiv_comment": "Full paper is accepted at IFIP Networking 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09131v2",
                "updated": "2025-04-18T09:38:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    38,
                    56,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-12T08:45:36Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    8,
                    45,
                    36,
                    5,
                    102,
                    0
                ],
                "title": "Haptic Perception via the Dynamics of Flexible Body Inspired by an\n  Ostrich's Neck",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Haptic Perception via the Dynamics of Flexible Body Inspired by an\n  Ostrich's Neck"
                },
                "summary": "In biological systems, both skin sensitivity and body flexibility play\ncrucial roles in haptic perception. Fully soft robots often suffer from\nstructural fragility and delayed sensory processing, limiting their practical\nfunctionality. The musculoskeletal system combines the adaptability of soft\nmaterials with the durability of rigid-body robots. It also leverages\nmorphological computation, where the morphological structures contribute to\ninformation processing, for dynamic and adaptive behaviors. This study focuses\non the pecking behaviors of birds, which enables precise haptic perception\nthrough the musculoskeletal system of their flexible neck. Physical reservoir\ncomputing is applied to flexible structures inspired by an ostrich neck to\nanalyze the relationship between haptic perception and physical\ncharacteristics. Experiments with both a physical robot and simulations reveal\nthat, with appropriate viscoelasticity, the flexible structure can discriminate\nobject softness and retain that information through behavior. Drawing on these\nfindings and anatomical insights from the ostrich neck, a haptic perception\nsystem is proposed that exhibits both separability and behavioral memory in\nflexible structures, enabling rapid learning and real-time inference. The\nresults demonstrate that through the dynamics of flexible structures, diverse\nfunctions can emerge beyond their original design as manipulators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In biological systems, both skin sensitivity and body flexibility play\ncrucial roles in haptic perception. Fully soft robots often suffer from\nstructural fragility and delayed sensory processing, limiting their practical\nfunctionality. The musculoskeletal system combines the adaptability of soft\nmaterials with the durability of rigid-body robots. It also leverages\nmorphological computation, where the morphological structures contribute to\ninformation processing, for dynamic and adaptive behaviors. This study focuses\non the pecking behaviors of birds, which enables precise haptic perception\nthrough the musculoskeletal system of their flexible neck. Physical reservoir\ncomputing is applied to flexible structures inspired by an ostrich neck to\nanalyze the relationship between haptic perception and physical\ncharacteristics. Experiments with both a physical robot and simulations reveal\nthat, with appropriate viscoelasticity, the flexible structure can discriminate\nobject softness and retain that information through behavior. Drawing on these\nfindings and anatomical insights from the ostrich neck, a haptic perception\nsystem is proposed that exhibits both separability and behavioral memory in\nflexible structures, enabling rapid learning and real-time inference. The\nresults demonstrate that through the dynamics of flexible structures, diverse\nfunctions can emerge beyond their original design as manipulators."
                },
                "authors": [
                    {
                        "name": "Kazashi Nakano"
                    },
                    {
                        "name": "Katsuma Inoue"
                    },
                    {
                        "name": "Yasuo Kuniyoshi"
                    },
                    {
                        "name": "Kohei Nakajima"
                    }
                ],
                "author_detail": {
                    "name": "Kohei Nakajima"
                },
                "author": "Kohei Nakajima",
                "arxiv_comment": "This paper includes a figure of a dissected ostrich. As the ostrich\n  was processed for food, its use does not raise any ethical concerns",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13587v1",
                "updated": "2025-04-18T09:38:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    38,
                    49,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T09:38:49Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    38,
                    49,
                    4,
                    108,
                    0
                ],
                "title": "RAG Without the Lag: Interactive Debugging for Retrieval-Augmented\n  Generation Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG Without the Lag: Interactive Debugging for Retrieval-Augmented\n  Generation Pipelines"
                },
                "summary": "Retrieval-augmented generation (RAG) pipelines have become the de-facto\napproach for building AI assistants with access to external, domain-specific\nknowledge. Given a user query, RAG pipelines typically first retrieve (R)\nrelevant information from external sources, before invoking a Large Language\nModel (LLM), augmented (A) with this information, to generate (G) responses.\nModern RAG pipelines frequently chain multiple retrieval and generation\ncomponents, in any order. However, developing effective RAG pipelines is\nchallenging because retrieval and generation components are intertwined, making\nit hard to identify which component(s) cause errors in the eventual output. The\nparameters with the greatest impact on output quality often require hours of\npre-processing after each change, creating prohibitively slow feedback cycles.\nTo address these challenges, we present RAGGY, a developer tool that combines a\nPython library of composable RAG primitives with an interactive interface for\nreal-time debugging. We contribute the design and implementation of RAGGY,\ninsights into expert debugging patterns through a qualitative study with 12\nengineers, and design implications for future RAG tools that better align with\ndevelopers' natural workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) pipelines have become the de-facto\napproach for building AI assistants with access to external, domain-specific\nknowledge. Given a user query, RAG pipelines typically first retrieve (R)\nrelevant information from external sources, before invoking a Large Language\nModel (LLM), augmented (A) with this information, to generate (G) responses.\nModern RAG pipelines frequently chain multiple retrieval and generation\ncomponents, in any order. However, developing effective RAG pipelines is\nchallenging because retrieval and generation components are intertwined, making\nit hard to identify which component(s) cause errors in the eventual output. The\nparameters with the greatest impact on output quality often require hours of\npre-processing after each change, creating prohibitively slow feedback cycles.\nTo address these challenges, we present RAGGY, a developer tool that combines a\nPython library of composable RAG primitives with an interactive interface for\nreal-time debugging. We contribute the design and implementation of RAGGY,\ninsights into expert debugging patterns through a qualitative study with 12\nengineers, and design implications for future RAG tools that better align with\ndevelopers' natural workflows."
                },
                "authors": [
                    {
                        "name": "Quentin Romero Lauro"
                    },
                    {
                        "name": "Shreya Shankar"
                    },
                    {
                        "name": "Sepanta Zeighami"
                    },
                    {
                        "name": "Aditya Parameswaran"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Parameswaran"
                },
                "author": "Aditya Parameswaran",
                "arxiv_comment": "15 pages, 7 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02505v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02505v2",
                "updated": "2025-04-18T09:26:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    26,
                    21,
                    4,
                    108,
                    0
                ],
                "published": "2025-01-05T11:04:30Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    11,
                    4,
                    30,
                    6,
                    5,
                    0
                ],
                "title": "Learning when to rank: Estimation of partial rankings from sparse, noisy\n  comparisons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning when to rank: Estimation of partial rankings from sparse, noisy\n  comparisons"
                },
                "summary": "A common task arising in various domains is that of ranking items based on\nthe outcomes of pairwise comparisons, from ranking players and teams in sports\nto ranking products or brands in marketing studies and recommendation systems.\nStatistical inference-based methods such as the Bradley-Terry model, which\nextract rankings based on an underlying generative model of the comparison\noutcomes, have emerged as flexible and powerful tools to tackle the task of\nranking in empirical data. In situations with limited and/or noisy comparisons,\nit is often challenging to confidently distinguish the performance of different\nitems based on the evidence available in the data. However, existing\ninference-based ranking methods overwhelmingly choose to assign each item to a\nunique rank or score, suggesting a meaningful distinction when there is none.\nHere, we address this problem by developing a principled Bayesian methodology\nfor learning partial rankings -- rankings with ties -- that distinguishes among\nthe ranks of different items only when there is sufficient evidence available\nin the data. Our framework is adaptable to any statistical ranking method in\nwhich the outcomes of pairwise observations depend on the ranks or scores of\nthe items being compared. We develop a fast agglomerative algorithm to perform\nMaximum A Posteriori (MAP) inference of partial rankings under our framework\nand examine the performance of our method on a variety of real and synthetic\nnetwork datasets, finding that it frequently gives a more parsimonious summary\nof the data than traditional ranking, particularly when observations are\nsparse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common task arising in various domains is that of ranking items based on\nthe outcomes of pairwise comparisons, from ranking players and teams in sports\nto ranking products or brands in marketing studies and recommendation systems.\nStatistical inference-based methods such as the Bradley-Terry model, which\nextract rankings based on an underlying generative model of the comparison\noutcomes, have emerged as flexible and powerful tools to tackle the task of\nranking in empirical data. In situations with limited and/or noisy comparisons,\nit is often challenging to confidently distinguish the performance of different\nitems based on the evidence available in the data. However, existing\ninference-based ranking methods overwhelmingly choose to assign each item to a\nunique rank or score, suggesting a meaningful distinction when there is none.\nHere, we address this problem by developing a principled Bayesian methodology\nfor learning partial rankings -- rankings with ties -- that distinguishes among\nthe ranks of different items only when there is sufficient evidence available\nin the data. Our framework is adaptable to any statistical ranking method in\nwhich the outcomes of pairwise observations depend on the ranks or scores of\nthe items being compared. We develop a fast agglomerative algorithm to perform\nMaximum A Posteriori (MAP) inference of partial rankings under our framework\nand examine the performance of our method on a variety of real and synthetic\nnetwork datasets, finding that it frequently gives a more parsimonious summary\nof the data than traditional ranking, particularly when observations are\nsparse."
                },
                "authors": [
                    {
                        "name": "Sebastian Morel-Balbi"
                    },
                    {
                        "name": "Alec Kirkley"
                    }
                ],
                "author_detail": {
                    "name": "Alec Kirkley"
                },
                "author": "Alec Kirkley",
                "arxiv_comment": "20 pages, 8 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02505v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02505v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14427v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14427v2",
                "updated": "2025-04-18T09:21:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    21,
                    41,
                    4,
                    108,
                    0
                ],
                "published": "2025-02-20T10:25:13Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    10,
                    25,
                    13,
                    3,
                    51,
                    0
                ],
                "title": "Token-Level Density-Based Uncertainty Quantification Methods for\n  Eliciting Truthfulness of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-Level Density-Based Uncertainty Quantification Methods for\n  Eliciting Truthfulness of Large Language Models"
                },
                "summary": "Uncertainty quantification (UQ) is a prominent approach for eliciting\ntruthful answers from large language models (LLMs). To date, information-based\nand consistency-based UQ have been the dominant UQ methods for text generation\nvia LLMs. Density-based methods, despite being very effective for UQ in text\nclassification with encoder-based models, have not been very successful with\ngenerative LLMs. In this work, we adapt Mahalanobis Distance (MD) - a\nwell-established UQ technique in classification tasks - for text generation and\nintroduce a new supervised UQ method. Our method extracts token embeddings from\nmultiple layers of LLMs, computes MD scores for each token, and uses linear\nregression trained on these features to provide robust uncertainty scores.\nThrough extensive experiments on eleven datasets, we demonstrate that our\napproach substantially improves over existing UQ methods, providing accurate\nand computationally efficient uncertainty scores for both sequence-level\nselective generation and claim-level fact-checking tasks. Our method also\nexhibits strong generalization to out-of-domain data, making it suitable for a\nwide range of LLM-based applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty quantification (UQ) is a prominent approach for eliciting\ntruthful answers from large language models (LLMs). To date, information-based\nand consistency-based UQ have been the dominant UQ methods for text generation\nvia LLMs. Density-based methods, despite being very effective for UQ in text\nclassification with encoder-based models, have not been very successful with\ngenerative LLMs. In this work, we adapt Mahalanobis Distance (MD) - a\nwell-established UQ technique in classification tasks - for text generation and\nintroduce a new supervised UQ method. Our method extracts token embeddings from\nmultiple layers of LLMs, computes MD scores for each token, and uses linear\nregression trained on these features to provide robust uncertainty scores.\nThrough extensive experiments on eleven datasets, we demonstrate that our\napproach substantially improves over existing UQ methods, providing accurate\nand computationally efficient uncertainty scores for both sequence-level\nselective generation and claim-level fact-checking tasks. Our method also\nexhibits strong generalization to out-of-domain data, making it suitable for a\nwide range of LLM-based applications."
                },
                "authors": [
                    {
                        "name": "Artem Vazhentsev"
                    },
                    {
                        "name": "Lyudmila Rvanova"
                    },
                    {
                        "name": "Ivan Lazichny"
                    },
                    {
                        "name": "Alexander Panchenko"
                    },
                    {
                        "name": "Maxim Panov"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Artem Shelmanov"
                    }
                ],
                "author_detail": {
                    "name": "Artem Shelmanov"
                },
                "author": "Artem Shelmanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14427v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14427v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13572v1",
                "updated": "2025-04-18T09:12:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    12,
                    46,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T09:12:46Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    12,
                    46,
                    4,
                    108,
                    0
                ],
                "title": "Contextualizing Spotify's Audiobook List Recommendations with\n  Descriptive Shelves",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextualizing Spotify's Audiobook List Recommendations with\n  Descriptive Shelves"
                },
                "summary": "In this paper, we propose a pipeline to generate contextualized list\nrecommendations with descriptive shelves in the domain of audiobooks. By\ncreating several shelves for topics the user has an affinity to, e.g. Uplifting\nWomen's Fiction, we can help them explore their recommendations according to\ntheir interests and at the same time recommend a diverse set of items. To do\nso, we use Large Language Models (LLMs) to enrich each item's metadata based on\na taxonomy created for this domain. Then we create diverse descriptive shelves\nfor each user. A/B tests show improvements in user engagement and audiobook\ndiscovery metrics, demonstrating benefits for users and content creators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a pipeline to generate contextualized list\nrecommendations with descriptive shelves in the domain of audiobooks. By\ncreating several shelves for topics the user has an affinity to, e.g. Uplifting\nWomen's Fiction, we can help them explore their recommendations according to\ntheir interests and at the same time recommend a diverse set of items. To do\nso, we use Large Language Models (LLMs) to enrich each item's metadata based on\na taxonomy created for this domain. Then we create diverse descriptive shelves\nfor each user. A/B tests show improvements in user engagement and audiobook\ndiscovery metrics, demonstrating benefits for users and content creators."
                },
                "authors": [
                    {
                        "name": "Gustavo Penha"
                    },
                    {
                        "name": "Alice Wang"
                    },
                    {
                        "name": "Martin Achenbach"
                    },
                    {
                        "name": "Kristen Sheets"
                    },
                    {
                        "name": "Sahitya Mantravadi"
                    },
                    {
                        "name": "Remi Galvez"
                    },
                    {
                        "name": "Nico Guetta-Jeanrenaud"
                    },
                    {
                        "name": "Divya Narayanan"
                    },
                    {
                        "name": "Ofeliya Kalaydzhyan"
                    },
                    {
                        "name": "Hugues Bouchard"
                    }
                ],
                "author_detail": {
                    "name": "Hugues Bouchard"
                },
                "author": "Hugues Bouchard",
                "arxiv_comment": "Accepted for publication in the 47th European Conference on\n  Information Retrieval (ECIR'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13569v1",
                "updated": "2025-04-18T09:11:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    11,
                    34,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T09:11:34Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    11,
                    34,
                    4,
                    108,
                    0
                ],
                "title": "Bayesian continual learning and forgetting in neural networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian continual learning and forgetting in neural networks"
                },
                "summary": "Biological synapses effortlessly balance memory retention and flexibility,\nyet artificial neural networks still struggle with the extremes of catastrophic\nforgetting and catastrophic remembering. Here, we introduce Metaplasticity from\nSynaptic Uncertainty (MESU), a Bayesian framework that updates network\nparameters according their uncertainty. This approach allows a principled\ncombination of learning and forgetting that ensures that critical knowledge is\npreserved while unused or outdated information is gradually released. Unlike\nstandard Bayesian approaches -- which risk becoming overly constrained, and\npopular continual-learning methods that rely on explicit task boundaries, MESU\nseamlessly adapts to streaming data. It further provides reliable epistemic\nuncertainty estimates, allowing out-of-distribution detection, the only\ncomputational cost being to sample the weights multiple times to provide proper\noutput statistics. Experiments on image-classification benchmarks demonstrate\nthat MESU mitigates catastrophic forgetting, while maintaining plasticity for\nnew tasks. When training 200 sequential permuted MNIST tasks, MESU outperforms\nestablished continual learning techniques in terms of accuracy, capability to\nlearn additional tasks, and out-of-distribution data detection. Additionally,\ndue to its non-reliance on task boundaries, MESU outperforms conventional\nlearning techniques on the incremental training of CIFAR-100 tasks consistently\nin a wide range of scenarios. Our results unify ideas from metaplasticity,\nBayesian inference, and Hessian-based regularization, offering a\nbiologically-inspired pathway to robust, perpetual learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biological synapses effortlessly balance memory retention and flexibility,\nyet artificial neural networks still struggle with the extremes of catastrophic\nforgetting and catastrophic remembering. Here, we introduce Metaplasticity from\nSynaptic Uncertainty (MESU), a Bayesian framework that updates network\nparameters according their uncertainty. This approach allows a principled\ncombination of learning and forgetting that ensures that critical knowledge is\npreserved while unused or outdated information is gradually released. Unlike\nstandard Bayesian approaches -- which risk becoming overly constrained, and\npopular continual-learning methods that rely on explicit task boundaries, MESU\nseamlessly adapts to streaming data. It further provides reliable epistemic\nuncertainty estimates, allowing out-of-distribution detection, the only\ncomputational cost being to sample the weights multiple times to provide proper\noutput statistics. Experiments on image-classification benchmarks demonstrate\nthat MESU mitigates catastrophic forgetting, while maintaining plasticity for\nnew tasks. When training 200 sequential permuted MNIST tasks, MESU outperforms\nestablished continual learning techniques in terms of accuracy, capability to\nlearn additional tasks, and out-of-distribution data detection. Additionally,\ndue to its non-reliance on task boundaries, MESU outperforms conventional\nlearning techniques on the incremental training of CIFAR-100 tasks consistently\nin a wide range of scenarios. Our results unify ideas from metaplasticity,\nBayesian inference, and Hessian-based regularization, offering a\nbiologically-inspired pathway to robust, perpetual learning."
                },
                "authors": [
                    {
                        "name": "Djohan Bonnet"
                    },
                    {
                        "name": "Kellian Cottart"
                    },
                    {
                        "name": "Tifenn Hirtzlin"
                    },
                    {
                        "name": "Tarcisius Januel"
                    },
                    {
                        "name": "Thomas Dalgaty"
                    },
                    {
                        "name": "Elisa Vianello"
                    },
                    {
                        "name": "Damien Querlioz"
                    }
                ],
                "author_detail": {
                    "name": "Damien Querlioz"
                },
                "author": "Damien Querlioz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13565v1",
                "updated": "2025-04-18T09:06:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    6,
                    26,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T09:06:26Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    6,
                    26,
                    4,
                    108,
                    0
                ],
                "title": "MR-MAGIC: Robust Causal Inference Using Many Weak Genetic Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MR-MAGIC: Robust Causal Inference Using Many Weak Genetic Interactions"
                },
                "summary": "Mendelian randomization (MR) studies commonly use genetic variants as\ninstrumental variables to estimate causal effects of exposures on outcomes.\nHowever, the presence of invalid instruments-even when numerous-can lead to\nbiased causal estimates. We propose a novel identification strategy that\nremains valid even when all candidate instruments are invalid by leveraging\ngenetic interactions that collectively explain substantial exposure variation.\nRecognizing that individual interaction effects may be weak, we develop\nMR-MAGIC (Mendelian Randomization with MAny weak Genetic Interactions for\nCausality), a robust method that simultaneously addresses instrument invalidity\nand improves estimation efficiency. MR-MAGIC provides consistent and\nasymptotically normal estimates under a many-weak-interactions asymptotic\nframework. Comprehensive simulations and applications to UK Biobank data\ndemonstrate that MR-MAGIC outperforms conventional MR methods in practice,\noffering reliable causal inference when standard approaches fail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mendelian randomization (MR) studies commonly use genetic variants as\ninstrumental variables to estimate causal effects of exposures on outcomes.\nHowever, the presence of invalid instruments-even when numerous-can lead to\nbiased causal estimates. We propose a novel identification strategy that\nremains valid even when all candidate instruments are invalid by leveraging\ngenetic interactions that collectively explain substantial exposure variation.\nRecognizing that individual interaction effects may be weak, we develop\nMR-MAGIC (Mendelian Randomization with MAny weak Genetic Interactions for\nCausality), a robust method that simultaneously addresses instrument invalidity\nand improves estimation efficiency. MR-MAGIC provides consistent and\nasymptotically normal estimates under a many-weak-interactions asymptotic\nframework. Comprehensive simulations and applications to UK Biobank data\ndemonstrate that MR-MAGIC outperforms conventional MR methods in practice,\noffering reliable causal inference when standard approaches fail."
                },
                "authors": [
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Minhao Yao"
                    },
                    {
                        "name": "Zhonghua Liu"
                    },
                    {
                        "name": "Baoluo Sun"
                    }
                ],
                "author_detail": {
                    "name": "Baoluo Sun"
                },
                "author": "Baoluo Sun",
                "arxiv_comment": "33 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20999v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20999v3",
                "updated": "2025-04-18T09:04:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    4,
                    15,
                    4,
                    108,
                    0
                ],
                "published": "2024-07-30T17:38:24Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    38,
                    24,
                    1,
                    212,
                    0
                ],
                "title": "MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM\n  Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM\n  Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks. Typically, LLMs are first pre-trained on large corpora\nand subsequently fine-tuned on task-specific datasets. However, during\nfine-tuning, LLMs may forget some knowledge acquired in the pre-training stage,\nleading to a decline in general capabilities. Existing approaches to mitigate\nforgetting often rely on access to pre-training data, which may be unavailable\nin many real-world scenarios--such as fine-tuning checkpoint-only open-source\nLLMs. To address this challenge, we propose a new fine-tuning algorithm termed\nMomentum-Filtered Optimizer (MoFO). MoFO is an extension of greedy block\ncoordinate descent (BCD) methods: in each iteration, MoFO only updates the\nmodel parameters with the largest momentum magnitudes, while keeping all other\nparameters fixed. MoFO achieves similar fine-tuning performance to the default\nfine-tuning algorithm while effectively mitigating knowledge forgetting. We\nvalidate MoFO through rigorous convergence analysis and extensive experiments,\ndemonstrating its effectiveness in mitigating forgetting without pre-training\ndata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks. Typically, LLMs are first pre-trained on large corpora\nand subsequently fine-tuned on task-specific datasets. However, during\nfine-tuning, LLMs may forget some knowledge acquired in the pre-training stage,\nleading to a decline in general capabilities. Existing approaches to mitigate\nforgetting often rely on access to pre-training data, which may be unavailable\nin many real-world scenarios--such as fine-tuning checkpoint-only open-source\nLLMs. To address this challenge, we propose a new fine-tuning algorithm termed\nMomentum-Filtered Optimizer (MoFO). MoFO is an extension of greedy block\ncoordinate descent (BCD) methods: in each iteration, MoFO only updates the\nmodel parameters with the largest momentum magnitudes, while keeping all other\nparameters fixed. MoFO achieves similar fine-tuning performance to the default\nfine-tuning algorithm while effectively mitigating knowledge forgetting. We\nvalidate MoFO through rigorous convergence analysis and extensive experiments,\ndemonstrating its effectiveness in mitigating forgetting without pre-training\ndata."
                },
                "authors": [
                    {
                        "name": "Yupeng Chen"
                    },
                    {
                        "name": "Senmiao Wang"
                    },
                    {
                        "name": "Yushun Zhang"
                    },
                    {
                        "name": "Zhihang Lin"
                    },
                    {
                        "name": "Haozhe Zhang"
                    },
                    {
                        "name": "Weijian Sun"
                    },
                    {
                        "name": "Tian Ding"
                    },
                    {
                        "name": "Ruoyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Ruoyu Sun"
                },
                "author": "Ruoyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20999v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20999v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13562v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13562v1",
                "updated": "2025-04-18T09:02:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    2,
                    12,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T09:02:12Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    2,
                    12,
                    4,
                    108,
                    0
                ],
                "title": "DETAM: Defending LLMs Against Jailbreak Attacks via Targeted Attention\n  Modification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DETAM: Defending LLMs Against Jailbreak Attacks via Targeted Attention\n  Modification"
                },
                "summary": "With the widespread adoption of Large Language Models (LLMs), jailbreak\nattacks have become an increasingly pressing safety concern. While\nsafety-aligned LLMs can effectively defend against normal harmful queries, they\nremain vulnerable to such attacks. Existing defense methods primarily rely on\nfine-tuning or input modification, which often suffer from limited\ngeneralization and reduced utility. To address this, we introduce DETAM, a\nfinetuning-free defense approach that improves the defensive capabilities\nagainst jailbreak attacks of LLMs via targeted attention modification.\nSpecifically, we analyze the differences in attention scores between successful\nand unsuccessful defenses to identify the attention heads sensitive to\njailbreak attacks. During inference, we reallocate attention to emphasize the\nuser's core intention, minimizing interference from attack tokens. Our\nexperimental results demonstrate that DETAM outperforms various baselines in\njailbreak defense and exhibits robust generalization across different attacks\nand models, maintaining its effectiveness even on in-the-wild jailbreak data.\nFurthermore, in evaluating the model's utility, we incorporated over-defense\ndatasets, which further validate the superior performance of our approach. The\ncode will be released immediately upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread adoption of Large Language Models (LLMs), jailbreak\nattacks have become an increasingly pressing safety concern. While\nsafety-aligned LLMs can effectively defend against normal harmful queries, they\nremain vulnerable to such attacks. Existing defense methods primarily rely on\nfine-tuning or input modification, which often suffer from limited\ngeneralization and reduced utility. To address this, we introduce DETAM, a\nfinetuning-free defense approach that improves the defensive capabilities\nagainst jailbreak attacks of LLMs via targeted attention modification.\nSpecifically, we analyze the differences in attention scores between successful\nand unsuccessful defenses to identify the attention heads sensitive to\njailbreak attacks. During inference, we reallocate attention to emphasize the\nuser's core intention, minimizing interference from attack tokens. Our\nexperimental results demonstrate that DETAM outperforms various baselines in\njailbreak defense and exhibits robust generalization across different attacks\nand models, maintaining its effectiveness even on in-the-wild jailbreak data.\nFurthermore, in evaluating the model's utility, we incorporated over-defense\ndatasets, which further validate the superior performance of our approach. The\ncode will be released immediately upon acceptance."
                },
                "authors": [
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Han Jiang"
                    },
                    {
                        "name": "Zhihua Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhihua Wei"
                },
                "author": "Zhihua Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13562v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13562v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13560v1",
                "updated": "2025-04-18T08:58:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    8,
                    58,
                    40,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T08:58:40Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    8,
                    58,
                    40,
                    4,
                    108,
                    0
                ],
                "title": "Zero-Shot Industrial Anomaly Segmentation with Image-Aware Prompt\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Industrial Anomaly Segmentation with Image-Aware Prompt\n  Generation"
                },
                "summary": "Anomaly segmentation is essential for industrial quality, maintenance, and\nstability. Existing text-guided zero-shot anomaly segmentation models are\neffective but rely on fixed prompts, limiting adaptability in diverse\nindustrial scenarios. This highlights the need for flexible, context-aware\nprompting strategies. We propose Image-Aware Prompt Anomaly Segmentation\n(IAP-AS), which enhances anomaly segmentation by generating dynamic,\ncontext-aware prompts using an image tagging model and a large language model\n(LLM). IAP-AS extracts object attributes from images to generate context-aware\nprompts, improving adaptability and generalization in dynamic and unstructured\nindustrial environments. In our experiments, IAP-AS improves the F1-max metric\nby up to 10%, demonstrating superior adaptability and generalization. It\nprovides a scalable solution for anomaly segmentation across industries",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anomaly segmentation is essential for industrial quality, maintenance, and\nstability. Existing text-guided zero-shot anomaly segmentation models are\neffective but rely on fixed prompts, limiting adaptability in diverse\nindustrial scenarios. This highlights the need for flexible, context-aware\nprompting strategies. We propose Image-Aware Prompt Anomaly Segmentation\n(IAP-AS), which enhances anomaly segmentation by generating dynamic,\ncontext-aware prompts using an image tagging model and a large language model\n(LLM). IAP-AS extracts object attributes from images to generate context-aware\nprompts, improving adaptability and generalization in dynamic and unstructured\nindustrial environments. In our experiments, IAP-AS improves the F1-max metric\nby up to 10%, demonstrating superior adaptability and generalization. It\nprovides a scalable solution for anomaly segmentation across industries"
                },
                "authors": [
                    {
                        "name": "SoYoung Park"
                    },
                    {
                        "name": "Hyewon Lee"
                    },
                    {
                        "name": "Mingyu Choi"
                    },
                    {
                        "name": "Seunghoon Han"
                    },
                    {
                        "name": "Jong-Ryul Lee"
                    },
                    {
                        "name": "Sungsu Lim"
                    },
                    {
                        "name": "Tae-Ho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Tae-Ho Kim"
                },
                "author": "Tae-Ho Kim",
                "arxiv_comment": "Accepted to PAKDD 2025, 12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13557v1",
                "updated": "2025-04-18T08:49:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    8,
                    49,
                    45,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T08:49:45Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    8,
                    49,
                    45,
                    4,
                    108,
                    0
                ],
                "title": "Integrating LLMs for Grading and Appeal Resolution in Computer Science\n  Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating LLMs for Grading and Appeal Resolution in Computer Science\n  Education"
                },
                "summary": "This study explores the integration of Large Language Models (LLMs) into the\ngrading and appeal resolution process in computer science education. We\nintroduce AI-PAT, an AI-powered assessment tool that leverages LLMs to evaluate\ncomputer science exams, generate feedback, and address student appeals. AI-PAT\nwas used to assess over 850 exam submissions and handle 185 appeal cases. Our\nmulti-model comparison (ChatGPT, Gemini) reveals strong correlations between\nmodel outputs, though significant variability persists depending on\nconfiguration and prompt design. Human graders, while internally consistent,\nshowed notable inter-rater disagreement, further highlighting subjectivity in\nmanual evaluation. The appeal process led to grade changes in 74% of cases,\nindicating the need for continued refinement of AI evaluation strategies. While\nstudents appreciated the speed and detail of AI feedback, survey responses\nrevealed trust and fairness concerns. We conclude that AI-PAT offers scalable\nbenefits for formative assessment and feedback, but must be accompanied by\ntransparent grading rubrics, human oversight, and appeal mechanisms to ensure\nequitable outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the integration of Large Language Models (LLMs) into the\ngrading and appeal resolution process in computer science education. We\nintroduce AI-PAT, an AI-powered assessment tool that leverages LLMs to evaluate\ncomputer science exams, generate feedback, and address student appeals. AI-PAT\nwas used to assess over 850 exam submissions and handle 185 appeal cases. Our\nmulti-model comparison (ChatGPT, Gemini) reveals strong correlations between\nmodel outputs, though significant variability persists depending on\nconfiguration and prompt design. Human graders, while internally consistent,\nshowed notable inter-rater disagreement, further highlighting subjectivity in\nmanual evaluation. The appeal process led to grade changes in 74% of cases,\nindicating the need for continued refinement of AI evaluation strategies. While\nstudents appreciated the speed and detail of AI feedback, survey responses\nrevealed trust and fairness concerns. We conclude that AI-PAT offers scalable\nbenefits for formative assessment and feedback, but must be accompanied by\ntransparent grading rubrics, human oversight, and appeal mechanisms to ensure\nequitable outcomes."
                },
                "authors": [
                    {
                        "name": "I. Aytutuldu"
                    },
                    {
                        "name": "O. Yol"
                    },
                    {
                        "name": "Y. S. Akgul"
                    }
                ],
                "author_detail": {
                    "name": "Y. S. Akgul"
                },
                "arxiv_affiliation": "Gebze Technical University",
                "author": "Y. S. Akgul",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08907v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08907v2",
                "updated": "2025-04-18T08:47:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    8,
                    47,
                    39,
                    4,
                    108,
                    0
                ],
                "published": "2024-12-12T03:39:44Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    39,
                    44,
                    3,
                    347,
                    0
                ],
                "title": "GaGA: Towards Interactive Global Geolocation Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaGA: Towards Interactive Global Geolocation Assistant"
                },
                "summary": "Global geolocation, which seeks to predict the geographical location of\nimages captured anywhere in the world, is one of the most challenging tasks in\nthe field of computer vision. In this paper, we introduce an innovative\ninteractive global geolocation assistant named GaGA, built upon the flourishing\nlarge vision-language models (LVLMs). GaGA uncovers geographical clues within\nimages and combines them with the extensive world knowledge embedded in LVLMs\nto determine the geolocations while also providing justifications and\nexplanations for the prediction results. We further designed a novel\ninteractive geolocation method that surpasses traditional static inference\napproaches. It allows users to intervene, correct, or provide clues for the\npredictions, making the model more flexible and practical. The development of\nGaGA relies on the newly proposed Multi-modal Global Geolocation (MG-Geo)\ndataset, a comprehensive collection of 5 million high-quality image-text pairs.\nGaGA achieves state-of-the-art performance on the GWS15k dataset, improving\naccuracy by 4.57% at the country level and 2.92% at the city level, setting a\nnew benchmark. These advancements represent a significant leap forward in\ndeveloping highly accurate, interactive geolocation systems with global\napplicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global geolocation, which seeks to predict the geographical location of\nimages captured anywhere in the world, is one of the most challenging tasks in\nthe field of computer vision. In this paper, we introduce an innovative\ninteractive global geolocation assistant named GaGA, built upon the flourishing\nlarge vision-language models (LVLMs). GaGA uncovers geographical clues within\nimages and combines them with the extensive world knowledge embedded in LVLMs\nto determine the geolocations while also providing justifications and\nexplanations for the prediction results. We further designed a novel\ninteractive geolocation method that surpasses traditional static inference\napproaches. It allows users to intervene, correct, or provide clues for the\npredictions, making the model more flexible and practical. The development of\nGaGA relies on the newly proposed Multi-modal Global Geolocation (MG-Geo)\ndataset, a comprehensive collection of 5 million high-quality image-text pairs.\nGaGA achieves state-of-the-art performance on the GWS15k dataset, improving\naccuracy by 4.57% at the country level and 2.92% at the city level, setting a\nnew benchmark. These advancements represent a significant leap forward in\ndeveloping highly accurate, interactive geolocation systems with global\napplicability."
                },
                "authors": [
                    {
                        "name": "Zhiyang Dou"
                    },
                    {
                        "name": "Zipeng Wang"
                    },
                    {
                        "name": "Xumeng Han"
                    },
                    {
                        "name": "Guorong Li"
                    },
                    {
                        "name": "Zhipei Huang"
                    },
                    {
                        "name": "Zhenjun Han"
                    }
                ],
                "author_detail": {
                    "name": "Zhenjun Han"
                },
                "author": "Zhenjun Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08907v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08907v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11900v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11900v2",
                "updated": "2025-04-18T08:44:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    8,
                    44,
                    4,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-16T09:25:54Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    25,
                    54,
                    2,
                    106,
                    0
                ],
                "title": "Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models\n  via Plot Hole Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models\n  via Plot Hole Detection"
                },
                "summary": "Stories are a fundamental aspect of human experience. Engaging deeply with\nstories and spotting plot holes -- inconsistencies in a storyline that break\nthe internal logic or rules of a story's world -- requires nuanced reasoning\nskills, including tracking entities and events and their interplay, abstract\nthinking, pragmatic narrative understanding, commonsense and social reasoning,\nand theory of mind. As Large Language Models (LLMs) increasingly generate,\ninterpret, and modify text, rigorously assessing their narrative consistency\nand deeper language understanding becomes critical. However, existing\nbenchmarks focus mainly on surface-level comprehension. In this work, we\npropose plot hole detection in stories as a proxy to evaluate language\nunderstanding and reasoning in LLMs. We introduce FlawedFictionsMaker, a novel\nalgorithm to controllably and carefully synthesize plot holes in human-written\nstories. Using this algorithm, we construct a benchmark to evaluate LLMs' plot\nhole detection abilities in stories -- FlawedFictions -- , which is robust to\ncontamination, with human filtering ensuring high quality. We find that\nstate-of-the-art LLMs struggle in accurately solving FlawedFictions regardless\nof the reasoning effort allowed, with performance significantly degrading as\nstory length increases. Finally, we show that LLM-based story summarization and\nstory generation are prone to introducing plot holes, with more than 50% and\n100% increases in plot hole detection rates with respect to human-written\noriginals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stories are a fundamental aspect of human experience. Engaging deeply with\nstories and spotting plot holes -- inconsistencies in a storyline that break\nthe internal logic or rules of a story's world -- requires nuanced reasoning\nskills, including tracking entities and events and their interplay, abstract\nthinking, pragmatic narrative understanding, commonsense and social reasoning,\nand theory of mind. As Large Language Models (LLMs) increasingly generate,\ninterpret, and modify text, rigorously assessing their narrative consistency\nand deeper language understanding becomes critical. However, existing\nbenchmarks focus mainly on surface-level comprehension. In this work, we\npropose plot hole detection in stories as a proxy to evaluate language\nunderstanding and reasoning in LLMs. We introduce FlawedFictionsMaker, a novel\nalgorithm to controllably and carefully synthesize plot holes in human-written\nstories. Using this algorithm, we construct a benchmark to evaluate LLMs' plot\nhole detection abilities in stories -- FlawedFictions -- , which is robust to\ncontamination, with human filtering ensuring high quality. We find that\nstate-of-the-art LLMs struggle in accurately solving FlawedFictions regardless\nof the reasoning effort allowed, with performance significantly degrading as\nstory length increases. Finally, we show that LLM-based story summarization and\nstory generation are prone to introducing plot holes, with more than 50% and\n100% increases in plot hole detection rates with respect to human-written\noriginals."
                },
                "authors": [
                    {
                        "name": "Kabir Ahuja"
                    },
                    {
                        "name": "Melanie Sclar"
                    },
                    {
                        "name": "Yulia Tsvetkov"
                    }
                ],
                "author_detail": {
                    "name": "Yulia Tsvetkov"
                },
                "author": "Yulia Tsvetkov",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11900v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11900v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12867v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12867v2",
                "updated": "2025-04-18T08:18:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    8,
                    18,
                    11,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-17T11:50:04Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    11,
                    50,
                    4,
                    3,
                    107,
                    0
                ],
                "title": "EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text\n  Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text\n  Prompting"
                },
                "summary": "Human speech goes beyond the mere transfer of information; it is a profound\nexchange of emotions and a connection between individuals. While Text-to-Speech\n(TTS) models have made huge progress, they still face challenges in controlling\nthe emotional expression in the generated speech. In this work, we propose\nEmoVoice, a novel emotion-controllable TTS model that exploits large language\nmodels (LLMs) to enable fine-grained freestyle natural language emotion\ncontrol, and a phoneme boost variant design that makes the model output phoneme\ntokens and audio tokens in parallel to enhance content consistency, inspired by\nchain-of-thought (CoT) and chain-of-modality (CoM) techniques. Besides, we\nintroduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring\nexpressive speech and fine-grained emotion labels with natural language\ndescriptions. EmoVoice achieves state-of-the-art performance on the English\nEmoVoice-DB test set using only synthetic training data, and on the Chinese\nSecap test set using our in-house data. We further investigate the reliability\nof existing emotion evaluation metrics and their alignment with human\nperceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and\nGemini to assess emotional speech. Demo samples are available at\nhttps://anonymous.4open.science/r/EmoVoice-DF55. Dataset, code, and checkpoints\nwill be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human speech goes beyond the mere transfer of information; it is a profound\nexchange of emotions and a connection between individuals. While Text-to-Speech\n(TTS) models have made huge progress, they still face challenges in controlling\nthe emotional expression in the generated speech. In this work, we propose\nEmoVoice, a novel emotion-controllable TTS model that exploits large language\nmodels (LLMs) to enable fine-grained freestyle natural language emotion\ncontrol, and a phoneme boost variant design that makes the model output phoneme\ntokens and audio tokens in parallel to enhance content consistency, inspired by\nchain-of-thought (CoT) and chain-of-modality (CoM) techniques. Besides, we\nintroduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring\nexpressive speech and fine-grained emotion labels with natural language\ndescriptions. EmoVoice achieves state-of-the-art performance on the English\nEmoVoice-DB test set using only synthetic training data, and on the Chinese\nSecap test set using our in-house data. We further investigate the reliability\nof existing emotion evaluation metrics and their alignment with human\nperceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and\nGemini to assess emotional speech. Demo samples are available at\nhttps://anonymous.4open.science/r/EmoVoice-DF55. Dataset, code, and checkpoints\nwill be released."
                },
                "authors": [
                    {
                        "name": "Guanrou Yang"
                    },
                    {
                        "name": "Chen Yang"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Wenxi Chen"
                    },
                    {
                        "name": "Wen Wang"
                    },
                    {
                        "name": "Tianrui Wang"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Zhikang Niu"
                    },
                    {
                        "name": "Wenrui Liu"
                    },
                    {
                        "name": "Fan Yu"
                    },
                    {
                        "name": "Zhihao Du"
                    },
                    {
                        "name": "Zhifu Gao"
                    },
                    {
                        "name": "ShiLiang Zhang"
                    },
                    {
                        "name": "Xie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xie Chen"
                },
                "author": "Xie Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12867v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12867v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15708v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15708v3",
                "updated": "2025-04-18T08:09:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    8,
                    9,
                    26,
                    4,
                    108,
                    0
                ],
                "published": "2025-01-27T00:05:12Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    0,
                    5,
                    12,
                    0,
                    27,
                    0
                ],
                "title": "StaICC: Standardized Evaluation for Classification Task in In-context\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StaICC: Standardized Evaluation for Classification Task in In-context\n  Learning"
                },
                "summary": "Classification tasks are widely investigated in the In-Context Learning (ICL)\nparadigm. However, current efforts are evaluated on disjoint benchmarks and\nsettings, while their performances are significantly influenced by some trivial\nvariables, such as prompt templates, data sampling, instructions, etc., which\nleads to significant inconsistencies in the results reported across various\nliterature, preventing fair comparison or meta-analysis across different\npapers. Therefore, this paper proposes a standardized and easy-to-use\nevaluation toolkit (StaICC) for in-context classification. Including, for the\nnormal classification task, we provide StaICC-Normal, selecting 10 widely used\ndatasets, and generating prompts with a fixed form, to mitigate the variance\namong the experiment implementations. To enrich the usage of our benchmark, we\nalso provide a sub-benchmark StaICC-Diag for diagnosing ICL from several\naspects, aiming for a more robust inference processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classification tasks are widely investigated in the In-Context Learning (ICL)\nparadigm. However, current efforts are evaluated on disjoint benchmarks and\nsettings, while their performances are significantly influenced by some trivial\nvariables, such as prompt templates, data sampling, instructions, etc., which\nleads to significant inconsistencies in the results reported across various\nliterature, preventing fair comparison or meta-analysis across different\npapers. Therefore, this paper proposes a standardized and easy-to-use\nevaluation toolkit (StaICC) for in-context classification. Including, for the\nnormal classification task, we provide StaICC-Normal, selecting 10 widely used\ndatasets, and generating prompts with a fixed form, to mitigate the variance\namong the experiment implementations. To enrich the usage of our benchmark, we\nalso provide a sub-benchmark StaICC-Diag for diagnosing ICL from several\naspects, aiming for a more robust inference processing."
                },
                "authors": [
                    {
                        "name": "Hakaze Cho"
                    },
                    {
                        "name": "Naoya Inoue"
                    }
                ],
                "author_detail": {
                    "name": "Naoya Inoue"
                },
                "author": "Naoya Inoue",
                "arxiv_comment": "20 pages, 8 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15708v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15708v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07880v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07880v2",
                "updated": "2025-04-18T08:05:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    8,
                    5,
                    53,
                    4,
                    108,
                    0
                ],
                "published": "2024-07-10T17:48:25Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    17,
                    48,
                    25,
                    2,
                    192,
                    0
                ],
                "title": "Towards Robust Alignment of Language Models: Distributionally\n  Robustifying Direct Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Robust Alignment of Language Models: Distributionally\n  Robustifying Direct Preference Optimization"
                },
                "summary": "This study addresses the challenge of noise in training datasets for Direct\nPreference Optimization (DPO), a method for aligning Large Language Models\n(LLMs) with human preferences. We categorize noise into pointwise noise, which\nincludes low-quality data points, and pairwise noise, which encompasses\nerroneous data pair associations that affect preference rankings. Utilizing\nDistributionally Robust Optimization (DRO), we enhance DPO's resilience to\nthese types of noise. Our theoretical insights reveal that DPO inherently\nembeds DRO principles, conferring robustness to pointwise noise, with the\nregularization coefficient $\\beta$ playing a critical role in its noise\nresistance. Extending this framework, we introduce Distributionally\nRobustifying DPO (Dr. DPO), which integrates pairwise robustness by optimizing\nagainst worst-case pairwise scenarios. The novel hyperparameter $\\beta'$ in Dr.\nDPO allows for fine-tuned control over data pair reliability, providing a\nstrategic balance between exploration and exploitation in noisy training\nenvironments. Empirical evaluations demonstrate that Dr. DPO substantially\nimproves the quality of generated text and response accuracy in preference\ndatasets, showcasing enhanced performance in both noisy and noise-free\nsettings. The code is available at https://github.com/junkangwu/Dr_DPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study addresses the challenge of noise in training datasets for Direct\nPreference Optimization (DPO), a method for aligning Large Language Models\n(LLMs) with human preferences. We categorize noise into pointwise noise, which\nincludes low-quality data points, and pairwise noise, which encompasses\nerroneous data pair associations that affect preference rankings. Utilizing\nDistributionally Robust Optimization (DRO), we enhance DPO's resilience to\nthese types of noise. Our theoretical insights reveal that DPO inherently\nembeds DRO principles, conferring robustness to pointwise noise, with the\nregularization coefficient $\\beta$ playing a critical role in its noise\nresistance. Extending this framework, we introduce Distributionally\nRobustifying DPO (Dr. DPO), which integrates pairwise robustness by optimizing\nagainst worst-case pairwise scenarios. The novel hyperparameter $\\beta'$ in Dr.\nDPO allows for fine-tuned control over data pair reliability, providing a\nstrategic balance between exploration and exploitation in noisy training\nenvironments. Empirical evaluations demonstrate that Dr. DPO substantially\nimproves the quality of generated text and response accuracy in preference\ndatasets, showcasing enhanced performance in both noisy and noise-free\nsettings. The code is available at https://github.com/junkangwu/Dr_DPO."
                },
                "authors": [
                    {
                        "name": "Junkang Wu"
                    },
                    {
                        "name": "Yuexiang Xie"
                    },
                    {
                        "name": "Zhengyi Yang"
                    },
                    {
                        "name": "Jiancan Wu"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Jinyang Gao"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07880v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07880v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15545v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15545v5",
                "updated": "2025-04-18T07:56:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    7,
                    56,
                    16,
                    4,
                    108,
                    0
                ],
                "published": "2024-08-28T05:41:52Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    5,
                    41,
                    52,
                    2,
                    241,
                    0
                ],
                "title": "SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding"
                },
                "summary": "Scientific literature understanding is crucial for extracting targeted\ninformation and garnering insights, thereby significantly advancing scientific\ndiscovery. Despite the remarkable success of Large Language Models (LLMs), they\nface challenges in scientific literature understanding, primarily due to (1) a\nlack of scientific knowledge and (2) unfamiliarity with specialized scientific\ntasks.\n  To develop an LLM specialized in scientific literature understanding, we\npropose a hybrid strategy that integrates continual pre-training (CPT) and\nsupervised fine-tuning (SFT), to simultaneously infuse scientific domain\nknowledge and enhance instruction-following capabilities for domain-specific\ntasks.cIn this process, we identify two key challenges: (1) constructing\nhigh-quality CPT corpora, and (2) generating diverse SFT instructions. We\naddress these challenges through a meticulous pipeline, including PDF text\nextraction, parsing content error correction, quality filtering, and synthetic\ninstruction creation. Applying this strategy, we present a suite of LLMs:\nSciLitLLM, specialized in scientific literature understanding. These models\ndemonstrate promising performance on scientific literature understanding\nbenchmarks.\n  Our contributions are threefold: (1) We present an effective framework that\nintegrates CPT and SFT to adapt LLMs to scientific literature understanding,\nwhich can also be easily adapted to other domains. (2) We propose an LLM-based\nsynthesis method to generate diverse and high-quality scientific instructions,\nresulting in a new instruction set -- SciLitIns -- for supervised fine-tuning\nin less-represented scientific domains. (3) SciLitLLM achieves promising\nperformance improvements on scientific literature understanding benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific literature understanding is crucial for extracting targeted\ninformation and garnering insights, thereby significantly advancing scientific\ndiscovery. Despite the remarkable success of Large Language Models (LLMs), they\nface challenges in scientific literature understanding, primarily due to (1) a\nlack of scientific knowledge and (2) unfamiliarity with specialized scientific\ntasks.\n  To develop an LLM specialized in scientific literature understanding, we\npropose a hybrid strategy that integrates continual pre-training (CPT) and\nsupervised fine-tuning (SFT), to simultaneously infuse scientific domain\nknowledge and enhance instruction-following capabilities for domain-specific\ntasks.cIn this process, we identify two key challenges: (1) constructing\nhigh-quality CPT corpora, and (2) generating diverse SFT instructions. We\naddress these challenges through a meticulous pipeline, including PDF text\nextraction, parsing content error correction, quality filtering, and synthetic\ninstruction creation. Applying this strategy, we present a suite of LLMs:\nSciLitLLM, specialized in scientific literature understanding. These models\ndemonstrate promising performance on scientific literature understanding\nbenchmarks.\n  Our contributions are threefold: (1) We present an effective framework that\nintegrates CPT and SFT to adapt LLMs to scientific literature understanding,\nwhich can also be easily adapted to other domains. (2) We propose an LLM-based\nsynthesis method to generate diverse and high-quality scientific instructions,\nresulting in a new instruction set -- SciLitIns -- for supervised fine-tuning\nin less-represented scientific domains. (3) SciLitLLM achieves promising\nperformance improvements on scientific literature understanding benchmarks."
                },
                "authors": [
                    {
                        "name": "Sihang Li"
                    },
                    {
                        "name": "Jin Huang"
                    },
                    {
                        "name": "Jiaxi Zhuang"
                    },
                    {
                        "name": "Yaorui Shi"
                    },
                    {
                        "name": "Xiaochen Cai"
                    },
                    {
                        "name": "Mingjun Xu"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Guolin Ke"
                    },
                    {
                        "name": "Hengxing Cai"
                    }
                ],
                "author_detail": {
                    "name": "Hengxing Cai"
                },
                "author": "Hengxing Cai",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15545v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15545v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13534v1",
                "updated": "2025-04-18T07:55:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    7,
                    55,
                    9,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T07:55:09Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    7,
                    55,
                    9,
                    4,
                    108,
                    0
                ],
                "title": "CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation\n  to Enhance Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation\n  to Enhance Reasoning in Large Language Models"
                },
                "summary": "While chain-of-thought (CoT) reasoning improves the performance of large\nlanguage models (LLMs) in complex tasks, it still has two main challenges: the\nlow reliability of relying solely on LLMs to generate reasoning chains and the\ninterference of natural language reasoning chains on the inference logic of\nLLMs. To address these issues, we propose CoT-RAG, a novel reasoning framework\nwith three key designs: (i) Knowledge Graph-driven CoT Generation, featuring\nknowledge graphs to modulate reasoning chain generation of LLMs, thereby\nenhancing reasoning credibility; (ii) Learnable Knowledge Case-aware RAG, which\nincorporates retrieval-augmented generation (RAG) into knowledge graphs to\nretrieve relevant sub-cases and sub-descriptions, providing LLMs with learnable\ninformation; (iii) Pseudo-Program Prompting Execution, which encourages LLMs to\nexecute reasoning tasks in pseudo-programs with greater logical rigor. We\nconduct a comprehensive evaluation on nine public datasets, covering three\nreasoning problems. Compared with the-state-of-the-art methods, CoT-RAG\nexhibits a significant accuracy improvement, ranging from 4.0% to 23.0%.\nFurthermore, testing on four domain-specific datasets, CoT-RAG shows remarkable\naccuracy and efficient execution, highlighting its strong practical\napplicability and scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While chain-of-thought (CoT) reasoning improves the performance of large\nlanguage models (LLMs) in complex tasks, it still has two main challenges: the\nlow reliability of relying solely on LLMs to generate reasoning chains and the\ninterference of natural language reasoning chains on the inference logic of\nLLMs. To address these issues, we propose CoT-RAG, a novel reasoning framework\nwith three key designs: (i) Knowledge Graph-driven CoT Generation, featuring\nknowledge graphs to modulate reasoning chain generation of LLMs, thereby\nenhancing reasoning credibility; (ii) Learnable Knowledge Case-aware RAG, which\nincorporates retrieval-augmented generation (RAG) into knowledge graphs to\nretrieve relevant sub-cases and sub-descriptions, providing LLMs with learnable\ninformation; (iii) Pseudo-Program Prompting Execution, which encourages LLMs to\nexecute reasoning tasks in pseudo-programs with greater logical rigor. We\nconduct a comprehensive evaluation on nine public datasets, covering three\nreasoning problems. Compared with the-state-of-the-art methods, CoT-RAG\nexhibits a significant accuracy improvement, ranging from 4.0% to 23.0%.\nFurthermore, testing on four domain-specific datasets, CoT-RAG shows remarkable\naccuracy and efficient execution, highlighting its strong practical\napplicability and scalability."
                },
                "authors": [
                    {
                        "name": "Feiyang Li"
                    },
                    {
                        "name": "Peng Fang"
                    },
                    {
                        "name": "Zhan Shi"
                    },
                    {
                        "name": "Arijit Khan"
                    },
                    {
                        "name": "Fang Wang"
                    },
                    {
                        "name": "Dan Feng"
                    },
                    {
                        "name": "Weihao Wang"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Yongjian Cui"
                    }
                ],
                "author_detail": {
                    "name": "Yongjian Cui"
                },
                "author": "Yongjian Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11056v2",
                "updated": "2025-04-18T07:50:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    7,
                    50,
                    8,
                    4,
                    108,
                    0
                ],
                "published": "2024-09-17T10:33:27Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    33,
                    27,
                    1,
                    261,
                    0
                ],
                "title": "Large Language Models are Good Multi-lingual Learners : When LLMs Meet\n  Cross-lingual Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are Good Multi-lingual Learners : When LLMs Meet\n  Cross-lingual Prompts"
                },
                "summary": "With the advent of Large Language Models (LLMs), generating rule-based data\nfor real-world applications has become more accessible. Due to the inherent\nambiguity of natural language and the complexity of rule sets, especially in\nlong contexts, LLMs often struggle to follow all specified rules, frequently\nomitting at least one. To enhance the reasoning and understanding of LLMs on\nlong and complex contexts, we propose a novel prompting strategy Multi-Lingual\nPrompt, namely MLPrompt, which automatically translates the error-prone rule\nthat an LLM struggles to follow into another language, thus drawing greater\nattention to it. Experimental results on public datasets across various tasks\nhave shown MLPrompt can outperform state-of-the-art prompting methods such as\nChain of Thought, Tree of Thought, and Self-Consistency. Additionally, we\nintroduce a framework integrating MLPrompt with an auto-checking mechanism for\nstructured data generation, with a specific case study in text-to-MIP\ninstances. Further, we extend the proposed framework for text-to-SQL to\ndemonstrate its generation ability towards structured data synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of Large Language Models (LLMs), generating rule-based data\nfor real-world applications has become more accessible. Due to the inherent\nambiguity of natural language and the complexity of rule sets, especially in\nlong contexts, LLMs often struggle to follow all specified rules, frequently\nomitting at least one. To enhance the reasoning and understanding of LLMs on\nlong and complex contexts, we propose a novel prompting strategy Multi-Lingual\nPrompt, namely MLPrompt, which automatically translates the error-prone rule\nthat an LLM struggles to follow into another language, thus drawing greater\nattention to it. Experimental results on public datasets across various tasks\nhave shown MLPrompt can outperform state-of-the-art prompting methods such as\nChain of Thought, Tree of Thought, and Self-Consistency. Additionally, we\nintroduce a framework integrating MLPrompt with an auto-checking mechanism for\nstructured data generation, with a specific case study in text-to-MIP\ninstances. Further, we extend the proposed framework for text-to-SQL to\ndemonstrate its generation ability towards structured data synthesis."
                },
                "authors": [
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Zhenqi He"
                    },
                    {
                        "name": "Wing-Yin Yu"
                    },
                    {
                        "name": "Xiaojin Fu"
                    },
                    {
                        "name": "Xiongwei Han"
                    }
                ],
                "author_detail": {
                    "name": "Xiongwei Han"
                },
                "author": "Xiongwei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13843v2",
                "updated": "2025-04-18T07:48:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    7,
                    48,
                    48,
                    4,
                    108,
                    0
                ],
                "published": "2025-02-19T16:02:59Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    2,
                    59,
                    2,
                    50,
                    0
                ],
                "title": "AgentCF++: Memory-enhanced LLM-based Agents for Popularity-aware\n  Cross-domain Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentCF++: Memory-enhanced LLM-based Agents for Popularity-aware\n  Cross-domain Recommendations"
                },
                "summary": "LLM-based user agents, which simulate user interaction behavior, are emerging\nas a promising approach to enhancing recommender systems. In real-world\nscenarios, users' interactions often exhibit cross-domain characteristics and\nare influenced by others. However, the memory design in current methods causes\nuser agents to introduce significant irrelevant information during\ndecision-making in cross-domain scenarios and makes them unable to recognize\nthe influence of other users' interactions, such as popularity factors. To\ntackle this issue, we propose a dual-layer memory architecture combined with a\ntwo-step fusion mechanism. This design avoids irrelevant information during\ndecision-making while ensuring effective integration of cross-domain\npreferences. We also introduce the concepts of interest groups and group-shared\nmemory to better capture the influence of popularity factors on users with\nsimilar interests. Comprehensive experiments validate the effectiveness of\nAgentCF++. Our code is available at https://github.com/jhliu0807/AgentCF-plus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based user agents, which simulate user interaction behavior, are emerging\nas a promising approach to enhancing recommender systems. In real-world\nscenarios, users' interactions often exhibit cross-domain characteristics and\nare influenced by others. However, the memory design in current methods causes\nuser agents to introduce significant irrelevant information during\ndecision-making in cross-domain scenarios and makes them unable to recognize\nthe influence of other users' interactions, such as popularity factors. To\ntackle this issue, we propose a dual-layer memory architecture combined with a\ntwo-step fusion mechanism. This design avoids irrelevant information during\ndecision-making while ensuring effective integration of cross-domain\npreferences. We also introduce the concepts of interest groups and group-shared\nmemory to better capture the influence of popularity factors on users with\nsimilar interests. Comprehensive experiments validate the effectiveness of\nAgentCF++. Our code is available at https://github.com/jhliu0807/AgentCF-plus."
                },
                "authors": [
                    {
                        "name": "Jiahao Liu"
                    },
                    {
                        "name": "Shengkang Gu"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Guangping Zhang"
                    },
                    {
                        "name": "Mingzhe Han"
                    },
                    {
                        "name": "Hansu Gu"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Tun Lu"
                    },
                    {
                        "name": "Li Shang"
                    },
                    {
                        "name": "Ning Gu"
                    }
                ],
                "author_detail": {
                    "name": "Ning Gu"
                },
                "author": "Ning Gu",
                "arxiv_comment": "Accepted by SIGIR 2025, 6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02497v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02497v2",
                "updated": "2025-04-18T07:46:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    7,
                    46,
                    25,
                    4,
                    108,
                    0
                ],
                "published": "2025-03-04T11:04:35Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    4,
                    35,
                    1,
                    63,
                    0
                ],
                "title": "PennyLang: Pioneering LLM-Based Quantum Code Generation with a Novel\n  PennyLane-Centric Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PennyLang: Pioneering LLM-Based Quantum Code Generation with a Novel\n  PennyLane-Centric Dataset"
                },
                "summary": "Large Language Models (LLMs) offer remarkable capabilities in code\ngeneration, natural language processing, and domain-specific reasoning.\nHowever, their application in quantum software development remains\nunderexplored, particularly for PennyLane-a leading framework for hybrid\nquantum-classical computing. To address this gap, we introduce a novel,\nhigh-quality dataset comprising 3,347 PennyLane-specific quantum code samples\nand contextual descriptions, specifically curated to support LLM training and\nfine-tuning for quantum code assistance. Our contributions are threefold: (1)\nthe automatic construction and open-source release of a comprehensive PennyLane\ndataset derived from textbooks, official documentation, and open-source\nrepositories; (2) a structured methodology for data curation, annotation, and\nformatting to enhance LLM usability and relevance; and (3) a rigorous\nevaluation of code generation capabilities using both baseline\nRetrieval-Augmented Generation (RAG) and a GraphRAG-enhanced pipeline. Using\nthe PennyLang framework, we demonstrate that GraphRAG, when applied to a GPT-4o\nMini model, substantially outperforms standard prompting and baseline RAG.\nAccuracy improves from 20.5% (without RAG) to 58.2% with GraphRAG, showcasing\nits effectiveness in reducing hallucinations and improving code correctness in\nquantum programming tasks. Compared to prior efforts focused largely on Qiskit,\nour work expands LLM-based assistance to the PennyLane ecosystem, contributing\npractical tools and reproducible methodologies for advancing AI-assisted\nquantum software development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) offer remarkable capabilities in code\ngeneration, natural language processing, and domain-specific reasoning.\nHowever, their application in quantum software development remains\nunderexplored, particularly for PennyLane-a leading framework for hybrid\nquantum-classical computing. To address this gap, we introduce a novel,\nhigh-quality dataset comprising 3,347 PennyLane-specific quantum code samples\nand contextual descriptions, specifically curated to support LLM training and\nfine-tuning for quantum code assistance. Our contributions are threefold: (1)\nthe automatic construction and open-source release of a comprehensive PennyLane\ndataset derived from textbooks, official documentation, and open-source\nrepositories; (2) a structured methodology for data curation, annotation, and\nformatting to enhance LLM usability and relevance; and (3) a rigorous\nevaluation of code generation capabilities using both baseline\nRetrieval-Augmented Generation (RAG) and a GraphRAG-enhanced pipeline. Using\nthe PennyLang framework, we demonstrate that GraphRAG, when applied to a GPT-4o\nMini model, substantially outperforms standard prompting and baseline RAG.\nAccuracy improves from 20.5% (without RAG) to 58.2% with GraphRAG, showcasing\nits effectiveness in reducing hallucinations and improving code correctness in\nquantum programming tasks. Compared to prior efforts focused largely on Qiskit,\nour work expands LLM-based assistance to the PennyLane ecosystem, contributing\npractical tools and reproducible methodologies for advancing AI-assisted\nquantum software development."
                },
                "authors": [
                    {
                        "name": "Abdul Basit"
                    },
                    {
                        "name": "Nouhaila Innan"
                    },
                    {
                        "name": "Haider Asif"
                    },
                    {
                        "name": "Minghao Shao"
                    },
                    {
                        "name": "Muhammad Kashif"
                    },
                    {
                        "name": "Alberto Marchisio"
                    },
                    {
                        "name": "Muhammad Shafique"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Shafique"
                },
                "author": "Muhammad Shafique",
                "arxiv_comment": "10 pages, 7 figures, 7 tables, submitted for review under QCE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02497v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02497v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50 (Primary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13845v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13845v2",
                "updated": "2025-04-18T07:45:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    7,
                    45,
                    55,
                    4,
                    108,
                    0
                ],
                "published": "2025-02-19T16:08:17Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    8,
                    17,
                    2,
                    50,
                    0
                ],
                "title": "Improving LLM-powered Recommendations with Personalized Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving LLM-powered Recommendations with Personalized Information"
                },
                "summary": "Due to the lack of explicit reasoning modeling, existing LLM-powered\nrecommendations fail to leverage LLMs' reasoning capabilities effectively. In\nthis paper, we propose a pipeline called CoT-Rec, which integrates two key\nChain-of-Thought (CoT) processes -- user preference analysis and item\nperception analysis -- into LLM-powered recommendations, thereby enhancing the\nutilization of LLMs' reasoning abilities. CoT-Rec consists of two stages: (1)\npersonalized information extraction, where user preferences and item perception\nare extracted, and (2) personalized information utilization, where this\ninformation is incorporated into the LLM-powered recommendation process.\nExperimental results demonstrate that CoT-Rec shows potential for improving\nLLM-powered recommendations. The implementation is publicly available at\nhttps://github.com/jhliu0807/CoT-Rec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the lack of explicit reasoning modeling, existing LLM-powered\nrecommendations fail to leverage LLMs' reasoning capabilities effectively. In\nthis paper, we propose a pipeline called CoT-Rec, which integrates two key\nChain-of-Thought (CoT) processes -- user preference analysis and item\nperception analysis -- into LLM-powered recommendations, thereby enhancing the\nutilization of LLMs' reasoning abilities. CoT-Rec consists of two stages: (1)\npersonalized information extraction, where user preferences and item perception\nare extracted, and (2) personalized information utilization, where this\ninformation is incorporated into the LLM-powered recommendation process.\nExperimental results demonstrate that CoT-Rec shows potential for improving\nLLM-powered recommendations. The implementation is publicly available at\nhttps://github.com/jhliu0807/CoT-Rec."
                },
                "authors": [
                    {
                        "name": "Jiahao Liu"
                    },
                    {
                        "name": "Xueshuo Yan"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Guangping Zhang"
                    },
                    {
                        "name": "Hansu Gu"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Tun Lu"
                    },
                    {
                        "name": "Li Shang"
                    },
                    {
                        "name": "Ning Gu"
                    }
                ],
                "author_detail": {
                    "name": "Ning Gu"
                },
                "author": "Ning Gu",
                "arxiv_comment": "Accepted by SIGIR 2025, 7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13845v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13845v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15449v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15449v4",
                "updated": "2025-04-18T07:31:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    7,
                    31,
                    11,
                    4,
                    108,
                    0
                ],
                "published": "2024-06-05T08:11:41Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    8,
                    11,
                    41,
                    2,
                    157,
                    0
                ],
                "title": "Exponential rate of epidemic spreading on complex networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exponential rate of epidemic spreading on complex networks"
                },
                "summary": "The initial phase of an epidemic is often characterized by an exponential\nincrease in the number of infected individuals. In this paper, we predict the\nexponential spreading rate of an epidemic on a complex network. We first find\nan expression of the reproduction number for a network, based on the degree\ndistribution, the network assortativity, and the level of clustering. We then\nconnect this reproduction number and the disease infectiousness to the\nspreading rate. Our result holds for a broad range of networks, apart from\nnetworks with very broad degree distribution, where no clear exponential regime\nis present. Our theory bridges the gap between classic epidemiology and the\ntheory of complex networks, with broad implications for model inference and\npolicy making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The initial phase of an epidemic is often characterized by an exponential\nincrease in the number of infected individuals. In this paper, we predict the\nexponential spreading rate of an epidemic on a complex network. We first find\nan expression of the reproduction number for a network, based on the degree\ndistribution, the network assortativity, and the level of clustering. We then\nconnect this reproduction number and the disease infectiousness to the\nspreading rate. Our result holds for a broad range of networks, apart from\nnetworks with very broad degree distribution, where no clear exponential regime\nis present. Our theory bridges the gap between classic epidemiology and the\ntheory of complex networks, with broad implications for model inference and\npolicy making."
                },
                "authors": [
                    {
                        "name": "Samuel Cure"
                    },
                    {
                        "name": "Florian G. Pflug"
                    },
                    {
                        "name": "Simone Pigolotti"
                    }
                ],
                "author_detail": {
                    "name": "Simone Pigolotti"
                },
                "author": "Simone Pigolotti",
                "arxiv_doi": "10.1103/PhysRevE.111.044311",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevE.111.044311",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.15449v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15449v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 13 figures, accepted version",
                "arxiv_journal_ref": "Phys. Rev. E 111, 044311 (2025)",
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13520v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13520v1",
                "updated": "2025-04-18T07:18:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    7,
                    18,
                    51,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T07:18:51Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    7,
                    18,
                    51,
                    4,
                    108,
                    0
                ],
                "title": "Bayesian Model Averaging in Causal Instrumental Variable Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Model Averaging in Causal Instrumental Variable Models"
                },
                "summary": "Instrumental variables are a popular tool to infer causal effects under\nunobserved confounding, but choosing suitable instruments is challenging in\npractice. We propose gIVBMA, a Bayesian model averaging procedure that\naddresses this challenge by averaging across different sets of instrumental\nvariables and covariates in a structural equation model. Our approach extends\nprevious work through a scale-invariant prior structure and accommodates\nnon-Gaussian outcomes and treatments, offering greater flexibility than\nexisting methods. The computational strategy uses conditional Bayes factors to\nupdate models separately for the outcome and treatments. We prove that this\nmodel selection procedure is consistent. By explicitly accounting for model\nuncertainty, gIVBMA allows instruments and covariates to switch roles and\nprovides robustness against invalid instruments. In simulation experiments,\ngIVBMA outperforms current state-of-the-art methods. We demonstrate its\nusefulness in two empirical applications: the effects of malaria and\ninstitutions on income per capita and the returns to schooling. A software\nimplementation of gIVBMA is available in Julia.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instrumental variables are a popular tool to infer causal effects under\nunobserved confounding, but choosing suitable instruments is challenging in\npractice. We propose gIVBMA, a Bayesian model averaging procedure that\naddresses this challenge by averaging across different sets of instrumental\nvariables and covariates in a structural equation model. Our approach extends\nprevious work through a scale-invariant prior structure and accommodates\nnon-Gaussian outcomes and treatments, offering greater flexibility than\nexisting methods. The computational strategy uses conditional Bayes factors to\nupdate models separately for the outcome and treatments. We prove that this\nmodel selection procedure is consistent. By explicitly accounting for model\nuncertainty, gIVBMA allows instruments and covariates to switch roles and\nprovides robustness against invalid instruments. In simulation experiments,\ngIVBMA outperforms current state-of-the-art methods. We demonstrate its\nusefulness in two empirical applications: the effects of malaria and\ninstitutions on income per capita and the returns to schooling. A software\nimplementation of gIVBMA is available in Julia."
                },
                "authors": [
                    {
                        "name": "Gregor Steiner"
                    },
                    {
                        "name": "Mark Steel"
                    }
                ],
                "author_detail": {
                    "name": "Mark Steel"
                },
                "author": "Mark Steel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13520v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13520v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11260v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11260v3",
                "updated": "2025-04-18T07:15:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    7,
                    15,
                    33,
                    4,
                    108,
                    0
                ],
                "published": "2024-06-17T07:00:41Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    7,
                    0,
                    41,
                    0,
                    169,
                    0
                ],
                "title": "Adversarial Style Augmentation via Large Language Model for Robust Fake\n  News Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Style Augmentation via Large Language Model for Robust Fake\n  News Detection"
                },
                "summary": "The spread of fake news harms individuals and presents a critical social\nchallenge that must be addressed. Although numerous algorithmic and insightful\nfeatures have been developed to detect fake news, many of these features can be\nmanipulated with style-conversion attacks, especially with the emergence of\nadvanced language models, making it more difficult to differentiate from\ngenuine news. This study proposes adversarial style augmentation, AdStyle,\ndesigned to train a fake news detector that remains robust against various\nstyle-conversion attacks. The primary mechanism involves the strategic use of\nLLMs to automatically generate a diverse and coherent array of style-conversion\nattack prompts, enhancing the generation of particularly challenging prompts\nfor the detector. Experiments indicate that our augmentation strategy\nsignificantly improves robustness and detection performance when evaluated on\nfake news benchmark datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The spread of fake news harms individuals and presents a critical social\nchallenge that must be addressed. Although numerous algorithmic and insightful\nfeatures have been developed to detect fake news, many of these features can be\nmanipulated with style-conversion attacks, especially with the emergence of\nadvanced language models, making it more difficult to differentiate from\ngenuine news. This study proposes adversarial style augmentation, AdStyle,\ndesigned to train a fake news detector that remains robust against various\nstyle-conversion attacks. The primary mechanism involves the strategic use of\nLLMs to automatically generate a diverse and coherent array of style-conversion\nattack prompts, enhancing the generation of particularly challenging prompts\nfor the detector. Experiments indicate that our augmentation strategy\nsignificantly improves robustness and detection performance when evaluated on\nfake news benchmark datasets."
                },
                "authors": [
                    {
                        "name": "Sungwon Park"
                    },
                    {
                        "name": "Sungwon Han"
                    },
                    {
                        "name": "Xing Xie"
                    },
                    {
                        "name": "Jae-Gil Lee"
                    },
                    {
                        "name": "Meeyoung Cha"
                    }
                ],
                "author_detail": {
                    "name": "Meeyoung Cha"
                },
                "author": "Meeyoung Cha",
                "arxiv_comment": "WWW'25 research track accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11260v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11260v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13515v1",
                "updated": "2025-04-18T07:09:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    7,
                    9,
                    56,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T07:09:56Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    7,
                    9,
                    56,
                    4,
                    108,
                    0
                ],
                "title": "Large Language Models for Validating Network Protocol Parsers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Validating Network Protocol Parsers"
                },
                "summary": "Network protocol parsers are essential for enabling correct and secure\ncommunication between devices. Bugs in these parsers can introduce critical\nvulnerabilities, including memory corruption, information leakage, and\ndenial-of-service attacks. An intuitive way to assess parser correctness is to\ncompare the implementation with its official protocol standard. However, this\ncomparison is challenging because protocol standards are typically written in\nnatural language, whereas implementations are in source code. Existing methods\nlike model checking, fuzzing, and differential testing have been used to find\nparsing bugs, but they either require significant manual effort or ignore the\nprotocol standards, limiting their ability to detect semantic violations. To\nenable more automated validation of parser implementations against protocol\nstandards, we propose PARVAL, a multi-agent framework built on large language\nmodels (LLMs). PARVAL leverages the capabilities of LLMs to understand both\nnatural language and code. It transforms both protocol standards and their\nimplementations into a unified intermediate representation, referred to as\nformat specifications, and performs a differential comparison to uncover\ninconsistencies. We evaluate PARVAL on the Bidirectional Forwarding Detection\n(BFD) protocol. Our experiments demonstrate that PARVAL successfully identifies\ninconsistencies between the implementation and its RFC standard, achieving a\nlow false positive rate of 5.6%. PARVAL uncovers seven unique bugs, including\nfive previously unknown issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network protocol parsers are essential for enabling correct and secure\ncommunication between devices. Bugs in these parsers can introduce critical\nvulnerabilities, including memory corruption, information leakage, and\ndenial-of-service attacks. An intuitive way to assess parser correctness is to\ncompare the implementation with its official protocol standard. However, this\ncomparison is challenging because protocol standards are typically written in\nnatural language, whereas implementations are in source code. Existing methods\nlike model checking, fuzzing, and differential testing have been used to find\nparsing bugs, but they either require significant manual effort or ignore the\nprotocol standards, limiting their ability to detect semantic violations. To\nenable more automated validation of parser implementations against protocol\nstandards, we propose PARVAL, a multi-agent framework built on large language\nmodels (LLMs). PARVAL leverages the capabilities of LLMs to understand both\nnatural language and code. It transforms both protocol standards and their\nimplementations into a unified intermediate representation, referred to as\nformat specifications, and performs a differential comparison to uncover\ninconsistencies. We evaluate PARVAL on the Bidirectional Forwarding Detection\n(BFD) protocol. Our experiments demonstrate that PARVAL successfully identifies\ninconsistencies between the implementation and its RFC standard, achieving a\nlow false positive rate of 5.6%. PARVAL uncovers seven unique bugs, including\nfive previously unknown issues."
                },
                "authors": [
                    {
                        "name": "Mingwei Zheng"
                    },
                    {
                        "name": "Danning Xie"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhang"
                },
                "author": "Xiangyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00641v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00641v3",
                "updated": "2025-04-18T07:00:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    7,
                    0,
                    39,
                    4,
                    108,
                    0
                ],
                "published": "2024-06-30T09:51:58Z",
                "published_parsed": [
                    2024,
                    6,
                    30,
                    9,
                    51,
                    58,
                    6,
                    182,
                    0
                ],
                "title": "NeuroNAS: Enhancing Efficiency of Neuromorphic In-Memory Computing for\n  Intelligent Mobile Agents through Hardware-Aware Spiking Neural Architecture\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuroNAS: Enhancing Efficiency of Neuromorphic In-Memory Computing for\n  Intelligent Mobile Agents through Hardware-Aware Spiking Neural Architecture\n  Search"
                },
                "summary": "Intelligent mobile agents (e.g., UGVs and UAVs) typically demand low\npower/energy consumption when solving their machine learning (ML)-based tasks,\nsince they are usually powered by portable batteries with limited capacity. A\npotential solution is employing neuromorphic computing with Spiking Neural\nNetworks (SNNs), which leverages event-based computation to enable ultra-low\npower/energy ML algorithms. To maximize the performance efficiency of SNN\ninference, the In-Memory Computing (IMC)-based hardware accelerators with\nemerging device technologies (e.g., RRAM) can be employed. However, SNN models\nare typically developed without considering constraints from the application\nand the underlying IMC hardware, thereby hindering SNNs from reaching their\nfull potential in performance and efficiency. To address this, we propose\nNeuroNAS, a novel framework for developing energyefficient neuromorphic IMC for\nintelligent mobile agents using hardware-aware spiking neural architecture\nsearch (NAS), i.e., by quickly finding an SNN architecture that offers high\naccuracy under the given constraints (e.g., memory, area, latency, and energy\nconsumption). Its key steps include: optimizing SNN operations to enable\nefficient NAS, employing quantization to minimize the memory footprint,\ndeveloping an SNN architecture that facilitates an effective learning, and\ndevising a systematic hardware-aware search algorithm to meet the constraints.\nCompared to the state-of-the-art techniques, NeuroNAS quickly finds SNN\narchitectures (with 8bit weight precision) that maintain high accuracy by up to\n6.6x search time speed-ups, while achieving up to 92% area savings, 1.2x\nlatency improvements, 84% energy savings across different datasets (i.e.,\nCIFAR-10, CIFAR-100, and TinyImageNet-200); while the state-of-the-art fail to\nmeet all constraints at once.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent mobile agents (e.g., UGVs and UAVs) typically demand low\npower/energy consumption when solving their machine learning (ML)-based tasks,\nsince they are usually powered by portable batteries with limited capacity. A\npotential solution is employing neuromorphic computing with Spiking Neural\nNetworks (SNNs), which leverages event-based computation to enable ultra-low\npower/energy ML algorithms. To maximize the performance efficiency of SNN\ninference, the In-Memory Computing (IMC)-based hardware accelerators with\nemerging device technologies (e.g., RRAM) can be employed. However, SNN models\nare typically developed without considering constraints from the application\nand the underlying IMC hardware, thereby hindering SNNs from reaching their\nfull potential in performance and efficiency. To address this, we propose\nNeuroNAS, a novel framework for developing energyefficient neuromorphic IMC for\nintelligent mobile agents using hardware-aware spiking neural architecture\nsearch (NAS), i.e., by quickly finding an SNN architecture that offers high\naccuracy under the given constraints (e.g., memory, area, latency, and energy\nconsumption). Its key steps include: optimizing SNN operations to enable\nefficient NAS, employing quantization to minimize the memory footprint,\ndeveloping an SNN architecture that facilitates an effective learning, and\ndevising a systematic hardware-aware search algorithm to meet the constraints.\nCompared to the state-of-the-art techniques, NeuroNAS quickly finds SNN\narchitectures (with 8bit weight precision) that maintain high accuracy by up to\n6.6x search time speed-ups, while achieving up to 92% area savings, 1.2x\nlatency improvements, 84% energy savings across different datasets (i.e.,\nCIFAR-10, CIFAR-100, and TinyImageNet-200); while the state-of-the-art fail to\nmeet all constraints at once."
                },
                "authors": [
                    {
                        "name": "Rachmad Vidya Wicaksana Putra"
                    },
                    {
                        "name": "Muhammad Shafique"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Shafique"
                },
                "author": "Muhammad Shafique",
                "arxiv_comment": "9 pages, 14 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00641v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00641v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00383v2",
                "updated": "2025-04-18T06:58:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    58,
                    12,
                    4,
                    108,
                    0
                ],
                "published": "2024-11-30T07:21:02Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    7,
                    21,
                    2,
                    5,
                    335,
                    0
                ],
                "title": "Unified Parameter-Efficient Unlearning for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Parameter-Efficient Unlearning for LLMs"
                },
                "summary": "The advent of Large Language Models (LLMs) has revolutionized natural\nlanguage processing, enabling advanced understanding and reasoning capabilities\nacross a variety of tasks. Fine-tuning these models for specific domains,\nparticularly through Parameter-Efficient Fine-Tuning (PEFT) strategies like\nLoRA, has become a prevalent practice due to its efficiency. However, this\nraises significant privacy and security concerns, as models may inadvertently\nretain and disseminate sensitive or undesirable information. To address these\nissues, we introduce a novel instance-wise unlearning framework, LLMEraser,\nwhich systematically categorizes unlearning tasks and applies precise parameter\nadjustments using influence functions. Unlike traditional unlearning techniques\nthat are often limited in scope and require extensive retraining, LLMEraser is\ndesigned to handle a broad spectrum of unlearning tasks without compromising\nmodel performance. Extensive experiments on benchmark datasets demonstrate that\nLLMEraser excels in efficiently managing various unlearning scenarios while\nmaintaining the overall integrity and efficacy of the models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has revolutionized natural\nlanguage processing, enabling advanced understanding and reasoning capabilities\nacross a variety of tasks. Fine-tuning these models for specific domains,\nparticularly through Parameter-Efficient Fine-Tuning (PEFT) strategies like\nLoRA, has become a prevalent practice due to its efficiency. However, this\nraises significant privacy and security concerns, as models may inadvertently\nretain and disseminate sensitive or undesirable information. To address these\nissues, we introduce a novel instance-wise unlearning framework, LLMEraser,\nwhich systematically categorizes unlearning tasks and applies precise parameter\nadjustments using influence functions. Unlike traditional unlearning techniques\nthat are often limited in scope and require extensive retraining, LLMEraser is\ndesigned to handle a broad spectrum of unlearning tasks without compromising\nmodel performance. Extensive experiments on benchmark datasets demonstrate that\nLLMEraser excels in efficiently managing various unlearning scenarios while\nmaintaining the overall integrity and efficacy of the models."
                },
                "authors": [
                    {
                        "name": "Chenlu Ding"
                    },
                    {
                        "name": "Jiancan Wu"
                    },
                    {
                        "name": "Yancheng Yuan"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Alex Su"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13502v1",
                "updated": "2025-04-18T06:45:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    45,
                    30,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T06:45:30Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    45,
                    30,
                    4,
                    108,
                    0
                ],
                "title": "Continuous-time filtering in Lie groups: estimation via the Fr{é}chet\n  mean of solutions to stochastic differential equations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous-time filtering in Lie groups: estimation via the Fr{é}chet\n  mean of solutions to stochastic differential equations"
                },
                "summary": "We compute the Fr\\'echet mean $\\mathscr{E}_t$ of the solution $X_{t}$ to a\ncontinuous-time stochastic differential equation in a Lie group. It provides an\nestimator with minimal variance of $X_{t}$. We use it in the context of Kalman\nfiltering and more precisely to infer rotation matrices. In this paper, we\nfocus on the prediction step between two consecutive observations. Compared to\nstate-of-the-art approaches, our assumptions on the model are minimal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We compute the Fr\\'echet mean $\\mathscr{E}_t$ of the solution $X_{t}$ to a\ncontinuous-time stochastic differential equation in a Lie group. It provides an\nestimator with minimal variance of $X_{t}$. We use it in the context of Kalman\nfiltering and more precisely to infer rotation matrices. In this paper, we\nfocus on the prediction step between two consecutive observations. Compared to\nstate-of-the-art approaches, our assumptions on the model are minimal."
                },
                "authors": [
                    {
                        "name": "Magalie Bénéfice"
                    },
                    {
                        "name": "Marc Arnaudon"
                    },
                    {
                        "name": "Audrey Giremus"
                    }
                ],
                "author_detail": {
                    "name": "Audrey Giremus"
                },
                "arxiv_affiliation": "IMS, UB",
                "author": "Audrey Giremus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.PR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13500v1",
                "updated": "2025-04-18T06:42:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    42,
                    30,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T06:42:30Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    42,
                    30,
                    4,
                    108,
                    0
                ],
                "title": "Prejudge-Before-Think: Enhancing Large Language Models at Test-Time by\n  Process Prejudge Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prejudge-Before-Think: Enhancing Large Language Models at Test-Time by\n  Process Prejudge Reasoning"
                },
                "summary": "In this paper, we introduce a new \\emph{process prejudge} strategy in LLM\nreasoning to demonstrate that bootstrapping with process prejudge allows the\nLLM to adaptively anticipate the errors encountered when advancing the\nsubsequent reasoning steps, similar to people sometimes pausing to think about\nwhat mistakes may occur and how to avoid them, rather than relying solely on\ntrial and error. Specifically, we define a prejudge node in the rationale,\nwhich represents a reasoning step, with at least one step that follows the\nprejudge node that has no paths toward the correct answer. To synthesize the\nprejudge reasoning process, we present an automated reasoning framework with a\ndynamic tree-searching strategy. This framework requires only one LLM to\nperform answer judging, response critiquing, prejudge generation, and thought\ncompletion. Furthermore, we develop a two-phase training mechanism with\nsupervised fine-tuning (SFT) and reinforcement learning (RL) to further enhance\nthe reasoning capabilities of LLMs. Experimental results from competition-level\ncomplex reasoning demonstrate that our method can teach the model to prejudge\nbefore thinking and significantly enhance the reasoning ability of LLMs. Code\nand data is released at https://github.com/wjn1996/Prejudge-Before-Think.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new \\emph{process prejudge} strategy in LLM\nreasoning to demonstrate that bootstrapping with process prejudge allows the\nLLM to adaptively anticipate the errors encountered when advancing the\nsubsequent reasoning steps, similar to people sometimes pausing to think about\nwhat mistakes may occur and how to avoid them, rather than relying solely on\ntrial and error. Specifically, we define a prejudge node in the rationale,\nwhich represents a reasoning step, with at least one step that follows the\nprejudge node that has no paths toward the correct answer. To synthesize the\nprejudge reasoning process, we present an automated reasoning framework with a\ndynamic tree-searching strategy. This framework requires only one LLM to\nperform answer judging, response critiquing, prejudge generation, and thought\ncompletion. Furthermore, we develop a two-phase training mechanism with\nsupervised fine-tuning (SFT) and reinforcement learning (RL) to further enhance\nthe reasoning capabilities of LLMs. Experimental results from competition-level\ncomplex reasoning demonstrate that our method can teach the model to prejudge\nbefore thinking and significantly enhance the reasoning ability of LLMs. Code\nand data is released at https://github.com/wjn1996/Prejudge-Before-Think."
                },
                "authors": [
                    {
                        "name": "Jianing Wang"
                    },
                    {
                        "name": "Jin Jiang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Mengdi Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xunliang Cai"
                },
                "author": "Xunliang Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13484v1",
                "updated": "2025-04-18T05:48:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    48,
                    35,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T05:48:35Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    48,
                    35,
                    4,
                    108,
                    0
                ],
                "title": "Monitor and Recover: A Paradigm for Future Research on Distribution\n  Shift in Learning-Enabled Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monitor and Recover: A Paradigm for Future Research on Distribution\n  Shift in Learning-Enabled Cyber-Physical Systems"
                },
                "summary": "With the known vulnerability of neural networks to distribution shift,\nmaintaining reliability in learning-enabled cyber-physical systems poses a\nsalient challenge. In response, many existing methods adopt a detect and\nabstain methodology, aiming to detect distribution shift at inference time so\nthat the learning-enabled component can abstain from decision-making. This\napproach, however, has limited use in real-world applications. We instead\npropose a monitor and recover paradigm as a promising direction for future\nresearch. This philosophy emphasizes 1) robust safety monitoring instead of\ndistribution shift detection and 2) distribution shift recovery instead of\nabstention. We discuss two examples from our recent work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the known vulnerability of neural networks to distribution shift,\nmaintaining reliability in learning-enabled cyber-physical systems poses a\nsalient challenge. In response, many existing methods adopt a detect and\nabstain methodology, aiming to detect distribution shift at inference time so\nthat the learning-enabled component can abstain from decision-making. This\napproach, however, has limited use in real-world applications. We instead\npropose a monitor and recover paradigm as a promising direction for future\nresearch. This philosophy emphasizes 1) robust safety monitoring instead of\ndistribution shift detection and 2) distribution shift recovery instead of\nabstention. We discuss two examples from our recent work."
                },
                "authors": [
                    {
                        "name": "Vivian Lin"
                    },
                    {
                        "name": "Insup Lee"
                    }
                ],
                "author_detail": {
                    "name": "Insup Lee"
                },
                "author": "Insup Lee",
                "arxiv_comment": "Accepted to ICCPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13475v1",
                "updated": "2025-04-18T05:35:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    35,
                    11,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T05:35:11Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    35,
                    11,
                    4,
                    108,
                    0
                ],
                "title": "LLM Sensitivity Evaluation Framework for Clinical Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Sensitivity Evaluation Framework for Clinical Diagnosis"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive performance across\nvarious domains. However, for clinical diagnosis, higher expectations are\nrequired for LLM's reliability and sensitivity: thinking like physicians and\nremaining sensitive to key medical information that affects diagnostic\nreasoning, as subtle variations can lead to different diagnosis results. Yet,\nexisting works focus mainly on investigating the sensitivity of LLMs to\nirrelevant context and overlook the importance of key information. In this\npaper, we investigate the sensitivity of LLMs, i.e. GPT-3.5, GPT-4, Gemini,\nClaude3 and LLaMA2-7b, to key medical information by introducing different\nperturbation strategies. The evaluation results highlight the limitations of\ncurrent LLMs in remaining sensitive to key medical information for diagnostic\ndecision-making. The evolution of LLMs must focus on improving their\nreliability, enhancing their ability to be sensitive to key information, and\neffectively utilizing this information. These improvements will enhance human\ntrust in LLMs and facilitate their practical application in real-world\nscenarios. Our code and dataset are available at\nhttps://github.com/chenwei23333/DiagnosisQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive performance across\nvarious domains. However, for clinical diagnosis, higher expectations are\nrequired for LLM's reliability and sensitivity: thinking like physicians and\nremaining sensitive to key medical information that affects diagnostic\nreasoning, as subtle variations can lead to different diagnosis results. Yet,\nexisting works focus mainly on investigating the sensitivity of LLMs to\nirrelevant context and overlook the importance of key information. In this\npaper, we investigate the sensitivity of LLMs, i.e. GPT-3.5, GPT-4, Gemini,\nClaude3 and LLaMA2-7b, to key medical information by introducing different\nperturbation strategies. The evaluation results highlight the limitations of\ncurrent LLMs in remaining sensitive to key medical information for diagnostic\ndecision-making. The evolution of LLMs must focus on improving their\nreliability, enhancing their ability to be sensitive to key information, and\neffectively utilizing this information. These improvements will enhance human\ntrust in LLMs and facilitate their practical application in real-world\nscenarios. Our code and dataset are available at\nhttps://github.com/chenwei23333/DiagnosisQA."
                },
                "authors": [
                    {
                        "name": "Chenwei Yan"
                    },
                    {
                        "name": "Xiangling Fu"
                    },
                    {
                        "name": "Yuxuan Xiong"
                    },
                    {
                        "name": "Tianyi Wang"
                    },
                    {
                        "name": "Siu Cheung Hui"
                    },
                    {
                        "name": "Ji Wu"
                    },
                    {
                        "name": "Xien Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xien Liu"
                },
                "author": "Xien Liu",
                "arxiv_journal_ref": "Proceedings of the 31st International Conference on Computational\n  Linguistics, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13474v1",
                "updated": "2025-04-18T05:32:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    32,
                    47,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T05:32:47Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    32,
                    47,
                    4,
                    108,
                    0
                ],
                "title": "Everything You Wanted to Know About LLM-based Vulnerability Detection\n  But Were Afraid to Ask",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Everything You Wanted to Know About LLM-based Vulnerability Detection\n  But Were Afraid to Ask"
                },
                "summary": "Large Language Models are a promising tool for automated vulnerability\ndetection, thanks to their success in code generation and repair. However,\ndespite widespread adoption, a critical question remains: Are LLMs truly\neffective at detecting real-world vulnerabilities? Current evaluations, which\noften assess models on isolated functions or files, ignore the broader\nexecution and data-flow context essential for understanding vulnerabilities.\nThis oversight leads to two types of misleading outcomes: incorrect conclusions\nand flawed rationales, collectively undermining the reliability of prior\nassessments. Therefore, in this paper, we challenge three widely held community\nbeliefs: that LLMs are (i) unreliable, (ii) insensitive to code patches, and\n(iii) performance-plateaued across model scales. We argue that these beliefs\nare artifacts of context-deprived evaluations. To address this, we propose\nCORRECT (Context-Rich Reasoning Evaluation of Code with Trust), a new\nevaluation framework that systematically incorporates contextual information\ninto LLM-based vulnerability detection. We construct a context-rich dataset of\n2,000 vulnerable-patched program pairs spanning 99 CWEs and evaluate 13 LLMs\nacross four model families. Our framework elicits both binary predictions and\nnatural-language rationales, which are further validated using LLM-as-a-judge\ntechniques. Our findings overturn existing misconceptions. When provided with\nsufficient context, SOTA LLMs achieve significantly improved performance (e.g.,\n0.7 F1-score on key CWEs), with 0.8 precision. We show that most false\npositives stem from reasoning errors rather than misclassification, and that\nwhile model and test-time scaling improve performance, they introduce\ndiminishing returns and trade-offs in recall. Finally, we uncover new flaws in\ncurrent LLM-based detection systems, such as limited generalization and\noverthinking biases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are a promising tool for automated vulnerability\ndetection, thanks to their success in code generation and repair. However,\ndespite widespread adoption, a critical question remains: Are LLMs truly\neffective at detecting real-world vulnerabilities? Current evaluations, which\noften assess models on isolated functions or files, ignore the broader\nexecution and data-flow context essential for understanding vulnerabilities.\nThis oversight leads to two types of misleading outcomes: incorrect conclusions\nand flawed rationales, collectively undermining the reliability of prior\nassessments. Therefore, in this paper, we challenge three widely held community\nbeliefs: that LLMs are (i) unreliable, (ii) insensitive to code patches, and\n(iii) performance-plateaued across model scales. We argue that these beliefs\nare artifacts of context-deprived evaluations. To address this, we propose\nCORRECT (Context-Rich Reasoning Evaluation of Code with Trust), a new\nevaluation framework that systematically incorporates contextual information\ninto LLM-based vulnerability detection. We construct a context-rich dataset of\n2,000 vulnerable-patched program pairs spanning 99 CWEs and evaluate 13 LLMs\nacross four model families. Our framework elicits both binary predictions and\nnatural-language rationales, which are further validated using LLM-as-a-judge\ntechniques. Our findings overturn existing misconceptions. When provided with\nsufficient context, SOTA LLMs achieve significantly improved performance (e.g.,\n0.7 F1-score on key CWEs), with 0.8 precision. We show that most false\npositives stem from reasoning errors rather than misclassification, and that\nwhile model and test-time scaling improve performance, they introduce\ndiminishing returns and trade-offs in recall. Finally, we uncover new flaws in\ncurrent LLM-based detection systems, such as limited generalization and\noverthinking biases."
                },
                "authors": [
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Minghui Xu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Xiuzhen Cheng"
                    },
                    {
                        "name": "Fengyuan Xu"
                    },
                    {
                        "name": "Sheng Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Zhong"
                },
                "author": "Sheng Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13472v1",
                "updated": "2025-04-18T05:26:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    26,
                    32,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T05:26:32Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    26,
                    32,
                    4,
                    108,
                    0
                ],
                "title": "CodeVisionary: An Agent-based Framework for Evaluating Large Language\n  Models in Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeVisionary: An Agent-based Framework for Evaluating Large Language\n  Models in Code Generation"
                },
                "summary": "Large language models (LLMs) have demonstrated strong capabilities in code\ngeneration, underscoring the critical need for rigorous and comprehensive\nevaluation. Existing evaluation approaches fall into three categories,\nincluding human-centered, metric-based, and LLM-based. Considering that\nhuman-centered approaches are labour-intensive and metric-based ones overly\nrely on reference answers, LLM-based approaches are gaining increasing\nattention due to their stronger contextual understanding capabilities and\nsuperior efficiency. However, the performance of LLM-based approaches remains\nlimited due to: (1) lack of multisource domain knowledge, and (2) insufficient\ncomprehension of complex code.\n  To mitigate the limitations, we propose CodeVisionary, the first LLM-based\nagent framework for evaluating LLMs in code generation. CodeVisionary consists\nof two stages: (1) Multiscore knowledge analysis stage, which aims to gather\nmultisource and comprehensive domain knowledge by formulating and executing a\nstepwise evaluation plan. (2) Negotiation-based scoring stage, which involves\nmultiple judges engaging in discussions to better comprehend the complex code\nand reach a consensus on the evaluation score. Extensive experiments\ndemonstrate that CodeVisionary achieves the best performance for evaluating\nLLMs in code generation, outperforming the best baseline methods with average\nimprovements of 0.202, 0.139, and 0.117 in Pearson, Spearman, and Kendall-Tau\ncoefficients, respectively. Besides, CodeVisionary provides detailed evaluation\nreports, which assist developers in identifying shortcomings and making\nimprovements. The resources of CodeVisionary are available at\nhttps://anonymous.4open.science/r/CodeVisionary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong capabilities in code\ngeneration, underscoring the critical need for rigorous and comprehensive\nevaluation. Existing evaluation approaches fall into three categories,\nincluding human-centered, metric-based, and LLM-based. Considering that\nhuman-centered approaches are labour-intensive and metric-based ones overly\nrely on reference answers, LLM-based approaches are gaining increasing\nattention due to their stronger contextual understanding capabilities and\nsuperior efficiency. However, the performance of LLM-based approaches remains\nlimited due to: (1) lack of multisource domain knowledge, and (2) insufficient\ncomprehension of complex code.\n  To mitigate the limitations, we propose CodeVisionary, the first LLM-based\nagent framework for evaluating LLMs in code generation. CodeVisionary consists\nof two stages: (1) Multiscore knowledge analysis stage, which aims to gather\nmultisource and comprehensive domain knowledge by formulating and executing a\nstepwise evaluation plan. (2) Negotiation-based scoring stage, which involves\nmultiple judges engaging in discussions to better comprehend the complex code\nand reach a consensus on the evaluation score. Extensive experiments\ndemonstrate that CodeVisionary achieves the best performance for evaluating\nLLMs in code generation, outperforming the best baseline methods with average\nimprovements of 0.202, 0.139, and 0.117 in Pearson, Spearman, and Kendall-Tau\ncoefficients, respectively. Besides, CodeVisionary provides detailed evaluation\nreports, which assist developers in identifying shortcomings and making\nimprovements. The resources of CodeVisionary are available at\nhttps://anonymous.4open.science/r/CodeVisionary."
                },
                "authors": [
                    {
                        "name": "Xinchen Wang"
                    },
                    {
                        "name": "Pengfei Gao"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Ruida Hu"
                    },
                    {
                        "name": "Cuiyun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Cuiyun Gao"
                },
                "author": "Cuiyun Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13471v1",
                "updated": "2025-04-18T05:25:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    25,
                    22,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T05:25:22Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    25,
                    22,
                    4,
                    108,
                    0
                ],
                "title": "From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient\n  LLMs"
                },
                "summary": "In recent years, Large Language Models (LLMs) have significantly advanced\nartificial intelligence by optimizing traditional Natural Language Processing\n(NLP) pipelines, improving performance and generalization. This has spurred\ntheir integration into various systems. Many NLP systems, including ours,\nemploy a \"one-stage\" pipeline directly incorporating LLMs. While effective,\nthis approach incurs substantial costs and latency due to the need for large\nmodel parameters to achieve satisfactory outcomes. This paper introduces a\nthree-stage cost-efficient end-to-end LLM deployment pipeline-including\nprototyping, knowledge transfer, and model compression-to tackle the\ncost-performance dilemma in LLM-based frameworks. Our approach yields a super\ntiny model optimized for cost and performance in online systems, simplifying\nthe system architecture. Initially, by transforming complex tasks into a\nfunction call-based LLM-driven pipeline, an optimal performance prototype\nsystem is constructed to produce high-quality data as a teacher model. The\nsecond stage combine techniques like rejection fine-tuning, reinforcement\nlearning and knowledge distillation to transfer knowledge to a smaller 0.5B\nstudent model, delivering effective performance at minimal cost. The final\nstage applies quantization and pruning to extremely compress model to 0.4B,\nachieving ultra-low latency and cost. The framework's modular design and\ncross-domain capabilities suggest potential applicability in other NLP areas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have significantly advanced\nartificial intelligence by optimizing traditional Natural Language Processing\n(NLP) pipelines, improving performance and generalization. This has spurred\ntheir integration into various systems. Many NLP systems, including ours,\nemploy a \"one-stage\" pipeline directly incorporating LLMs. While effective,\nthis approach incurs substantial costs and latency due to the need for large\nmodel parameters to achieve satisfactory outcomes. This paper introduces a\nthree-stage cost-efficient end-to-end LLM deployment pipeline-including\nprototyping, knowledge transfer, and model compression-to tackle the\ncost-performance dilemma in LLM-based frameworks. Our approach yields a super\ntiny model optimized for cost and performance in online systems, simplifying\nthe system architecture. Initially, by transforming complex tasks into a\nfunction call-based LLM-driven pipeline, an optimal performance prototype\nsystem is constructed to produce high-quality data as a teacher model. The\nsecond stage combine techniques like rejection fine-tuning, reinforcement\nlearning and knowledge distillation to transfer knowledge to a smaller 0.5B\nstudent model, delivering effective performance at minimal cost. The final\nstage applies quantization and pruning to extremely compress model to 0.4B,\nachieving ultra-low latency and cost. The framework's modular design and\ncross-domain capabilities suggest potential applicability in other NLP areas."
                },
                "authors": [
                    {
                        "name": "Jiliang Ni"
                    },
                    {
                        "name": "Jiachen Pu"
                    },
                    {
                        "name": "Zhongyi Yang"
                    },
                    {
                        "name": "Kun Zhou"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Xiaoliang Xiao"
                    },
                    {
                        "name": "Dakui Wang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Jingfeng Luo"
                    },
                    {
                        "name": "Conggang Hu"
                    }
                ],
                "author_detail": {
                    "name": "Conggang Hu"
                },
                "author": "Conggang Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13469v1",
                "updated": "2025-04-18T05:24:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    24,
                    8,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T05:24:08Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    24,
                    8,
                    4,
                    108,
                    0
                ],
                "title": "HMPE:HeatMap Embedding for Efficient Transformer-Based Small Object\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HMPE:HeatMap Embedding for Efficient Transformer-Based Small Object\n  Detection"
                },
                "summary": "Current Transformer-based methods for small object detection continue\nemerging, yet they have still exhibited significant shortcomings. This paper\nintroduces HeatMap Position Embedding (HMPE), a novel Transformer Optimization\ntechnique that enhances object detection performance by dynamically integrating\npositional encoding with semantic detection information through heatmap-guided\nadaptive learning.We also innovatively visualize the HMPE method, offering\nclear visualization of embedded information for parameter fine-tuning.We then\ncreate Multi-Scale ObjectBox-Heatmap Fusion Encoder (MOHFE) and HeatMap Induced\nHigh-Quality Queries for Decoder (HIDQ) modules. These are designed for the\nencoder and decoder, respectively, to generate high-quality queries and reduce\nbackground noise queries.Using both heatmap embedding and Linear-Snake\nConv(LSConv) feature engineering, we enhance the embedding of massively diverse\nsmall object categories and reduced the decoder multihead layers, thereby\naccelerating both inference and training.In the generalization experiments, our\napproach outperforme the baseline mAP by 1.9% on the small object dataset (NWPU\nVHR-10) and by 1.2% on the general dataset (PASCAL VOC). By employing\nHMPE-enhanced embedding, we are able to reduce the number of decoder layers\nfrom eight to a minimum of three, significantly decreasing both inference and\ntraining costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Transformer-based methods for small object detection continue\nemerging, yet they have still exhibited significant shortcomings. This paper\nintroduces HeatMap Position Embedding (HMPE), a novel Transformer Optimization\ntechnique that enhances object detection performance by dynamically integrating\npositional encoding with semantic detection information through heatmap-guided\nadaptive learning.We also innovatively visualize the HMPE method, offering\nclear visualization of embedded information for parameter fine-tuning.We then\ncreate Multi-Scale ObjectBox-Heatmap Fusion Encoder (MOHFE) and HeatMap Induced\nHigh-Quality Queries for Decoder (HIDQ) modules. These are designed for the\nencoder and decoder, respectively, to generate high-quality queries and reduce\nbackground noise queries.Using both heatmap embedding and Linear-Snake\nConv(LSConv) feature engineering, we enhance the embedding of massively diverse\nsmall object categories and reduced the decoder multihead layers, thereby\naccelerating both inference and training.In the generalization experiments, our\napproach outperforme the baseline mAP by 1.9% on the small object dataset (NWPU\nVHR-10) and by 1.2% on the general dataset (PASCAL VOC). By employing\nHMPE-enhanced embedding, we are able to reduce the number of decoder layers\nfrom eight to a minimum of three, significantly decreasing both inference and\ntraining costs."
                },
                "authors": [
                    {
                        "name": "YangChen Zeng"
                    }
                ],
                "author_detail": {
                    "name": "YangChen Zeng"
                },
                "author": "YangChen Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13467v1",
                "updated": "2025-04-18T05:19:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    19,
                    19,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T05:19:19Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    19,
                    19,
                    4,
                    108,
                    0
                ],
                "title": "Efficient Estimation under Multiple Missing Patterns via Balancing\n  Weights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Estimation under Multiple Missing Patterns via Balancing\n  Weights"
                },
                "summary": "As one of the most commonly seen data challenges, missing data, in\nparticular, multiple, non-monotone missing patterns, complicates estimation and\ninference due to the fact that missingness mechanisms are often not missing at\nrandom, and conventional methods cannot be applied. Pattern graphs have\nrecently been proposed as a tool to systematically relate various observed\npatterns in the sample. We extend its scope to the estimation of parameters\ndefined by moment equations, including common regression models, via solving\nweighted estimating equations with weights constructed using a sequential\nbalancing approach. These novel weights are carefully crafted to address the\ninstability issue of the straightforward approach based on local balancing. We\nderive the efficiency bound for the model parameters and show that our proposed\nmethod, albeit relatively simple, is asymptotically efficient. Simulation\nresults demonstrate the superior performance of the proposed method, and\nreal-data applications illustrate how the results are robust to the choice of\nidentification assumptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As one of the most commonly seen data challenges, missing data, in\nparticular, multiple, non-monotone missing patterns, complicates estimation and\ninference due to the fact that missingness mechanisms are often not missing at\nrandom, and conventional methods cannot be applied. Pattern graphs have\nrecently been proposed as a tool to systematically relate various observed\npatterns in the sample. We extend its scope to the estimation of parameters\ndefined by moment equations, including common regression models, via solving\nweighted estimating equations with weights constructed using a sequential\nbalancing approach. These novel weights are carefully crafted to address the\ninstability issue of the straightforward approach based on local balancing. We\nderive the efficiency bound for the model parameters and show that our proposed\nmethod, albeit relatively simple, is asymptotically efficient. Simulation\nresults demonstrate the superior performance of the proposed method, and\nreal-data applications illustrate how the results are robust to the choice of\nidentification assumptions."
                },
                "authors": [
                    {
                        "name": "Jianing Dong"
                    },
                    {
                        "name": "Raymond K. W. Wong"
                    },
                    {
                        "name": "Kwun Chuen Gary Chan"
                    }
                ],
                "author_detail": {
                    "name": "Kwun Chuen Gary Chan"
                },
                "author": "Kwun Chuen Gary Chan",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2402.08873",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15288v2",
                "updated": "2025-04-18T04:46:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    4,
                    46,
                    0,
                    4,
                    108,
                    0
                ],
                "published": "2024-10-20T05:02:18Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    5,
                    2,
                    18,
                    6,
                    294,
                    0
                ],
                "title": "If LLMs Would Just Look: Simple Line-by-line Checking Improves\n  Vulnerability Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "If LLMs Would Just Look: Simple Line-by-line Checking Improves\n  Vulnerability Localization"
                },
                "summary": "The rapid expansion of software systems and the growing number of reported\nvulnerabilities have emphasized the importance of accurately identifying\nvulnerable code segments. Traditional methods for vulnerability localization,\nsuch as manual code audits or rule-based tools, are often time-consuming and\nlimited in scope, typically focusing on specific programming languages or types\nof vulnerabilities. In recent years, the introduction of large language models\n(LLMs) such as GPT and LLaMA has opened new possibilities for automating\nvulnerability detection. However, while LLMs show promise in this area, they\nface challenges, particularly in maintaining accuracy over longer code\ncontexts. This paper introduces LOVA, a novel framework leveraging the\nself-attention mechanisms inherent in LLMs to enhance vulnerability\nlocalization. Our key insight is that self-attention mechanisms assign varying\nimportance to different parts of the input, making it possible to track how\nmuch attention the model focuses on specific lines of code. In the context of\nvulnerability localization, the hypothesis is that vulnerable lines of code\nwill naturally attract higher attention weights because they have a greater\ninfluence on the model's output. By systematically tracking changes in\nattention weights and focusing on specific lines of code, LOVA improves the\nprecision of identifying vulnerable lines across various programming languages.\nThrough rigorous experimentation and evaluation, we demonstrate that LOVA\nsignificantly outperforms existing LLM-based approaches, achieving up to a 5.3x\nimprovement in F1-scores. LOVA also demonstrated strong scalability, with up to\na 14.6x improvement in smart contract vulnerability localization across\nlanguages like C, Python, Java, and Solidity. Its robustness was proven through\nconsistent performance across different LLM architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of software systems and the growing number of reported\nvulnerabilities have emphasized the importance of accurately identifying\nvulnerable code segments. Traditional methods for vulnerability localization,\nsuch as manual code audits or rule-based tools, are often time-consuming and\nlimited in scope, typically focusing on specific programming languages or types\nof vulnerabilities. In recent years, the introduction of large language models\n(LLMs) such as GPT and LLaMA has opened new possibilities for automating\nvulnerability detection. However, while LLMs show promise in this area, they\nface challenges, particularly in maintaining accuracy over longer code\ncontexts. This paper introduces LOVA, a novel framework leveraging the\nself-attention mechanisms inherent in LLMs to enhance vulnerability\nlocalization. Our key insight is that self-attention mechanisms assign varying\nimportance to different parts of the input, making it possible to track how\nmuch attention the model focuses on specific lines of code. In the context of\nvulnerability localization, the hypothesis is that vulnerable lines of code\nwill naturally attract higher attention weights because they have a greater\ninfluence on the model's output. By systematically tracking changes in\nattention weights and focusing on specific lines of code, LOVA improves the\nprecision of identifying vulnerable lines across various programming languages.\nThrough rigorous experimentation and evaluation, we demonstrate that LOVA\nsignificantly outperforms existing LLM-based approaches, achieving up to a 5.3x\nimprovement in F1-scores. LOVA also demonstrated strong scalability, with up to\na 14.6x improvement in smart contract vulnerability localization across\nlanguages like C, Python, Java, and Solidity. Its robustness was proven through\nconsistent performance across different LLM architectures."
                },
                "authors": [
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Xiuzhen Cheng"
                    },
                    {
                        "name": "Yating Liu"
                    },
                    {
                        "name": "Fengyuan Xu"
                    },
                    {
                        "name": "Sheng Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Zhong"
                },
                "author": "Sheng Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13460v1",
                "updated": "2025-04-18T04:35:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    4,
                    35,
                    35,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T04:35:35Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    4,
                    35,
                    35,
                    4,
                    108,
                    0
                ],
                "title": "Chain-of-Thought Textual Reasoning for Few-shot Temporal Action\n  Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought Textual Reasoning for Few-shot Temporal Action\n  Localization"
                },
                "summary": "Traditional temporal action localization (TAL) methods rely on large amounts\nof detailed annotated data, whereas few-shot TAL reduces this dependence by\nusing only a few training samples to identify unseen action categories.\nHowever, existing few-shot TAL methods typically focus solely on video-level\ninformation, neglecting textual information, which can provide valuable\nsemantic support for the localization task. Therefore, we propose a new\nfew-shot temporal action localization method by Chain-of-Thought textual\nreasoning to improve localization performance. Specifically, we design a novel\nfew-shot learning framework that leverages textual semantic information to\nenhance the model's ability to capture action commonalities and variations,\nwhich includes a semantic-aware text-visual alignment module designed to align\nthe query and support videos at different levels. Meanwhile, to better express\nthe temporal dependencies and causal relationships between actions at the\ntextual level to assist action localization, we design a Chain of Thought\n(CoT)-like reasoning method that progressively guides the Vision Language Model\n(VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for\nvideos. The generated texts can capture more variance of action than visual\nfeatures. We conduct extensive experiments on the publicly available\nActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named\nHuman-related Anomaly Localization and explore the application of the TAL task\nin human anomaly detection. The experimental results demonstrate that our\nproposed method significantly outperforms existing methods in single-instance\nand multi-instance scenarios. We will release our code, data and benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional temporal action localization (TAL) methods rely on large amounts\nof detailed annotated data, whereas few-shot TAL reduces this dependence by\nusing only a few training samples to identify unseen action categories.\nHowever, existing few-shot TAL methods typically focus solely on video-level\ninformation, neglecting textual information, which can provide valuable\nsemantic support for the localization task. Therefore, we propose a new\nfew-shot temporal action localization method by Chain-of-Thought textual\nreasoning to improve localization performance. Specifically, we design a novel\nfew-shot learning framework that leverages textual semantic information to\nenhance the model's ability to capture action commonalities and variations,\nwhich includes a semantic-aware text-visual alignment module designed to align\nthe query and support videos at different levels. Meanwhile, to better express\nthe temporal dependencies and causal relationships between actions at the\ntextual level to assist action localization, we design a Chain of Thought\n(CoT)-like reasoning method that progressively guides the Vision Language Model\n(VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for\nvideos. The generated texts can capture more variance of action than visual\nfeatures. We conduct extensive experiments on the publicly available\nActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named\nHuman-related Anomaly Localization and explore the application of the TAL task\nin human anomaly detection. The experimental results demonstrate that our\nproposed method significantly outperforms existing methods in single-instance\nand multi-instance scenarios. We will release our code, data and benchmark."
                },
                "authors": [
                    {
                        "name": "Hongwei Ji"
                    },
                    {
                        "name": "Wulian Yun"
                    },
                    {
                        "name": "Mengshi Qi"
                    },
                    {
                        "name": "Huadong Ma"
                    }
                ],
                "author_detail": {
                    "name": "Huadong Ma"
                },
                "author": "Huadong Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.13837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13837v1",
                "updated": "2025-04-18T17:59:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    59,
                    56,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T17:59:56Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    59,
                    56,
                    4,
                    108,
                    0
                ],
                "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in\n  LLMs Beyond the Base Model?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in\n  LLMs Beyond the Base Model?"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently\ndemonstrated notable success in enhancing the reasoning capabilities of LLMs,\nparticularly in mathematics and programming tasks. It is widely believed that\nRLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning\nabilities that exceed corresponding base models' capacity. In this study,\nhowever, we critically re-examines this assumption by measuring the\npass@\\textit{k} metric with large values of \\textit{k} to explore the reasoning\ncapability boundary of the models across a wide range of model families and\nbenchmarks. Surprisingly, the RL does \\emph{not}, in fact, elicit fundamentally\nnew reasoning patterns. While RL-trained models outperform their base models at\nsmaller values of $k$ (\\eg, $k$=1), base models can achieve a comparable or\neven higher pass@$k$ score compared to their RL counterparts at large $k$\nvalues. The reasoning paths generated by RL-trained models are already included\nin the base models' sampling distribution, suggesting that most reasoning\nabilities manifested in RL-trained models are already obtained by base models.\nFurther analysis shows that RL training boosts the performance by biasing the\nmodel's output distribution toward paths that are more likely to yield rewards,\ntherefore sampling correct responses more efficiently. But this also results in\na narrower reasoning capability boundary compared to base models. Similar\nresults are observed in visual reasoning tasks trained with RLVR. Moreover, we\nfind that distillation can genuinely introduce new knowledge into the model,\ndifferent from RLVR. These findings underscore a critical limitation of RLVR in\nadvancing LLM reasoning abilities which requires us to fundamentally rethink\nthe impact of RL training in reasoning LLMs and the need of a better paradigm.\nProject Page: https://limit-of-RLVR.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently\ndemonstrated notable success in enhancing the reasoning capabilities of LLMs,\nparticularly in mathematics and programming tasks. It is widely believed that\nRLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning\nabilities that exceed corresponding base models' capacity. In this study,\nhowever, we critically re-examines this assumption by measuring the\npass@\\textit{k} metric with large values of \\textit{k} to explore the reasoning\ncapability boundary of the models across a wide range of model families and\nbenchmarks. Surprisingly, the RL does \\emph{not}, in fact, elicit fundamentally\nnew reasoning patterns. While RL-trained models outperform their base models at\nsmaller values of $k$ (\\eg, $k$=1), base models can achieve a comparable or\neven higher pass@$k$ score compared to their RL counterparts at large $k$\nvalues. The reasoning paths generated by RL-trained models are already included\nin the base models' sampling distribution, suggesting that most reasoning\nabilities manifested in RL-trained models are already obtained by base models.\nFurther analysis shows that RL training boosts the performance by biasing the\nmodel's output distribution toward paths that are more likely to yield rewards,\ntherefore sampling correct responses more efficiently. But this also results in\na narrower reasoning capability boundary compared to base models. Similar\nresults are observed in visual reasoning tasks trained with RLVR. Moreover, we\nfind that distillation can genuinely introduce new knowledge into the model,\ndifferent from RLVR. These findings underscore a critical limitation of RLVR in\nadvancing LLM reasoning abilities which requires us to fundamentally rethink\nthe impact of RL training in reasoning LLMs and the need of a better paradigm.\nProject Page: https://limit-of-RLVR.github.io"
                },
                "authors": [
                    {
                        "name": "Yang Yue"
                    },
                    {
                        "name": "Zhiqi Chen"
                    },
                    {
                        "name": "Rui Lu"
                    },
                    {
                        "name": "Andrew Zhao"
                    },
                    {
                        "name": "Zhaokai Wang"
                    },
                    {
                        "name": "Yang Yue"
                    },
                    {
                        "name": "Shiji Song"
                    },
                    {
                        "name": "Gao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Gao Huang"
                },
                "author": "Gao Huang",
                "arxiv_comment": "24 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13834v1",
                "updated": "2025-04-18T17:59:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    59,
                    29,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T17:59:29Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    59,
                    29,
                    4,
                    108,
                    0
                ],
                "title": "Science Hierarchography: Hierarchical Organization of Science Literature",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Science Hierarchography: Hierarchical Organization of Science Literature"
                },
                "summary": "Scientific knowledge is growing rapidly, making it challenging to track\nprogress and high-level conceptual links across broad disciplines. While\nexisting tools like citation networks and search engines make it easy to access\na few related papers, they fundamentally lack the flexible abstraction needed\nto represent the density of activity in various scientific subfields. We\nmotivate SCIENCE HIERARCHOGRAPHY, the goal of organizing scientific literature\ninto a high-quality hierarchical structure that allows for the categorization\nof scientific work across varying levels of abstraction, from very broad fields\nto very specific studies. Such a representation can provide insights into which\nfields are well-explored and which are under-explored. To achieve the goals of\nSCIENCE HIERARCHOGRAPHY, we develop a range of algorithms. Our primary approach\ncombines fast embedding-based clustering with LLM-based prompting to balance\nthe computational efficiency of embedding methods with the semantic precision\noffered by LLM prompting. We demonstrate that this approach offers the best\ntrade-off between quality and speed compared to methods that heavily rely on\nLLM prompting, such as iterative tree construction with LLMs. To better reflect\nthe interdisciplinary and multifaceted nature of research papers, our hierarchy\ncaptures multiple dimensions of categorization beyond simple topic labels. We\nevaluate the utility of our framework by assessing how effectively an LLM-based\nagent can locate target papers using the hierarchy. Results show that this\nstructured approach enhances interpretability, supports trend discovery, and\noffers an alternative pathway for exploring scientific literature beyond\ntraditional search methods. Code, data and demo:\n$\\href{https://github.com/JHU-CLSP/science-hierarchography}{https://github.com/JHU-CLSP/science-hierarchography}$",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific knowledge is growing rapidly, making it challenging to track\nprogress and high-level conceptual links across broad disciplines. While\nexisting tools like citation networks and search engines make it easy to access\na few related papers, they fundamentally lack the flexible abstraction needed\nto represent the density of activity in various scientific subfields. We\nmotivate SCIENCE HIERARCHOGRAPHY, the goal of organizing scientific literature\ninto a high-quality hierarchical structure that allows for the categorization\nof scientific work across varying levels of abstraction, from very broad fields\nto very specific studies. Such a representation can provide insights into which\nfields are well-explored and which are under-explored. To achieve the goals of\nSCIENCE HIERARCHOGRAPHY, we develop a range of algorithms. Our primary approach\ncombines fast embedding-based clustering with LLM-based prompting to balance\nthe computational efficiency of embedding methods with the semantic precision\noffered by LLM prompting. We demonstrate that this approach offers the best\ntrade-off between quality and speed compared to methods that heavily rely on\nLLM prompting, such as iterative tree construction with LLMs. To better reflect\nthe interdisciplinary and multifaceted nature of research papers, our hierarchy\ncaptures multiple dimensions of categorization beyond simple topic labels. We\nevaluate the utility of our framework by assessing how effectively an LLM-based\nagent can locate target papers using the hierarchy. Results show that this\nstructured approach enhances interpretability, supports trend discovery, and\noffers an alternative pathway for exploring scientific literature beyond\ntraditional search methods. Code, data and demo:\n$\\href{https://github.com/JHU-CLSP/science-hierarchography}{https://github.com/JHU-CLSP/science-hierarchography}$"
                },
                "authors": [
                    {
                        "name": "Muhan Gao"
                    },
                    {
                        "name": "Jash Shah"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Daniel Khashabi"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Khashabi"
                },
                "author": "Daniel Khashabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13818v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13818v1",
                "updated": "2025-04-18T17:49:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    49,
                    55,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T17:49:55Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    49,
                    55,
                    4,
                    108,
                    0
                ],
                "title": "Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement\n  Learning"
                },
                "summary": "Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing\nreasoning capabilities in large language models, but faces a fundamental\nasymmetry in computation and memory requirements: inference is embarrassingly\nparallel with a minimal memory footprint, while policy updates require\nextensive synchronization and are memory-intensive. To address this asymmetry,\nwe introduce PODS (Policy Optimization with Down-Sampling), a framework that\nstrategically decouples these phases by generating numerous rollouts in\nparallel but updating only on an informative subset. Within this framework, we\ndevelop max-variance down-sampling, a theoretically motivated method that\nselects rollouts with maximally diverse reward signals. We prove that this\napproach has an efficient algorithmic solution, and empirically demonstrate\nthat GRPO with PODS using max-variance down-sampling achieves superior\nperformance over standard GRPO on the GSM8K benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing\nreasoning capabilities in large language models, but faces a fundamental\nasymmetry in computation and memory requirements: inference is embarrassingly\nparallel with a minimal memory footprint, while policy updates require\nextensive synchronization and are memory-intensive. To address this asymmetry,\nwe introduce PODS (Policy Optimization with Down-Sampling), a framework that\nstrategically decouples these phases by generating numerous rollouts in\nparallel but updating only on an informative subset. Within this framework, we\ndevelop max-variance down-sampling, a theoretically motivated method that\nselects rollouts with maximally diverse reward signals. We prove that this\napproach has an efficient algorithmic solution, and empirically demonstrate\nthat GRPO with PODS using max-variance down-sampling achieves superior\nperformance over standard GRPO on the GSM8K benchmark."
                },
                "authors": [
                    {
                        "name": "Yixuan Even Xu"
                    },
                    {
                        "name": "Yash Savani"
                    },
                    {
                        "name": "Fei Fang"
                    },
                    {
                        "name": "Zico Kolter"
                    }
                ],
                "author_detail": {
                    "name": "Zico Kolter"
                },
                "author": "Zico Kolter",
                "arxiv_comment": "9 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13818v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13816v1",
                "updated": "2025-04-18T17:44:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    44,
                    12,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T17:44:12Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    44,
                    12,
                    4,
                    108,
                    0
                ],
                "title": "Analyzing LLMs' Knowledge Boundary Cognition Across Languages Through\n  the Lens of Internal Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing LLMs' Knowledge Boundary Cognition Across Languages Through\n  the Lens of Internal Representations"
                },
                "summary": "While understanding the knowledge boundaries of LLMs is crucial to prevent\nhallucination, research on knowledge boundaries of LLMs has predominantly\nfocused on English. In this work, we present the first study to analyze how\nLLMs recognize knowledge boundaries across different languages by probing their\ninternal representations when processing known and unknown questions in\nmultiple languages. Our empirical studies reveal three key findings: 1) LLMs'\nperceptions of knowledge boundaries are encoded in the middle to middle-upper\nlayers across different languages. 2) Language differences in knowledge\nboundary perception follow a linear structure, which motivates our proposal of\na training-free alignment method that effectively transfers knowledge boundary\nperception ability across languages, thereby helping reduce hallucination risk\nin low-resource languages; 3) Fine-tuning on bilingual question pair\ntranslation further enhances LLMs' recognition of knowledge boundaries across\nlanguages. Given the absence of standard testbeds for cross-lingual knowledge\nboundary analysis, we construct a multilingual evaluation suite comprising\nthree representative types of knowledge boundary data. Our code and datasets\nare publicly available at\nhttps://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While understanding the knowledge boundaries of LLMs is crucial to prevent\nhallucination, research on knowledge boundaries of LLMs has predominantly\nfocused on English. In this work, we present the first study to analyze how\nLLMs recognize knowledge boundaries across different languages by probing their\ninternal representations when processing known and unknown questions in\nmultiple languages. Our empirical studies reveal three key findings: 1) LLMs'\nperceptions of knowledge boundaries are encoded in the middle to middle-upper\nlayers across different languages. 2) Language differences in knowledge\nboundary perception follow a linear structure, which motivates our proposal of\na training-free alignment method that effectively transfers knowledge boundary\nperception ability across languages, thereby helping reduce hallucination risk\nin low-resource languages; 3) Fine-tuning on bilingual question pair\ntranslation further enhances LLMs' recognition of knowledge boundaries across\nlanguages. Given the absence of standard testbeds for cross-lingual knowledge\nboundary analysis, we construct a multilingual evaluation suite comprising\nthree representative types of knowledge boundary data. Our code and datasets\nare publicly available at\nhttps://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundaries."
                },
                "authors": [
                    {
                        "name": "Chenghao Xiao"
                    },
                    {
                        "name": "Hou Pong Chan"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Mahani Aljunied"
                    },
                    {
                        "name": "Lidong Bing"
                    },
                    {
                        "name": "Noura Al Moubayed"
                    },
                    {
                        "name": "Yu Rong"
                    }
                ],
                "author_detail": {
                    "name": "Yu Rong"
                },
                "author": "Yu Rong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12110v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12110v5",
                "updated": "2025-04-18T17:26:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    26,
                    57,
                    4,
                    108,
                    0
                ],
                "published": "2025-02-17T18:36:14Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    18,
                    36,
                    14,
                    0,
                    48,
                    0
                ],
                "title": "A-MEM: Agentic Memory for LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A-MEM: Agentic Memory for LLM Agents"
                },
                "summary": "While large language model (LLM) agents can effectively use external tools\nfor complex real-world tasks, they require memory systems to leverage\nhistorical experiences. Current memory systems enable basic storage and\nretrieval but lack sophisticated memory organization, despite recent attempts\nto incorporate graph databases. Moreover, these systems' fixed operations and\nstructures limit their adaptability across diverse tasks. To address this\nlimitation, this paper proposes a novel agentic memory system for LLM agents\nthat can dynamically organize memories in an agentic way. Following the basic\nprinciples of the Zettelkasten method, we designed our memory system to create\ninterconnected knowledge networks through dynamic indexing and linking. When a\nnew memory is added, we generate a comprehensive note containing multiple\nstructured attributes, including contextual descriptions, keywords, and tags.\nThe system then analyzes historical memories to identify relevant connections,\nestablishing links where meaningful similarities exist. Additionally, this\nprocess enables memory evolution - as new memories are integrated, they can\ntrigger updates to the contextual representations and attributes of existing\nhistorical memories, allowing the memory network to continuously refine its\nunderstanding. Our approach combines the structured organization principles of\nZettelkasten with the flexibility of agent-driven decision making, allowing for\nmore adaptive and context-aware memory management. Empirical experiments on six\nfoundation models show superior improvement against existing SOTA baselines.\nThe source code for evaluating performance is available at\nhttps://github.com/WujiangXu/AgenticMemory, while the source code of agentic\nmemory system is available at https://github.com/agiresearch/A-mem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language model (LLM) agents can effectively use external tools\nfor complex real-world tasks, they require memory systems to leverage\nhistorical experiences. Current memory systems enable basic storage and\nretrieval but lack sophisticated memory organization, despite recent attempts\nto incorporate graph databases. Moreover, these systems' fixed operations and\nstructures limit their adaptability across diverse tasks. To address this\nlimitation, this paper proposes a novel agentic memory system for LLM agents\nthat can dynamically organize memories in an agentic way. Following the basic\nprinciples of the Zettelkasten method, we designed our memory system to create\ninterconnected knowledge networks through dynamic indexing and linking. When a\nnew memory is added, we generate a comprehensive note containing multiple\nstructured attributes, including contextual descriptions, keywords, and tags.\nThe system then analyzes historical memories to identify relevant connections,\nestablishing links where meaningful similarities exist. Additionally, this\nprocess enables memory evolution - as new memories are integrated, they can\ntrigger updates to the contextual representations and attributes of existing\nhistorical memories, allowing the memory network to continuously refine its\nunderstanding. Our approach combines the structured organization principles of\nZettelkasten with the flexibility of agent-driven decision making, allowing for\nmore adaptive and context-aware memory management. Empirical experiments on six\nfoundation models show superior improvement against existing SOTA baselines.\nThe source code for evaluating performance is available at\nhttps://github.com/WujiangXu/AgenticMemory, while the source code of agentic\nmemory system is available at https://github.com/agiresearch/A-mem."
                },
                "authors": [
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Hang Gao"
                    },
                    {
                        "name": "Juntao Tan"
                    },
                    {
                        "name": "Zujie Liang"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12110v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12110v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13807v1",
                "updated": "2025-04-18T17:20:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    20,
                    27,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T17:20:27Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    20,
                    27,
                    4,
                    108,
                    0
                ],
                "title": "DiffOG: Differentiable Policy Trajectory Optimization with\n  Generalizability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffOG: Differentiable Policy Trajectory Optimization with\n  Generalizability"
                },
                "summary": "Imitation learning-based visuomotor policies excel at manipulation tasks but\noften produce suboptimal action trajectories compared to model-based methods.\nDirectly mapping camera data to actions via neural networks can result in jerky\nmotions and difficulties in meeting critical constraints, compromising safety\nand robustness in real-world deployment. For tasks that require high robustness\nor strict adherence to constraints, ensuring trajectory quality is crucial.\nHowever, the lack of interpretability in neural networks makes it challenging\nto generate constraint-compliant actions in a controlled manner. This paper\nintroduces differentiable policy trajectory optimization with generalizability\n(DiffOG), a learning-based trajectory optimization framework designed to\nenhance visuomotor policies. By leveraging the proposed differentiable\nformulation of trajectory optimization with transformer, DiffOG seamlessly\nintegrates policies with a generalizable optimization layer. Visuomotor\npolicies enhanced by DiffOG generate smoother, constraint-compliant action\ntrajectories in a more interpretable way. DiffOG exhibits strong generalization\ncapabilities and high flexibility. We evaluated DiffOG across 11 simulated\ntasks and 2 real-world tasks. The results demonstrate that DiffOG significantly\nenhances the trajectory quality of visuomotor policies while having minimal\nimpact on policy performance, outperforming trajectory processing baselines\nsuch as greedy constraint clipping and penalty-based trajectory optimization.\nFurthermore, DiffOG achieves superior performance compared to existing\nconstrained visuomotor policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imitation learning-based visuomotor policies excel at manipulation tasks but\noften produce suboptimal action trajectories compared to model-based methods.\nDirectly mapping camera data to actions via neural networks can result in jerky\nmotions and difficulties in meeting critical constraints, compromising safety\nand robustness in real-world deployment. For tasks that require high robustness\nor strict adherence to constraints, ensuring trajectory quality is crucial.\nHowever, the lack of interpretability in neural networks makes it challenging\nto generate constraint-compliant actions in a controlled manner. This paper\nintroduces differentiable policy trajectory optimization with generalizability\n(DiffOG), a learning-based trajectory optimization framework designed to\nenhance visuomotor policies. By leveraging the proposed differentiable\nformulation of trajectory optimization with transformer, DiffOG seamlessly\nintegrates policies with a generalizable optimization layer. Visuomotor\npolicies enhanced by DiffOG generate smoother, constraint-compliant action\ntrajectories in a more interpretable way. DiffOG exhibits strong generalization\ncapabilities and high flexibility. We evaluated DiffOG across 11 simulated\ntasks and 2 real-world tasks. The results demonstrate that DiffOG significantly\nenhances the trajectory quality of visuomotor policies while having minimal\nimpact on policy performance, outperforming trajectory processing baselines\nsuch as greedy constraint clipping and penalty-based trajectory optimization.\nFurthermore, DiffOG achieves superior performance compared to existing\nconstrained visuomotor policy."
                },
                "authors": [
                    {
                        "name": "Zhengtong Xu"
                    },
                    {
                        "name": "Zichen Miao"
                    },
                    {
                        "name": "Qiang Qiu"
                    },
                    {
                        "name": "Zhe Zhang"
                    },
                    {
                        "name": "Yu She"
                    }
                ],
                "author_detail": {
                    "name": "Yu She"
                },
                "author": "Yu She",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13805v1",
                "updated": "2025-04-18T17:13:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    13,
                    34,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T17:13:34Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    13,
                    34,
                    4,
                    108,
                    0
                ],
                "title": "LearnAct: Few-Shot Mobile GUI Agent with a Unified Demonstration\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LearnAct: Few-Shot Mobile GUI Agent with a Unified Demonstration\n  Benchmark"
                },
                "summary": "Mobile GUI agents show promise in automating tasks but face generalization\nchallenges in diverse real-world scenarios. Traditional approaches using\npre-training or fine-tuning with massive datasets struggle with the diversity\nof mobile applications and user-specific tasks. We propose enhancing mobile GUI\nagent capabilities through human demonstrations, focusing on improving\nperformance in unseen scenarios rather than pursuing universal generalization\nthrough larger datasets. To realize this paradigm, we introduce LearnGUI, the\nfirst comprehensive dataset specifically designed for studying\ndemonstration-based learning in mobile GUI agents, comprising 2,252 offline\ntasks and 101 online tasks with high-quality human demonstrations. We further\ndevelop LearnAct, a sophisticated multi-agent framework that automatically\nextracts knowledge from demonstrations to enhance task completion. This\nframework integrates three specialized agents: DemoParser for knowledge\nextraction, KnowSeeker for relevant knowledge retrieval, and ActExecutor for\ndemonstration-enhanced task execution. Our experimental results show\nsignificant performance gains in both offline and online evaluations. In\noffline assessments, a single demonstration improves model performance,\nincreasing Gemini-1.5-Pro's accuracy from 19.3% to 51.7%. In online\nevaluations, our framework enhances UI-TARS-7B-SFT's task success rate from\n18.1% to 32.8%. LearnAct framework and LearnGUI benchmark establish\ndemonstration-based learning as a promising direction for more adaptable,\npersonalized, and deployable mobile GUI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile GUI agents show promise in automating tasks but face generalization\nchallenges in diverse real-world scenarios. Traditional approaches using\npre-training or fine-tuning with massive datasets struggle with the diversity\nof mobile applications and user-specific tasks. We propose enhancing mobile GUI\nagent capabilities through human demonstrations, focusing on improving\nperformance in unseen scenarios rather than pursuing universal generalization\nthrough larger datasets. To realize this paradigm, we introduce LearnGUI, the\nfirst comprehensive dataset specifically designed for studying\ndemonstration-based learning in mobile GUI agents, comprising 2,252 offline\ntasks and 101 online tasks with high-quality human demonstrations. We further\ndevelop LearnAct, a sophisticated multi-agent framework that automatically\nextracts knowledge from demonstrations to enhance task completion. This\nframework integrates three specialized agents: DemoParser for knowledge\nextraction, KnowSeeker for relevant knowledge retrieval, and ActExecutor for\ndemonstration-enhanced task execution. Our experimental results show\nsignificant performance gains in both offline and online evaluations. In\noffline assessments, a single demonstration improves model performance,\nincreasing Gemini-1.5-Pro's accuracy from 19.3% to 51.7%. In online\nevaluations, our framework enhances UI-TARS-7B-SFT's task success rate from\n18.1% to 32.8%. LearnAct framework and LearnGUI benchmark establish\ndemonstration-based learning as a promising direction for more adaptable,\npersonalized, and deployable mobile GUI agents."
                },
                "authors": [
                    {
                        "name": "Guangyi Liu"
                    },
                    {
                        "name": "Pengxiang Zhao"
                    },
                    {
                        "name": "Liang Liu"
                    },
                    {
                        "name": "Zhiming Chen"
                    },
                    {
                        "name": "Yuxiang Chai"
                    },
                    {
                        "name": "Shuai Ren"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Shibo He"
                    },
                    {
                        "name": "Wenchao Meng"
                    }
                ],
                "author_detail": {
                    "name": "Wenchao Meng"
                },
                "author": "Wenchao Meng",
                "arxiv_comment": "23 pages, 16 figures, the project resources are available at\n  https://lgy0404.github.io/LearnAct",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07433v2",
                "updated": "2025-04-18T17:03:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    3,
                    1,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-10T04:03:25Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    4,
                    3,
                    25,
                    3,
                    100,
                    0
                ],
                "title": "From Token to Line: Enhancing Code Generation with a Long-Term\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Token to Line: Enhancing Code Generation with a Long-Term\n  Perspective"
                },
                "summary": "The emergence of large language models (LLMs) has significantly promoted the\ndevelopment of code generation task, sparking a surge in pertinent literature.\nCurrent research is hindered by redundant generation results and a tendency to\noverfit local patterns in the short term. Although existing studies attempt to\nalleviate the issue by adopting a multi-token prediction strategy, there\nremains limited focus on choosing the appropriate processing length for\ngenerations. By analyzing the attention between tokens during the generation\nprocess of LLMs, it can be observed that the high spikes of the attention\nscores typically appear at the end of lines. This insight suggests that it is\nreasonable to treat each line of code as a fundamental processing unit and\ngenerate them sequentially. Inspired by this, we propose the \\textbf{LSR-MCTS}\nalgorithm, which leverages MCTS to determine the code line-by-line and select\nthe optimal path. Further, we integrate a self-refine mechanism at each node to\nenhance diversity and generate higher-quality programs through error\ncorrection. Extensive experiments and comprehensive analyses on three public\ncoding benchmarks demonstrate that our method outperforms the state-of-the-art\nperformance approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) has significantly promoted the\ndevelopment of code generation task, sparking a surge in pertinent literature.\nCurrent research is hindered by redundant generation results and a tendency to\noverfit local patterns in the short term. Although existing studies attempt to\nalleviate the issue by adopting a multi-token prediction strategy, there\nremains limited focus on choosing the appropriate processing length for\ngenerations. By analyzing the attention between tokens during the generation\nprocess of LLMs, it can be observed that the high spikes of the attention\nscores typically appear at the end of lines. This insight suggests that it is\nreasonable to treat each line of code as a fundamental processing unit and\ngenerate them sequentially. Inspired by this, we propose the \\textbf{LSR-MCTS}\nalgorithm, which leverages MCTS to determine the code line-by-line and select\nthe optimal path. Further, we integrate a self-refine mechanism at each node to\nenhance diversity and generate higher-quality programs through error\ncorrection. Extensive experiments and comprehensive analyses on three public\ncoding benchmarks demonstrate that our method outperforms the state-of-the-art\nperformance approaches."
                },
                "authors": [
                    {
                        "name": "Tingwei Lu"
                    },
                    {
                        "name": "Yangning Li"
                    },
                    {
                        "name": "Liyuan Wang"
                    },
                    {
                        "name": "Binghuai Lin"
                    },
                    {
                        "name": "Jiwei Tang"
                    },
                    {
                        "name": "Wanshi Xu"
                    },
                    {
                        "name": "Hai-Tao Zheng"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Bingxu An"
                    },
                    {
                        "name": "Zhao Wei"
                    },
                    {
                        "name": "Yong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Xu"
                },
                "author": "Yong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19653v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19653v4",
                "updated": "2025-04-18T16:49:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    16,
                    49,
                    44,
                    4,
                    108,
                    0
                ],
                "published": "2024-05-30T03:12:04Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    3,
                    12,
                    4,
                    3,
                    151,
                    0
                ],
                "title": "SysCaps: Language Interfaces for Simulation Surrogates of Complex\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SysCaps: Language Interfaces for Simulation Surrogates of Complex\n  Systems"
                },
                "summary": "Surrogate models are used to predict the behavior of complex energy systems\nthat are too expensive to simulate with traditional numerical methods. Our work\nintroduces the use of language descriptions, which we call ``system captions''\nor SysCaps, to interface with such surrogates. We argue that interacting with\nsurrogates through text, particularly natural language, makes these models more\naccessible for both experts and non-experts. We introduce a lightweight\nmultimodal text and timeseries regression model and a training pipeline that\nuses large language models (LLMs) to synthesize high-quality captions from\nsimulation metadata. Our experiments on two real-world simulators of buildings\nand wind farms show that our SysCaps-augmented surrogates have better accuracy\non held-out systems than traditional methods while enjoying new generalization\nabilities, such as handling semantically related descriptions of the same test\nsystem. Additional experiments also highlight the potential of SysCaps to\nunlock language-driven design space exploration and to regularize training\nthrough prompt augmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models are used to predict the behavior of complex energy systems\nthat are too expensive to simulate with traditional numerical methods. Our work\nintroduces the use of language descriptions, which we call ``system captions''\nor SysCaps, to interface with such surrogates. We argue that interacting with\nsurrogates through text, particularly natural language, makes these models more\naccessible for both experts and non-experts. We introduce a lightweight\nmultimodal text and timeseries regression model and a training pipeline that\nuses large language models (LLMs) to synthesize high-quality captions from\nsimulation metadata. Our experiments on two real-world simulators of buildings\nand wind farms show that our SysCaps-augmented surrogates have better accuracy\non held-out systems than traditional methods while enjoying new generalization\nabilities, such as handling semantically related descriptions of the same test\nsystem. Additional experiments also highlight the potential of SysCaps to\nunlock language-driven design space exploration and to regularize training\nthrough prompt augmentation."
                },
                "authors": [
                    {
                        "name": "Patrick Emami"
                    },
                    {
                        "name": "Zhaonan Li"
                    },
                    {
                        "name": "Saumya Sinha"
                    },
                    {
                        "name": "Truc Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Truc Nguyen"
                },
                "author": "Truc Nguyen",
                "arxiv_comment": "Accepted at ICLR 2025. 23 pages. Updated with final camera ready\n  version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19653v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19653v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13793v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13793v1",
                "updated": "2025-04-18T16:48:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    16,
                    48,
                    30,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T16:48:30Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    16,
                    48,
                    30,
                    4,
                    108,
                    0
                ],
                "title": "ChatNekoHacker: Real-Time Fan Engagement with Conversational Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatNekoHacker: Real-Time Fan Engagement with Conversational Agents"
                },
                "summary": "ChatNekoHacker is a real-time conversational agent system that strengthens\nfan engagement for musicians. It integrates Amazon Bedrock Agents for\nautonomous dialogue, Unity for immersive 3D livestream sets, and VOICEVOX for\nhigh quality Japanese text-to-speech, enabling two virtual personas to\nrepresent the music duo Neko Hacker. In a one-hour YouTube Live with 30\nparticipants, we evaluated the impact of the system. Regression analysis showed\nthat agent interaction significantly elevated fan interest, with perceived fun\nas the dominant predictor. The participants also expressed a stronger intention\nto listen to the duo's music and attend future concerts. These findings\nhighlight entertaining, interactive broadcasts as pivotal to cultivating\nfandom. Our work offers actionable insights for the deployment of\nconversational agents in entertainment while pointing to next steps: broader\nresponse diversity, lower latency, and tighter fact-checking to curb potential\nmisinformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatNekoHacker is a real-time conversational agent system that strengthens\nfan engagement for musicians. It integrates Amazon Bedrock Agents for\nautonomous dialogue, Unity for immersive 3D livestream sets, and VOICEVOX for\nhigh quality Japanese text-to-speech, enabling two virtual personas to\nrepresent the music duo Neko Hacker. In a one-hour YouTube Live with 30\nparticipants, we evaluated the impact of the system. Regression analysis showed\nthat agent interaction significantly elevated fan interest, with perceived fun\nas the dominant predictor. The participants also expressed a stronger intention\nto listen to the duo's music and attend future concerts. These findings\nhighlight entertaining, interactive broadcasts as pivotal to cultivating\nfandom. Our work offers actionable insights for the deployment of\nconversational agents in entertainment while pointing to next steps: broader\nresponse diversity, lower latency, and tighter fact-checking to curb potential\nmisinformation."
                },
                "authors": [
                    {
                        "name": "Takuya Sera"
                    },
                    {
                        "name": "Yusuke Hamano"
                    }
                ],
                "author_detail": {
                    "name": "Yusuke Hamano"
                },
                "author": "Yusuke Hamano",
                "arxiv_comment": "Accepted to GenAICHI 2025: Generative AI and HCI at CHI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13793v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13793v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13792v1",
                "updated": "2025-04-18T16:44:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    16,
                    44,
                    12,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T16:44:12Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    16,
                    44,
                    12,
                    4,
                    108,
                    0
                ],
                "title": "The Binary and Ternary Quantization Can Improve Feature Discrimination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Binary and Ternary Quantization Can Improve Feature Discrimination"
                },
                "summary": "In machine learning, quantization is widely used to simplify data\nrepresentation and facilitate algorithm deployment on hardware. Given the\nfundamental role of classification in machine learning, it is crucial to\ninvestigate the impact of quantization on classification. Current research\nprimarily focuses on quantization errors, operating under the premise that\nhigher quantization errors generally result in lower classification\nperformance. However, this premise lacks a solid theoretical foundation and\noften contradicts empirical findings. For instance, certain extremely low\nbit-width quantization methods, such as $\\{0,1\\}$-binary quantization and $\\{0,\n\\pm1\\}$-ternary quantization, can achieve comparable or even superior\nclassification accuracy compared to the original non-quantized data, despite\nexhibiting high quantization errors. To more accurately evaluate classification\nperformance, we propose to directly investigate the feature discrimination of\nquantized data, instead of analyzing its quantization error. Interestingly, it\nis found that both binary and ternary quantization methods can improve, rather\nthan degrade, the feature discrimination of the original data. This remarkable\nperformance is validated through classification experiments across various data\ntypes, including images, speech, and texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In machine learning, quantization is widely used to simplify data\nrepresentation and facilitate algorithm deployment on hardware. Given the\nfundamental role of classification in machine learning, it is crucial to\ninvestigate the impact of quantization on classification. Current research\nprimarily focuses on quantization errors, operating under the premise that\nhigher quantization errors generally result in lower classification\nperformance. However, this premise lacks a solid theoretical foundation and\noften contradicts empirical findings. For instance, certain extremely low\nbit-width quantization methods, such as $\\{0,1\\}$-binary quantization and $\\{0,\n\\pm1\\}$-ternary quantization, can achieve comparable or even superior\nclassification accuracy compared to the original non-quantized data, despite\nexhibiting high quantization errors. To more accurately evaluate classification\nperformance, we propose to directly investigate the feature discrimination of\nquantized data, instead of analyzing its quantization error. Interestingly, it\nis found that both binary and ternary quantization methods can improve, rather\nthan degrade, the feature discrimination of the original data. This remarkable\nperformance is validated through classification experiments across various data\ntypes, including images, speech, and texts."
                },
                "authors": [
                    {
                        "name": "Weizhi Lu"
                    },
                    {
                        "name": "Mingrui Chen"
                    },
                    {
                        "name": "Weiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Weiyu Li"
                },
                "author": "Weiyu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13775v1",
                "updated": "2025-04-18T16:22:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    16,
                    22,
                    41,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T16:22:41Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    16,
                    22,
                    41,
                    4,
                    108,
                    0
                ],
                "title": "BadApex: Backdoor Attack Based on Adaptive Optimization Mechanism of\n  Black-box Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BadApex: Backdoor Attack Based on Adaptive Optimization Mechanism of\n  Black-box Large Language Models"
                },
                "summary": "Previous insertion-based and paraphrase-based backdoors have achieved great\nsuccess in attack efficacy, but they ignore the text quality and semantic\nconsistency between poisoned and clean texts. Although recent studies introduce\nLLMs to generate poisoned texts and improve the stealthiness, semantic\nconsistency, and text quality, their hand-crafted prompts rely on expert\nexperiences, facing significant challenges in prompt adaptability and attack\nperformance after defenses. In this paper, we propose a novel backdoor attack\nbased on adaptive optimization mechanism of black-box large language models\n(BadApex), which leverages a black-box LLM to generate poisoned text through a\nrefined prompt. Specifically, an Adaptive Optimization Mechanism is designed to\nrefine an initial prompt iteratively using the generation and modification\nagents. The generation agent generates the poisoned text based on the initial\nprompt. Then the modification agent evaluates the quality of the poisoned text\nand refines a new prompt. After several iterations of the above process, the\nrefined prompt is used to generate poisoned texts through LLMs. We conduct\nextensive experiments on three dataset with six backdoor attacks and two\ndefenses. Extensive experimental results demonstrate that BadApex significantly\noutperforms state-of-the-art attacks. It improves prompt adaptability, semantic\nconsistency, and text quality. Furthermore, when two defense methods are\napplied, the average attack success rate (ASR) still up to 96.75%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous insertion-based and paraphrase-based backdoors have achieved great\nsuccess in attack efficacy, but they ignore the text quality and semantic\nconsistency between poisoned and clean texts. Although recent studies introduce\nLLMs to generate poisoned texts and improve the stealthiness, semantic\nconsistency, and text quality, their hand-crafted prompts rely on expert\nexperiences, facing significant challenges in prompt adaptability and attack\nperformance after defenses. In this paper, we propose a novel backdoor attack\nbased on adaptive optimization mechanism of black-box large language models\n(BadApex), which leverages a black-box LLM to generate poisoned text through a\nrefined prompt. Specifically, an Adaptive Optimization Mechanism is designed to\nrefine an initial prompt iteratively using the generation and modification\nagents. The generation agent generates the poisoned text based on the initial\nprompt. Then the modification agent evaluates the quality of the poisoned text\nand refines a new prompt. After several iterations of the above process, the\nrefined prompt is used to generate poisoned texts through LLMs. We conduct\nextensive experiments on three dataset with six backdoor attacks and two\ndefenses. Extensive experimental results demonstrate that BadApex significantly\noutperforms state-of-the-art attacks. It improves prompt adaptability, semantic\nconsistency, and text quality. Furthermore, when two defense methods are\napplied, the average attack success rate (ASR) still up to 96.75%."
                },
                "authors": [
                    {
                        "name": "Zhengxian Wu"
                    },
                    {
                        "name": "Juan Wen"
                    },
                    {
                        "name": "Wanli Peng"
                    },
                    {
                        "name": "Ziwei Zhang"
                    },
                    {
                        "name": "Yinghan Zhou"
                    },
                    {
                        "name": "Yiming Xue"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Xue"
                },
                "author": "Yiming Xue",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13774v1",
                "updated": "2025-04-18T16:22:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    16,
                    22,
                    20,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T16:22:20Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    16,
                    22,
                    20,
                    4,
                    108,
                    0
                ],
                "title": "DP2Unlearning: An Efficient and Guaranteed Unlearning Framework for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DP2Unlearning: An Efficient and Guaranteed Unlearning Framework for LLMs"
                },
                "summary": "Large language models (LLMs) have recently revolutionized language processing\ntasks but have also brought ethical and legal issues. LLMs have a tendency to\nmemorize potentially private or copyrighted information present in the training\ndata, which might then be delivered to end users at inference time. When this\nhappens, a naive solution is to retrain the model from scratch after excluding\nthe undesired data. Although this guarantees that the target data have been\nforgotten, it is also prohibitively expensive for LLMs. Approximate unlearning\noffers a more efficient alternative, as it consists of ex post modifications of\nthe trained model itself to prevent undesirable results, but it lacks\nforgetting guarantees because it relies solely on empirical evidence. In this\nwork, we present DP2Unlearning, a novel LLM unlearning framework that offers\nformal forgetting guarantees at a significantly lower cost than retraining from\nscratch on the data to be retained. DP2Unlearning involves training LLMs on\ntextual data protected using {\\epsilon}-differential privacy (DP), which later\nenables efficient unlearning with the guarantees against disclosure associated\nwith the chosen {\\epsilon}. Our experiments demonstrate that DP2Unlearning\nachieves similar model performance post-unlearning, compared to an LLM\nretraining from scratch on retained data -- the gold standard exact unlearning\n-- but at approximately half the unlearning cost. In addition, with a\nreasonable computational cost, it outperforms approximate unlearning methods at\nboth preserving the utility of the model post-unlearning and effectively\nforgetting the targeted information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently revolutionized language processing\ntasks but have also brought ethical and legal issues. LLMs have a tendency to\nmemorize potentially private or copyrighted information present in the training\ndata, which might then be delivered to end users at inference time. When this\nhappens, a naive solution is to retrain the model from scratch after excluding\nthe undesired data. Although this guarantees that the target data have been\nforgotten, it is also prohibitively expensive for LLMs. Approximate unlearning\noffers a more efficient alternative, as it consists of ex post modifications of\nthe trained model itself to prevent undesirable results, but it lacks\nforgetting guarantees because it relies solely on empirical evidence. In this\nwork, we present DP2Unlearning, a novel LLM unlearning framework that offers\nformal forgetting guarantees at a significantly lower cost than retraining from\nscratch on the data to be retained. DP2Unlearning involves training LLMs on\ntextual data protected using {\\epsilon}-differential privacy (DP), which later\nenables efficient unlearning with the guarantees against disclosure associated\nwith the chosen {\\epsilon}. Our experiments demonstrate that DP2Unlearning\nachieves similar model performance post-unlearning, compared to an LLM\nretraining from scratch on retained data -- the gold standard exact unlearning\n-- but at approximately half the unlearning cost. In addition, with a\nreasonable computational cost, it outperforms approximate unlearning methods at\nboth preserving the utility of the model post-unlearning and effectively\nforgetting the targeted information."
                },
                "authors": [
                    {
                        "name": "Tamim Al Mahmud"
                    },
                    {
                        "name": "Najeeb Jebreel"
                    },
                    {
                        "name": "Josep Domingo-Ferrer"
                    },
                    {
                        "name": "David Sanchez"
                    }
                ],
                "author_detail": {
                    "name": "David Sanchez"
                },
                "author": "David Sanchez",
                "arxiv_doi": "10.2139/ssrn.5217160",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.2139/ssrn.5217160",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.13774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "49 pages",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13769v1",
                "updated": "2025-04-18T16:11:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    16,
                    11,
                    59,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T16:11:59Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    16,
                    11,
                    59,
                    4,
                    108,
                    0
                ],
                "title": "Detecting Malicious Source Code in PyPI Packages with LLMs: Does RAG\n  Come in Handy?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Malicious Source Code in PyPI Packages with LLMs: Does RAG\n  Come in Handy?"
                },
                "summary": "Malicious software packages in open-source ecosystems, such as PyPI, pose\ngrowing security risks. Unlike traditional vulnerabilities, these packages are\nintentionally designed to deceive users, making detection challenging due to\nevolving attack methods and the lack of structured datasets. In this work, we\nempirically evaluate the effectiveness of Large Language Models (LLMs),\nRetrieval-Augmented Generation (RAG), and few-shot learning for detecting\nmalicious source code. We fine-tune LLMs on curated datasets and integrate YARA\nrules, GitHub Security Advisories, and malicious code snippets with the aim of\nenhancing classification accuracy. We came across a counterintuitive outcome:\nWhile RAG is expected to boost up the prediction performance, it fails in the\nperformed evaluation, obtaining a mediocre accuracy. In contrast, few-shot\nlearning is more effective as it significantly improves the detection of\nmalicious code, achieving 97% accuracy and 95% balanced accuracy, outperforming\ntraditional RAG approaches. Thus, future work should expand structured\nknowledge bases, refine retrieval models, and explore hybrid AI-driven\ncybersecurity solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Malicious software packages in open-source ecosystems, such as PyPI, pose\ngrowing security risks. Unlike traditional vulnerabilities, these packages are\nintentionally designed to deceive users, making detection challenging due to\nevolving attack methods and the lack of structured datasets. In this work, we\nempirically evaluate the effectiveness of Large Language Models (LLMs),\nRetrieval-Augmented Generation (RAG), and few-shot learning for detecting\nmalicious source code. We fine-tune LLMs on curated datasets and integrate YARA\nrules, GitHub Security Advisories, and malicious code snippets with the aim of\nenhancing classification accuracy. We came across a counterintuitive outcome:\nWhile RAG is expected to boost up the prediction performance, it fails in the\nperformed evaluation, obtaining a mediocre accuracy. In contrast, few-shot\nlearning is more effective as it significantly improves the detection of\nmalicious code, achieving 97% accuracy and 95% balanced accuracy, outperforming\ntraditional RAG approaches. Thus, future work should expand structured\nknowledge bases, refine retrieval models, and explore hybrid AI-driven\ncybersecurity solutions."
                },
                "authors": [
                    {
                        "name": "Motunrayo Ibiyo"
                    },
                    {
                        "name": "Thinakone Louangdy"
                    },
                    {
                        "name": "Phuong T. Nguyen"
                    },
                    {
                        "name": "Claudio Di Sipio"
                    },
                    {
                        "name": "Davide Di Ruscio"
                    }
                ],
                "author_detail": {
                    "name": "Davide Di Ruscio"
                },
                "author": "Davide Di Ruscio",
                "arxiv_comment": "The paper has been peer-reviewed and accepted for publication to the\n  29th International Conference on Evaluation and Assessment in Software\n  Engineering (EASE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19413v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19413v2",
                "updated": "2025-04-18T15:48:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    15,
                    48,
                    1,
                    4,
                    108,
                    0
                ],
                "published": "2025-02-26T18:56:52Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    56,
                    52,
                    2,
                    57,
                    0
                ],
                "title": "Project Alexandria: Towards Freeing Scientific Knowledge from Copyright\n  Burdens via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Project Alexandria: Towards Freeing Scientific Knowledge from Copyright\n  Burdens via LLMs"
                },
                "summary": "Paywalls, licenses and copyright rules often restrict the broad dissemination\nand reuse of scientific knowledge. We take the position that it is both legally\nand technically feasible to extract the scientific knowledge in scholarly\ntexts. Current methods, like text embeddings, fail to reliably preserve factual\ncontent, and simple paraphrasing may not be legally sound. We propose a new\nidea for the community to adopt: convert scholarly documents into knowledge\npreserving, but style agnostic representations we term Knowledge Units using\nLLMs. These units use structured data capturing entities, attributes and\nrelationships without stylistic content. We provide evidence that Knowledge\nUnits (1) form a legally defensible framework for sharing knowledge from\ncopyrighted research texts, based on legal analyses of German copyright law and\nU.S. Fair Use doctrine, and (2) preserve most (~95\\%) factual knowledge from\noriginal text, measured by MCQ performance on facts from the original\ncopyrighted text across four research domains. Freeing scientific knowledge\nfrom copyright promises transformative benefits for scientific research and\neducation by allowing language models to reuse important facts from copyrighted\ntext. To support this, we share open-source tools for converting research\ndocuments into Knowledge Units. Overall, our work posits the feasibility of\ndemocratizing access to scientific knowledge while respecting copyright.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Paywalls, licenses and copyright rules often restrict the broad dissemination\nand reuse of scientific knowledge. We take the position that it is both legally\nand technically feasible to extract the scientific knowledge in scholarly\ntexts. Current methods, like text embeddings, fail to reliably preserve factual\ncontent, and simple paraphrasing may not be legally sound. We propose a new\nidea for the community to adopt: convert scholarly documents into knowledge\npreserving, but style agnostic representations we term Knowledge Units using\nLLMs. These units use structured data capturing entities, attributes and\nrelationships without stylistic content. We provide evidence that Knowledge\nUnits (1) form a legally defensible framework for sharing knowledge from\ncopyrighted research texts, based on legal analyses of German copyright law and\nU.S. Fair Use doctrine, and (2) preserve most (~95\\%) factual knowledge from\noriginal text, measured by MCQ performance on facts from the original\ncopyrighted text across four research domains. Freeing scientific knowledge\nfrom copyright promises transformative benefits for scientific research and\neducation by allowing language models to reuse important facts from copyrighted\ntext. To support this, we share open-source tools for converting research\ndocuments into Knowledge Units. Overall, our work posits the feasibility of\ndemocratizing access to scientific knowledge while respecting copyright."
                },
                "authors": [
                    {
                        "name": "Christoph Schuhmann"
                    },
                    {
                        "name": "Gollam Rabby"
                    },
                    {
                        "name": "Ameya Prabhu"
                    },
                    {
                        "name": "Tawsif Ahmed"
                    },
                    {
                        "name": "Andreas Hochlehnert"
                    },
                    {
                        "name": "Huu Nguyen"
                    },
                    {
                        "name": "Nick Akinci"
                    },
                    {
                        "name": "Ludwig Schmidt"
                    },
                    {
                        "name": "Robert Kaczmarczyk"
                    },
                    {
                        "name": "Sören Auer"
                    },
                    {
                        "name": "Jenia Jitsev"
                    },
                    {
                        "name": "Matthias Bethge"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Bethge"
                },
                "author": "Matthias Bethge",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19413v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19413v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13754v1",
                "updated": "2025-04-18T15:39:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    15,
                    39,
                    46,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T15:39:46Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    15,
                    39,
                    46,
                    4,
                    108,
                    0
                ],
                "title": "Towards Accurate and Interpretable Neuroblastoma Diagnosis via\n  Contrastive Multi-scale Pathological Image Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Accurate and Interpretable Neuroblastoma Diagnosis via\n  Contrastive Multi-scale Pathological Image Analysis"
                },
                "summary": "Neuroblastoma, adrenal-derived, is among the most common pediatric solid\nmalignancies, characterized by significant clinical heterogeneity. Timely and\naccurate pathological diagnosis from hematoxylin and eosin-stained whole slide\nimages is critical for patient prognosis. However, current diagnostic practices\nprimarily rely on subjective manual examination by pathologists, leading to\ninconsistent accuracy. Existing automated whole slide image classification\nmethods encounter challenges such as poor interpretability, limited feature\nextraction capabilities, and high computational costs, restricting their\npractical clinical deployment. To overcome these limitations, we propose\nCMSwinKAN, a contrastive-learning-based multi-scale feature fusion model\ntailored for pathological image classification, which enhances the Swin\nTransformer architecture by integrating a Kernel Activation Network within its\nmultilayer perceptron and classification head modules, significantly improving\nboth interpretability and accuracy. By fusing multi-scale features and\nleveraging contrastive learning strategies, CMSwinKAN mimics clinicians'\ncomprehensive approach, effectively capturing global and local tissue\ncharacteristics. Additionally, we introduce a heuristic soft voting mechanism\nguided by clinical insights to seamlessly bridge patch-level predictions to\nwhole slide image-level classifications. We validate CMSwinKAN on the PpNTs\ndataset, which was collaboratively established with our partner hospital and\nthe publicly accessible BreakHis dataset. Results demonstrate that CMSwinKAN\nperforms better than existing state-of-the-art pathology-specific models\npre-trained on large datasets. Our source code is available at\nhttps://github.com/JSLiam94/CMSwinKAN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuroblastoma, adrenal-derived, is among the most common pediatric solid\nmalignancies, characterized by significant clinical heterogeneity. Timely and\naccurate pathological diagnosis from hematoxylin and eosin-stained whole slide\nimages is critical for patient prognosis. However, current diagnostic practices\nprimarily rely on subjective manual examination by pathologists, leading to\ninconsistent accuracy. Existing automated whole slide image classification\nmethods encounter challenges such as poor interpretability, limited feature\nextraction capabilities, and high computational costs, restricting their\npractical clinical deployment. To overcome these limitations, we propose\nCMSwinKAN, a contrastive-learning-based multi-scale feature fusion model\ntailored for pathological image classification, which enhances the Swin\nTransformer architecture by integrating a Kernel Activation Network within its\nmultilayer perceptron and classification head modules, significantly improving\nboth interpretability and accuracy. By fusing multi-scale features and\nleveraging contrastive learning strategies, CMSwinKAN mimics clinicians'\ncomprehensive approach, effectively capturing global and local tissue\ncharacteristics. Additionally, we introduce a heuristic soft voting mechanism\nguided by clinical insights to seamlessly bridge patch-level predictions to\nwhole slide image-level classifications. We validate CMSwinKAN on the PpNTs\ndataset, which was collaboratively established with our partner hospital and\nthe publicly accessible BreakHis dataset. Results demonstrate that CMSwinKAN\nperforms better than existing state-of-the-art pathology-specific models\npre-trained on large datasets. Our source code is available at\nhttps://github.com/JSLiam94/CMSwinKAN."
                },
                "authors": [
                    {
                        "name": "Zhu Zhu"
                    },
                    {
                        "name": "Shuo Jiang"
                    },
                    {
                        "name": "Jingyuan Zheng"
                    },
                    {
                        "name": "Yawen Li"
                    },
                    {
                        "name": "Yifei Chen"
                    },
                    {
                        "name": "Manli Zhao"
                    },
                    {
                        "name": "Weizhong Gu"
                    },
                    {
                        "name": "Feiwei Qin"
                    },
                    {
                        "name": "Jinhu Wang"
                    },
                    {
                        "name": "Gang Yu"
                    }
                ],
                "author_detail": {
                    "name": "Gang Yu"
                },
                "author": "Gang Yu",
                "arxiv_comment": "14pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12022v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12022v2",
                "updated": "2025-04-18T15:31:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    15,
                    31,
                    32,
                    4,
                    108,
                    0
                ],
                "published": "2024-08-21T22:29:56Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    22,
                    29,
                    56,
                    2,
                    234,
                    0
                ],
                "title": "Understanding Epistemic Language with a Language-augmented Bayesian\n  Theory of Mind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Epistemic Language with a Language-augmented Bayesian\n  Theory of Mind"
                },
                "summary": "How do people understand and evaluate claims about others' beliefs, even\nthough these beliefs cannot be directly observed? In this paper, we introduce a\ncognitive model of epistemic language interpretation, grounded in Bayesian\ninferences about other agents' goals, beliefs, and intentions: a\nlanguage-augmented Bayesian theory-of-mind (LaBToM). By translating natural\nlanguage into an epistemic ``language-of-thought'' with grammar-constrained LLM\ndecoding, then evaluating these translations against the inferences produced by\ninverting a generative model of rational action and perception, LaBToM captures\ngraded plausibility judgments of epistemic claims. We validate our model in an\nexperiment where participants watch an agent navigate a maze to find keys\nhidden in boxes needed to reach their goal, then rate sentences about the\nagent's beliefs. In contrast with multimodal LLMs (GPT-4o, Gemini Pro) and\nablated models, our model correlates highly with human judgments for a wide\nrange of expressions, including modal language, uncertainty expressions,\nknowledge claims, likelihood comparisons, and attributions of false belief.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How do people understand and evaluate claims about others' beliefs, even\nthough these beliefs cannot be directly observed? In this paper, we introduce a\ncognitive model of epistemic language interpretation, grounded in Bayesian\ninferences about other agents' goals, beliefs, and intentions: a\nlanguage-augmented Bayesian theory-of-mind (LaBToM). By translating natural\nlanguage into an epistemic ``language-of-thought'' with grammar-constrained LLM\ndecoding, then evaluating these translations against the inferences produced by\ninverting a generative model of rational action and perception, LaBToM captures\ngraded plausibility judgments of epistemic claims. We validate our model in an\nexperiment where participants watch an agent navigate a maze to find keys\nhidden in boxes needed to reach their goal, then rate sentences about the\nagent's beliefs. In contrast with multimodal LLMs (GPT-4o, Gemini Pro) and\nablated models, our model correlates highly with human judgments for a wide\nrange of expressions, including modal language, uncertainty expressions,\nknowledge claims, likelihood comparisons, and attributions of false belief."
                },
                "authors": [
                    {
                        "name": "Lance Ying"
                    },
                    {
                        "name": "Tan Zhi-Xuan"
                    },
                    {
                        "name": "Lionel Wong"
                    },
                    {
                        "name": "Vikash Mansinghka"
                    },
                    {
                        "name": "Joshua B. Tenenbaum"
                    }
                ],
                "author_detail": {
                    "name": "Joshua B. Tenenbaum"
                },
                "author": "Joshua B. Tenenbaum",
                "arxiv_comment": "23 pages; Published at the Transactions of the Association for\n  Computational Linguistics (TACL); Presented at NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12022v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12022v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13736v1",
                "updated": "2025-04-18T15:04:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    15,
                    4,
                    53,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T15:04:53Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    15,
                    4,
                    53,
                    4,
                    108,
                    0
                ],
                "title": "LimitNet: Progressive, Content-Aware Image Offloading for Extremely Weak\n  Devices & Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LimitNet: Progressive, Content-Aware Image Offloading for Extremely Weak\n  Devices & Networks"
                },
                "summary": "IoT devices have limited hardware capabilities and are often deployed in\nremote areas. Consequently, advanced vision models surpass such devices'\nprocessing and storage capabilities, requiring offloading of such tasks to the\ncloud. However, remote areas often rely on LPWANs technology with limited\nbandwidth, high packet loss rates, and extremely low duty cycles, which makes\nfast offloading for time-sensitive inference challenging. Today's approaches,\nwhich are deployable on weak devices, generate a non-progressive bit stream,\nand therefore, their decoding quality suffers strongly when data is only\npartially available on the cloud at a deadline due to limited bandwidth or\npacket losses.\n  In this paper, we introduce LimitNet, a progressive, content-aware image\ncompression model designed for extremely weak devices and networks. LimitNet's\nlightweight progressive encoder prioritizes critical data during transmission\nbased on the content of the image, which gives the cloud the opportunity to run\ninference even with partial data availability.\n  Experimental results demonstrate that LimitNet, on average, compared to SOTA,\nachieves 14.01 p.p. (percentage point) higher accuracy on ImageNet1000, 18.01\npp on CIFAR100, and 0.1 higher mAP@0.5 on COCO. Also, on average, LimitNet\nsaves 61.24% bandwidth on ImageNet1000, 83.68% on CIFAR100, and 42.25% on the\nCOCO dataset compared to SOTA, while it only has 4% more encoding time compared\nto JPEG (with a fixed quality) on STM32F7 (Cortex-M7).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IoT devices have limited hardware capabilities and are often deployed in\nremote areas. Consequently, advanced vision models surpass such devices'\nprocessing and storage capabilities, requiring offloading of such tasks to the\ncloud. However, remote areas often rely on LPWANs technology with limited\nbandwidth, high packet loss rates, and extremely low duty cycles, which makes\nfast offloading for time-sensitive inference challenging. Today's approaches,\nwhich are deployable on weak devices, generate a non-progressive bit stream,\nand therefore, their decoding quality suffers strongly when data is only\npartially available on the cloud at a deadline due to limited bandwidth or\npacket losses.\n  In this paper, we introduce LimitNet, a progressive, content-aware image\ncompression model designed for extremely weak devices and networks. LimitNet's\nlightweight progressive encoder prioritizes critical data during transmission\nbased on the content of the image, which gives the cloud the opportunity to run\ninference even with partial data availability.\n  Experimental results demonstrate that LimitNet, on average, compared to SOTA,\nachieves 14.01 p.p. (percentage point) higher accuracy on ImageNet1000, 18.01\npp on CIFAR100, and 0.1 higher mAP@0.5 on COCO. Also, on average, LimitNet\nsaves 61.24% bandwidth on ImageNet1000, 83.68% on CIFAR100, and 42.25% on the\nCOCO dataset compared to SOTA, while it only has 4% more encoding time compared\nto JPEG (with a fixed quality) on STM32F7 (Cortex-M7)."
                },
                "authors": [
                    {
                        "name": "Ali Hojjat"
                    },
                    {
                        "name": "Janek Haberer"
                    },
                    {
                        "name": "Tayyaba Zainab"
                    },
                    {
                        "name": "Olaf Landsiedel"
                    }
                ],
                "author_detail": {
                    "name": "Olaf Landsiedel"
                },
                "author": "Olaf Landsiedel",
                "arxiv_doi": "10.1145/3643832.3661856",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3643832.3661856",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.13736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This is the author's accepted manuscript. The Version of Record is\n  available at: https://doi.org/10.1145/3643832.3661856",
                "arxiv_journal_ref": "In Proceedings of the 22nd ACM International Conference on Mobile\n  Systems, Applications, and Services (MobiSys '24), June 3-7, 2024, Minato-ku,\n  Tokyo, Japan. ACM, New York, NY, USA",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13730v1",
                "updated": "2025-04-18T14:57:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    14,
                    57,
                    7,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T14:57:07Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    14,
                    57,
                    7,
                    4,
                    108,
                    0
                ],
                "title": "Controlled Territory and Conflict Tracking (CONTACT): (Geo-)Mapping\n  Occupied Territory from Open Source Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlled Territory and Conflict Tracking (CONTACT): (Geo-)Mapping\n  Occupied Territory from Open Source Intelligence"
                },
                "summary": "Open-source intelligence provides a stream of unstructured textual data that\ncan inform assessments of territorial control. We present CONTACT, a framework\nfor territorial control prediction using large language models (LLMs) and\nminimal supervision. We evaluate two approaches: SetFit, an embedding-based\nfew-shot classifier, and a prompt tuning method applied to BLOOMZ-560m, a\nmultilingual generative LLM. Our model is trained on a small hand-labeled\ndataset of news articles covering ISIS activity in Syria and Iraq, using\nprompt-conditioned extraction of control-relevant signals such as military\noperations, casualties, and location references. We show that the BLOOMZ-based\nmodel outperforms the SetFit baseline, and that prompt-based supervision\nimproves generalization in low-resource settings. CONTACT demonstrates that\nLLMs fine-tuned using few-shot methods can reduce annotation burdens and\nsupport structured inference from open-ended OSINT streams. Our code is\navailable at https://github.com/PaulKMandal/CONTACT/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source intelligence provides a stream of unstructured textual data that\ncan inform assessments of territorial control. We present CONTACT, a framework\nfor territorial control prediction using large language models (LLMs) and\nminimal supervision. We evaluate two approaches: SetFit, an embedding-based\nfew-shot classifier, and a prompt tuning method applied to BLOOMZ-560m, a\nmultilingual generative LLM. Our model is trained on a small hand-labeled\ndataset of news articles covering ISIS activity in Syria and Iraq, using\nprompt-conditioned extraction of control-relevant signals such as military\noperations, casualties, and location references. We show that the BLOOMZ-based\nmodel outperforms the SetFit baseline, and that prompt-based supervision\nimproves generalization in low-resource settings. CONTACT demonstrates that\nLLMs fine-tuned using few-shot methods can reduce annotation burdens and\nsupport structured inference from open-ended OSINT streams. Our code is\navailable at https://github.com/PaulKMandal/CONTACT/."
                },
                "authors": [
                    {
                        "name": "Paul K. Mandal"
                    },
                    {
                        "name": "Cole Leo"
                    },
                    {
                        "name": "Connor Hurley"
                    }
                ],
                "author_detail": {
                    "name": "Connor Hurley"
                },
                "author": "Connor Hurley",
                "arxiv_comment": "7 pages, 1 figure, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; I.2.8; H.3.1; K.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13717v1",
                "updated": "2025-04-18T14:40:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    14,
                    40,
                    58,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T14:40:58Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    14,
                    40,
                    58,
                    4,
                    108,
                    0
                ],
                "title": "Human-aligned Deep Learning: Explainability, Causality, and Biological\n  Inspiration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-aligned Deep Learning: Explainability, Causality, and Biological\n  Inspiration"
                },
                "summary": "This work aligns deep learning (DL) with human reasoning capabilities and\nneeds to enable more efficient, interpretable, and robust image classification.\nWe approach this from three perspectives: explainability, causality, and\nbiological vision. Introduction and background open this work before diving\ninto operative chapters. First, we assess neural networks' visualization\ntechniques for medical images and validate an explainable-by-design method for\nbreast mass classification. A comprehensive review at the intersection of XAI\nand causality follows, where we introduce a general scaffold to organize past\nand future research, laying the groundwork for our second perspective. In the\ncausality direction, we propose novel modules that exploit feature\nco-occurrence in medical images, leading to more effective and explainable\npredictions. We further introduce CROCODILE, a general framework that\nintegrates causal concepts, contrastive learning, feature disentanglement, and\nprior knowledge to enhance generalization. Lastly, we explore biological\nvision, examining how humans recognize objects, and propose CoCoReco, a\nconnectivity-inspired network with context-aware attention mechanisms. Overall,\nour key findings include: (i) simple activation maximization lacks insight for\nmedical imaging DL models; (ii) prototypical-part learning is effective and\nradiologically aligned; (iii) XAI and causal ML are deeply connected; (iv) weak\ncausal signals can be leveraged without a priori information to improve\nperformance and interpretability; (v) our framework generalizes across medical\ndomains and out-of-distribution data; (vi) incorporating biological circuit\nmotifs improves human-aligned recognition. This work contributes toward\nhuman-aligned DL and highlights pathways to bridge the gap between research and\nclinical adoption, with implications for improved trust, diagnostic accuracy,\nand safe deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work aligns deep learning (DL) with human reasoning capabilities and\nneeds to enable more efficient, interpretable, and robust image classification.\nWe approach this from three perspectives: explainability, causality, and\nbiological vision. Introduction and background open this work before diving\ninto operative chapters. First, we assess neural networks' visualization\ntechniques for medical images and validate an explainable-by-design method for\nbreast mass classification. A comprehensive review at the intersection of XAI\nand causality follows, where we introduce a general scaffold to organize past\nand future research, laying the groundwork for our second perspective. In the\ncausality direction, we propose novel modules that exploit feature\nco-occurrence in medical images, leading to more effective and explainable\npredictions. We further introduce CROCODILE, a general framework that\nintegrates causal concepts, contrastive learning, feature disentanglement, and\nprior knowledge to enhance generalization. Lastly, we explore biological\nvision, examining how humans recognize objects, and propose CoCoReco, a\nconnectivity-inspired network with context-aware attention mechanisms. Overall,\nour key findings include: (i) simple activation maximization lacks insight for\nmedical imaging DL models; (ii) prototypical-part learning is effective and\nradiologically aligned; (iii) XAI and causal ML are deeply connected; (iv) weak\ncausal signals can be leveraged without a priori information to improve\nperformance and interpretability; (v) our framework generalizes across medical\ndomains and out-of-distribution data; (vi) incorporating biological circuit\nmotifs improves human-aligned recognition. This work contributes toward\nhuman-aligned DL and highlights pathways to bridge the gap between research and\nclinical adoption, with implications for improved trust, diagnostic accuracy,\nand safe deployment."
                },
                "authors": [
                    {
                        "name": "Gianluca Carloni"
                    }
                ],
                "author_detail": {
                    "name": "Gianluca Carloni"
                },
                "author": "Gianluca Carloni",
                "arxiv_comment": "Personal adaptation and expansion of doctoral thesis (originally\n  submitted in Oct 2024, revisioned in Jan 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.2.6; I.4; I.4.7; I.5; J.3; J.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09024v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09024v3",
                "updated": "2025-04-18T14:30:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    14,
                    30,
                    31,
                    4,
                    108,
                    0
                ],
                "published": "2024-10-11T17:39:22Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    39,
                    22,
                    4,
                    285,
                    0
                ],
                "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents"
                },
                "summary": "The robustness of LLMs to jailbreak attacks, where users design prompts to\ncircumvent safety measures and misuse model capabilities, has been studied\nprimarily for LLMs acting as simple chatbots. Meanwhile, LLM agents -- which\nuse external tools and can execute multi-stage tasks -- may pose a greater risk\nif misused, but their robustness remains underexplored. To facilitate research\non LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark\nincludes a diverse set of 110 explicitly malicious agent tasks (440 with\naugmentations), covering 11 harm categories including fraud, cybercrime, and\nharassment. In addition to measuring whether models refuse harmful agentic\nrequests, scoring well on AgentHarm requires jailbroken agents to maintain\ntheir capabilities following an attack to complete a multi-step task. We\nevaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly\ncompliant with malicious agent requests without jailbreaking, (2) simple\nuniversal jailbreak templates can be adapted to effectively jailbreak agents,\nand (3) these jailbreaks enable coherent and malicious multi-step agent\nbehavior and retain model capabilities. To enable simple and reliable\nevaluation of attacks and defenses for LLM-based agents, we publicly release\nAgentHarm at https://huggingface.co/datasets/ai-safety-institute/AgentHarm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The robustness of LLMs to jailbreak attacks, where users design prompts to\ncircumvent safety measures and misuse model capabilities, has been studied\nprimarily for LLMs acting as simple chatbots. Meanwhile, LLM agents -- which\nuse external tools and can execute multi-stage tasks -- may pose a greater risk\nif misused, but their robustness remains underexplored. To facilitate research\non LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark\nincludes a diverse set of 110 explicitly malicious agent tasks (440 with\naugmentations), covering 11 harm categories including fraud, cybercrime, and\nharassment. In addition to measuring whether models refuse harmful agentic\nrequests, scoring well on AgentHarm requires jailbroken agents to maintain\ntheir capabilities following an attack to complete a multi-step task. We\nevaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly\ncompliant with malicious agent requests without jailbreaking, (2) simple\nuniversal jailbreak templates can be adapted to effectively jailbreak agents,\nand (3) these jailbreaks enable coherent and malicious multi-step agent\nbehavior and retain model capabilities. To enable simple and reliable\nevaluation of attacks and defenses for LLM-based agents, we publicly release\nAgentHarm at https://huggingface.co/datasets/ai-safety-institute/AgentHarm."
                },
                "authors": [
                    {
                        "name": "Maksym Andriushchenko"
                    },
                    {
                        "name": "Alexandra Souly"
                    },
                    {
                        "name": "Mateusz Dziemian"
                    },
                    {
                        "name": "Derek Duenas"
                    },
                    {
                        "name": "Maxwell Lin"
                    },
                    {
                        "name": "Justin Wang"
                    },
                    {
                        "name": "Dan Hendrycks"
                    },
                    {
                        "name": "Andy Zou"
                    },
                    {
                        "name": "Zico Kolter"
                    },
                    {
                        "name": "Matt Fredrikson"
                    },
                    {
                        "name": "Eric Winsor"
                    },
                    {
                        "name": "Jerome Wynne"
                    },
                    {
                        "name": "Yarin Gal"
                    },
                    {
                        "name": "Xander Davies"
                    }
                ],
                "author_detail": {
                    "name": "Xander Davies"
                },
                "author": "Xander Davies",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09024v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09024v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13707v1",
                "updated": "2025-04-18T14:11:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    14,
                    11,
                    27,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T14:11:27Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    14,
                    11,
                    27,
                    4,
                    108,
                    0
                ],
                "title": "OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via\n  Open-ended Interaction Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via\n  Open-ended Interaction Simulation"
                },
                "summary": "As the general capabilities of large language models (LLMs) improve and agent\napplications become more widespread, the underlying deception risks urgently\nrequire systematic evaluation and effective oversight. Unlike existing\nevaluation which uses simulated games or presents limited choices, we introduce\nOpenDeception, a novel deception evaluation framework with an open-ended\nscenario dataset. OpenDeception jointly evaluates both the deception intention\nand capabilities of LLM-based agents by inspecting their internal reasoning\nprocess. Specifically, we construct five types of common use cases where LLMs\nintensively interact with the user, each consisting of ten diverse, concrete\nscenarios from the real world. To avoid ethical concerns and costs of high-risk\ndeceptive interactions with human testers, we propose to simulate the\nmulti-turn dialogue via agent simulation. Extensive evaluation of eleven\nmainstream LLMs on OpenDeception highlights the urgent need to address\ndeception risks and security concerns in LLM-based agents: the deception\nintention ratio across the models exceeds 80%, while the deception success rate\nsurpasses 50%. Furthermore, we observe that LLMs with stronger capabilities do\nexhibit a higher risk of deception, which calls for more alignment efforts on\ninhibiting deceptive behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the general capabilities of large language models (LLMs) improve and agent\napplications become more widespread, the underlying deception risks urgently\nrequire systematic evaluation and effective oversight. Unlike existing\nevaluation which uses simulated games or presents limited choices, we introduce\nOpenDeception, a novel deception evaluation framework with an open-ended\nscenario dataset. OpenDeception jointly evaluates both the deception intention\nand capabilities of LLM-based agents by inspecting their internal reasoning\nprocess. Specifically, we construct five types of common use cases where LLMs\nintensively interact with the user, each consisting of ten diverse, concrete\nscenarios from the real world. To avoid ethical concerns and costs of high-risk\ndeceptive interactions with human testers, we propose to simulate the\nmulti-turn dialogue via agent simulation. Extensive evaluation of eleven\nmainstream LLMs on OpenDeception highlights the urgent need to address\ndeception risks and security concerns in LLM-based agents: the deception\nintention ratio across the models exceeds 80%, while the deception success rate\nsurpasses 50%. Furthermore, we observe that LLMs with stronger capabilities do\nexhibit a higher risk of deception, which calls for more alignment efforts on\ninhibiting deceptive behaviors."
                },
                "authors": [
                    {
                        "name": "Yichen Wu"
                    },
                    {
                        "name": "Xudong Pan"
                    },
                    {
                        "name": "Geng Hong"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11005v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11005v3",
                "updated": "2025-04-18T14:01:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    14,
                    1,
                    42,
                    4,
                    108,
                    0
                ],
                "published": "2024-02-16T18:28:43Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    18,
                    28,
                    43,
                    4,
                    47,
                    0
                ],
                "title": "A Theory of LLM Sampling: Part Descriptive and Part Prescriptive",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Theory of LLM Sampling: Part Descriptive and Part Prescriptive"
                },
                "summary": "Large Language Models (LLMs) are increasingly utilized in autonomous\ndecision-making, where they sample options from vast action spaces. However,\nthe heuristics that guide this sampling process remain under-explored. We study\nthis sampling behavior and show that this underlying heuristics resembles that\nof human decision-making: comprising a descriptive component (reflecting\nstatistical norm) and a prescriptive component (implicit ideal encoded in the\nLLM) of a concept. We show that this deviation of a sample from the statistical\nnorm towards a prescriptive component consistently appears in concepts across\ndiverse real-world domains like public health, and economic trends. To further\nillustrate the theory, we demonstrate that concept prototypes in LLMs are\naffected by prescriptive norms, similar to the concept of normality in humans.\nThrough case studies and comparison with human studies, we illustrate that in\nreal-world applications, the shift of samples toward an ideal value in LLMs'\noutputs can result in significantly biased decision-making, raising ethical\nconcerns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly utilized in autonomous\ndecision-making, where they sample options from vast action spaces. However,\nthe heuristics that guide this sampling process remain under-explored. We study\nthis sampling behavior and show that this underlying heuristics resembles that\nof human decision-making: comprising a descriptive component (reflecting\nstatistical norm) and a prescriptive component (implicit ideal encoded in the\nLLM) of a concept. We show that this deviation of a sample from the statistical\nnorm towards a prescriptive component consistently appears in concepts across\ndiverse real-world domains like public health, and economic trends. To further\nillustrate the theory, we demonstrate that concept prototypes in LLMs are\naffected by prescriptive norms, similar to the concept of normality in humans.\nThrough case studies and comparison with human studies, we illustrate that in\nreal-world applications, the shift of samples toward an ideal value in LLMs'\noutputs can result in significantly biased decision-making, raising ethical\nconcerns."
                },
                "authors": [
                    {
                        "name": "Sarath Sivaprasad"
                    },
                    {
                        "name": "Pramod Kaushik"
                    },
                    {
                        "name": "Sahar Abdelnabi"
                    },
                    {
                        "name": "Mario Fritz"
                    }
                ],
                "author_detail": {
                    "name": "Mario Fritz"
                },
                "author": "Mario Fritz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11005v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11005v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13700v1",
                "updated": "2025-04-18T14:00:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    14,
                    0,
                    55,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T14:00:55Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    14,
                    0,
                    55,
                    4,
                    108,
                    0
                ],
                "title": "Exploring Multimodal Prompt for Visualization Authoring with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Multimodal Prompt for Visualization Authoring with Large\n  Language Models"
                },
                "summary": "Recent advances in large language models (LLMs) have shown great potential in\nautomating the process of visualization authoring through simple natural\nlanguage utterances. However, instructing LLMs using natural language is\nlimited in precision and expressiveness for conveying visualization intent,\nleading to misinterpretation and time-consuming iterations. To address these\nlimitations, we conduct an empirical study to understand how LLMs interpret\nambiguous or incomplete text prompts in the context of visualization authoring,\nand the conditions making LLMs misinterpret user intent. Informed by the\nfindings, we introduce visual prompts as a complementary input modality to text\nprompts, which help clarify user intent and improve LLMs' interpretation\nabilities. To explore the potential of multimodal prompting in visualization\nauthoring, we design VisPilot, which enables users to easily create\nvisualizations using multimodal prompts, including text, sketches, and direct\nmanipulations on existing visualizations. Through two case studies and a\ncontrolled user study, we demonstrate that VisPilot provides a more intuitive\nway to create visualizations without affecting the overall task efficiency\ncompared to text-only prompting approaches. Furthermore, we analyze the impact\nof text and visual prompts in different visualization tasks. Our findings\nhighlight the importance of multimodal prompting in improving the usability of\nLLMs for visualization authoring. We discuss design implications for future\nvisualization systems and provide insights into how multimodal prompts can\nenhance human-AI collaboration in creative visualization tasks. All materials\nare available at https://OSF.IO/2QRAK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have shown great potential in\nautomating the process of visualization authoring through simple natural\nlanguage utterances. However, instructing LLMs using natural language is\nlimited in precision and expressiveness for conveying visualization intent,\nleading to misinterpretation and time-consuming iterations. To address these\nlimitations, we conduct an empirical study to understand how LLMs interpret\nambiguous or incomplete text prompts in the context of visualization authoring,\nand the conditions making LLMs misinterpret user intent. Informed by the\nfindings, we introduce visual prompts as a complementary input modality to text\nprompts, which help clarify user intent and improve LLMs' interpretation\nabilities. To explore the potential of multimodal prompting in visualization\nauthoring, we design VisPilot, which enables users to easily create\nvisualizations using multimodal prompts, including text, sketches, and direct\nmanipulations on existing visualizations. Through two case studies and a\ncontrolled user study, we demonstrate that VisPilot provides a more intuitive\nway to create visualizations without affecting the overall task efficiency\ncompared to text-only prompting approaches. Furthermore, we analyze the impact\nof text and visual prompts in different visualization tasks. Our findings\nhighlight the importance of multimodal prompting in improving the usability of\nLLMs for visualization authoring. We discuss design implications for future\nvisualization systems and provide insights into how multimodal prompts can\nenhance human-AI collaboration in creative visualization tasks. All materials\nare available at https://OSF.IO/2QRAK."
                },
                "authors": [
                    {
                        "name": "Zhen Wen"
                    },
                    {
                        "name": "Luoxuan Weng"
                    },
                    {
                        "name": "Yinghao Tang"
                    },
                    {
                        "name": "Runjin Zhang"
                    },
                    {
                        "name": "Yuxin Liu"
                    },
                    {
                        "name": "Bo Pan"
                    },
                    {
                        "name": "Minfeng Zhu"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "arxiv_comment": "11 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13684v1",
                "updated": "2025-04-18T13:35:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    35,
                    21,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T13:35:21Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    35,
                    21,
                    4,
                    108,
                    0
                ],
                "title": "Intelligent Interaction Strategies for Context-Aware Cognitive\n  Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Interaction Strategies for Context-Aware Cognitive\n  Augmentation"
                },
                "summary": "Human cognition is constrained by processing limitations, leading to\ncognitive overload and inefficiencies in knowledge synthesis and\ndecision-making. Large Language Models (LLMs) present an opportunity for\ncognitive augmentation, but their current reactive nature limits their\nreal-world applicability. This position paper explores the potential of\ncontext-aware cognitive augmentation, where LLMs dynamically adapt to users'\ncognitive states and task environments to provide appropriate support. Through\na think-aloud study in an exhibition setting, we examine how individuals\ninteract with multi-modal information and identify key cognitive challenges in\nstructuring, retrieving, and applying knowledge. Our findings highlight the\nneed for AI-driven cognitive support systems that integrate real-time\ncontextual awareness, personalized reasoning assistance, and socially adaptive\ninteractions. We propose a framework for AI augmentation that seamlessly\ntransitions between real-time cognitive support and post-experience knowledge\norganization, contributing to the design of more effective human-centered AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human cognition is constrained by processing limitations, leading to\ncognitive overload and inefficiencies in knowledge synthesis and\ndecision-making. Large Language Models (LLMs) present an opportunity for\ncognitive augmentation, but their current reactive nature limits their\nreal-world applicability. This position paper explores the potential of\ncontext-aware cognitive augmentation, where LLMs dynamically adapt to users'\ncognitive states and task environments to provide appropriate support. Through\na think-aloud study in an exhibition setting, we examine how individuals\ninteract with multi-modal information and identify key cognitive challenges in\nstructuring, retrieving, and applying knowledge. Our findings highlight the\nneed for AI-driven cognitive support systems that integrate real-time\ncontextual awareness, personalized reasoning assistance, and socially adaptive\ninteractions. We propose a framework for AI augmentation that seamlessly\ntransitions between real-time cognitive support and post-experience knowledge\norganization, contributing to the design of more effective human-centered AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Xiangrong"
                    },
                    {
                        "name": "Zhu"
                    },
                    {
                        "name": "Yuan Xu"
                    },
                    {
                        "name": "Tianjian Liu"
                    },
                    {
                        "name": "Jingwei Sun"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Xin Tong"
                    }
                ],
                "author_detail": {
                    "name": "Xin Tong"
                },
                "arxiv_affiliation": "Daniel",
                "author": "Xin Tong",
                "arxiv_comment": "Presented at the 2025 ACM Workshop on Human-AI Interaction for\n  Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING",
                "arxiv_journal_ref": "Proceedings of the 2025 ACM CHI Workshop on Human-AI Interaction\n  for Augmented Reasoning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13677v1",
                "updated": "2025-04-18T13:13:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    13,
                    42,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T13:13:42Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    13,
                    42,
                    4,
                    108,
                    0
                ],
                "title": "Revisiting Uncertainty Quantification Evaluation in Language Models:\n  Spurious Interactions with Response Length Bias Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Uncertainty Quantification Evaluation in Language Models:\n  Spurious Interactions with Response Length Bias Results"
                },
                "summary": "Uncertainty Quantification (UQ) in Language Models (LMs) is crucial for\nimproving their safety and reliability. Evaluations often use performance\nmetrics like AUROC to assess how well UQ methods (e.g., negative sequence\nprobabilities) correlate with task correctness functions (e.g., ROUGE-L). In\nthis paper, we show that commonly used correctness functions bias UQ\nevaluations by inflating the performance of certain UQ methods. We evaluate 7\ncorrectness functions -- from lexical-based and embedding-based metrics to\nLLM-as-a-judge approaches -- across 4 datasets x 4 models x 6 UQ methods. Our\nanalysis reveals that length biases in the errors of these correctness\nfunctions distort UQ assessments by interacting with length biases in UQ\nmethods. We identify LLM-as-a-judge approaches as among the least length-biased\nchoices and hence a potential solution to mitigate these biases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty Quantification (UQ) in Language Models (LMs) is crucial for\nimproving their safety and reliability. Evaluations often use performance\nmetrics like AUROC to assess how well UQ methods (e.g., negative sequence\nprobabilities) correlate with task correctness functions (e.g., ROUGE-L). In\nthis paper, we show that commonly used correctness functions bias UQ\nevaluations by inflating the performance of certain UQ methods. We evaluate 7\ncorrectness functions -- from lexical-based and embedding-based metrics to\nLLM-as-a-judge approaches -- across 4 datasets x 4 models x 6 UQ methods. Our\nanalysis reveals that length biases in the errors of these correctness\nfunctions distort UQ assessments by interacting with length biases in UQ\nmethods. We identify LLM-as-a-judge approaches as among the least length-biased\nchoices and hence a potential solution to mitigate these biases."
                },
                "authors": [
                    {
                        "name": "Andrea Santilli"
                    },
                    {
                        "name": "Adam Golinski"
                    },
                    {
                        "name": "Michael Kirchhof"
                    },
                    {
                        "name": "Federico Danieli"
                    },
                    {
                        "name": "Arno Blaas"
                    },
                    {
                        "name": "Miao Xiong"
                    },
                    {
                        "name": "Luca Zappella"
                    },
                    {
                        "name": "Sinead Williamson"
                    }
                ],
                "author_detail": {
                    "name": "Sinead Williamson"
                },
                "author": "Sinead Williamson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12319v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12319v4",
                "updated": "2025-04-18T13:12:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    12,
                    42,
                    4,
                    108,
                    0
                ],
                "published": "2024-06-18T06:43:04Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    6,
                    43,
                    4,
                    1,
                    170,
                    0
                ],
                "title": "The Comparative Trap: Pairwise Comparisons Amplifies Biased Preferences\n  of LLM Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Comparative Trap: Pairwise Comparisons Amplifies Biased Preferences\n  of LLM Evaluators"
                },
                "summary": "As large language models (LLMs) are increasingly used as evaluators for\nnatural language generation tasks, ensuring unbiased assessments is essential.\nHowever, LLM evaluators often display biased preferences, such as favoring\nverbosity and authoritative tones. Our empirical analysis reveals that these\nbiases are exacerbated in pairwise evaluation, where LLMs directly compare two\noutputs and easily prioritize superficial attributes. In contrast, pointwise\nevaluation, which assesses outputs independently, is less susceptible to such\nbias because each output is judged in isolation. To address the limitations of\nthe pairwise evaluation, we introduce a novel evaluation method, PRePair, which\nintegrates pointwise reasoning within a pairwise framework. PRePair effectively\nalleviates biased preference, improving performance on the adversarial\nbenchmark (LLMBar) while outperforming pointwise evaluation on the standard\nbenchmark (MT-Bench).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly used as evaluators for\nnatural language generation tasks, ensuring unbiased assessments is essential.\nHowever, LLM evaluators often display biased preferences, such as favoring\nverbosity and authoritative tones. Our empirical analysis reveals that these\nbiases are exacerbated in pairwise evaluation, where LLMs directly compare two\noutputs and easily prioritize superficial attributes. In contrast, pointwise\nevaluation, which assesses outputs independently, is less susceptible to such\nbias because each output is judged in isolation. To address the limitations of\nthe pairwise evaluation, we introduce a novel evaluation method, PRePair, which\nintegrates pointwise reasoning within a pairwise framework. PRePair effectively\nalleviates biased preference, improving performance on the adversarial\nbenchmark (LLMBar) while outperforming pointwise evaluation on the standard\nbenchmark (MT-Bench)."
                },
                "authors": [
                    {
                        "name": "Hawon Jeong"
                    },
                    {
                        "name": "ChaeHun Park"
                    },
                    {
                        "name": "Jimin Hong"
                    },
                    {
                        "name": "Hojoon Lee"
                    },
                    {
                        "name": "Jaegul Choo"
                    }
                ],
                "author_detail": {
                    "name": "Jaegul Choo"
                },
                "author": "Jaegul Choo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12319v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12319v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12307v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12307v4",
                "updated": "2025-04-18T13:07:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    7,
                    8,
                    4,
                    108,
                    0
                ],
                "published": "2024-06-18T06:28:06Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    6,
                    28,
                    6,
                    1,
                    170,
                    0
                ],
                "title": "Can Tool-augmented Large Language Models be Aware of Incomplete\n  Conditions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Tool-augmented Large Language Models be Aware of Incomplete\n  Conditions?"
                },
                "summary": "Recent advancements in integrating large language models (LLMs) with tools\nhave allowed the models to interact with real-world environments. However,\nthese tool-augmented LLMs often encounter incomplete scenarios when users\nprovide partial information or the necessary tools are unavailable. Recognizing\nand managing such scenarios is crucial for LLMs to ensure their reliability,\nbut this exploration remains understudied. This study examines whether LLMs can\nidentify incomplete conditions and appropriately determine when to refrain from\nusing tools. To this end, we address a dataset by manipulating instances from\ntwo datasets by removing necessary tools or essential information for tool\ninvocation. Our experiments show that LLMs often struggle to identify the\nabsence of information required to utilize specific tools and recognize the\nabsence of appropriate tools. We further analyze model behaviors in different\nenvironments and compare their performance against humans. Our research can\ncontribute to advancing reliable LLMs by addressing common scenarios during\ninteractions between humans and LLMs. Our code and dataset will be publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in integrating large language models (LLMs) with tools\nhave allowed the models to interact with real-world environments. However,\nthese tool-augmented LLMs often encounter incomplete scenarios when users\nprovide partial information or the necessary tools are unavailable. Recognizing\nand managing such scenarios is crucial for LLMs to ensure their reliability,\nbut this exploration remains understudied. This study examines whether LLMs can\nidentify incomplete conditions and appropriately determine when to refrain from\nusing tools. To this end, we address a dataset by manipulating instances from\ntwo datasets by removing necessary tools or essential information for tool\ninvocation. Our experiments show that LLMs often struggle to identify the\nabsence of information required to utilize specific tools and recognize the\nabsence of appropriate tools. We further analyze model behaviors in different\nenvironments and compare their performance against humans. Our research can\ncontribute to advancing reliable LLMs by addressing common scenarios during\ninteractions between humans and LLMs. Our code and dataset will be publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Seungbin Yang"
                    },
                    {
                        "name": "ChaeHun Park"
                    },
                    {
                        "name": "Taehee Kim"
                    },
                    {
                        "name": "Jaegul Choo"
                    }
                ],
                "author_detail": {
                    "name": "Jaegul Choo"
                },
                "author": "Jaegul Choo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12307v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12307v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13667v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13667v1",
                "updated": "2025-04-18T13:01:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    1,
                    27,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T13:01:27Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    1,
                    27,
                    4,
                    108,
                    0
                ],
                "title": "Large Language Models Will Change The Way Children Think About\n  Technology And Impact Every Interaction Paradigm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Will Change The Way Children Think About\n  Technology And Impact Every Interaction Paradigm"
                },
                "summary": "This paper presents a hopeful perspective on the potentially dramatic impacts\nof Large Language Models on how we children learn and how they will expect to\ninteract with technology. We review the effects of LLMs on education so far,\nand make the case that these effects are minor compared to the upcoming changes\nthat are occurring. We present a small scenario and self-ethnographic study\ndemonstrating the effects of these changes, and define five significant\nconsiderations that interactive systems designers will have to accommodate in\nthe future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a hopeful perspective on the potentially dramatic impacts\nof Large Language Models on how we children learn and how they will expect to\ninteract with technology. We review the effects of LLMs on education so far,\nand make the case that these effects are minor compared to the upcoming changes\nthat are occurring. We present a small scenario and self-ethnographic study\ndemonstrating the effects of these changes, and define five significant\nconsiderations that interactive systems designers will have to accommodate in\nthe future."
                },
                "authors": [
                    {
                        "name": "Russell Beale"
                    }
                ],
                "author_detail": {
                    "name": "Russell Beale"
                },
                "author": "Russell Beale",
                "arxiv_comment": "Accepted for IDC 2025. Citation: Russell Beale. 2025. Large Language\n  Models Will Change The Way Children Think About Technology And Impact Every\n  Interaction Paradigm. In Proceedings of Interaction Design and Children\n  Conference (IDC2025). ACM, New York, NY, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13667v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13667v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13656v1",
                "updated": "2025-04-18T12:37:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    12,
                    37,
                    2,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T12:37:02Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    12,
                    37,
                    2,
                    4,
                    108,
                    0
                ],
                "title": "Do Prompt Patterns Affect Code Quality? A First Empirical Assessment of\n  ChatGPT-Generated Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Prompt Patterns Affect Code Quality? A First Empirical Assessment of\n  ChatGPT-Generated Code"
                },
                "summary": "Large Language Models (LLMs) have rapidly transformed software development,\nespecially in code generation. However, their inconsistent performance, prone\nto hallucinations and quality issues, complicates program comprehension and\nhinders maintainability. Research indicates that prompt engineering-the\npractice of designing inputs to direct LLMs toward generating relevant\noutputs-may help address these challenges. In this regard, researchers have\nintroduced prompt patterns, structured templates intended to guide users in\nformulating their requests. However, the influence of prompt patterns on code\nquality has yet to be thoroughly investigated. An improved understanding of\nthis relationship would be essential to advancing our collective knowledge on\nhow to effectively use LLMs for code generation, thereby enhancing their\nunderstandability in contemporary software development. This paper empirically\ninvestigates the impact of prompt patterns on code quality, specifically\nmaintainability, security, and reliability, using the Dev-GPT dataset. Results\nshow that Zero-Shot prompting is most common, followed by Zero-Shot with\nChain-of-Thought and Few-Shot. Analysis of 7583 code files across quality\nmetrics revealed minimal issues, with Kruskal-Wallis tests indicating no\nsignificant differences among patterns, suggesting that prompt structure may\nnot substantially impact these quality metrics in ChatGPT-assisted code\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have rapidly transformed software development,\nespecially in code generation. However, their inconsistent performance, prone\nto hallucinations and quality issues, complicates program comprehension and\nhinders maintainability. Research indicates that prompt engineering-the\npractice of designing inputs to direct LLMs toward generating relevant\noutputs-may help address these challenges. In this regard, researchers have\nintroduced prompt patterns, structured templates intended to guide users in\nformulating their requests. However, the influence of prompt patterns on code\nquality has yet to be thoroughly investigated. An improved understanding of\nthis relationship would be essential to advancing our collective knowledge on\nhow to effectively use LLMs for code generation, thereby enhancing their\nunderstandability in contemporary software development. This paper empirically\ninvestigates the impact of prompt patterns on code quality, specifically\nmaintainability, security, and reliability, using the Dev-GPT dataset. Results\nshow that Zero-Shot prompting is most common, followed by Zero-Shot with\nChain-of-Thought and Few-Shot. Analysis of 7583 code files across quality\nmetrics revealed minimal issues, with Kruskal-Wallis tests indicating no\nsignificant differences among patterns, suggesting that prompt structure may\nnot substantially impact these quality metrics in ChatGPT-assisted code\ngeneration."
                },
                "authors": [
                    {
                        "name": "Antonio Della Porta"
                    },
                    {
                        "name": "Stefano Lambiase"
                    },
                    {
                        "name": "Fabio Palomba"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Palomba"
                },
                "author": "Fabio Palomba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19874v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19874v3",
                "updated": "2025-04-18T12:31:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    12,
                    31,
                    18,
                    4,
                    108,
                    0
                ],
                "published": "2024-05-30T09:28:56Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    9,
                    28,
                    56,
                    3,
                    151,
                    0
                ],
                "title": "Is In-Context Learning Sufficient for Instruction Following in LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is In-Context Learning Sufficient for Instruction Following in LLMs?"
                },
                "summary": "In-context learning (ICL) allows LLMs to learn from examples without changing\ntheir weights: this is a particularly promising capability for long-context\nLLMs that can potentially learn from many examples. Recently, Lin et al. (2024)\nproposed URIAL, a method using only three in-context examples to align base\nLLMs, achieving non-trivial instruction following performance. In this work, we\nshow that, while effective, ICL alignment with URIAL still underperforms\ncompared to instruction fine-tuning on the established benchmark MT-Bench,\nespecially with more capable base LLMs. We then uncover the most relevant\nelements for successful in-context alignment, finding the crucial role of the\ndecoding parameters. Based on these insights, we show that the approach of\nURIAL can indeed be improved by adding high-quality, potentially carefully\nselected via greedy search, demonstrations in context, getting closer to the\nperformance of instruct models. Finally, we provide the first, to our\nknowledge, systematic comparison of ICL and instruction fine-tuning (IFT) for\ninstruction following in the low data regime, where ICL can be a viable\nalternative to IFT. Overall, our work advances the understanding of ICL as an\nalignment technique and its relationship to IFT. We provide our code at\nhttps://github.com/tml-epfl/icl-alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) allows LLMs to learn from examples without changing\ntheir weights: this is a particularly promising capability for long-context\nLLMs that can potentially learn from many examples. Recently, Lin et al. (2024)\nproposed URIAL, a method using only three in-context examples to align base\nLLMs, achieving non-trivial instruction following performance. In this work, we\nshow that, while effective, ICL alignment with URIAL still underperforms\ncompared to instruction fine-tuning on the established benchmark MT-Bench,\nespecially with more capable base LLMs. We then uncover the most relevant\nelements for successful in-context alignment, finding the crucial role of the\ndecoding parameters. Based on these insights, we show that the approach of\nURIAL can indeed be improved by adding high-quality, potentially carefully\nselected via greedy search, demonstrations in context, getting closer to the\nperformance of instruct models. Finally, we provide the first, to our\nknowledge, systematic comparison of ICL and instruction fine-tuning (IFT) for\ninstruction following in the low data regime, where ICL can be a viable\nalternative to IFT. Overall, our work advances the understanding of ICL as an\nalignment technique and its relationship to IFT. We provide our code at\nhttps://github.com/tml-epfl/icl-alignment."
                },
                "authors": [
                    {
                        "name": "Hao Zhao"
                    },
                    {
                        "name": "Maksym Andriushchenko"
                    },
                    {
                        "name": "Francesco Croce"
                    },
                    {
                        "name": "Nicolas Flammarion"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Flammarion"
                },
                "author": "Nicolas Flammarion",
                "arxiv_comment": "Accepted at ICLR 2025. This camera-ready version v3 adds multi-turn\n  alignment via ICL, revisiting main results on instruct models, and simple\n  mechanistic study. Updates in the v2: experiment with decoding schemes,\n  scaling in-context alignment, ICL vs IFT for instruction following. Code at\n  https://github.com/tml-epfl/icl-alignment",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19874v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19874v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13647v1",
                "updated": "2025-04-18T11:59:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    11,
                    59,
                    34,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T11:59:34Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    11,
                    59,
                    34,
                    4,
                    108,
                    0
                ],
                "title": "Lightweight LiDAR-Camera 3D Dynamic Object Detection and Multi-Class\n  Trajectory Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight LiDAR-Camera 3D Dynamic Object Detection and Multi-Class\n  Trajectory Prediction"
                },
                "summary": "Service mobile robots are often required to avoid dynamic objects while\nperforming their tasks, but they usually have only limited computational\nresources. So we present a lightweight multi-modal framework for 3D object\ndetection and trajectory prediction. Our system synergistically integrates\nLiDAR and camera inputs to achieve real-time perception of pedestrians,\nvehicles, and riders in 3D space. The framework proposes two novel modules: 1)\na Cross-Modal Deformable Transformer (CMDT) for object detection with high\naccuracy and acceptable amount of computation, and 2) a Reference\nTrajectory-based Multi-Class Transformer (RTMCT) for efficient and diverse\ntrajectory prediction of mult-class objects with flexible trajectory lengths.\nEvaluations on the CODa benchmark demonstrate superior performance over\nexisting methods across detection (+2.03% in mAP) and trajectory prediction\n(-0.408m in minADE5 of pedestrians) metrics. Remarkably, the system exhibits\nexceptional deployability - when implemented on a wheelchair robot with an\nentry-level NVIDIA 3060 GPU, it achieves real-time inference at 13.2 fps. To\nfacilitate reproducibility and practical deployment, we release the related\ncode of the method at https://github.com/TossherO/3D_Perception and its ROS\ninference version at https://github.com/TossherO/ros_packages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Service mobile robots are often required to avoid dynamic objects while\nperforming their tasks, but they usually have only limited computational\nresources. So we present a lightweight multi-modal framework for 3D object\ndetection and trajectory prediction. Our system synergistically integrates\nLiDAR and camera inputs to achieve real-time perception of pedestrians,\nvehicles, and riders in 3D space. The framework proposes two novel modules: 1)\na Cross-Modal Deformable Transformer (CMDT) for object detection with high\naccuracy and acceptable amount of computation, and 2) a Reference\nTrajectory-based Multi-Class Transformer (RTMCT) for efficient and diverse\ntrajectory prediction of mult-class objects with flexible trajectory lengths.\nEvaluations on the CODa benchmark demonstrate superior performance over\nexisting methods across detection (+2.03% in mAP) and trajectory prediction\n(-0.408m in minADE5 of pedestrians) metrics. Remarkably, the system exhibits\nexceptional deployability - when implemented on a wheelchair robot with an\nentry-level NVIDIA 3060 GPU, it achieves real-time inference at 13.2 fps. To\nfacilitate reproducibility and practical deployment, we release the related\ncode of the method at https://github.com/TossherO/3D_Perception and its ROS\ninference version at https://github.com/TossherO/ros_packages."
                },
                "authors": [
                    {
                        "name": "Yushen He"
                    },
                    {
                        "name": "Lei Zhao"
                    },
                    {
                        "name": "Tianchen Deng"
                    },
                    {
                        "name": "Zipeng Fang"
                    },
                    {
                        "name": "Weidong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weidong Chen"
                },
                "author": "Weidong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13644v1",
                "updated": "2025-04-18T11:50:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    11,
                    50,
                    30,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T11:50:30Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    11,
                    50,
                    30,
                    4,
                    108,
                    0
                ],
                "title": "Exploring the Potential for Large Language Models to Demonstrate\n  Rational Probabilistic Beliefs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Potential for Large Language Models to Demonstrate\n  Rational Probabilistic Beliefs"
                },
                "summary": "Advances in the general capabilities of large language models (LLMs) have led\nto their use for information retrieval, and as components in automated decision\nsystems. A faithful representation of probabilistic reasoning in these models\nmay be essential to ensure trustworthy, explainable and effective performance\nin these tasks. Despite previous work suggesting that LLMs can perform complex\nreasoning and well-calibrated uncertainty quantification, we find that current\nversions of this class of model lack the ability to provide rational and\ncoherent representations of probabilistic beliefs. To demonstrate this, we\nintroduce a novel dataset of claims with indeterminate truth values and apply a\nnumber of well-established techniques for uncertainty quantification to measure\nthe ability of LLM's to adhere to fundamental properties of probabilistic\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in the general capabilities of large language models (LLMs) have led\nto their use for information retrieval, and as components in automated decision\nsystems. A faithful representation of probabilistic reasoning in these models\nmay be essential to ensure trustworthy, explainable and effective performance\nin these tasks. Despite previous work suggesting that LLMs can perform complex\nreasoning and well-calibrated uncertainty quantification, we find that current\nversions of this class of model lack the ability to provide rational and\ncoherent representations of probabilistic beliefs. To demonstrate this, we\nintroduce a novel dataset of claims with indeterminate truth values and apply a\nnumber of well-established techniques for uncertainty quantification to measure\nthe ability of LLM's to adhere to fundamental properties of probabilistic\nreasoning."
                },
                "authors": [
                    {
                        "name": "Gabriel Freedman"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02079v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02079v3",
                "updated": "2025-04-18T11:20:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    11,
                    20,
                    24,
                    4,
                    108,
                    0
                ],
                "published": "2024-05-03T13:12:28Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    13,
                    12,
                    28,
                    4,
                    124,
                    0
                ],
                "title": "Argumentative Large Language Models for Explainable and Contestable\n  Claim Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Argumentative Large Language Models for Explainable and Contestable\n  Claim Verification"
                },
                "summary": "The profusion of knowledge encoded in large language models (LLMs) and their\nability to apply this knowledge zero-shot in a range of settings makes them\npromising candidates for use in decision-making. However, they are currently\nlimited by their inability to provide outputs which can be faithfully explained\nand effectively contested to correct mistakes. In this paper, we attempt to\nreconcile these strengths and weaknesses by introducing \\emph{argumentative\nLLMs (ArgLLMs)}, a method for augmenting LLMs with argumentative reasoning.\nConcretely, ArgLLMs construct argumentation frameworks, which then serve as the\nbasis for formal reasoning in support of decision-making. The interpretable\nnature of these argumentation frameworks and formal reasoning means that any\ndecision made by ArgLLMs may be explained and contested. We evaluate ArgLLMs'\nperformance experimentally in comparison with state-of-the-art techniques, in\nthe context of the decision-making task of claim verification. We also define\nnovel properties to characterise contestability and assess ArgLLMs formally in\nterms of these properties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The profusion of knowledge encoded in large language models (LLMs) and their\nability to apply this knowledge zero-shot in a range of settings makes them\npromising candidates for use in decision-making. However, they are currently\nlimited by their inability to provide outputs which can be faithfully explained\nand effectively contested to correct mistakes. In this paper, we attempt to\nreconcile these strengths and weaknesses by introducing \\emph{argumentative\nLLMs (ArgLLMs)}, a method for augmenting LLMs with argumentative reasoning.\nConcretely, ArgLLMs construct argumentation frameworks, which then serve as the\nbasis for formal reasoning in support of decision-making. The interpretable\nnature of these argumentation frameworks and formal reasoning means that any\ndecision made by ArgLLMs may be explained and contested. We evaluate ArgLLMs'\nperformance experimentally in comparison with state-of-the-art techniques, in\nthe context of the decision-making task of claim verification. We also define\nnovel properties to characterise contestability and assess ArgLLMs formally in\nterms of these properties."
                },
                "authors": [
                    {
                        "name": "Gabriel Freedman"
                    },
                    {
                        "name": "Adam Dejl"
                    },
                    {
                        "name": "Deniz Gorur"
                    },
                    {
                        "name": "Xiang Yin"
                    },
                    {
                        "name": "Antonio Rago"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "arxiv_comment": "18 pages, 18 figures. Accepted as an oral presentation at AAAI 2025",
                "arxiv_journal_ref": "Proceedings of the AAAI Conference on Artificial Intelligence,\n  39(14), 14930-14939. 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02079v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02079v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13630v1",
                "updated": "2025-04-18T11:11:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    11,
                    11,
                    14,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T11:11:14Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    11,
                    11,
                    14,
                    4,
                    108,
                    0
                ],
                "title": "Remedy: Learning Machine Translation Evaluation from Human Preferences\n  with Reward Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remedy: Learning Machine Translation Evaluation from Human Preferences\n  with Reward Modeling"
                },
                "summary": "A key challenge in MT evaluation is the inherent noise and inconsistency of\nhuman ratings. Regression-based neural metrics struggle with this noise, while\nprompting LLMs shows promise at system-level evaluation but performs poorly at\nsegment level. In this work, we propose ReMedy, a novel MT metric framework\nthat reformulates translation evaluation as a reward modeling task. Instead of\nregressing on imperfect human ratings directly, ReMedy learns relative\ntranslation quality using pairwise preference data, resulting in a more\nreliable evaluation. In extensive experiments across WMT22-24 shared tasks (39\nlanguage pairs, 111 MT systems), ReMedy achieves state-of-the-art performance\nat both segment- and system-level evaluation. Specifically, ReMedy-9B surpasses\nlarger WMT winners and massive closed LLMs such as MetricX-13B,\nXCOMET-Ensemble, GEMBA-GPT-4, PaLM-540B, and finetuned PaLM2. Further analyses\ndemonstrate that ReMedy delivers superior capability in detecting translation\nerrors and evaluating low-quality translations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key challenge in MT evaluation is the inherent noise and inconsistency of\nhuman ratings. Regression-based neural metrics struggle with this noise, while\nprompting LLMs shows promise at system-level evaluation but performs poorly at\nsegment level. In this work, we propose ReMedy, a novel MT metric framework\nthat reformulates translation evaluation as a reward modeling task. Instead of\nregressing on imperfect human ratings directly, ReMedy learns relative\ntranslation quality using pairwise preference data, resulting in a more\nreliable evaluation. In extensive experiments across WMT22-24 shared tasks (39\nlanguage pairs, 111 MT systems), ReMedy achieves state-of-the-art performance\nat both segment- and system-level evaluation. Specifically, ReMedy-9B surpasses\nlarger WMT winners and massive closed LLMs such as MetricX-13B,\nXCOMET-Ensemble, GEMBA-GPT-4, PaLM-540B, and finetuned PaLM2. Further analyses\ndemonstrate that ReMedy delivers superior capability in detecting translation\nerrors and evaluating low-quality translations."
                },
                "authors": [
                    {
                        "name": "Shaomu Tan"
                    },
                    {
                        "name": "Christof Monz"
                    }
                ],
                "author_detail": {
                    "name": "Christof Monz"
                },
                "author": "Christof Monz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13629v1",
                "updated": "2025-04-18T11:09:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    11,
                    9,
                    16,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T11:09:16Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    11,
                    9,
                    16,
                    4,
                    108,
                    0
                ],
                "title": "Divergent LLM Adoption and Heterogeneous Convergence Paths in Research\n  Writing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Divergent LLM Adoption and Heterogeneous Convergence Paths in Research\n  Writing"
                },
                "summary": "Large Language Models (LLMs), such as ChatGPT, are reshaping content creation\nand academic writing. This study investigates the impact of AI-assisted\ngenerative revisions on research manuscripts, focusing on heterogeneous\nadoption patterns and their influence on writing convergence. Leveraging a\ndataset of over 627,000 academic papers from arXiv, we develop a novel\nclassification framework by fine-tuning prompt- and discipline-specific large\nlanguage models to detect the style of ChatGPT-revised texts. Our findings\nreveal substantial disparities in LLM adoption across academic disciplines,\ngender, native language status, and career stage, alongside a rapid evolution\nin scholarly writing styles. Moreover, LLM usage enhances clarity, conciseness,\nand adherence to formal writing conventions, with improvements varying by\nrevision type. Finally, a difference-in-differences analysis shows that while\nLLMs drive convergence in academic writing, early adopters, male researchers,\nnon-native speakers, and junior scholars exhibit the most pronounced stylistic\nshifts, aligning their writing more closely with that of established\nresearchers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as ChatGPT, are reshaping content creation\nand academic writing. This study investigates the impact of AI-assisted\ngenerative revisions on research manuscripts, focusing on heterogeneous\nadoption patterns and their influence on writing convergence. Leveraging a\ndataset of over 627,000 academic papers from arXiv, we develop a novel\nclassification framework by fine-tuning prompt- and discipline-specific large\nlanguage models to detect the style of ChatGPT-revised texts. Our findings\nreveal substantial disparities in LLM adoption across academic disciplines,\ngender, native language status, and career stage, alongside a rapid evolution\nin scholarly writing styles. Moreover, LLM usage enhances clarity, conciseness,\nand adherence to formal writing conventions, with improvements varying by\nrevision type. Finally, a difference-in-differences analysis shows that while\nLLMs drive convergence in academic writing, early adopters, male researchers,\nnon-native speakers, and junior scholars exhibit the most pronounced stylistic\nshifts, aligning their writing more closely with that of established\nresearchers."
                },
                "authors": [
                    {
                        "name": "Cong William Lin"
                    },
                    {
                        "name": "Wu Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Wu Zhu"
                },
                "author": "Wu Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13621v1",
                "updated": "2025-04-18T10:54:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    54,
                    52,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T10:54:52Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    54,
                    52,
                    4,
                    108,
                    0
                ],
                "title": "Visual Intention Grounding for Egocentric Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Intention Grounding for Egocentric Assistants"
                },
                "summary": "Visual grounding associates textual descriptions with objects in an image.\nConventional methods target third-person image inputs and named object queries.\nIn applications such as AI assistants, the perspective shifts -- inputs are\negocentric, and objects may be referred to implicitly through needs and\nintentions. To bridge this gap, we introduce EgoIntention, the first dataset\nfor egocentric visual intention grounding. EgoIntention challenges multimodal\nLLMs to 1) understand and ignore unintended contextual objects and 2) reason\nabout uncommon object functionalities. Benchmark results show that current\nmodels misidentify context objects and lack affordance understanding in\negocentric views. We also propose Reason-to-Ground (RoG) instruction tuning; it\nenables hybrid training with normal descriptions and egocentric intentions with\na chained intention reasoning and object grounding mechanism. RoG significantly\noutperforms naive finetuning and hybrid training on EgoIntention, while\nmaintaining or slightly improving naive description grounding. This advancement\nenables unified visual grounding for egocentric and exocentric visual inputs\nwhile handling explicit object queries and implicit human intentions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual grounding associates textual descriptions with objects in an image.\nConventional methods target third-person image inputs and named object queries.\nIn applications such as AI assistants, the perspective shifts -- inputs are\negocentric, and objects may be referred to implicitly through needs and\nintentions. To bridge this gap, we introduce EgoIntention, the first dataset\nfor egocentric visual intention grounding. EgoIntention challenges multimodal\nLLMs to 1) understand and ignore unintended contextual objects and 2) reason\nabout uncommon object functionalities. Benchmark results show that current\nmodels misidentify context objects and lack affordance understanding in\negocentric views. We also propose Reason-to-Ground (RoG) instruction tuning; it\nenables hybrid training with normal descriptions and egocentric intentions with\na chained intention reasoning and object grounding mechanism. RoG significantly\noutperforms naive finetuning and hybrid training on EgoIntention, while\nmaintaining or slightly improving naive description grounding. This advancement\nenables unified visual grounding for egocentric and exocentric visual inputs\nwhile handling explicit object queries and implicit human intentions."
                },
                "authors": [
                    {
                        "name": "Pengzhan Sun"
                    },
                    {
                        "name": "Junbin Xiao"
                    },
                    {
                        "name": "Tze Ho Elden Tse"
                    },
                    {
                        "name": "Yicong Li"
                    },
                    {
                        "name": "Arjun Akula"
                    },
                    {
                        "name": "Angela Yao"
                    }
                ],
                "author_detail": {
                    "name": "Angela Yao"
                },
                "author": "Angela Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13619v1",
                "updated": "2025-04-18T10:49:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    49,
                    7,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T10:49:07Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    49,
                    7,
                    4,
                    108,
                    0
                ],
                "title": "Robust Humanoid Walking on Compliant and Uneven Terrain with Deep\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Humanoid Walking on Compliant and Uneven Terrain with Deep\n  Reinforcement Learning"
                },
                "summary": "For the deployment of legged robots in real-world environments, it is\nessential to develop robust locomotion control methods for challenging terrains\nthat may exhibit unexpected deformability and irregularity. In this paper, we\nexplore the application of sim-to-real deep reinforcement learning (RL) for the\ndesign of bipedal locomotion controllers for humanoid robots on compliant and\nuneven terrains. Our key contribution is to show that a simple training\ncurriculum for exposing the RL agent to randomized terrains in simulation can\nachieve robust walking on a real humanoid robot using only proprioceptive\nfeedback. We train an end-to-end bipedal locomotion policy using the proposed\napproach, and show extensive real-robot demonstration on the HRP-5P humanoid\nover several difficult terrains inside and outside the lab environment.\nFurther, we argue that the robustness of a bipedal walking policy can be\nimproved if the robot is allowed to exhibit aperiodic motion with variable\nstepping frequency. We propose a new control policy to enable modification of\nthe observed clock signal, leading to adaptive gait frequencies depending on\nthe terrain and command velocity. Through simulation experiments, we show the\neffectiveness of this policy specifically for walking over challenging terrains\nby controlling swing and stance durations. The code for training and evaluation\nis available online at https://github.com/rohanpsingh/LearningHumanoidWalking.\nDemo video is available at https://www.youtube.com/watch?v=ZgfNzGAkk2Q.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For the deployment of legged robots in real-world environments, it is\nessential to develop robust locomotion control methods for challenging terrains\nthat may exhibit unexpected deformability and irregularity. In this paper, we\nexplore the application of sim-to-real deep reinforcement learning (RL) for the\ndesign of bipedal locomotion controllers for humanoid robots on compliant and\nuneven terrains. Our key contribution is to show that a simple training\ncurriculum for exposing the RL agent to randomized terrains in simulation can\nachieve robust walking on a real humanoid robot using only proprioceptive\nfeedback. We train an end-to-end bipedal locomotion policy using the proposed\napproach, and show extensive real-robot demonstration on the HRP-5P humanoid\nover several difficult terrains inside and outside the lab environment.\nFurther, we argue that the robustness of a bipedal walking policy can be\nimproved if the robot is allowed to exhibit aperiodic motion with variable\nstepping frequency. We propose a new control policy to enable modification of\nthe observed clock signal, leading to adaptive gait frequencies depending on\nthe terrain and command velocity. Through simulation experiments, we show the\neffectiveness of this policy specifically for walking over challenging terrains\nby controlling swing and stance durations. The code for training and evaluation\nis available online at https://github.com/rohanpsingh/LearningHumanoidWalking.\nDemo video is available at https://www.youtube.com/watch?v=ZgfNzGAkk2Q."
                },
                "authors": [
                    {
                        "name": "Rohan P. Singh"
                    },
                    {
                        "name": "Mitsuharu Morisawa"
                    },
                    {
                        "name": "Mehdi Benallegue"
                    },
                    {
                        "name": "Zhaoming Xie"
                    },
                    {
                        "name": "Fumio Kanehiro"
                    }
                ],
                "author_detail": {
                    "name": "Fumio Kanehiro"
                },
                "author": "Fumio Kanehiro",
                "arxiv_doi": "10.1109/Humanoids58906.2024.10769793",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/Humanoids58906.2024.10769793",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.13619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "2024 IEEE-RAS 23rd International Conference on Humanoid Robots\n  (Humanoids)",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13617v1",
                "updated": "2025-04-18T10:46:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    46,
                    22,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T10:46:22Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    46,
                    22,
                    4,
                    108,
                    0
                ],
                "title": "Compile Scene Graphs with Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compile Scene Graphs with Reinforcement Learning"
                },
                "summary": "Next token prediction is the fundamental principle for training large\nlanguage models (LLMs), and reinforcement learning (RL) further enhances their\nreasoning performance. As an effective way to model language, image, video, and\nother modalities, the use of LLMs for end-to-end extraction of structured\nvisual representations, such as scene graphs, remains underexplored. It\nrequires the model to accurately produce a set of objects and relationship\ntriplets, rather than generating text token by token. To achieve this, we\nintroduce R1-SGG, a multimodal LLM (M-LLM) initially trained via supervised\nfine-tuning (SFT) on the scene graph dataset and subsequently refined using\nreinforcement learning to enhance its ability to generate scene graphs in an\nend-to-end manner. The SFT follows a conventional prompt-response paradigm,\nwhile RL requires the design of effective reward signals. Given the structured\nnature of scene graphs, we design a graph-centric reward function that\nintegrates node-level rewards, edge-level rewards, and a format consistency\nreward. Our experiments demonstrate that rule-based RL substantially enhances\nmodel performance in the SGG task, achieving a zero failure rate--unlike\nsupervised fine-tuning (SFT), which struggles to generalize effectively. Our\ncode is available at https://github.com/gpt4vision/R1-SGG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next token prediction is the fundamental principle for training large\nlanguage models (LLMs), and reinforcement learning (RL) further enhances their\nreasoning performance. As an effective way to model language, image, video, and\nother modalities, the use of LLMs for end-to-end extraction of structured\nvisual representations, such as scene graphs, remains underexplored. It\nrequires the model to accurately produce a set of objects and relationship\ntriplets, rather than generating text token by token. To achieve this, we\nintroduce R1-SGG, a multimodal LLM (M-LLM) initially trained via supervised\nfine-tuning (SFT) on the scene graph dataset and subsequently refined using\nreinforcement learning to enhance its ability to generate scene graphs in an\nend-to-end manner. The SFT follows a conventional prompt-response paradigm,\nwhile RL requires the design of effective reward signals. Given the structured\nnature of scene graphs, we design a graph-centric reward function that\nintegrates node-level rewards, edge-level rewards, and a format consistency\nreward. Our experiments demonstrate that rule-based RL substantially enhances\nmodel performance in the SGG task, achieving a zero failure rate--unlike\nsupervised fine-tuning (SFT), which struggles to generalize effectively. Our\ncode is available at https://github.com/gpt4vision/R1-SGG."
                },
                "authors": [
                    {
                        "name": "Zuyao Chen"
                    },
                    {
                        "name": "Jinlin Wu"
                    },
                    {
                        "name": "Zhen Lei"
                    },
                    {
                        "name": "Marc Pollefeys"
                    },
                    {
                        "name": "Chang Wen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chang Wen Chen"
                },
                "author": "Chang Wen Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13615v1",
                "updated": "2025-04-18T10:43:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    43,
                    21,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T10:43:21Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    43,
                    21,
                    4,
                    108,
                    0
                ],
                "title": "Long-context Non-factoid Question Answering in Indic Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Non-factoid Question Answering in Indic Languages"
                },
                "summary": "Question Answering (QA) tasks, which involve extracting answers from a given\ncontext, are relatively straightforward for modern Large Language Models (LLMs)\nwhen the context is short. However, long contexts pose challenges due to the\nquadratic complexity of the self-attention mechanism. This challenge is\ncompounded in Indic languages, which are often low-resource. This study\nexplores context-shortening techniques, including Open Information Extraction\n(OIE), coreference resolution, Answer Paragraph Selection (APS), and their\ncombinations, to improve QA performance. Compared to the baseline of\nunshortened (long) contexts, our experiments on four Indic languages (Hindi,\nTamil, Telugu, and Urdu) demonstrate that context-shortening techniques yield\nan average improvement of 4\\% in semantic scores and 47\\% in token-level scores\nwhen evaluated on three popular LLMs without fine-tuning. Furthermore, with\nfine-tuning, we achieve an average increase of 2\\% in both semantic and\ntoken-level scores. Additionally, context-shortening reduces computational\noverhead. Explainability techniques like LIME and SHAP reveal that when the APS\nmodel confidently identifies the paragraph containing the answer, nearly all\ntokens within the selected text receive high relevance scores. However, the\nstudy also highlights the limitations of LLM-based QA systems in addressing\nnon-factoid questions, particularly those requiring reasoning or debate.\nMoreover, verbalizing OIE-generated triples does not enhance system\nperformance. These findings emphasize the potential of context-shortening\ntechniques to improve the efficiency and effectiveness of LLM-based QA systems,\nespecially for low-resource languages. The source code and resources are\navailable at https://github.com/ritwikmishra/IndicGenQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question Answering (QA) tasks, which involve extracting answers from a given\ncontext, are relatively straightforward for modern Large Language Models (LLMs)\nwhen the context is short. However, long contexts pose challenges due to the\nquadratic complexity of the self-attention mechanism. This challenge is\ncompounded in Indic languages, which are often low-resource. This study\nexplores context-shortening techniques, including Open Information Extraction\n(OIE), coreference resolution, Answer Paragraph Selection (APS), and their\ncombinations, to improve QA performance. Compared to the baseline of\nunshortened (long) contexts, our experiments on four Indic languages (Hindi,\nTamil, Telugu, and Urdu) demonstrate that context-shortening techniques yield\nan average improvement of 4\\% in semantic scores and 47\\% in token-level scores\nwhen evaluated on three popular LLMs without fine-tuning. Furthermore, with\nfine-tuning, we achieve an average increase of 2\\% in both semantic and\ntoken-level scores. Additionally, context-shortening reduces computational\noverhead. Explainability techniques like LIME and SHAP reveal that when the APS\nmodel confidently identifies the paragraph containing the answer, nearly all\ntokens within the selected text receive high relevance scores. However, the\nstudy also highlights the limitations of LLM-based QA systems in addressing\nnon-factoid questions, particularly those requiring reasoning or debate.\nMoreover, verbalizing OIE-generated triples does not enhance system\nperformance. These findings emphasize the potential of context-shortening\ntechniques to improve the efficiency and effectiveness of LLM-based QA systems,\nespecially for low-resource languages. The source code and resources are\navailable at https://github.com/ritwikmishra/IndicGenQA."
                },
                "authors": [
                    {
                        "name": "Ritwik Mishra"
                    },
                    {
                        "name": "Rajiv Ratn Shah"
                    },
                    {
                        "name": "Ponnurangam Kumaraguru"
                    }
                ],
                "author_detail": {
                    "name": "Ponnurangam Kumaraguru"
                },
                "author": "Ponnurangam Kumaraguru",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11094v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11094v2",
                "updated": "2025-04-18T10:39:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    39,
                    23,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-15T11:40:12Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    40,
                    12,
                    1,
                    105,
                    0
                ],
                "title": "Evaluation Report on MCP Servers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation Report on MCP Servers"
                },
                "summary": "With the rise of LLMs, a large number of Model Context Protocol (MCP)\nservices have emerged since the end of 2024. However, the effectiveness and\nefficiency of MCP servers have not been well studied. To study these questions,\nwe propose an evaluation framework, called MCPBench. We selected several widely\nused MCP server and conducted an experimental evaluation on their accuracy,\ntime, and token usage. Our experiments showed that the most effective MCP, Bing\nWeb Search, achieved an accuracy of 64%. Importantly, we found that the\naccuracy of MCP servers can be substantially enhanced by involving declarative\ninterface. This research paves the way for further investigations into\noptimized MCP implementations, ultimately leading to better AI-driven\napplications and data retrieval solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of LLMs, a large number of Model Context Protocol (MCP)\nservices have emerged since the end of 2024. However, the effectiveness and\nefficiency of MCP servers have not been well studied. To study these questions,\nwe propose an evaluation framework, called MCPBench. We selected several widely\nused MCP server and conducted an experimental evaluation on their accuracy,\ntime, and token usage. Our experiments showed that the most effective MCP, Bing\nWeb Search, achieved an accuracy of 64%. Importantly, we found that the\naccuracy of MCP servers can be substantially enhanced by involving declarative\ninterface. This research paves the way for further investigations into\noptimized MCP implementations, ultimately leading to better AI-driven\napplications and data retrieval solutions."
                },
                "authors": [
                    {
                        "name": "Zhiling Luo"
                    },
                    {
                        "name": "Xiaorong Shi"
                    },
                    {
                        "name": "Xuanrui Lin"
                    },
                    {
                        "name": "Jinyang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jinyang Gao"
                },
                "author": "Jinyang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11094v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11094v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17176v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17176v2",
                "updated": "2025-04-18T10:35:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    35,
                    50,
                    4,
                    108,
                    0
                ],
                "published": "2025-01-24T08:15:05Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    15,
                    5,
                    4,
                    24,
                    0
                ],
                "title": "Prompt-Based Cost-Effective Evaluation and Operation of ChatGPT as a\n  Computer Programming Teaching Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-Based Cost-Effective Evaluation and Operation of ChatGPT as a\n  Computer Programming Teaching Assistant"
                },
                "summary": "The dream of achieving a student-teacher ratio of 1:1 is closer than ever\nthanks to the emergence of large language models (LLMs). One potential\napplication of these models in the educational field would be to provide\nfeedback to students in university introductory programming courses, so that a\nstudent struggling to solve a basic implementation problem could seek help from\nan LLM available 24/7. This article focuses on studying three aspects related\nto such an application. First, the performance of two well-known models,\nGPT-3.5T and GPT-4T, in providing feedback to students is evaluated. The\nempirical results showed that GPT-4T performs much better than GPT-3.5T,\nhowever, it is not yet ready for use in a real-world scenario. This is due to\nthe possibility of generating incorrect information that potential users may\nnot always be able to detect. Second, the article proposes a carefully designed\nprompt using in-context learning techniques that allows automating important\nparts of the evaluation process, as well as providing a lower bound for the\nfraction of feedbacks containing incorrect information, saving time and effort.\nThis was possible because the resulting feedback has a programmatically\nanalyzable structure that incorporates diagnostic information about the LLM's\nperformance in solving the requested task. Third, the article also suggests a\npossible strategy for implementing a practical learning tool based on LLMs,\nwhich is rooted on the proposed prompting techniques. This strategy opens up a\nwhole range of interesting possibilities from a pedagogical perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dream of achieving a student-teacher ratio of 1:1 is closer than ever\nthanks to the emergence of large language models (LLMs). One potential\napplication of these models in the educational field would be to provide\nfeedback to students in university introductory programming courses, so that a\nstudent struggling to solve a basic implementation problem could seek help from\nan LLM available 24/7. This article focuses on studying three aspects related\nto such an application. First, the performance of two well-known models,\nGPT-3.5T and GPT-4T, in providing feedback to students is evaluated. The\nempirical results showed that GPT-4T performs much better than GPT-3.5T,\nhowever, it is not yet ready for use in a real-world scenario. This is due to\nthe possibility of generating incorrect information that potential users may\nnot always be able to detect. Second, the article proposes a carefully designed\nprompt using in-context learning techniques that allows automating important\nparts of the evaluation process, as well as providing a lower bound for the\nfraction of feedbacks containing incorrect information, saving time and effort.\nThis was possible because the resulting feedback has a programmatically\nanalyzable structure that incorporates diagnostic information about the LLM's\nperformance in solving the requested task. Third, the article also suggests a\npossible strategy for implementing a practical learning tool based on LLMs,\nwhich is rooted on the proposed prompting techniques. This strategy opens up a\nwhole range of interesting possibilities from a pedagogical perspective."
                },
                "authors": [
                    {
                        "name": "Marc Ballestero-Ribó"
                    },
                    {
                        "name": "Daniel Ortiz-Martínez"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Ortiz-Martínez"
                },
                "author": "Daniel Ortiz-Martínez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17176v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17176v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18337v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18337v2",
                "updated": "2025-04-18T10:26:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    26,
                    12,
                    4,
                    108,
                    0
                ],
                "published": "2024-11-27T13:35:32Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    13,
                    35,
                    32,
                    2,
                    332,
                    0
                ],
                "title": "Can LLMs assist with Ambiguity? A Quantitative Evaluation of various\n  Large Language Models on Word Sense Disambiguation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs assist with Ambiguity? A Quantitative Evaluation of various\n  Large Language Models on Word Sense Disambiguation"
                },
                "summary": "Ambiguous words are often found in modern digital communications. Lexical\nambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due\nto limited data. Consequently, the efficiency of translation, information\nretrieval, and question-answering systems is hindered by these limitations.\nThis study investigates the use of Large Language Models (LLMs) to improve WSD\nusing a novel approach combining a systematic prompt augmentation mechanism\nwith a knowledge base (KB) consisting of different sense interpretations. The\nproposed method incorporates a human-in-loop approach for prompt augmentation\nwhere prompt is supported by Part-of-Speech (POS) tagging, synonyms of\nambiguous words, aspect-based sense filtering and few-shot prompting to guide\nthe LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based\napproach, this work demonstrates a substantial improvement in performance. The\nevaluation was conducted using FEWS test data and sense tags. This research\nadvances accurate word interpretation in social media and digital\ncommunication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ambiguous words are often found in modern digital communications. Lexical\nambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due\nto limited data. Consequently, the efficiency of translation, information\nretrieval, and question-answering systems is hindered by these limitations.\nThis study investigates the use of Large Language Models (LLMs) to improve WSD\nusing a novel approach combining a systematic prompt augmentation mechanism\nwith a knowledge base (KB) consisting of different sense interpretations. The\nproposed method incorporates a human-in-loop approach for prompt augmentation\nwhere prompt is supported by Part-of-Speech (POS) tagging, synonyms of\nambiguous words, aspect-based sense filtering and few-shot prompting to guide\nthe LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based\napproach, this work demonstrates a substantial improvement in performance. The\nevaluation was conducted using FEWS test data and sense tags. This research\nadvances accurate word interpretation in social media and digital\ncommunication."
                },
                "authors": [
                    {
                        "name": "T. G. D. K. Sumanathilaka"
                    },
                    {
                        "name": "Nicholas Micallef"
                    },
                    {
                        "name": "Julian Hough"
                    }
                ],
                "author_detail": {
                    "name": "Julian Hough"
                },
                "author": "Julian Hough",
                "arxiv_comment": "12 pages,6 tables, 1 figure, Proceedings of the 1st International\n  Conference on NLP & AI for Cyber Security",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18337v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18337v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13604v1",
                "updated": "2025-04-18T10:18:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    18,
                    7,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T10:18:07Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    18,
                    7,
                    4,
                    108,
                    0
                ],
                "title": "FocusTrack: A Self-Adaptive Local Sampling Algorithm for Efficient\n  Anti-UAV Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FocusTrack: A Self-Adaptive Local Sampling Algorithm for Efficient\n  Anti-UAV Tracking"
                },
                "summary": "Anti-UAV tracking poses significant challenges, including small target sizes,\nabrupt camera motion, and cluttered infrared backgrounds. Existing tracking\nparadigms can be broadly categorized into global- and local-based methods.\nGlobal-based trackers, such as SiamDT, achieve high accuracy by scanning the\nentire field of view but suffer from excessive computational overhead, limiting\nreal-world deployment. In contrast, local-based methods, including OSTrack and\nROMTrack, efficiently restrict the search region but struggle when targets\nundergo significant displacements due to abrupt camera motion. Through\npreliminary experiments, it is evident that a local tracker, when paired with\nadaptive search region adjustment, can significantly enhance tracking accuracy,\nnarrowing the gap between local and global trackers. To address this challenge,\nwe propose FocusTrack, a novel framework that dynamically refines the search\nregion and strengthens feature representations, achieving an optimal balance\nbetween computational efficiency and tracking accuracy. Specifically, our\nSearch Region Adjustment (SRA) strategy estimates the target presence\nprobability and adaptively adjusts the field of view, ensuring the target\nremains within focus. Furthermore, to counteract feature degradation caused by\nvarying search regions, the Attention-to-Mask (ATM) module is proposed. This\nmodule integrates hierarchical information, enriching the target\nrepresentations with fine-grained details. Experimental results demonstrate\nthat FocusTrack achieves state-of-the-art performance, obtaining 67.7% AUC on\nAntiUAV and 62.8% AUC on AntiUAV410, outperforming the baseline tracker by 8.5%\nand 9.1% AUC, respectively. In terms of efficiency, FocusTrack surpasses\nglobal-based trackers, requiring only 30G MACs and achieving 143 fps with\nFocusTrack (SRA) and 44 fps with the full version, both enabling real-time\ntracking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anti-UAV tracking poses significant challenges, including small target sizes,\nabrupt camera motion, and cluttered infrared backgrounds. Existing tracking\nparadigms can be broadly categorized into global- and local-based methods.\nGlobal-based trackers, such as SiamDT, achieve high accuracy by scanning the\nentire field of view but suffer from excessive computational overhead, limiting\nreal-world deployment. In contrast, local-based methods, including OSTrack and\nROMTrack, efficiently restrict the search region but struggle when targets\nundergo significant displacements due to abrupt camera motion. Through\npreliminary experiments, it is evident that a local tracker, when paired with\nadaptive search region adjustment, can significantly enhance tracking accuracy,\nnarrowing the gap between local and global trackers. To address this challenge,\nwe propose FocusTrack, a novel framework that dynamically refines the search\nregion and strengthens feature representations, achieving an optimal balance\nbetween computational efficiency and tracking accuracy. Specifically, our\nSearch Region Adjustment (SRA) strategy estimates the target presence\nprobability and adaptively adjusts the field of view, ensuring the target\nremains within focus. Furthermore, to counteract feature degradation caused by\nvarying search regions, the Attention-to-Mask (ATM) module is proposed. This\nmodule integrates hierarchical information, enriching the target\nrepresentations with fine-grained details. Experimental results demonstrate\nthat FocusTrack achieves state-of-the-art performance, obtaining 67.7% AUC on\nAntiUAV and 62.8% AUC on AntiUAV410, outperforming the baseline tracker by 8.5%\nand 9.1% AUC, respectively. In terms of efficiency, FocusTrack surpasses\nglobal-based trackers, requiring only 30G MACs and achieving 143 fps with\nFocusTrack (SRA) and 44 fps with the full version, both enabling real-time\ntracking."
                },
                "authors": [
                    {
                        "name": "Ying Wang"
                    },
                    {
                        "name": "Tingfa Xu"
                    },
                    {
                        "name": "Jianan Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianan Li"
                },
                "author": "Jianan Li",
                "arxiv_comment": "13pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13603v1",
                "updated": "2025-04-18T10:14:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    14,
                    51,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T10:14:51Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    14,
                    51,
                    4,
                    108,
                    0
                ],
                "title": "Continual Pre-Training is (not) What You Need in Domain Adaption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Pre-Training is (not) What You Need in Domain Adaption"
                },
                "summary": "The recent advances in Legal Large Language Models (LLMs) have transformed\nthe landscape of legal research and practice by automating tasks, enhancing\nresearch precision, and supporting complex decision-making processes. However,\neffectively adapting LLMs to the legal domain remains challenging due to the\ncomplexity of legal reasoning, the need for precise interpretation of\nspecialized language, and the potential for hallucinations. This paper examines\nthe efficacy of Domain-Adaptive Continual Pre-Training (DACP) in improving the\nlegal reasoning capabilities of LLMs. Through a series of experiments on legal\nreasoning tasks within the Taiwanese legal framework, we demonstrate that while\nDACP enhances domain-specific knowledge, it does not uniformly improve\nperformance across all legal tasks. We discuss the trade-offs involved in DACP,\nparticularly its impact on model generalization and performance in prompt-based\ntasks, and propose directions for future research to optimize domain adaptation\nstrategies in legal AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advances in Legal Large Language Models (LLMs) have transformed\nthe landscape of legal research and practice by automating tasks, enhancing\nresearch precision, and supporting complex decision-making processes. However,\neffectively adapting LLMs to the legal domain remains challenging due to the\ncomplexity of legal reasoning, the need for precise interpretation of\nspecialized language, and the potential for hallucinations. This paper examines\nthe efficacy of Domain-Adaptive Continual Pre-Training (DACP) in improving the\nlegal reasoning capabilities of LLMs. Through a series of experiments on legal\nreasoning tasks within the Taiwanese legal framework, we demonstrate that while\nDACP enhances domain-specific knowledge, it does not uniformly improve\nperformance across all legal tasks. We discuss the trade-offs involved in DACP,\nparticularly its impact on model generalization and performance in prompt-based\ntasks, and propose directions for future research to optimize domain adaptation\nstrategies in legal AI."
                },
                "authors": [
                    {
                        "name": "Pin-Er Chen"
                    },
                    {
                        "name": "Da-Chen Lian"
                    },
                    {
                        "name": "Shu-Kai Hsieh"
                    },
                    {
                        "name": "Sieh-Chuen Huang"
                    },
                    {
                        "name": "Hsuan-Lei Shao"
                    },
                    {
                        "name": "Jun-Wei Chiu"
                    },
                    {
                        "name": "Yang-Hsien Lin"
                    },
                    {
                        "name": "Zih-Ching Chen"
                    },
                    {
                        "name": "Cheng-Kuang"
                    },
                    {
                        "name": "Eddie TC Huang"
                    },
                    {
                        "name": "Simon See"
                    }
                ],
                "author_detail": {
                    "name": "Simon See"
                },
                "author": "Simon See",
                "arxiv_comment": "11 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13592v1",
                "updated": "2025-04-18T09:52:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    52,
                    12,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T09:52:12Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    52,
                    12,
                    4,
                    108,
                    0
                ],
                "title": "Improving Generalization in Intent Detection: GRPO with Reward-Based\n  Curriculum Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Generalization in Intent Detection: GRPO with Reward-Based\n  Curriculum Sampling"
                },
                "summary": "Intent detection, a critical component in task-oriented dialogue (TOD)\nsystems, faces significant challenges in adapting to the rapid influx of\nintegrable tools with complex interrelationships. Existing approaches, such as\nzero-shot reformulations and LLM-based dynamic recognition, struggle with\nperformance degradation when encountering unseen intents, leading to erroneous\ntask routing. To enhance the model's generalization performance on unseen\ntasks, we employ Reinforcement Learning (RL) combined with a Reward-based\nCurriculum Sampling (RCS) during Group Relative Policy Optimization (GRPO)\ntraining in intent detection tasks. Experiments demonstrate that RL-trained\nmodels substantially outperform supervised fine-tuning (SFT) baselines in\ngeneralization. Besides, the introduction of the RCS, significantly bolsters\nthe effectiveness of RL in intent detection by focusing the model on\nchallenging cases during training. Moreover, incorporating Chain-of-Thought\n(COT) processes in RL notably improves generalization in complex intent\ndetection tasks, underscoring the importance of thought in challenging\nscenarios. This work advances the generalization of intent detection tasks,\noffering practical insights for deploying adaptable dialogue systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent detection, a critical component in task-oriented dialogue (TOD)\nsystems, faces significant challenges in adapting to the rapid influx of\nintegrable tools with complex interrelationships. Existing approaches, such as\nzero-shot reformulations and LLM-based dynamic recognition, struggle with\nperformance degradation when encountering unseen intents, leading to erroneous\ntask routing. To enhance the model's generalization performance on unseen\ntasks, we employ Reinforcement Learning (RL) combined with a Reward-based\nCurriculum Sampling (RCS) during Group Relative Policy Optimization (GRPO)\ntraining in intent detection tasks. Experiments demonstrate that RL-trained\nmodels substantially outperform supervised fine-tuning (SFT) baselines in\ngeneralization. Besides, the introduction of the RCS, significantly bolsters\nthe effectiveness of RL in intent detection by focusing the model on\nchallenging cases during training. Moreover, incorporating Chain-of-Thought\n(COT) processes in RL notably improves generalization in complex intent\ndetection tasks, underscoring the importance of thought in challenging\nscenarios. This work advances the generalization of intent detection tasks,\noffering practical insights for deploying adaptable dialogue systems."
                },
                "authors": [
                    {
                        "name": "Zihao Feng"
                    },
                    {
                        "name": "Xiaoxue Wang"
                    },
                    {
                        "name": "Ziwei Bai"
                    },
                    {
                        "name": "Donghang Su"
                    },
                    {
                        "name": "Bowen Wu"
                    },
                    {
                        "name": "Qun Yu"
                    },
                    {
                        "name": "Baoxun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Baoxun Wang"
                },
                "author": "Baoxun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13074v2",
                "updated": "2025-04-18T09:46:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    46,
                    32,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-17T16:37:27Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    37,
                    27,
                    3,
                    107,
                    0
                ],
                "title": "SkyReels-V2: Infinite-length Film Generative Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyReels-V2: Infinite-length Film Generative Model"
                },
                "summary": "Recent advances in video generation have been driven by diffusion models and\nautoregressive frameworks, yet critical challenges persist in harmonizing\nprompt adherence, visual quality, motion dynamics, and duration: compromises in\nmotion dynamics to enhance temporal visual quality, constrained video duration\n(5-10 seconds) to prioritize resolution, and inadequate shot-aware generation\nstemming from general-purpose MLLMs' inability to interpret cinematic grammar,\nsuch as shot composition, actor expressions, and camera motions. These\nintertwined limitations hinder realistic long-form synthesis and professional\nfilm-style generation. To address these limitations, we propose SkyReels-V2, an\nInfinite-length Film Generative Model, that synergizes Multi-modal Large\nLanguage Model (MLLM), Multi-stage Pretraining, Reinforcement Learning, and\nDiffusion Forcing Framework. Firstly, we design a comprehensive structural\nrepresentation of video that combines the general descriptions by the\nMulti-modal LLM and the detailed shot language by sub-expert models. Aided with\nhuman annotation, we then train a unified Video Captioner, named\nSkyCaptioner-V1, to efficiently label the video data. Secondly, we establish\nprogressive-resolution pretraining for the fundamental video generation,\nfollowed by a four-stage post-training enhancement: Initial concept-balanced\nSupervised Fine-Tuning (SFT) improves baseline quality; Motion-specific\nReinforcement Learning (RL) training with human-annotated and synthetic\ndistortion data addresses dynamic artifacts; Our diffusion forcing framework\nwith non-decreasing noise schedules enables long-video synthesis in an\nefficient search space; Final high-quality SFT refines visual fidelity. All the\ncode and models are available at https://github.com/SkyworkAI/SkyReels-V2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in video generation have been driven by diffusion models and\nautoregressive frameworks, yet critical challenges persist in harmonizing\nprompt adherence, visual quality, motion dynamics, and duration: compromises in\nmotion dynamics to enhance temporal visual quality, constrained video duration\n(5-10 seconds) to prioritize resolution, and inadequate shot-aware generation\nstemming from general-purpose MLLMs' inability to interpret cinematic grammar,\nsuch as shot composition, actor expressions, and camera motions. These\nintertwined limitations hinder realistic long-form synthesis and professional\nfilm-style generation. To address these limitations, we propose SkyReels-V2, an\nInfinite-length Film Generative Model, that synergizes Multi-modal Large\nLanguage Model (MLLM), Multi-stage Pretraining, Reinforcement Learning, and\nDiffusion Forcing Framework. Firstly, we design a comprehensive structural\nrepresentation of video that combines the general descriptions by the\nMulti-modal LLM and the detailed shot language by sub-expert models. Aided with\nhuman annotation, we then train a unified Video Captioner, named\nSkyCaptioner-V1, to efficiently label the video data. Secondly, we establish\nprogressive-resolution pretraining for the fundamental video generation,\nfollowed by a four-stage post-training enhancement: Initial concept-balanced\nSupervised Fine-Tuning (SFT) improves baseline quality; Motion-specific\nReinforcement Learning (RL) training with human-annotated and synthetic\ndistortion data addresses dynamic artifacts; Our diffusion forcing framework\nwith non-decreasing noise schedules enables long-video synthesis in an\nefficient search space; Final high-quality SFT refines visual fidelity. All the\ncode and models are available at https://github.com/SkyworkAI/SkyReels-V2."
                },
                "authors": [
                    {
                        "name": "Guibin Chen"
                    },
                    {
                        "name": "Dixuan Lin"
                    },
                    {
                        "name": "Jiangping Yang"
                    },
                    {
                        "name": "Chunze Lin"
                    },
                    {
                        "name": "Juncheng Zhu"
                    },
                    {
                        "name": "Mingyuan Fan"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Sheng Chen"
                    },
                    {
                        "name": "Zheng Chen"
                    },
                    {
                        "name": "Chengchen Ma"
                    },
                    {
                        "name": "Weiming Xiong"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Nuo Pang"
                    },
                    {
                        "name": "Kang Kang"
                    },
                    {
                        "name": "Zhiheng Xu"
                    },
                    {
                        "name": "Yuzhe Jin"
                    },
                    {
                        "name": "Yupeng Liang"
                    },
                    {
                        "name": "Yubing Song"
                    },
                    {
                        "name": "Peng Zhao"
                    },
                    {
                        "name": "Boyuan Xu"
                    },
                    {
                        "name": "Di Qiu"
                    },
                    {
                        "name": "Debang Li"
                    },
                    {
                        "name": "Zhengcong Fei"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yahui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yahui Zhou"
                },
                "author": "Yahui Zhou",
                "arxiv_comment": "31 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13589v1",
                "updated": "2025-04-18T09:41:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    41,
                    35,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T09:41:35Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    41,
                    35,
                    4,
                    108,
                    0
                ],
                "title": "Towards End-to-End Network Intent Management with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards End-to-End Network Intent Management with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are likely to play a key role in Intent-Based\nNetworking (IBN) as they show remarkable performance in interpreting human\nlanguage as well as code generation, enabling the translation of high-level\nintents expressed by humans into low-level network configurations. In this\npaper, we leverage closed-source language models (i.e., Google Gemini 1.5 pro,\nChatGPT-4) and open-source models (i.e., LLama, Mistral) to investigate their\ncapacity to generate E2E network configurations for radio access networks\n(RANs) and core networks in 5G/6G mobile networks. We introduce a novel\nperformance metrics, known as FEACI, to quantitatively assess the format (F),\nexplainability (E), accuracy (A), cost (C), and inference time (I) of the\ngenerated answer; existing general metrics are unable to capture these\nfeatures. The results of our study demonstrate that open-source models can\nachieve comparable or even superior translation performance compared with the\nclosed-source models requiring costly hardware setup and not accessible to all\nusers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are likely to play a key role in Intent-Based\nNetworking (IBN) as they show remarkable performance in interpreting human\nlanguage as well as code generation, enabling the translation of high-level\nintents expressed by humans into low-level network configurations. In this\npaper, we leverage closed-source language models (i.e., Google Gemini 1.5 pro,\nChatGPT-4) and open-source models (i.e., LLama, Mistral) to investigate their\ncapacity to generate E2E network configurations for radio access networks\n(RANs) and core networks in 5G/6G mobile networks. We introduce a novel\nperformance metrics, known as FEACI, to quantitatively assess the format (F),\nexplainability (E), accuracy (A), cost (C), and inference time (I) of the\ngenerated answer; existing general metrics are unable to capture these\nfeatures. The results of our study demonstrate that open-source models can\nachieve comparable or even superior translation performance compared with the\nclosed-source models requiring costly hardware setup and not accessible to all\nusers."
                },
                "authors": [
                    {
                        "name": "Lam Dinh"
                    },
                    {
                        "name": "Sihem Cherrared"
                    },
                    {
                        "name": "Xiaofeng Huang"
                    },
                    {
                        "name": "Fabrice Guillemin"
                    }
                ],
                "author_detail": {
                    "name": "Fabrice Guillemin"
                },
                "author": "Fabrice Guillemin",
                "arxiv_comment": "Full paper is accepted at IFIP Networking 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13587v1",
                "updated": "2025-04-18T09:38:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    38,
                    49,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T09:38:49Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    38,
                    49,
                    4,
                    108,
                    0
                ],
                "title": "RAG Without the Lag: Interactive Debugging for Retrieval-Augmented\n  Generation Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG Without the Lag: Interactive Debugging for Retrieval-Augmented\n  Generation Pipelines"
                },
                "summary": "Retrieval-augmented generation (RAG) pipelines have become the de-facto\napproach for building AI assistants with access to external, domain-specific\nknowledge. Given a user query, RAG pipelines typically first retrieve (R)\nrelevant information from external sources, before invoking a Large Language\nModel (LLM), augmented (A) with this information, to generate (G) responses.\nModern RAG pipelines frequently chain multiple retrieval and generation\ncomponents, in any order. However, developing effective RAG pipelines is\nchallenging because retrieval and generation components are intertwined, making\nit hard to identify which component(s) cause errors in the eventual output. The\nparameters with the greatest impact on output quality often require hours of\npre-processing after each change, creating prohibitively slow feedback cycles.\nTo address these challenges, we present RAGGY, a developer tool that combines a\nPython library of composable RAG primitives with an interactive interface for\nreal-time debugging. We contribute the design and implementation of RAGGY,\ninsights into expert debugging patterns through a qualitative study with 12\nengineers, and design implications for future RAG tools that better align with\ndevelopers' natural workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) pipelines have become the de-facto\napproach for building AI assistants with access to external, domain-specific\nknowledge. Given a user query, RAG pipelines typically first retrieve (R)\nrelevant information from external sources, before invoking a Large Language\nModel (LLM), augmented (A) with this information, to generate (G) responses.\nModern RAG pipelines frequently chain multiple retrieval and generation\ncomponents, in any order. However, developing effective RAG pipelines is\nchallenging because retrieval and generation components are intertwined, making\nit hard to identify which component(s) cause errors in the eventual output. The\nparameters with the greatest impact on output quality often require hours of\npre-processing after each change, creating prohibitively slow feedback cycles.\nTo address these challenges, we present RAGGY, a developer tool that combines a\nPython library of composable RAG primitives with an interactive interface for\nreal-time debugging. We contribute the design and implementation of RAGGY,\ninsights into expert debugging patterns through a qualitative study with 12\nengineers, and design implications for future RAG tools that better align with\ndevelopers' natural workflows."
                },
                "authors": [
                    {
                        "name": "Quentin Romero Lauro"
                    },
                    {
                        "name": "Shreya Shankar"
                    },
                    {
                        "name": "Sepanta Zeighami"
                    },
                    {
                        "name": "Aditya Parameswaran"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Parameswaran"
                },
                "author": "Aditya Parameswaran",
                "arxiv_comment": "15 pages, 7 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14427v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14427v2",
                "updated": "2025-04-18T09:21:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    21,
                    41,
                    4,
                    108,
                    0
                ],
                "published": "2025-02-20T10:25:13Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    10,
                    25,
                    13,
                    3,
                    51,
                    0
                ],
                "title": "Token-Level Density-Based Uncertainty Quantification Methods for\n  Eliciting Truthfulness of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-Level Density-Based Uncertainty Quantification Methods for\n  Eliciting Truthfulness of Large Language Models"
                },
                "summary": "Uncertainty quantification (UQ) is a prominent approach for eliciting\ntruthful answers from large language models (LLMs). To date, information-based\nand consistency-based UQ have been the dominant UQ methods for text generation\nvia LLMs. Density-based methods, despite being very effective for UQ in text\nclassification with encoder-based models, have not been very successful with\ngenerative LLMs. In this work, we adapt Mahalanobis Distance (MD) - a\nwell-established UQ technique in classification tasks - for text generation and\nintroduce a new supervised UQ method. Our method extracts token embeddings from\nmultiple layers of LLMs, computes MD scores for each token, and uses linear\nregression trained on these features to provide robust uncertainty scores.\nThrough extensive experiments on eleven datasets, we demonstrate that our\napproach substantially improves over existing UQ methods, providing accurate\nand computationally efficient uncertainty scores for both sequence-level\nselective generation and claim-level fact-checking tasks. Our method also\nexhibits strong generalization to out-of-domain data, making it suitable for a\nwide range of LLM-based applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty quantification (UQ) is a prominent approach for eliciting\ntruthful answers from large language models (LLMs). To date, information-based\nand consistency-based UQ have been the dominant UQ methods for text generation\nvia LLMs. Density-based methods, despite being very effective for UQ in text\nclassification with encoder-based models, have not been very successful with\ngenerative LLMs. In this work, we adapt Mahalanobis Distance (MD) - a\nwell-established UQ technique in classification tasks - for text generation and\nintroduce a new supervised UQ method. Our method extracts token embeddings from\nmultiple layers of LLMs, computes MD scores for each token, and uses linear\nregression trained on these features to provide robust uncertainty scores.\nThrough extensive experiments on eleven datasets, we demonstrate that our\napproach substantially improves over existing UQ methods, providing accurate\nand computationally efficient uncertainty scores for both sequence-level\nselective generation and claim-level fact-checking tasks. Our method also\nexhibits strong generalization to out-of-domain data, making it suitable for a\nwide range of LLM-based applications."
                },
                "authors": [
                    {
                        "name": "Artem Vazhentsev"
                    },
                    {
                        "name": "Lyudmila Rvanova"
                    },
                    {
                        "name": "Ivan Lazichny"
                    },
                    {
                        "name": "Alexander Panchenko"
                    },
                    {
                        "name": "Maxim Panov"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Artem Shelmanov"
                    }
                ],
                "author_detail": {
                    "name": "Artem Shelmanov"
                },
                "author": "Artem Shelmanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14427v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14427v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13574v1",
                "updated": "2025-04-18T09:19:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    19,
                    7,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T09:19:07Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    19,
                    7,
                    4,
                    108,
                    0
                ],
                "title": "MAAM: A Lightweight Multi-Agent Aggregation Module for Efficient Image\n  Classification Based on the MindSpore Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAAM: A Lightweight Multi-Agent Aggregation Module for Efficient Image\n  Classification Based on the MindSpore Framework"
                },
                "summary": "The demand for lightweight models in image classification tasks under\nresource-constrained environments necessitates a balance between computational\nefficiency and robust feature representation. Traditional attention mechanisms,\ndespite their strong feature modeling capability, often struggle with high\ncomputational complexity and structural rigidity, limiting their applicability\nin scenarios with limited computational resources (e.g., edge devices or\nreal-time systems). To address this, we propose the Multi-Agent Aggregation\nModule (MAAM), a lightweight attention architecture integrated with the\nMindSpore framework. MAAM employs three parallel agent branches with\nindependently parameterized operations to extract heterogeneous features,\nadaptively fused via learnable scalar weights, and refined through a\nconvolutional compression layer. Leveraging MindSpore's dynamic computational\ngraph and operator fusion, MAAM achieves 87.0% accuracy on the CIFAR-10\ndataset, significantly outperforming conventional CNN (58.3%) and MLP (49.6%)\nmodels, while improving training efficiency by 30%. Ablation studies confirm\nthe critical role of agent attention (accuracy drops to 32.0% if removed) and\ncompression modules (25.5% if omitted), validating their necessity for\nmaintaining discriminative feature learning. The framework's hardware\nacceleration capabilities and minimal memory footprint further demonstrate its\npracticality, offering a deployable solution for image classification in\nresource-constrained scenarios without compromising accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The demand for lightweight models in image classification tasks under\nresource-constrained environments necessitates a balance between computational\nefficiency and robust feature representation. Traditional attention mechanisms,\ndespite their strong feature modeling capability, often struggle with high\ncomputational complexity and structural rigidity, limiting their applicability\nin scenarios with limited computational resources (e.g., edge devices or\nreal-time systems). To address this, we propose the Multi-Agent Aggregation\nModule (MAAM), a lightweight attention architecture integrated with the\nMindSpore framework. MAAM employs three parallel agent branches with\nindependently parameterized operations to extract heterogeneous features,\nadaptively fused via learnable scalar weights, and refined through a\nconvolutional compression layer. Leveraging MindSpore's dynamic computational\ngraph and operator fusion, MAAM achieves 87.0% accuracy on the CIFAR-10\ndataset, significantly outperforming conventional CNN (58.3%) and MLP (49.6%)\nmodels, while improving training efficiency by 30%. Ablation studies confirm\nthe critical role of agent attention (accuracy drops to 32.0% if removed) and\ncompression modules (25.5% if omitted), validating their necessity for\nmaintaining discriminative feature learning. The framework's hardware\nacceleration capabilities and minimal memory footprint further demonstrate its\npracticality, offering a deployable solution for image classification in\nresource-constrained scenarios without compromising accuracy."
                },
                "authors": [
                    {
                        "name": "Zhenkai Qin"
                    },
                    {
                        "name": "Feng Zhu"
                    },
                    {
                        "name": "Huan Zeng"
                    },
                    {
                        "name": "Xunyi Nong"
                    }
                ],
                "author_detail": {
                    "name": "Xunyi Nong"
                },
                "author": "Xunyi Nong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13572v1",
                "updated": "2025-04-18T09:12:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    12,
                    46,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T09:12:46Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    12,
                    46,
                    4,
                    108,
                    0
                ],
                "title": "Contextualizing Spotify's Audiobook List Recommendations with\n  Descriptive Shelves",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextualizing Spotify's Audiobook List Recommendations with\n  Descriptive Shelves"
                },
                "summary": "In this paper, we propose a pipeline to generate contextualized list\nrecommendations with descriptive shelves in the domain of audiobooks. By\ncreating several shelves for topics the user has an affinity to, e.g. Uplifting\nWomen's Fiction, we can help them explore their recommendations according to\ntheir interests and at the same time recommend a diverse set of items. To do\nso, we use Large Language Models (LLMs) to enrich each item's metadata based on\na taxonomy created for this domain. Then we create diverse descriptive shelves\nfor each user. A/B tests show improvements in user engagement and audiobook\ndiscovery metrics, demonstrating benefits for users and content creators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a pipeline to generate contextualized list\nrecommendations with descriptive shelves in the domain of audiobooks. By\ncreating several shelves for topics the user has an affinity to, e.g. Uplifting\nWomen's Fiction, we can help them explore their recommendations according to\ntheir interests and at the same time recommend a diverse set of items. To do\nso, we use Large Language Models (LLMs) to enrich each item's metadata based on\na taxonomy created for this domain. Then we create diverse descriptive shelves\nfor each user. A/B tests show improvements in user engagement and audiobook\ndiscovery metrics, demonstrating benefits for users and content creators."
                },
                "authors": [
                    {
                        "name": "Gustavo Penha"
                    },
                    {
                        "name": "Alice Wang"
                    },
                    {
                        "name": "Martin Achenbach"
                    },
                    {
                        "name": "Kristen Sheets"
                    },
                    {
                        "name": "Sahitya Mantravadi"
                    },
                    {
                        "name": "Remi Galvez"
                    },
                    {
                        "name": "Nico Guetta-Jeanrenaud"
                    },
                    {
                        "name": "Divya Narayanan"
                    },
                    {
                        "name": "Ofeliya Kalaydzhyan"
                    },
                    {
                        "name": "Hugues Bouchard"
                    }
                ],
                "author_detail": {
                    "name": "Hugues Bouchard"
                },
                "author": "Hugues Bouchard",
                "arxiv_comment": "Accepted for publication in the 47th European Conference on\n  Information Retrieval (ECIR'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20999v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20999v3",
                "updated": "2025-04-18T09:04:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    4,
                    15,
                    4,
                    108,
                    0
                ],
                "published": "2024-07-30T17:38:24Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    38,
                    24,
                    1,
                    212,
                    0
                ],
                "title": "MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM\n  Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM\n  Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks. Typically, LLMs are first pre-trained on large corpora\nand subsequently fine-tuned on task-specific datasets. However, during\nfine-tuning, LLMs may forget some knowledge acquired in the pre-training stage,\nleading to a decline in general capabilities. Existing approaches to mitigate\nforgetting often rely on access to pre-training data, which may be unavailable\nin many real-world scenarios--such as fine-tuning checkpoint-only open-source\nLLMs. To address this challenge, we propose a new fine-tuning algorithm termed\nMomentum-Filtered Optimizer (MoFO). MoFO is an extension of greedy block\ncoordinate descent (BCD) methods: in each iteration, MoFO only updates the\nmodel parameters with the largest momentum magnitudes, while keeping all other\nparameters fixed. MoFO achieves similar fine-tuning performance to the default\nfine-tuning algorithm while effectively mitigating knowledge forgetting. We\nvalidate MoFO through rigorous convergence analysis and extensive experiments,\ndemonstrating its effectiveness in mitigating forgetting without pre-training\ndata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks. Typically, LLMs are first pre-trained on large corpora\nand subsequently fine-tuned on task-specific datasets. However, during\nfine-tuning, LLMs may forget some knowledge acquired in the pre-training stage,\nleading to a decline in general capabilities. Existing approaches to mitigate\nforgetting often rely on access to pre-training data, which may be unavailable\nin many real-world scenarios--such as fine-tuning checkpoint-only open-source\nLLMs. To address this challenge, we propose a new fine-tuning algorithm termed\nMomentum-Filtered Optimizer (MoFO). MoFO is an extension of greedy block\ncoordinate descent (BCD) methods: in each iteration, MoFO only updates the\nmodel parameters with the largest momentum magnitudes, while keeping all other\nparameters fixed. MoFO achieves similar fine-tuning performance to the default\nfine-tuning algorithm while effectively mitigating knowledge forgetting. We\nvalidate MoFO through rigorous convergence analysis and extensive experiments,\ndemonstrating its effectiveness in mitigating forgetting without pre-training\ndata."
                },
                "authors": [
                    {
                        "name": "Yupeng Chen"
                    },
                    {
                        "name": "Senmiao Wang"
                    },
                    {
                        "name": "Yushun Zhang"
                    },
                    {
                        "name": "Zhihang Lin"
                    },
                    {
                        "name": "Haozhe Zhang"
                    },
                    {
                        "name": "Weijian Sun"
                    },
                    {
                        "name": "Tian Ding"
                    },
                    {
                        "name": "Ruoyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Ruoyu Sun"
                },
                "author": "Ruoyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20999v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20999v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13562v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13562v1",
                "updated": "2025-04-18T09:02:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    2,
                    12,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T09:02:12Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    2,
                    12,
                    4,
                    108,
                    0
                ],
                "title": "DETAM: Defending LLMs Against Jailbreak Attacks via Targeted Attention\n  Modification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DETAM: Defending LLMs Against Jailbreak Attacks via Targeted Attention\n  Modification"
                },
                "summary": "With the widespread adoption of Large Language Models (LLMs), jailbreak\nattacks have become an increasingly pressing safety concern. While\nsafety-aligned LLMs can effectively defend against normal harmful queries, they\nremain vulnerable to such attacks. Existing defense methods primarily rely on\nfine-tuning or input modification, which often suffer from limited\ngeneralization and reduced utility. To address this, we introduce DETAM, a\nfinetuning-free defense approach that improves the defensive capabilities\nagainst jailbreak attacks of LLMs via targeted attention modification.\nSpecifically, we analyze the differences in attention scores between successful\nand unsuccessful defenses to identify the attention heads sensitive to\njailbreak attacks. During inference, we reallocate attention to emphasize the\nuser's core intention, minimizing interference from attack tokens. Our\nexperimental results demonstrate that DETAM outperforms various baselines in\njailbreak defense and exhibits robust generalization across different attacks\nand models, maintaining its effectiveness even on in-the-wild jailbreak data.\nFurthermore, in evaluating the model's utility, we incorporated over-defense\ndatasets, which further validate the superior performance of our approach. The\ncode will be released immediately upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread adoption of Large Language Models (LLMs), jailbreak\nattacks have become an increasingly pressing safety concern. While\nsafety-aligned LLMs can effectively defend against normal harmful queries, they\nremain vulnerable to such attacks. Existing defense methods primarily rely on\nfine-tuning or input modification, which often suffer from limited\ngeneralization and reduced utility. To address this, we introduce DETAM, a\nfinetuning-free defense approach that improves the defensive capabilities\nagainst jailbreak attacks of LLMs via targeted attention modification.\nSpecifically, we analyze the differences in attention scores between successful\nand unsuccessful defenses to identify the attention heads sensitive to\njailbreak attacks. During inference, we reallocate attention to emphasize the\nuser's core intention, minimizing interference from attack tokens. Our\nexperimental results demonstrate that DETAM outperforms various baselines in\njailbreak defense and exhibits robust generalization across different attacks\nand models, maintaining its effectiveness even on in-the-wild jailbreak data.\nFurthermore, in evaluating the model's utility, we incorporated over-defense\ndatasets, which further validate the superior performance of our approach. The\ncode will be released immediately upon acceptance."
                },
                "authors": [
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Han Jiang"
                    },
                    {
                        "name": "Zhihua Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhihua Wei"
                },
                "author": "Zhihua Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13562v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13562v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13560v1",
                "updated": "2025-04-18T08:58:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    8,
                    58,
                    40,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T08:58:40Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    8,
                    58,
                    40,
                    4,
                    108,
                    0
                ],
                "title": "Zero-Shot Industrial Anomaly Segmentation with Image-Aware Prompt\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Industrial Anomaly Segmentation with Image-Aware Prompt\n  Generation"
                },
                "summary": "Anomaly segmentation is essential for industrial quality, maintenance, and\nstability. Existing text-guided zero-shot anomaly segmentation models are\neffective but rely on fixed prompts, limiting adaptability in diverse\nindustrial scenarios. This highlights the need for flexible, context-aware\nprompting strategies. We propose Image-Aware Prompt Anomaly Segmentation\n(IAP-AS), which enhances anomaly segmentation by generating dynamic,\ncontext-aware prompts using an image tagging model and a large language model\n(LLM). IAP-AS extracts object attributes from images to generate context-aware\nprompts, improving adaptability and generalization in dynamic and unstructured\nindustrial environments. In our experiments, IAP-AS improves the F1-max metric\nby up to 10%, demonstrating superior adaptability and generalization. It\nprovides a scalable solution for anomaly segmentation across industries",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anomaly segmentation is essential for industrial quality, maintenance, and\nstability. Existing text-guided zero-shot anomaly segmentation models are\neffective but rely on fixed prompts, limiting adaptability in diverse\nindustrial scenarios. This highlights the need for flexible, context-aware\nprompting strategies. We propose Image-Aware Prompt Anomaly Segmentation\n(IAP-AS), which enhances anomaly segmentation by generating dynamic,\ncontext-aware prompts using an image tagging model and a large language model\n(LLM). IAP-AS extracts object attributes from images to generate context-aware\nprompts, improving adaptability and generalization in dynamic and unstructured\nindustrial environments. In our experiments, IAP-AS improves the F1-max metric\nby up to 10%, demonstrating superior adaptability and generalization. It\nprovides a scalable solution for anomaly segmentation across industries"
                },
                "authors": [
                    {
                        "name": "SoYoung Park"
                    },
                    {
                        "name": "Hyewon Lee"
                    },
                    {
                        "name": "Mingyu Choi"
                    },
                    {
                        "name": "Seunghoon Han"
                    },
                    {
                        "name": "Jong-Ryul Lee"
                    },
                    {
                        "name": "Sungsu Lim"
                    },
                    {
                        "name": "Tae-Ho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Tae-Ho Kim"
                },
                "author": "Tae-Ho Kim",
                "arxiv_comment": "Accepted to PAKDD 2025, 12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13557v1",
                "updated": "2025-04-18T08:49:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    8,
                    49,
                    45,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T08:49:45Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    8,
                    49,
                    45,
                    4,
                    108,
                    0
                ],
                "title": "Integrating LLMs for Grading and Appeal Resolution in Computer Science\n  Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating LLMs for Grading and Appeal Resolution in Computer Science\n  Education"
                },
                "summary": "This study explores the integration of Large Language Models (LLMs) into the\ngrading and appeal resolution process in computer science education. We\nintroduce AI-PAT, an AI-powered assessment tool that leverages LLMs to evaluate\ncomputer science exams, generate feedback, and address student appeals. AI-PAT\nwas used to assess over 850 exam submissions and handle 185 appeal cases. Our\nmulti-model comparison (ChatGPT, Gemini) reveals strong correlations between\nmodel outputs, though significant variability persists depending on\nconfiguration and prompt design. Human graders, while internally consistent,\nshowed notable inter-rater disagreement, further highlighting subjectivity in\nmanual evaluation. The appeal process led to grade changes in 74% of cases,\nindicating the need for continued refinement of AI evaluation strategies. While\nstudents appreciated the speed and detail of AI feedback, survey responses\nrevealed trust and fairness concerns. We conclude that AI-PAT offers scalable\nbenefits for formative assessment and feedback, but must be accompanied by\ntransparent grading rubrics, human oversight, and appeal mechanisms to ensure\nequitable outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the integration of Large Language Models (LLMs) into the\ngrading and appeal resolution process in computer science education. We\nintroduce AI-PAT, an AI-powered assessment tool that leverages LLMs to evaluate\ncomputer science exams, generate feedback, and address student appeals. AI-PAT\nwas used to assess over 850 exam submissions and handle 185 appeal cases. Our\nmulti-model comparison (ChatGPT, Gemini) reveals strong correlations between\nmodel outputs, though significant variability persists depending on\nconfiguration and prompt design. Human graders, while internally consistent,\nshowed notable inter-rater disagreement, further highlighting subjectivity in\nmanual evaluation. The appeal process led to grade changes in 74% of cases,\nindicating the need for continued refinement of AI evaluation strategies. While\nstudents appreciated the speed and detail of AI feedback, survey responses\nrevealed trust and fairness concerns. We conclude that AI-PAT offers scalable\nbenefits for formative assessment and feedback, but must be accompanied by\ntransparent grading rubrics, human oversight, and appeal mechanisms to ensure\nequitable outcomes."
                },
                "authors": [
                    {
                        "name": "I. Aytutuldu"
                    },
                    {
                        "name": "O. Yol"
                    },
                    {
                        "name": "Y. S. Akgul"
                    }
                ],
                "author_detail": {
                    "name": "Y. S. Akgul"
                },
                "arxiv_affiliation": "Gebze Technical University",
                "author": "Y. S. Akgul",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11900v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11900v2",
                "updated": "2025-04-18T08:44:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    8,
                    44,
                    4,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-16T09:25:54Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    25,
                    54,
                    2,
                    106,
                    0
                ],
                "title": "Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models\n  via Plot Hole Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models\n  via Plot Hole Detection"
                },
                "summary": "Stories are a fundamental aspect of human experience. Engaging deeply with\nstories and spotting plot holes -- inconsistencies in a storyline that break\nthe internal logic or rules of a story's world -- requires nuanced reasoning\nskills, including tracking entities and events and their interplay, abstract\nthinking, pragmatic narrative understanding, commonsense and social reasoning,\nand theory of mind. As Large Language Models (LLMs) increasingly generate,\ninterpret, and modify text, rigorously assessing their narrative consistency\nand deeper language understanding becomes critical. However, existing\nbenchmarks focus mainly on surface-level comprehension. In this work, we\npropose plot hole detection in stories as a proxy to evaluate language\nunderstanding and reasoning in LLMs. We introduce FlawedFictionsMaker, a novel\nalgorithm to controllably and carefully synthesize plot holes in human-written\nstories. Using this algorithm, we construct a benchmark to evaluate LLMs' plot\nhole detection abilities in stories -- FlawedFictions -- , which is robust to\ncontamination, with human filtering ensuring high quality. We find that\nstate-of-the-art LLMs struggle in accurately solving FlawedFictions regardless\nof the reasoning effort allowed, with performance significantly degrading as\nstory length increases. Finally, we show that LLM-based story summarization and\nstory generation are prone to introducing plot holes, with more than 50% and\n100% increases in plot hole detection rates with respect to human-written\noriginals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stories are a fundamental aspect of human experience. Engaging deeply with\nstories and spotting plot holes -- inconsistencies in a storyline that break\nthe internal logic or rules of a story's world -- requires nuanced reasoning\nskills, including tracking entities and events and their interplay, abstract\nthinking, pragmatic narrative understanding, commonsense and social reasoning,\nand theory of mind. As Large Language Models (LLMs) increasingly generate,\ninterpret, and modify text, rigorously assessing their narrative consistency\nand deeper language understanding becomes critical. However, existing\nbenchmarks focus mainly on surface-level comprehension. In this work, we\npropose plot hole detection in stories as a proxy to evaluate language\nunderstanding and reasoning in LLMs. We introduce FlawedFictionsMaker, a novel\nalgorithm to controllably and carefully synthesize plot holes in human-written\nstories. Using this algorithm, we construct a benchmark to evaluate LLMs' plot\nhole detection abilities in stories -- FlawedFictions -- , which is robust to\ncontamination, with human filtering ensuring high quality. We find that\nstate-of-the-art LLMs struggle in accurately solving FlawedFictions regardless\nof the reasoning effort allowed, with performance significantly degrading as\nstory length increases. Finally, we show that LLM-based story summarization and\nstory generation are prone to introducing plot holes, with more than 50% and\n100% increases in plot hole detection rates with respect to human-written\noriginals."
                },
                "authors": [
                    {
                        "name": "Kabir Ahuja"
                    },
                    {
                        "name": "Melanie Sclar"
                    },
                    {
                        "name": "Yulia Tsvetkov"
                    }
                ],
                "author_detail": {
                    "name": "Yulia Tsvetkov"
                },
                "author": "Yulia Tsvetkov",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11900v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11900v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12867v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12867v2",
                "updated": "2025-04-18T08:18:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    8,
                    18,
                    11,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-17T11:50:04Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    11,
                    50,
                    4,
                    3,
                    107,
                    0
                ],
                "title": "EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text\n  Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text\n  Prompting"
                },
                "summary": "Human speech goes beyond the mere transfer of information; it is a profound\nexchange of emotions and a connection between individuals. While Text-to-Speech\n(TTS) models have made huge progress, they still face challenges in controlling\nthe emotional expression in the generated speech. In this work, we propose\nEmoVoice, a novel emotion-controllable TTS model that exploits large language\nmodels (LLMs) to enable fine-grained freestyle natural language emotion\ncontrol, and a phoneme boost variant design that makes the model output phoneme\ntokens and audio tokens in parallel to enhance content consistency, inspired by\nchain-of-thought (CoT) and chain-of-modality (CoM) techniques. Besides, we\nintroduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring\nexpressive speech and fine-grained emotion labels with natural language\ndescriptions. EmoVoice achieves state-of-the-art performance on the English\nEmoVoice-DB test set using only synthetic training data, and on the Chinese\nSecap test set using our in-house data. We further investigate the reliability\nof existing emotion evaluation metrics and their alignment with human\nperceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and\nGemini to assess emotional speech. Demo samples are available at\nhttps://anonymous.4open.science/r/EmoVoice-DF55. Dataset, code, and checkpoints\nwill be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human speech goes beyond the mere transfer of information; it is a profound\nexchange of emotions and a connection between individuals. While Text-to-Speech\n(TTS) models have made huge progress, they still face challenges in controlling\nthe emotional expression in the generated speech. In this work, we propose\nEmoVoice, a novel emotion-controllable TTS model that exploits large language\nmodels (LLMs) to enable fine-grained freestyle natural language emotion\ncontrol, and a phoneme boost variant design that makes the model output phoneme\ntokens and audio tokens in parallel to enhance content consistency, inspired by\nchain-of-thought (CoT) and chain-of-modality (CoM) techniques. Besides, we\nintroduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring\nexpressive speech and fine-grained emotion labels with natural language\ndescriptions. EmoVoice achieves state-of-the-art performance on the English\nEmoVoice-DB test set using only synthetic training data, and on the Chinese\nSecap test set using our in-house data. We further investigate the reliability\nof existing emotion evaluation metrics and their alignment with human\nperceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and\nGemini to assess emotional speech. Demo samples are available at\nhttps://anonymous.4open.science/r/EmoVoice-DF55. Dataset, code, and checkpoints\nwill be released."
                },
                "authors": [
                    {
                        "name": "Guanrou Yang"
                    },
                    {
                        "name": "Chen Yang"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Wenxi Chen"
                    },
                    {
                        "name": "Wen Wang"
                    },
                    {
                        "name": "Tianrui Wang"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Zhikang Niu"
                    },
                    {
                        "name": "Wenrui Liu"
                    },
                    {
                        "name": "Fan Yu"
                    },
                    {
                        "name": "Zhihao Du"
                    },
                    {
                        "name": "Zhifu Gao"
                    },
                    {
                        "name": "ShiLiang Zhang"
                    },
                    {
                        "name": "Xie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xie Chen"
                },
                "author": "Xie Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12867v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12867v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07880v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07880v2",
                "updated": "2025-04-18T08:05:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    8,
                    5,
                    53,
                    4,
                    108,
                    0
                ],
                "published": "2024-07-10T17:48:25Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    17,
                    48,
                    25,
                    2,
                    192,
                    0
                ],
                "title": "Towards Robust Alignment of Language Models: Distributionally\n  Robustifying Direct Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Robust Alignment of Language Models: Distributionally\n  Robustifying Direct Preference Optimization"
                },
                "summary": "This study addresses the challenge of noise in training datasets for Direct\nPreference Optimization (DPO), a method for aligning Large Language Models\n(LLMs) with human preferences. We categorize noise into pointwise noise, which\nincludes low-quality data points, and pairwise noise, which encompasses\nerroneous data pair associations that affect preference rankings. Utilizing\nDistributionally Robust Optimization (DRO), we enhance DPO's resilience to\nthese types of noise. Our theoretical insights reveal that DPO inherently\nembeds DRO principles, conferring robustness to pointwise noise, with the\nregularization coefficient $\\beta$ playing a critical role in its noise\nresistance. Extending this framework, we introduce Distributionally\nRobustifying DPO (Dr. DPO), which integrates pairwise robustness by optimizing\nagainst worst-case pairwise scenarios. The novel hyperparameter $\\beta'$ in Dr.\nDPO allows for fine-tuned control over data pair reliability, providing a\nstrategic balance between exploration and exploitation in noisy training\nenvironments. Empirical evaluations demonstrate that Dr. DPO substantially\nimproves the quality of generated text and response accuracy in preference\ndatasets, showcasing enhanced performance in both noisy and noise-free\nsettings. The code is available at https://github.com/junkangwu/Dr_DPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study addresses the challenge of noise in training datasets for Direct\nPreference Optimization (DPO), a method for aligning Large Language Models\n(LLMs) with human preferences. We categorize noise into pointwise noise, which\nincludes low-quality data points, and pairwise noise, which encompasses\nerroneous data pair associations that affect preference rankings. Utilizing\nDistributionally Robust Optimization (DRO), we enhance DPO's resilience to\nthese types of noise. Our theoretical insights reveal that DPO inherently\nembeds DRO principles, conferring robustness to pointwise noise, with the\nregularization coefficient $\\beta$ playing a critical role in its noise\nresistance. Extending this framework, we introduce Distributionally\nRobustifying DPO (Dr. DPO), which integrates pairwise robustness by optimizing\nagainst worst-case pairwise scenarios. The novel hyperparameter $\\beta'$ in Dr.\nDPO allows for fine-tuned control over data pair reliability, providing a\nstrategic balance between exploration and exploitation in noisy training\nenvironments. Empirical evaluations demonstrate that Dr. DPO substantially\nimproves the quality of generated text and response accuracy in preference\ndatasets, showcasing enhanced performance in both noisy and noise-free\nsettings. The code is available at https://github.com/junkangwu/Dr_DPO."
                },
                "authors": [
                    {
                        "name": "Junkang Wu"
                    },
                    {
                        "name": "Yuexiang Xie"
                    },
                    {
                        "name": "Zhengyi Yang"
                    },
                    {
                        "name": "Jiancan Wu"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Jinyang Gao"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07880v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07880v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15545v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15545v5",
                "updated": "2025-04-18T07:56:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    7,
                    56,
                    16,
                    4,
                    108,
                    0
                ],
                "published": "2024-08-28T05:41:52Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    5,
                    41,
                    52,
                    2,
                    241,
                    0
                ],
                "title": "SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding"
                },
                "summary": "Scientific literature understanding is crucial for extracting targeted\ninformation and garnering insights, thereby significantly advancing scientific\ndiscovery. Despite the remarkable success of Large Language Models (LLMs), they\nface challenges in scientific literature understanding, primarily due to (1) a\nlack of scientific knowledge and (2) unfamiliarity with specialized scientific\ntasks.\n  To develop an LLM specialized in scientific literature understanding, we\npropose a hybrid strategy that integrates continual pre-training (CPT) and\nsupervised fine-tuning (SFT), to simultaneously infuse scientific domain\nknowledge and enhance instruction-following capabilities for domain-specific\ntasks.cIn this process, we identify two key challenges: (1) constructing\nhigh-quality CPT corpora, and (2) generating diverse SFT instructions. We\naddress these challenges through a meticulous pipeline, including PDF text\nextraction, parsing content error correction, quality filtering, and synthetic\ninstruction creation. Applying this strategy, we present a suite of LLMs:\nSciLitLLM, specialized in scientific literature understanding. These models\ndemonstrate promising performance on scientific literature understanding\nbenchmarks.\n  Our contributions are threefold: (1) We present an effective framework that\nintegrates CPT and SFT to adapt LLMs to scientific literature understanding,\nwhich can also be easily adapted to other domains. (2) We propose an LLM-based\nsynthesis method to generate diverse and high-quality scientific instructions,\nresulting in a new instruction set -- SciLitIns -- for supervised fine-tuning\nin less-represented scientific domains. (3) SciLitLLM achieves promising\nperformance improvements on scientific literature understanding benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific literature understanding is crucial for extracting targeted\ninformation and garnering insights, thereby significantly advancing scientific\ndiscovery. Despite the remarkable success of Large Language Models (LLMs), they\nface challenges in scientific literature understanding, primarily due to (1) a\nlack of scientific knowledge and (2) unfamiliarity with specialized scientific\ntasks.\n  To develop an LLM specialized in scientific literature understanding, we\npropose a hybrid strategy that integrates continual pre-training (CPT) and\nsupervised fine-tuning (SFT), to simultaneously infuse scientific domain\nknowledge and enhance instruction-following capabilities for domain-specific\ntasks.cIn this process, we identify two key challenges: (1) constructing\nhigh-quality CPT corpora, and (2) generating diverse SFT instructions. We\naddress these challenges through a meticulous pipeline, including PDF text\nextraction, parsing content error correction, quality filtering, and synthetic\ninstruction creation. Applying this strategy, we present a suite of LLMs:\nSciLitLLM, specialized in scientific literature understanding. These models\ndemonstrate promising performance on scientific literature understanding\nbenchmarks.\n  Our contributions are threefold: (1) We present an effective framework that\nintegrates CPT and SFT to adapt LLMs to scientific literature understanding,\nwhich can also be easily adapted to other domains. (2) We propose an LLM-based\nsynthesis method to generate diverse and high-quality scientific instructions,\nresulting in a new instruction set -- SciLitIns -- for supervised fine-tuning\nin less-represented scientific domains. (3) SciLitLLM achieves promising\nperformance improvements on scientific literature understanding benchmarks."
                },
                "authors": [
                    {
                        "name": "Sihang Li"
                    },
                    {
                        "name": "Jin Huang"
                    },
                    {
                        "name": "Jiaxi Zhuang"
                    },
                    {
                        "name": "Yaorui Shi"
                    },
                    {
                        "name": "Xiaochen Cai"
                    },
                    {
                        "name": "Mingjun Xu"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Guolin Ke"
                    },
                    {
                        "name": "Hengxing Cai"
                    }
                ],
                "author_detail": {
                    "name": "Hengxing Cai"
                },
                "author": "Hengxing Cai",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15545v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15545v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13534v1",
                "updated": "2025-04-18T07:55:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    7,
                    55,
                    9,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T07:55:09Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    7,
                    55,
                    9,
                    4,
                    108,
                    0
                ],
                "title": "CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation\n  to Enhance Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation\n  to Enhance Reasoning in Large Language Models"
                },
                "summary": "While chain-of-thought (CoT) reasoning improves the performance of large\nlanguage models (LLMs) in complex tasks, it still has two main challenges: the\nlow reliability of relying solely on LLMs to generate reasoning chains and the\ninterference of natural language reasoning chains on the inference logic of\nLLMs. To address these issues, we propose CoT-RAG, a novel reasoning framework\nwith three key designs: (i) Knowledge Graph-driven CoT Generation, featuring\nknowledge graphs to modulate reasoning chain generation of LLMs, thereby\nenhancing reasoning credibility; (ii) Learnable Knowledge Case-aware RAG, which\nincorporates retrieval-augmented generation (RAG) into knowledge graphs to\nretrieve relevant sub-cases and sub-descriptions, providing LLMs with learnable\ninformation; (iii) Pseudo-Program Prompting Execution, which encourages LLMs to\nexecute reasoning tasks in pseudo-programs with greater logical rigor. We\nconduct a comprehensive evaluation on nine public datasets, covering three\nreasoning problems. Compared with the-state-of-the-art methods, CoT-RAG\nexhibits a significant accuracy improvement, ranging from 4.0% to 23.0%.\nFurthermore, testing on four domain-specific datasets, CoT-RAG shows remarkable\naccuracy and efficient execution, highlighting its strong practical\napplicability and scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While chain-of-thought (CoT) reasoning improves the performance of large\nlanguage models (LLMs) in complex tasks, it still has two main challenges: the\nlow reliability of relying solely on LLMs to generate reasoning chains and the\ninterference of natural language reasoning chains on the inference logic of\nLLMs. To address these issues, we propose CoT-RAG, a novel reasoning framework\nwith three key designs: (i) Knowledge Graph-driven CoT Generation, featuring\nknowledge graphs to modulate reasoning chain generation of LLMs, thereby\nenhancing reasoning credibility; (ii) Learnable Knowledge Case-aware RAG, which\nincorporates retrieval-augmented generation (RAG) into knowledge graphs to\nretrieve relevant sub-cases and sub-descriptions, providing LLMs with learnable\ninformation; (iii) Pseudo-Program Prompting Execution, which encourages LLMs to\nexecute reasoning tasks in pseudo-programs with greater logical rigor. We\nconduct a comprehensive evaluation on nine public datasets, covering three\nreasoning problems. Compared with the-state-of-the-art methods, CoT-RAG\nexhibits a significant accuracy improvement, ranging from 4.0% to 23.0%.\nFurthermore, testing on four domain-specific datasets, CoT-RAG shows remarkable\naccuracy and efficient execution, highlighting its strong practical\napplicability and scalability."
                },
                "authors": [
                    {
                        "name": "Feiyang Li"
                    },
                    {
                        "name": "Peng Fang"
                    },
                    {
                        "name": "Zhan Shi"
                    },
                    {
                        "name": "Arijit Khan"
                    },
                    {
                        "name": "Fang Wang"
                    },
                    {
                        "name": "Dan Feng"
                    },
                    {
                        "name": "Weihao Wang"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Yongjian Cui"
                    }
                ],
                "author_detail": {
                    "name": "Yongjian Cui"
                },
                "author": "Yongjian Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11056v2",
                "updated": "2025-04-18T07:50:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    7,
                    50,
                    8,
                    4,
                    108,
                    0
                ],
                "published": "2024-09-17T10:33:27Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    33,
                    27,
                    1,
                    261,
                    0
                ],
                "title": "Large Language Models are Good Multi-lingual Learners : When LLMs Meet\n  Cross-lingual Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are Good Multi-lingual Learners : When LLMs Meet\n  Cross-lingual Prompts"
                },
                "summary": "With the advent of Large Language Models (LLMs), generating rule-based data\nfor real-world applications has become more accessible. Due to the inherent\nambiguity of natural language and the complexity of rule sets, especially in\nlong contexts, LLMs often struggle to follow all specified rules, frequently\nomitting at least one. To enhance the reasoning and understanding of LLMs on\nlong and complex contexts, we propose a novel prompting strategy Multi-Lingual\nPrompt, namely MLPrompt, which automatically translates the error-prone rule\nthat an LLM struggles to follow into another language, thus drawing greater\nattention to it. Experimental results on public datasets across various tasks\nhave shown MLPrompt can outperform state-of-the-art prompting methods such as\nChain of Thought, Tree of Thought, and Self-Consistency. Additionally, we\nintroduce a framework integrating MLPrompt with an auto-checking mechanism for\nstructured data generation, with a specific case study in text-to-MIP\ninstances. Further, we extend the proposed framework for text-to-SQL to\ndemonstrate its generation ability towards structured data synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of Large Language Models (LLMs), generating rule-based data\nfor real-world applications has become more accessible. Due to the inherent\nambiguity of natural language and the complexity of rule sets, especially in\nlong contexts, LLMs often struggle to follow all specified rules, frequently\nomitting at least one. To enhance the reasoning and understanding of LLMs on\nlong and complex contexts, we propose a novel prompting strategy Multi-Lingual\nPrompt, namely MLPrompt, which automatically translates the error-prone rule\nthat an LLM struggles to follow into another language, thus drawing greater\nattention to it. Experimental results on public datasets across various tasks\nhave shown MLPrompt can outperform state-of-the-art prompting methods such as\nChain of Thought, Tree of Thought, and Self-Consistency. Additionally, we\nintroduce a framework integrating MLPrompt with an auto-checking mechanism for\nstructured data generation, with a specific case study in text-to-MIP\ninstances. Further, we extend the proposed framework for text-to-SQL to\ndemonstrate its generation ability towards structured data synthesis."
                },
                "authors": [
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Zhenqi He"
                    },
                    {
                        "name": "Wing-Yin Yu"
                    },
                    {
                        "name": "Xiaojin Fu"
                    },
                    {
                        "name": "Xiongwei Han"
                    }
                ],
                "author_detail": {
                    "name": "Xiongwei Han"
                },
                "author": "Xiongwei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13843v2",
                "updated": "2025-04-18T07:48:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    7,
                    48,
                    48,
                    4,
                    108,
                    0
                ],
                "published": "2025-02-19T16:02:59Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    2,
                    59,
                    2,
                    50,
                    0
                ],
                "title": "AgentCF++: Memory-enhanced LLM-based Agents for Popularity-aware\n  Cross-domain Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentCF++: Memory-enhanced LLM-based Agents for Popularity-aware\n  Cross-domain Recommendations"
                },
                "summary": "LLM-based user agents, which simulate user interaction behavior, are emerging\nas a promising approach to enhancing recommender systems. In real-world\nscenarios, users' interactions often exhibit cross-domain characteristics and\nare influenced by others. However, the memory design in current methods causes\nuser agents to introduce significant irrelevant information during\ndecision-making in cross-domain scenarios and makes them unable to recognize\nthe influence of other users' interactions, such as popularity factors. To\ntackle this issue, we propose a dual-layer memory architecture combined with a\ntwo-step fusion mechanism. This design avoids irrelevant information during\ndecision-making while ensuring effective integration of cross-domain\npreferences. We also introduce the concepts of interest groups and group-shared\nmemory to better capture the influence of popularity factors on users with\nsimilar interests. Comprehensive experiments validate the effectiveness of\nAgentCF++. Our code is available at https://github.com/jhliu0807/AgentCF-plus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based user agents, which simulate user interaction behavior, are emerging\nas a promising approach to enhancing recommender systems. In real-world\nscenarios, users' interactions often exhibit cross-domain characteristics and\nare influenced by others. However, the memory design in current methods causes\nuser agents to introduce significant irrelevant information during\ndecision-making in cross-domain scenarios and makes them unable to recognize\nthe influence of other users' interactions, such as popularity factors. To\ntackle this issue, we propose a dual-layer memory architecture combined with a\ntwo-step fusion mechanism. This design avoids irrelevant information during\ndecision-making while ensuring effective integration of cross-domain\npreferences. We also introduce the concepts of interest groups and group-shared\nmemory to better capture the influence of popularity factors on users with\nsimilar interests. Comprehensive experiments validate the effectiveness of\nAgentCF++. Our code is available at https://github.com/jhliu0807/AgentCF-plus."
                },
                "authors": [
                    {
                        "name": "Jiahao Liu"
                    },
                    {
                        "name": "Shengkang Gu"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Guangping Zhang"
                    },
                    {
                        "name": "Mingzhe Han"
                    },
                    {
                        "name": "Hansu Gu"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Tun Lu"
                    },
                    {
                        "name": "Li Shang"
                    },
                    {
                        "name": "Ning Gu"
                    }
                ],
                "author_detail": {
                    "name": "Ning Gu"
                },
                "author": "Ning Gu",
                "arxiv_comment": "Accepted by SIGIR 2025, 6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02497v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02497v2",
                "updated": "2025-04-18T07:46:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    7,
                    46,
                    25,
                    4,
                    108,
                    0
                ],
                "published": "2025-03-04T11:04:35Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    4,
                    35,
                    1,
                    63,
                    0
                ],
                "title": "PennyLang: Pioneering LLM-Based Quantum Code Generation with a Novel\n  PennyLane-Centric Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PennyLang: Pioneering LLM-Based Quantum Code Generation with a Novel\n  PennyLane-Centric Dataset"
                },
                "summary": "Large Language Models (LLMs) offer remarkable capabilities in code\ngeneration, natural language processing, and domain-specific reasoning.\nHowever, their application in quantum software development remains\nunderexplored, particularly for PennyLane-a leading framework for hybrid\nquantum-classical computing. To address this gap, we introduce a novel,\nhigh-quality dataset comprising 3,347 PennyLane-specific quantum code samples\nand contextual descriptions, specifically curated to support LLM training and\nfine-tuning for quantum code assistance. Our contributions are threefold: (1)\nthe automatic construction and open-source release of a comprehensive PennyLane\ndataset derived from textbooks, official documentation, and open-source\nrepositories; (2) a structured methodology for data curation, annotation, and\nformatting to enhance LLM usability and relevance; and (3) a rigorous\nevaluation of code generation capabilities using both baseline\nRetrieval-Augmented Generation (RAG) and a GraphRAG-enhanced pipeline. Using\nthe PennyLang framework, we demonstrate that GraphRAG, when applied to a GPT-4o\nMini model, substantially outperforms standard prompting and baseline RAG.\nAccuracy improves from 20.5% (without RAG) to 58.2% with GraphRAG, showcasing\nits effectiveness in reducing hallucinations and improving code correctness in\nquantum programming tasks. Compared to prior efforts focused largely on Qiskit,\nour work expands LLM-based assistance to the PennyLane ecosystem, contributing\npractical tools and reproducible methodologies for advancing AI-assisted\nquantum software development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) offer remarkable capabilities in code\ngeneration, natural language processing, and domain-specific reasoning.\nHowever, their application in quantum software development remains\nunderexplored, particularly for PennyLane-a leading framework for hybrid\nquantum-classical computing. To address this gap, we introduce a novel,\nhigh-quality dataset comprising 3,347 PennyLane-specific quantum code samples\nand contextual descriptions, specifically curated to support LLM training and\nfine-tuning for quantum code assistance. Our contributions are threefold: (1)\nthe automatic construction and open-source release of a comprehensive PennyLane\ndataset derived from textbooks, official documentation, and open-source\nrepositories; (2) a structured methodology for data curation, annotation, and\nformatting to enhance LLM usability and relevance; and (3) a rigorous\nevaluation of code generation capabilities using both baseline\nRetrieval-Augmented Generation (RAG) and a GraphRAG-enhanced pipeline. Using\nthe PennyLang framework, we demonstrate that GraphRAG, when applied to a GPT-4o\nMini model, substantially outperforms standard prompting and baseline RAG.\nAccuracy improves from 20.5% (without RAG) to 58.2% with GraphRAG, showcasing\nits effectiveness in reducing hallucinations and improving code correctness in\nquantum programming tasks. Compared to prior efforts focused largely on Qiskit,\nour work expands LLM-based assistance to the PennyLane ecosystem, contributing\npractical tools and reproducible methodologies for advancing AI-assisted\nquantum software development."
                },
                "authors": [
                    {
                        "name": "Abdul Basit"
                    },
                    {
                        "name": "Nouhaila Innan"
                    },
                    {
                        "name": "Haider Asif"
                    },
                    {
                        "name": "Minghao Shao"
                    },
                    {
                        "name": "Muhammad Kashif"
                    },
                    {
                        "name": "Alberto Marchisio"
                    },
                    {
                        "name": "Muhammad Shafique"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Shafique"
                },
                "author": "Muhammad Shafique",
                "arxiv_comment": "10 pages, 7 figures, 7 tables, submitted for review under QCE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02497v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02497v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50 (Primary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13845v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13845v2",
                "updated": "2025-04-18T07:45:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    7,
                    45,
                    55,
                    4,
                    108,
                    0
                ],
                "published": "2025-02-19T16:08:17Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    8,
                    17,
                    2,
                    50,
                    0
                ],
                "title": "Improving LLM-powered Recommendations with Personalized Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving LLM-powered Recommendations with Personalized Information"
                },
                "summary": "Due to the lack of explicit reasoning modeling, existing LLM-powered\nrecommendations fail to leverage LLMs' reasoning capabilities effectively. In\nthis paper, we propose a pipeline called CoT-Rec, which integrates two key\nChain-of-Thought (CoT) processes -- user preference analysis and item\nperception analysis -- into LLM-powered recommendations, thereby enhancing the\nutilization of LLMs' reasoning abilities. CoT-Rec consists of two stages: (1)\npersonalized information extraction, where user preferences and item perception\nare extracted, and (2) personalized information utilization, where this\ninformation is incorporated into the LLM-powered recommendation process.\nExperimental results demonstrate that CoT-Rec shows potential for improving\nLLM-powered recommendations. The implementation is publicly available at\nhttps://github.com/jhliu0807/CoT-Rec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the lack of explicit reasoning modeling, existing LLM-powered\nrecommendations fail to leverage LLMs' reasoning capabilities effectively. In\nthis paper, we propose a pipeline called CoT-Rec, which integrates two key\nChain-of-Thought (CoT) processes -- user preference analysis and item\nperception analysis -- into LLM-powered recommendations, thereby enhancing the\nutilization of LLMs' reasoning abilities. CoT-Rec consists of two stages: (1)\npersonalized information extraction, where user preferences and item perception\nare extracted, and (2) personalized information utilization, where this\ninformation is incorporated into the LLM-powered recommendation process.\nExperimental results demonstrate that CoT-Rec shows potential for improving\nLLM-powered recommendations. The implementation is publicly available at\nhttps://github.com/jhliu0807/CoT-Rec."
                },
                "authors": [
                    {
                        "name": "Jiahao Liu"
                    },
                    {
                        "name": "Xueshuo Yan"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Guangping Zhang"
                    },
                    {
                        "name": "Hansu Gu"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Tun Lu"
                    },
                    {
                        "name": "Li Shang"
                    },
                    {
                        "name": "Ning Gu"
                    }
                ],
                "author_detail": {
                    "name": "Ning Gu"
                },
                "author": "Ning Gu",
                "arxiv_comment": "Accepted by SIGIR 2025, 7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13845v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13845v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13380v2",
                "updated": "2025-04-18T07:22:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    7,
                    22,
                    9,
                    4,
                    108,
                    0
                ],
                "published": "2025-01-23T04:49:36Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    4,
                    49,
                    36,
                    3,
                    23,
                    0
                ],
                "title": "Joint Power and Bit Allocation for Precoded Massive MIMO Channels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Power and Bit Allocation for Precoded Massive MIMO Channels"
                },
                "summary": "This work addresses the joint optimization of power and bit allocation in\nprecoded large-scale n x n MIMO systems with discrete input alphabets,\nspecifically QAM constellations. We propose an adaptive QAM scheme that\nmaintains a fixed gap to the Gaussian-input capacity for a given n. A key\nfinding is that, under the proposed scheme, the mercury/waterfilling (MWF)\nsolution reduces analytically to the classical water-filling (WF) policy.\nFurthermore, the adaptive QAM configuration can be precomputed under the\nlarge-system assumption, enabling the replacement of full SVD with truncated\nSVD and yielding substantial computational savings. To support practical\ndeployment, we develop a bit-allocation algorithm that meets a target\ntransmission data rate while minimizing the overall decoding error rate and\npreserving computational complexity at O(n log n). Simulation results confirm\nthat the proposed truncated SVD precoding, paired with the joint power and bit\nallocation, achieves superior decoding performance relative to conventional\napproaches, while operating at significantly lower complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work addresses the joint optimization of power and bit allocation in\nprecoded large-scale n x n MIMO systems with discrete input alphabets,\nspecifically QAM constellations. We propose an adaptive QAM scheme that\nmaintains a fixed gap to the Gaussian-input capacity for a given n. A key\nfinding is that, under the proposed scheme, the mercury/waterfilling (MWF)\nsolution reduces analytically to the classical water-filling (WF) policy.\nFurthermore, the adaptive QAM configuration can be precomputed under the\nlarge-system assumption, enabling the replacement of full SVD with truncated\nSVD and yielding substantial computational savings. To support practical\ndeployment, we develop a bit-allocation algorithm that meets a target\ntransmission data rate while minimizing the overall decoding error rate and\npreserving computational complexity at O(n log n). Simulation results confirm\nthat the proposed truncated SVD precoding, paired with the joint power and bit\nallocation, achieves superior decoding performance relative to conventional\napproaches, while operating at significantly lower complexity."
                },
                "authors": [
                    {
                        "name": "Shuiyin Liu"
                    },
                    {
                        "name": "Amin Sakzad"
                    }
                ],
                "author_detail": {
                    "name": "Amin Sakzad"
                },
                "author": "Amin Sakzad",
                "arxiv_comment": "6 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11260v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11260v3",
                "updated": "2025-04-18T07:15:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    7,
                    15,
                    33,
                    4,
                    108,
                    0
                ],
                "published": "2024-06-17T07:00:41Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    7,
                    0,
                    41,
                    0,
                    169,
                    0
                ],
                "title": "Adversarial Style Augmentation via Large Language Model for Robust Fake\n  News Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Style Augmentation via Large Language Model for Robust Fake\n  News Detection"
                },
                "summary": "The spread of fake news harms individuals and presents a critical social\nchallenge that must be addressed. Although numerous algorithmic and insightful\nfeatures have been developed to detect fake news, many of these features can be\nmanipulated with style-conversion attacks, especially with the emergence of\nadvanced language models, making it more difficult to differentiate from\ngenuine news. This study proposes adversarial style augmentation, AdStyle,\ndesigned to train a fake news detector that remains robust against various\nstyle-conversion attacks. The primary mechanism involves the strategic use of\nLLMs to automatically generate a diverse and coherent array of style-conversion\nattack prompts, enhancing the generation of particularly challenging prompts\nfor the detector. Experiments indicate that our augmentation strategy\nsignificantly improves robustness and detection performance when evaluated on\nfake news benchmark datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The spread of fake news harms individuals and presents a critical social\nchallenge that must be addressed. Although numerous algorithmic and insightful\nfeatures have been developed to detect fake news, many of these features can be\nmanipulated with style-conversion attacks, especially with the emergence of\nadvanced language models, making it more difficult to differentiate from\ngenuine news. This study proposes adversarial style augmentation, AdStyle,\ndesigned to train a fake news detector that remains robust against various\nstyle-conversion attacks. The primary mechanism involves the strategic use of\nLLMs to automatically generate a diverse and coherent array of style-conversion\nattack prompts, enhancing the generation of particularly challenging prompts\nfor the detector. Experiments indicate that our augmentation strategy\nsignificantly improves robustness and detection performance when evaluated on\nfake news benchmark datasets."
                },
                "authors": [
                    {
                        "name": "Sungwon Park"
                    },
                    {
                        "name": "Sungwon Han"
                    },
                    {
                        "name": "Xing Xie"
                    },
                    {
                        "name": "Jae-Gil Lee"
                    },
                    {
                        "name": "Meeyoung Cha"
                    }
                ],
                "author_detail": {
                    "name": "Meeyoung Cha"
                },
                "author": "Meeyoung Cha",
                "arxiv_comment": "WWW'25 research track accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11260v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11260v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13517v1",
                "updated": "2025-04-18T07:10:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    7,
                    10,
                    48,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T07:10:48Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    7,
                    10,
                    48,
                    4,
                    108,
                    0
                ],
                "title": "Optimizing Electric Vehicle Charging Station Locations: A Data-driven\n  System with Multi-source Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Electric Vehicle Charging Station Locations: A Data-driven\n  System with Multi-source Fusion"
                },
                "summary": "With the growing electric vehicles (EVs) charging demand, urban planners face\nthe challenges of providing charging infrastructure at optimal locations. For\nexample, range anxiety during long-distance travel and the inadequate\ndistribution of residential charging stations are the major issues many cities\nface. To achieve reasonable estimation and deployment of the charging demand,\nwe develop a data-driven system based on existing EV trips in New South Wales\n(NSW) state, Australia, incorporating multiple factors that enhance the\ngeographical feasibility of recommended charging stations. Our system\nintegrates data sources including EV trip data, geographical data such as route\ndata and Local Government Area (LGA) boundaries, as well as features like fire\nand flood risks, and Points of Interest (POIs). We visualize our results to\nintuitively demonstrate the findings from our data-driven, multi-source fusion\nsystem, and evaluate them through case studies. The outcome of this work can\nprovide a platform for discussion to develop new insights that could be used to\ngive guidance on where to position future EV charging stations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing electric vehicles (EVs) charging demand, urban planners face\nthe challenges of providing charging infrastructure at optimal locations. For\nexample, range anxiety during long-distance travel and the inadequate\ndistribution of residential charging stations are the major issues many cities\nface. To achieve reasonable estimation and deployment of the charging demand,\nwe develop a data-driven system based on existing EV trips in New South Wales\n(NSW) state, Australia, incorporating multiple factors that enhance the\ngeographical feasibility of recommended charging stations. Our system\nintegrates data sources including EV trip data, geographical data such as route\ndata and Local Government Area (LGA) boundaries, as well as features like fire\nand flood risks, and Points of Interest (POIs). We visualize our results to\nintuitively demonstrate the findings from our data-driven, multi-source fusion\nsystem, and evaluate them through case studies. The outcome of this work can\nprovide a platform for discussion to develop new insights that could be used to\ngive guidance on where to position future EV charging stations."
                },
                "authors": [
                    {
                        "name": "Lihuan Li"
                    },
                    {
                        "name": "Du Yin"
                    },
                    {
                        "name": "Hao Xue"
                    },
                    {
                        "name": "David Lillo-Trynes"
                    },
                    {
                        "name": "Flora Salim"
                    }
                ],
                "author_detail": {
                    "name": "Flora Salim"
                },
                "author": "Flora Salim",
                "arxiv_comment": "4-page short paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13515v1",
                "updated": "2025-04-18T07:09:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    7,
                    9,
                    56,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T07:09:56Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    7,
                    9,
                    56,
                    4,
                    108,
                    0
                ],
                "title": "Large Language Models for Validating Network Protocol Parsers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Validating Network Protocol Parsers"
                },
                "summary": "Network protocol parsers are essential for enabling correct and secure\ncommunication between devices. Bugs in these parsers can introduce critical\nvulnerabilities, including memory corruption, information leakage, and\ndenial-of-service attacks. An intuitive way to assess parser correctness is to\ncompare the implementation with its official protocol standard. However, this\ncomparison is challenging because protocol standards are typically written in\nnatural language, whereas implementations are in source code. Existing methods\nlike model checking, fuzzing, and differential testing have been used to find\nparsing bugs, but they either require significant manual effort or ignore the\nprotocol standards, limiting their ability to detect semantic violations. To\nenable more automated validation of parser implementations against protocol\nstandards, we propose PARVAL, a multi-agent framework built on large language\nmodels (LLMs). PARVAL leverages the capabilities of LLMs to understand both\nnatural language and code. It transforms both protocol standards and their\nimplementations into a unified intermediate representation, referred to as\nformat specifications, and performs a differential comparison to uncover\ninconsistencies. We evaluate PARVAL on the Bidirectional Forwarding Detection\n(BFD) protocol. Our experiments demonstrate that PARVAL successfully identifies\ninconsistencies between the implementation and its RFC standard, achieving a\nlow false positive rate of 5.6%. PARVAL uncovers seven unique bugs, including\nfive previously unknown issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network protocol parsers are essential for enabling correct and secure\ncommunication between devices. Bugs in these parsers can introduce critical\nvulnerabilities, including memory corruption, information leakage, and\ndenial-of-service attacks. An intuitive way to assess parser correctness is to\ncompare the implementation with its official protocol standard. However, this\ncomparison is challenging because protocol standards are typically written in\nnatural language, whereas implementations are in source code. Existing methods\nlike model checking, fuzzing, and differential testing have been used to find\nparsing bugs, but they either require significant manual effort or ignore the\nprotocol standards, limiting their ability to detect semantic violations. To\nenable more automated validation of parser implementations against protocol\nstandards, we propose PARVAL, a multi-agent framework built on large language\nmodels (LLMs). PARVAL leverages the capabilities of LLMs to understand both\nnatural language and code. It transforms both protocol standards and their\nimplementations into a unified intermediate representation, referred to as\nformat specifications, and performs a differential comparison to uncover\ninconsistencies. We evaluate PARVAL on the Bidirectional Forwarding Detection\n(BFD) protocol. Our experiments demonstrate that PARVAL successfully identifies\ninconsistencies between the implementation and its RFC standard, achieving a\nlow false positive rate of 5.6%. PARVAL uncovers seven unique bugs, including\nfive previously unknown issues."
                },
                "authors": [
                    {
                        "name": "Mingwei Zheng"
                    },
                    {
                        "name": "Danning Xie"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhang"
                },
                "author": "Xiangyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00383v2",
                "updated": "2025-04-18T06:58:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    58,
                    12,
                    4,
                    108,
                    0
                ],
                "published": "2024-11-30T07:21:02Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    7,
                    21,
                    2,
                    5,
                    335,
                    0
                ],
                "title": "Unified Parameter-Efficient Unlearning for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Parameter-Efficient Unlearning for LLMs"
                },
                "summary": "The advent of Large Language Models (LLMs) has revolutionized natural\nlanguage processing, enabling advanced understanding and reasoning capabilities\nacross a variety of tasks. Fine-tuning these models for specific domains,\nparticularly through Parameter-Efficient Fine-Tuning (PEFT) strategies like\nLoRA, has become a prevalent practice due to its efficiency. However, this\nraises significant privacy and security concerns, as models may inadvertently\nretain and disseminate sensitive or undesirable information. To address these\nissues, we introduce a novel instance-wise unlearning framework, LLMEraser,\nwhich systematically categorizes unlearning tasks and applies precise parameter\nadjustments using influence functions. Unlike traditional unlearning techniques\nthat are often limited in scope and require extensive retraining, LLMEraser is\ndesigned to handle a broad spectrum of unlearning tasks without compromising\nmodel performance. Extensive experiments on benchmark datasets demonstrate that\nLLMEraser excels in efficiently managing various unlearning scenarios while\nmaintaining the overall integrity and efficacy of the models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has revolutionized natural\nlanguage processing, enabling advanced understanding and reasoning capabilities\nacross a variety of tasks. Fine-tuning these models for specific domains,\nparticularly through Parameter-Efficient Fine-Tuning (PEFT) strategies like\nLoRA, has become a prevalent practice due to its efficiency. However, this\nraises significant privacy and security concerns, as models may inadvertently\nretain and disseminate sensitive or undesirable information. To address these\nissues, we introduce a novel instance-wise unlearning framework, LLMEraser,\nwhich systematically categorizes unlearning tasks and applies precise parameter\nadjustments using influence functions. Unlike traditional unlearning techniques\nthat are often limited in scope and require extensive retraining, LLMEraser is\ndesigned to handle a broad spectrum of unlearning tasks without compromising\nmodel performance. Extensive experiments on benchmark datasets demonstrate that\nLLMEraser excels in efficiently managing various unlearning scenarios while\nmaintaining the overall integrity and efficacy of the models."
                },
                "authors": [
                    {
                        "name": "Chenlu Ding"
                    },
                    {
                        "name": "Jiancan Wu"
                    },
                    {
                        "name": "Yancheng Yuan"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Alex Su"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13500v1",
                "updated": "2025-04-18T06:42:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    42,
                    30,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T06:42:30Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    42,
                    30,
                    4,
                    108,
                    0
                ],
                "title": "Prejudge-Before-Think: Enhancing Large Language Models at Test-Time by\n  Process Prejudge Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prejudge-Before-Think: Enhancing Large Language Models at Test-Time by\n  Process Prejudge Reasoning"
                },
                "summary": "In this paper, we introduce a new \\emph{process prejudge} strategy in LLM\nreasoning to demonstrate that bootstrapping with process prejudge allows the\nLLM to adaptively anticipate the errors encountered when advancing the\nsubsequent reasoning steps, similar to people sometimes pausing to think about\nwhat mistakes may occur and how to avoid them, rather than relying solely on\ntrial and error. Specifically, we define a prejudge node in the rationale,\nwhich represents a reasoning step, with at least one step that follows the\nprejudge node that has no paths toward the correct answer. To synthesize the\nprejudge reasoning process, we present an automated reasoning framework with a\ndynamic tree-searching strategy. This framework requires only one LLM to\nperform answer judging, response critiquing, prejudge generation, and thought\ncompletion. Furthermore, we develop a two-phase training mechanism with\nsupervised fine-tuning (SFT) and reinforcement learning (RL) to further enhance\nthe reasoning capabilities of LLMs. Experimental results from competition-level\ncomplex reasoning demonstrate that our method can teach the model to prejudge\nbefore thinking and significantly enhance the reasoning ability of LLMs. Code\nand data is released at https://github.com/wjn1996/Prejudge-Before-Think.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new \\emph{process prejudge} strategy in LLM\nreasoning to demonstrate that bootstrapping with process prejudge allows the\nLLM to adaptively anticipate the errors encountered when advancing the\nsubsequent reasoning steps, similar to people sometimes pausing to think about\nwhat mistakes may occur and how to avoid them, rather than relying solely on\ntrial and error. Specifically, we define a prejudge node in the rationale,\nwhich represents a reasoning step, with at least one step that follows the\nprejudge node that has no paths toward the correct answer. To synthesize the\nprejudge reasoning process, we present an automated reasoning framework with a\ndynamic tree-searching strategy. This framework requires only one LLM to\nperform answer judging, response critiquing, prejudge generation, and thought\ncompletion. Furthermore, we develop a two-phase training mechanism with\nsupervised fine-tuning (SFT) and reinforcement learning (RL) to further enhance\nthe reasoning capabilities of LLMs. Experimental results from competition-level\ncomplex reasoning demonstrate that our method can teach the model to prejudge\nbefore thinking and significantly enhance the reasoning ability of LLMs. Code\nand data is released at https://github.com/wjn1996/Prejudge-Before-Think."
                },
                "authors": [
                    {
                        "name": "Jianing Wang"
                    },
                    {
                        "name": "Jin Jiang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Mengdi Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xunliang Cai"
                },
                "author": "Xunliang Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11506v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11506v2",
                "updated": "2025-04-18T05:59:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    59,
                    29,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-15T08:22:35Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    8,
                    22,
                    35,
                    1,
                    105,
                    0
                ],
                "title": "Cross-cultural Deployment of Autonomous Vehicles Using Data-light\n  Inverse Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-cultural Deployment of Autonomous Vehicles Using Data-light\n  Inverse Reinforcement Learning"
                },
                "summary": "More than the adherence to specific traffic regulations, driving culture\ntouches upon a more implicit part - an informal, conventional, collective\nbehavioral pattern followed by drivers - that varies across countries, regions,\nand even cities. Such cultural divergence has become one of the biggest\nchallenges in deploying autonomous vehicles (AVs) across diverse regions today.\nThe current emergence of data-driven methods has shown a potential solution to\nenable culture-compatible driving through learning from data, but what if some\nunderdeveloped regions cannot provide sufficient local data to inform driving\nculture? This issue is particularly significant for a broader global AV market.\nHere, we propose a cross-cultural deployment scheme for AVs, called data-light\ninverse reinforcement learning, designed to re-calibrate culture-specific AVs\nand assimilate them into other cultures. First, we report the divergence in\ndriving cultures through a comprehensive comparative analysis of naturalistic\ndriving datasets on highways from three countries: Germany, China, and the USA.\nThen, we demonstrate the effectiveness of our scheme by testing the expeditious\ncross-cultural deployment across these three countries, with cumulative testing\nmileage of over 56084 km. The performance is particularly advantageous when\ncross-cultural deployment is carried out without affluent local data. Results\nshow that we can reduce the dependence on local data by a margin of 98.67% at\nbest. This study is expected to bring a broader, fairer AV global market,\nparticularly in those regions that lack enough local data to develop\nculture-compatible AVs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More than the adherence to specific traffic regulations, driving culture\ntouches upon a more implicit part - an informal, conventional, collective\nbehavioral pattern followed by drivers - that varies across countries, regions,\nand even cities. Such cultural divergence has become one of the biggest\nchallenges in deploying autonomous vehicles (AVs) across diverse regions today.\nThe current emergence of data-driven methods has shown a potential solution to\nenable culture-compatible driving through learning from data, but what if some\nunderdeveloped regions cannot provide sufficient local data to inform driving\nculture? This issue is particularly significant for a broader global AV market.\nHere, we propose a cross-cultural deployment scheme for AVs, called data-light\ninverse reinforcement learning, designed to re-calibrate culture-specific AVs\nand assimilate them into other cultures. First, we report the divergence in\ndriving cultures through a comprehensive comparative analysis of naturalistic\ndriving datasets on highways from three countries: Germany, China, and the USA.\nThen, we demonstrate the effectiveness of our scheme by testing the expeditious\ncross-cultural deployment across these three countries, with cumulative testing\nmileage of over 56084 km. The performance is particularly advantageous when\ncross-cultural deployment is carried out without affluent local data. Results\nshow that we can reduce the dependence on local data by a margin of 98.67% at\nbest. This study is expected to bring a broader, fairer AV global market,\nparticularly in those regions that lack enough local data to develop\nculture-compatible AVs."
                },
                "authors": [
                    {
                        "name": "Hongliang Lu"
                    },
                    {
                        "name": "Shuqi Shen"
                    },
                    {
                        "name": "Junjie Yang"
                    },
                    {
                        "name": "Chao Lu"
                    },
                    {
                        "name": "Xinhu Zheng"
                    },
                    {
                        "name": "Hai Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hai Yang"
                },
                "author": "Hai Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11506v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11506v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13475v1",
                "updated": "2025-04-18T05:35:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    35,
                    11,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T05:35:11Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    35,
                    11,
                    4,
                    108,
                    0
                ],
                "title": "LLM Sensitivity Evaluation Framework for Clinical Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Sensitivity Evaluation Framework for Clinical Diagnosis"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive performance across\nvarious domains. However, for clinical diagnosis, higher expectations are\nrequired for LLM's reliability and sensitivity: thinking like physicians and\nremaining sensitive to key medical information that affects diagnostic\nreasoning, as subtle variations can lead to different diagnosis results. Yet,\nexisting works focus mainly on investigating the sensitivity of LLMs to\nirrelevant context and overlook the importance of key information. In this\npaper, we investigate the sensitivity of LLMs, i.e. GPT-3.5, GPT-4, Gemini,\nClaude3 and LLaMA2-7b, to key medical information by introducing different\nperturbation strategies. The evaluation results highlight the limitations of\ncurrent LLMs in remaining sensitive to key medical information for diagnostic\ndecision-making. The evolution of LLMs must focus on improving their\nreliability, enhancing their ability to be sensitive to key information, and\neffectively utilizing this information. These improvements will enhance human\ntrust in LLMs and facilitate their practical application in real-world\nscenarios. Our code and dataset are available at\nhttps://github.com/chenwei23333/DiagnosisQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive performance across\nvarious domains. However, for clinical diagnosis, higher expectations are\nrequired for LLM's reliability and sensitivity: thinking like physicians and\nremaining sensitive to key medical information that affects diagnostic\nreasoning, as subtle variations can lead to different diagnosis results. Yet,\nexisting works focus mainly on investigating the sensitivity of LLMs to\nirrelevant context and overlook the importance of key information. In this\npaper, we investigate the sensitivity of LLMs, i.e. GPT-3.5, GPT-4, Gemini,\nClaude3 and LLaMA2-7b, to key medical information by introducing different\nperturbation strategies. The evaluation results highlight the limitations of\ncurrent LLMs in remaining sensitive to key medical information for diagnostic\ndecision-making. The evolution of LLMs must focus on improving their\nreliability, enhancing their ability to be sensitive to key information, and\neffectively utilizing this information. These improvements will enhance human\ntrust in LLMs and facilitate their practical application in real-world\nscenarios. Our code and dataset are available at\nhttps://github.com/chenwei23333/DiagnosisQA."
                },
                "authors": [
                    {
                        "name": "Chenwei Yan"
                    },
                    {
                        "name": "Xiangling Fu"
                    },
                    {
                        "name": "Yuxuan Xiong"
                    },
                    {
                        "name": "Tianyi Wang"
                    },
                    {
                        "name": "Siu Cheung Hui"
                    },
                    {
                        "name": "Ji Wu"
                    },
                    {
                        "name": "Xien Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xien Liu"
                },
                "author": "Xien Liu",
                "arxiv_journal_ref": "Proceedings of the 31st International Conference on Computational\n  Linguistics, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13474v1",
                "updated": "2025-04-18T05:32:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    32,
                    47,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T05:32:47Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    32,
                    47,
                    4,
                    108,
                    0
                ],
                "title": "Everything You Wanted to Know About LLM-based Vulnerability Detection\n  But Were Afraid to Ask",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Everything You Wanted to Know About LLM-based Vulnerability Detection\n  But Were Afraid to Ask"
                },
                "summary": "Large Language Models are a promising tool for automated vulnerability\ndetection, thanks to their success in code generation and repair. However,\ndespite widespread adoption, a critical question remains: Are LLMs truly\neffective at detecting real-world vulnerabilities? Current evaluations, which\noften assess models on isolated functions or files, ignore the broader\nexecution and data-flow context essential for understanding vulnerabilities.\nThis oversight leads to two types of misleading outcomes: incorrect conclusions\nand flawed rationales, collectively undermining the reliability of prior\nassessments. Therefore, in this paper, we challenge three widely held community\nbeliefs: that LLMs are (i) unreliable, (ii) insensitive to code patches, and\n(iii) performance-plateaued across model scales. We argue that these beliefs\nare artifacts of context-deprived evaluations. To address this, we propose\nCORRECT (Context-Rich Reasoning Evaluation of Code with Trust), a new\nevaluation framework that systematically incorporates contextual information\ninto LLM-based vulnerability detection. We construct a context-rich dataset of\n2,000 vulnerable-patched program pairs spanning 99 CWEs and evaluate 13 LLMs\nacross four model families. Our framework elicits both binary predictions and\nnatural-language rationales, which are further validated using LLM-as-a-judge\ntechniques. Our findings overturn existing misconceptions. When provided with\nsufficient context, SOTA LLMs achieve significantly improved performance (e.g.,\n0.7 F1-score on key CWEs), with 0.8 precision. We show that most false\npositives stem from reasoning errors rather than misclassification, and that\nwhile model and test-time scaling improve performance, they introduce\ndiminishing returns and trade-offs in recall. Finally, we uncover new flaws in\ncurrent LLM-based detection systems, such as limited generalization and\noverthinking biases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are a promising tool for automated vulnerability\ndetection, thanks to their success in code generation and repair. However,\ndespite widespread adoption, a critical question remains: Are LLMs truly\neffective at detecting real-world vulnerabilities? Current evaluations, which\noften assess models on isolated functions or files, ignore the broader\nexecution and data-flow context essential for understanding vulnerabilities.\nThis oversight leads to two types of misleading outcomes: incorrect conclusions\nand flawed rationales, collectively undermining the reliability of prior\nassessments. Therefore, in this paper, we challenge three widely held community\nbeliefs: that LLMs are (i) unreliable, (ii) insensitive to code patches, and\n(iii) performance-plateaued across model scales. We argue that these beliefs\nare artifacts of context-deprived evaluations. To address this, we propose\nCORRECT (Context-Rich Reasoning Evaluation of Code with Trust), a new\nevaluation framework that systematically incorporates contextual information\ninto LLM-based vulnerability detection. We construct a context-rich dataset of\n2,000 vulnerable-patched program pairs spanning 99 CWEs and evaluate 13 LLMs\nacross four model families. Our framework elicits both binary predictions and\nnatural-language rationales, which are further validated using LLM-as-a-judge\ntechniques. Our findings overturn existing misconceptions. When provided with\nsufficient context, SOTA LLMs achieve significantly improved performance (e.g.,\n0.7 F1-score on key CWEs), with 0.8 precision. We show that most false\npositives stem from reasoning errors rather than misclassification, and that\nwhile model and test-time scaling improve performance, they introduce\ndiminishing returns and trade-offs in recall. Finally, we uncover new flaws in\ncurrent LLM-based detection systems, such as limited generalization and\noverthinking biases."
                },
                "authors": [
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Minghui Xu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Xiuzhen Cheng"
                    },
                    {
                        "name": "Fengyuan Xu"
                    },
                    {
                        "name": "Sheng Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Zhong"
                },
                "author": "Sheng Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13472v1",
                "updated": "2025-04-18T05:26:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    26,
                    32,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T05:26:32Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    26,
                    32,
                    4,
                    108,
                    0
                ],
                "title": "CodeVisionary: An Agent-based Framework for Evaluating Large Language\n  Models in Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeVisionary: An Agent-based Framework for Evaluating Large Language\n  Models in Code Generation"
                },
                "summary": "Large language models (LLMs) have demonstrated strong capabilities in code\ngeneration, underscoring the critical need for rigorous and comprehensive\nevaluation. Existing evaluation approaches fall into three categories,\nincluding human-centered, metric-based, and LLM-based. Considering that\nhuman-centered approaches are labour-intensive and metric-based ones overly\nrely on reference answers, LLM-based approaches are gaining increasing\nattention due to their stronger contextual understanding capabilities and\nsuperior efficiency. However, the performance of LLM-based approaches remains\nlimited due to: (1) lack of multisource domain knowledge, and (2) insufficient\ncomprehension of complex code.\n  To mitigate the limitations, we propose CodeVisionary, the first LLM-based\nagent framework for evaluating LLMs in code generation. CodeVisionary consists\nof two stages: (1) Multiscore knowledge analysis stage, which aims to gather\nmultisource and comprehensive domain knowledge by formulating and executing a\nstepwise evaluation plan. (2) Negotiation-based scoring stage, which involves\nmultiple judges engaging in discussions to better comprehend the complex code\nand reach a consensus on the evaluation score. Extensive experiments\ndemonstrate that CodeVisionary achieves the best performance for evaluating\nLLMs in code generation, outperforming the best baseline methods with average\nimprovements of 0.202, 0.139, and 0.117 in Pearson, Spearman, and Kendall-Tau\ncoefficients, respectively. Besides, CodeVisionary provides detailed evaluation\nreports, which assist developers in identifying shortcomings and making\nimprovements. The resources of CodeVisionary are available at\nhttps://anonymous.4open.science/r/CodeVisionary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong capabilities in code\ngeneration, underscoring the critical need for rigorous and comprehensive\nevaluation. Existing evaluation approaches fall into three categories,\nincluding human-centered, metric-based, and LLM-based. Considering that\nhuman-centered approaches are labour-intensive and metric-based ones overly\nrely on reference answers, LLM-based approaches are gaining increasing\nattention due to their stronger contextual understanding capabilities and\nsuperior efficiency. However, the performance of LLM-based approaches remains\nlimited due to: (1) lack of multisource domain knowledge, and (2) insufficient\ncomprehension of complex code.\n  To mitigate the limitations, we propose CodeVisionary, the first LLM-based\nagent framework for evaluating LLMs in code generation. CodeVisionary consists\nof two stages: (1) Multiscore knowledge analysis stage, which aims to gather\nmultisource and comprehensive domain knowledge by formulating and executing a\nstepwise evaluation plan. (2) Negotiation-based scoring stage, which involves\nmultiple judges engaging in discussions to better comprehend the complex code\nand reach a consensus on the evaluation score. Extensive experiments\ndemonstrate that CodeVisionary achieves the best performance for evaluating\nLLMs in code generation, outperforming the best baseline methods with average\nimprovements of 0.202, 0.139, and 0.117 in Pearson, Spearman, and Kendall-Tau\ncoefficients, respectively. Besides, CodeVisionary provides detailed evaluation\nreports, which assist developers in identifying shortcomings and making\nimprovements. The resources of CodeVisionary are available at\nhttps://anonymous.4open.science/r/CodeVisionary."
                },
                "authors": [
                    {
                        "name": "Xinchen Wang"
                    },
                    {
                        "name": "Pengfei Gao"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Ruida Hu"
                    },
                    {
                        "name": "Cuiyun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Cuiyun Gao"
                },
                "author": "Cuiyun Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13471v1",
                "updated": "2025-04-18T05:25:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    25,
                    22,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T05:25:22Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    25,
                    22,
                    4,
                    108,
                    0
                ],
                "title": "From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient\n  LLMs"
                },
                "summary": "In recent years, Large Language Models (LLMs) have significantly advanced\nartificial intelligence by optimizing traditional Natural Language Processing\n(NLP) pipelines, improving performance and generalization. This has spurred\ntheir integration into various systems. Many NLP systems, including ours,\nemploy a \"one-stage\" pipeline directly incorporating LLMs. While effective,\nthis approach incurs substantial costs and latency due to the need for large\nmodel parameters to achieve satisfactory outcomes. This paper introduces a\nthree-stage cost-efficient end-to-end LLM deployment pipeline-including\nprototyping, knowledge transfer, and model compression-to tackle the\ncost-performance dilemma in LLM-based frameworks. Our approach yields a super\ntiny model optimized for cost and performance in online systems, simplifying\nthe system architecture. Initially, by transforming complex tasks into a\nfunction call-based LLM-driven pipeline, an optimal performance prototype\nsystem is constructed to produce high-quality data as a teacher model. The\nsecond stage combine techniques like rejection fine-tuning, reinforcement\nlearning and knowledge distillation to transfer knowledge to a smaller 0.5B\nstudent model, delivering effective performance at minimal cost. The final\nstage applies quantization and pruning to extremely compress model to 0.4B,\nachieving ultra-low latency and cost. The framework's modular design and\ncross-domain capabilities suggest potential applicability in other NLP areas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have significantly advanced\nartificial intelligence by optimizing traditional Natural Language Processing\n(NLP) pipelines, improving performance and generalization. This has spurred\ntheir integration into various systems. Many NLP systems, including ours,\nemploy a \"one-stage\" pipeline directly incorporating LLMs. While effective,\nthis approach incurs substantial costs and latency due to the need for large\nmodel parameters to achieve satisfactory outcomes. This paper introduces a\nthree-stage cost-efficient end-to-end LLM deployment pipeline-including\nprototyping, knowledge transfer, and model compression-to tackle the\ncost-performance dilemma in LLM-based frameworks. Our approach yields a super\ntiny model optimized for cost and performance in online systems, simplifying\nthe system architecture. Initially, by transforming complex tasks into a\nfunction call-based LLM-driven pipeline, an optimal performance prototype\nsystem is constructed to produce high-quality data as a teacher model. The\nsecond stage combine techniques like rejection fine-tuning, reinforcement\nlearning and knowledge distillation to transfer knowledge to a smaller 0.5B\nstudent model, delivering effective performance at minimal cost. The final\nstage applies quantization and pruning to extremely compress model to 0.4B,\nachieving ultra-low latency and cost. The framework's modular design and\ncross-domain capabilities suggest potential applicability in other NLP areas."
                },
                "authors": [
                    {
                        "name": "Jiliang Ni"
                    },
                    {
                        "name": "Jiachen Pu"
                    },
                    {
                        "name": "Zhongyi Yang"
                    },
                    {
                        "name": "Kun Zhou"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Xiaoliang Xiao"
                    },
                    {
                        "name": "Dakui Wang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Jingfeng Luo"
                    },
                    {
                        "name": "Conggang Hu"
                    }
                ],
                "author_detail": {
                    "name": "Conggang Hu"
                },
                "author": "Conggang Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15288v2",
                "updated": "2025-04-18T04:46:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    4,
                    46,
                    0,
                    4,
                    108,
                    0
                ],
                "published": "2024-10-20T05:02:18Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    5,
                    2,
                    18,
                    6,
                    294,
                    0
                ],
                "title": "If LLMs Would Just Look: Simple Line-by-line Checking Improves\n  Vulnerability Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "If LLMs Would Just Look: Simple Line-by-line Checking Improves\n  Vulnerability Localization"
                },
                "summary": "The rapid expansion of software systems and the growing number of reported\nvulnerabilities have emphasized the importance of accurately identifying\nvulnerable code segments. Traditional methods for vulnerability localization,\nsuch as manual code audits or rule-based tools, are often time-consuming and\nlimited in scope, typically focusing on specific programming languages or types\nof vulnerabilities. In recent years, the introduction of large language models\n(LLMs) such as GPT and LLaMA has opened new possibilities for automating\nvulnerability detection. However, while LLMs show promise in this area, they\nface challenges, particularly in maintaining accuracy over longer code\ncontexts. This paper introduces LOVA, a novel framework leveraging the\nself-attention mechanisms inherent in LLMs to enhance vulnerability\nlocalization. Our key insight is that self-attention mechanisms assign varying\nimportance to different parts of the input, making it possible to track how\nmuch attention the model focuses on specific lines of code. In the context of\nvulnerability localization, the hypothesis is that vulnerable lines of code\nwill naturally attract higher attention weights because they have a greater\ninfluence on the model's output. By systematically tracking changes in\nattention weights and focusing on specific lines of code, LOVA improves the\nprecision of identifying vulnerable lines across various programming languages.\nThrough rigorous experimentation and evaluation, we demonstrate that LOVA\nsignificantly outperforms existing LLM-based approaches, achieving up to a 5.3x\nimprovement in F1-scores. LOVA also demonstrated strong scalability, with up to\na 14.6x improvement in smart contract vulnerability localization across\nlanguages like C, Python, Java, and Solidity. Its robustness was proven through\nconsistent performance across different LLM architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of software systems and the growing number of reported\nvulnerabilities have emphasized the importance of accurately identifying\nvulnerable code segments. Traditional methods for vulnerability localization,\nsuch as manual code audits or rule-based tools, are often time-consuming and\nlimited in scope, typically focusing on specific programming languages or types\nof vulnerabilities. In recent years, the introduction of large language models\n(LLMs) such as GPT and LLaMA has opened new possibilities for automating\nvulnerability detection. However, while LLMs show promise in this area, they\nface challenges, particularly in maintaining accuracy over longer code\ncontexts. This paper introduces LOVA, a novel framework leveraging the\nself-attention mechanisms inherent in LLMs to enhance vulnerability\nlocalization. Our key insight is that self-attention mechanisms assign varying\nimportance to different parts of the input, making it possible to track how\nmuch attention the model focuses on specific lines of code. In the context of\nvulnerability localization, the hypothesis is that vulnerable lines of code\nwill naturally attract higher attention weights because they have a greater\ninfluence on the model's output. By systematically tracking changes in\nattention weights and focusing on specific lines of code, LOVA improves the\nprecision of identifying vulnerable lines across various programming languages.\nThrough rigorous experimentation and evaluation, we demonstrate that LOVA\nsignificantly outperforms existing LLM-based approaches, achieving up to a 5.3x\nimprovement in F1-scores. LOVA also demonstrated strong scalability, with up to\na 14.6x improvement in smart contract vulnerability localization across\nlanguages like C, Python, Java, and Solidity. Its robustness was proven through\nconsistent performance across different LLM architectures."
                },
                "authors": [
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Xiuzhen Cheng"
                    },
                    {
                        "name": "Yating Liu"
                    },
                    {
                        "name": "Fengyuan Xu"
                    },
                    {
                        "name": "Sheng Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Zhong"
                },
                "author": "Sheng Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13461v1",
                "updated": "2025-04-18T04:38:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    4,
                    38,
                    51,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T04:38:51Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    4,
                    38,
                    51,
                    4,
                    108,
                    0
                ],
                "title": "An Addendum to NeBula: Towards Extending TEAM CoSTAR's Solution to\n  Larger Scale Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Addendum to NeBula: Towards Extending TEAM CoSTAR's Solution to\n  Larger Scale Environments"
                },
                "summary": "This paper presents an appendix to the original NeBula autonomy solution\ndeveloped by the TEAM CoSTAR (Collaborative SubTerranean Autonomous Robots),\nparticipating in the DARPA Subterranean Challenge. Specifically, this paper\npresents extensions to NeBula's hardware, software, and algorithmic components\nthat focus on increasing the range and scale of the exploration environment.\nFrom the algorithmic perspective, we discuss the following extensions to the\noriginal NeBula framework: (i) large-scale geometric and semantic environment\nmapping; (ii) an adaptive positioning system; (iii) probabilistic\ntraversability analysis and local planning; (iv) large-scale POMDP-based global\nmotion planning and exploration behavior; (v) large-scale networking and\ndecentralized reasoning; (vi) communication-aware mission planning; and (vii)\nmulti-modal ground-aerial exploration solutions. We demonstrate the application\nand deployment of the presented systems and solutions in various large-scale\nunderground environments, including limestone mine exploration scenarios as\nwell as deployment in the DARPA Subterranean challenge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an appendix to the original NeBula autonomy solution\ndeveloped by the TEAM CoSTAR (Collaborative SubTerranean Autonomous Robots),\nparticipating in the DARPA Subterranean Challenge. Specifically, this paper\npresents extensions to NeBula's hardware, software, and algorithmic components\nthat focus on increasing the range and scale of the exploration environment.\nFrom the algorithmic perspective, we discuss the following extensions to the\noriginal NeBula framework: (i) large-scale geometric and semantic environment\nmapping; (ii) an adaptive positioning system; (iii) probabilistic\ntraversability analysis and local planning; (iv) large-scale POMDP-based global\nmotion planning and exploration behavior; (v) large-scale networking and\ndecentralized reasoning; (vi) communication-aware mission planning; and (vii)\nmulti-modal ground-aerial exploration solutions. We demonstrate the application\nand deployment of the presented systems and solutions in various large-scale\nunderground environments, including limestone mine exploration scenarios as\nwell as deployment in the DARPA Subterranean challenge."
                },
                "authors": [
                    {
                        "name": "Ali Agha"
                    },
                    {
                        "name": "Kyohei Otsu"
                    },
                    {
                        "name": "Benjamin Morrell"
                    },
                    {
                        "name": "David D. Fan"
                    },
                    {
                        "name": "Sung-Kyun Kim"
                    },
                    {
                        "name": "Muhammad Fadhil Ginting"
                    },
                    {
                        "name": "Xianmei Lei"
                    },
                    {
                        "name": "Jeffrey Edlund"
                    },
                    {
                        "name": "Seyed Fakoorian"
                    },
                    {
                        "name": "Amanda Bouman"
                    },
                    {
                        "name": "Fernando Chavez"
                    },
                    {
                        "name": "Taeyeon Kim"
                    },
                    {
                        "name": "Gustavo J. Correa"
                    },
                    {
                        "name": "Maira Saboia"
                    },
                    {
                        "name": "Angel Santamaria-Navarro"
                    },
                    {
                        "name": "Brett Lopez"
                    },
                    {
                        "name": "Boseong Kim"
                    },
                    {
                        "name": "Chanyoung Jung"
                    },
                    {
                        "name": "Mamoru Sobue"
                    },
                    {
                        "name": "Oriana Claudia Peltzer"
                    },
                    {
                        "name": "Joshua Ott"
                    },
                    {
                        "name": "Robert Trybula"
                    },
                    {
                        "name": "Thomas Touma"
                    },
                    {
                        "name": "Marcel Kaufmann"
                    },
                    {
                        "name": "Tiago Stegun Vaquero"
                    },
                    {
                        "name": "Torkom Pailevanian"
                    },
                    {
                        "name": "Matteo Palieri"
                    },
                    {
                        "name": "Yun Chang"
                    },
                    {
                        "name": "Andrzej Reinke"
                    },
                    {
                        "name": "Matthew Anderson"
                    },
                    {
                        "name": "Frederik E. T. Schöller"
                    },
                    {
                        "name": "Patrick Spieler"
                    },
                    {
                        "name": "Lillian M. Clark"
                    },
                    {
                        "name": "Avak Archanian"
                    },
                    {
                        "name": "Kenny Chen"
                    },
                    {
                        "name": "Hovhannes Melikyan"
                    },
                    {
                        "name": "Anushri Dixit"
                    },
                    {
                        "name": "Harrison Delecki"
                    },
                    {
                        "name": "Daniel Pastor"
                    },
                    {
                        "name": "Barry Ridge"
                    },
                    {
                        "name": "Nicolas Marchal"
                    },
                    {
                        "name": "Jose Uribe"
                    },
                    {
                        "name": "Sharmita Dey"
                    },
                    {
                        "name": "Kamak Ebadi"
                    },
                    {
                        "name": "Kyle Coble"
                    },
                    {
                        "name": "Alexander Nikitas Dimopoulos"
                    },
                    {
                        "name": "Vivek Thangavelu"
                    },
                    {
                        "name": "Vivek S. Varadharajan"
                    },
                    {
                        "name": "Nicholas Palomo"
                    },
                    {
                        "name": "Antoni Rosinol"
                    },
                    {
                        "name": "Arghya Chatterjee"
                    },
                    {
                        "name": "Christoforos Kanellakis"
                    },
                    {
                        "name": "Bjorn Lindqvist"
                    },
                    {
                        "name": "Micah Corah"
                    },
                    {
                        "name": "Kyle Strickland"
                    },
                    {
                        "name": "Ryan Stonebraker"
                    },
                    {
                        "name": "Michael Milano"
                    },
                    {
                        "name": "Christopher E. Denniston"
                    },
                    {
                        "name": "Sami Sahnoune"
                    },
                    {
                        "name": "Thomas Claudet"
                    },
                    {
                        "name": "Seungwook Lee"
                    },
                    {
                        "name": "Gautam Salhotra"
                    },
                    {
                        "name": "Edward Terry"
                    },
                    {
                        "name": "Rithvik Musuku"
                    },
                    {
                        "name": "Robin Schmid"
                    },
                    {
                        "name": "Tony Tran"
                    },
                    {
                        "name": "Ara Kourchians"
                    },
                    {
                        "name": "Justin Schachter"
                    },
                    {
                        "name": "Hector Azpurua"
                    },
                    {
                        "name": "Levi Resende"
                    },
                    {
                        "name": "Arash Kalantari"
                    },
                    {
                        "name": "Jeremy Nash"
                    },
                    {
                        "name": "Josh Lee"
                    },
                    {
                        "name": "Christopher Patterson"
                    },
                    {
                        "name": "Jennifer G. Blank"
                    },
                    {
                        "name": "Kartik Patath"
                    },
                    {
                        "name": "Yuki Kubo"
                    },
                    {
                        "name": "Ryan Alimo"
                    },
                    {
                        "name": "Yasin Almalioglu"
                    },
                    {
                        "name": "Aaron Curtis"
                    },
                    {
                        "name": "Jacqueline Sly"
                    },
                    {
                        "name": "Tesla Wells"
                    },
                    {
                        "name": "Nhut T. Ho"
                    },
                    {
                        "name": "Mykel Kochenderfer"
                    },
                    {
                        "name": "Giovanni Beltrame"
                    },
                    {
                        "name": "George Nikolakopoulos"
                    },
                    {
                        "name": "David Shim"
                    },
                    {
                        "name": "Luca Carlone"
                    },
                    {
                        "name": "Joel Burdick"
                    }
                ],
                "author_detail": {
                    "name": "Joel Burdick"
                },
                "author": "Joel Burdick",
                "arxiv_doi": "10.1109/TFR.2024.3430891",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TFR.2024.3430891",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.13461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Transactions on Field Robotics, vol. 1, pp. 476-526, 2024",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13460v1",
                "updated": "2025-04-18T04:35:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    4,
                    35,
                    35,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T04:35:35Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    4,
                    35,
                    35,
                    4,
                    108,
                    0
                ],
                "title": "Chain-of-Thought Textual Reasoning for Few-shot Temporal Action\n  Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought Textual Reasoning for Few-shot Temporal Action\n  Localization"
                },
                "summary": "Traditional temporal action localization (TAL) methods rely on large amounts\nof detailed annotated data, whereas few-shot TAL reduces this dependence by\nusing only a few training samples to identify unseen action categories.\nHowever, existing few-shot TAL methods typically focus solely on video-level\ninformation, neglecting textual information, which can provide valuable\nsemantic support for the localization task. Therefore, we propose a new\nfew-shot temporal action localization method by Chain-of-Thought textual\nreasoning to improve localization performance. Specifically, we design a novel\nfew-shot learning framework that leverages textual semantic information to\nenhance the model's ability to capture action commonalities and variations,\nwhich includes a semantic-aware text-visual alignment module designed to align\nthe query and support videos at different levels. Meanwhile, to better express\nthe temporal dependencies and causal relationships between actions at the\ntextual level to assist action localization, we design a Chain of Thought\n(CoT)-like reasoning method that progressively guides the Vision Language Model\n(VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for\nvideos. The generated texts can capture more variance of action than visual\nfeatures. We conduct extensive experiments on the publicly available\nActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named\nHuman-related Anomaly Localization and explore the application of the TAL task\nin human anomaly detection. The experimental results demonstrate that our\nproposed method significantly outperforms existing methods in single-instance\nand multi-instance scenarios. We will release our code, data and benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional temporal action localization (TAL) methods rely on large amounts\nof detailed annotated data, whereas few-shot TAL reduces this dependence by\nusing only a few training samples to identify unseen action categories.\nHowever, existing few-shot TAL methods typically focus solely on video-level\ninformation, neglecting textual information, which can provide valuable\nsemantic support for the localization task. Therefore, we propose a new\nfew-shot temporal action localization method by Chain-of-Thought textual\nreasoning to improve localization performance. Specifically, we design a novel\nfew-shot learning framework that leverages textual semantic information to\nenhance the model's ability to capture action commonalities and variations,\nwhich includes a semantic-aware text-visual alignment module designed to align\nthe query and support videos at different levels. Meanwhile, to better express\nthe temporal dependencies and causal relationships between actions at the\ntextual level to assist action localization, we design a Chain of Thought\n(CoT)-like reasoning method that progressively guides the Vision Language Model\n(VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for\nvideos. The generated texts can capture more variance of action than visual\nfeatures. We conduct extensive experiments on the publicly available\nActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named\nHuman-related Anomaly Localization and explore the application of the TAL task\nin human anomaly detection. The experimental results demonstrate that our\nproposed method significantly outperforms existing methods in single-instance\nand multi-instance scenarios. We will release our code, data and benchmark."
                },
                "authors": [
                    {
                        "name": "Hongwei Ji"
                    },
                    {
                        "name": "Wulian Yun"
                    },
                    {
                        "name": "Mengshi Qi"
                    },
                    {
                        "name": "Huadong Ma"
                    }
                ],
                "author_detail": {
                    "name": "Huadong Ma"
                },
                "author": "Huadong Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08725v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08725v2",
                "updated": "2025-04-18T04:32:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    4,
                    32,
                    43,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-11T17:50:08Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    50,
                    8,
                    4,
                    101,
                    0
                ],
                "title": "DocAgent: A Multi-Agent System for Automated Code Documentation\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DocAgent: A Multi-Agent System for Automated Code Documentation\n  Generation"
                },
                "summary": "High-quality code documentation is crucial for software development\nespecially in the era of AI. However, generating it automatically using Large\nLanguage Models (LLMs) remains challenging, as existing approaches often\nproduce incomplete, unhelpful, or factually incorrect outputs. We introduce\nDocAgent, a novel multi-agent collaborative system using topological code\nprocessing for incremental context building. Specialized agents (Reader,\nSearcher, Writer, Verifier, Orchestrator) then collaboratively generate\ndocumentation. We also propose a multi-faceted evaluation framework assessing\nCompleteness, Helpfulness, and Truthfulness. Comprehensive experiments show\nDocAgent significantly outperforms baselines consistently. Our ablation study\nconfirms the vital role of the topological processing order. DocAgent offers a\nrobust approach for reliable code documentation generation in complex and\nproprietary repositories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-quality code documentation is crucial for software development\nespecially in the era of AI. However, generating it automatically using Large\nLanguage Models (LLMs) remains challenging, as existing approaches often\nproduce incomplete, unhelpful, or factually incorrect outputs. We introduce\nDocAgent, a novel multi-agent collaborative system using topological code\nprocessing for incremental context building. Specialized agents (Reader,\nSearcher, Writer, Verifier, Orchestrator) then collaboratively generate\ndocumentation. We also propose a multi-faceted evaluation framework assessing\nCompleteness, Helpfulness, and Truthfulness. Comprehensive experiments show\nDocAgent significantly outperforms baselines consistently. Our ablation study\nconfirms the vital role of the topological processing order. DocAgent offers a\nrobust approach for reliable code documentation generation in complex and\nproprietary repositories."
                },
                "authors": [
                    {
                        "name": "Dayu Yang"
                    },
                    {
                        "name": "Antoine Simoulin"
                    },
                    {
                        "name": "Xin Qian"
                    },
                    {
                        "name": "Xiaoyi Liu"
                    },
                    {
                        "name": "Yuwei Cao"
                    },
                    {
                        "name": "Zhaopu Teng"
                    },
                    {
                        "name": "Grey Yang"
                    }
                ],
                "author_detail": {
                    "name": "Grey Yang"
                },
                "author": "Grey Yang",
                "arxiv_comment": "Public Repo: https://github.com/facebookresearch/DocAgent",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08725v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08725v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12943v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12943v2",
                "updated": "2025-04-18T04:23:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    4,
                    23,
                    0,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-17T13:43:13Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    13,
                    43,
                    13,
                    3,
                    107,
                    0
                ],
                "title": "Customizing Emotional Support: How Do Individuals Construct and Interact\n  With LLM-Powered Chatbots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customizing Emotional Support: How Do Individuals Construct and Interact\n  With LLM-Powered Chatbots"
                },
                "summary": "Personalized support is essential to fulfill individuals' emotional needs and\nsustain their mental well-being. Large language models (LLMs), with great\ncustomization flexibility, hold promises to enable individuals to create their\nown emotional support agents. In this work, we developed ChatLab, where users\ncould construct LLM-powered chatbots with additional interaction features\nincluding voices and avatars. Using a Research through Design approach, we\nconducted a week-long field study followed by interviews and design activities\n(N = 22), which uncovered how participants created diverse chatbot personas for\nemotional reliance, confronting stressors, connecting to intellectual\ndiscourse, reflecting mirrored selves, etc. We found that participants actively\nenriched the personas they constructed, shaping the dynamics between themselves\nand the chatbot to foster open and honest conversations. They also suggested\nother customizable features, such as integrating online activities and\nadjustable memory settings. Based on these findings, we discuss opportunities\nfor enhancing personalized emotional support through emerging AI technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized support is essential to fulfill individuals' emotional needs and\nsustain their mental well-being. Large language models (LLMs), with great\ncustomization flexibility, hold promises to enable individuals to create their\nown emotional support agents. In this work, we developed ChatLab, where users\ncould construct LLM-powered chatbots with additional interaction features\nincluding voices and avatars. Using a Research through Design approach, we\nconducted a week-long field study followed by interviews and design activities\n(N = 22), which uncovered how participants created diverse chatbot personas for\nemotional reliance, confronting stressors, connecting to intellectual\ndiscourse, reflecting mirrored selves, etc. We found that participants actively\nenriched the personas they constructed, shaping the dynamics between themselves\nand the chatbot to foster open and honest conversations. They also suggested\nother customizable features, such as integrating online activities and\nadjustable memory settings. Based on these findings, we discuss opportunities\nfor enhancing personalized emotional support through emerging AI technologies."
                },
                "authors": [
                    {
                        "name": "Xi Zheng"
                    },
                    {
                        "name": "Zhuoyang Li"
                    },
                    {
                        "name": "Xinning Gui"
                    },
                    {
                        "name": "Yuhan Luo"
                    }
                ],
                "author_detail": {
                    "name": "Yuhan Luo"
                },
                "author": "Yuhan Luo",
                "arxiv_comment": "20 pages, 3 figures, 3 tables. Accepted to CHI 2025, ACM Conference\n  on Human Factors in Computing Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12943v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13455v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13455v1",
                "updated": "2025-04-18T04:12:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    4,
                    12,
                    47,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T04:12:47Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    4,
                    12,
                    47,
                    4,
                    108,
                    0
                ],
                "title": "Modular XL-Array-Enabled 3-D Localization based on Hybrid\n  Spherical-Planar Wave Model in Terahertz Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modular XL-Array-Enabled 3-D Localization based on Hybrid\n  Spherical-Planar Wave Model in Terahertz Systems"
                },
                "summary": "This work considers the three-dimensional (3-D) positioning problem in a\nTerahertz (THz) system enabled by a modular extra-large (XL) array with\nsub-connected architecture. Our purpose is to estimate the Cartesian\nCoordinates of multiple user equipments (UEs) with the received signal of the\nRF chains while considering the spatial non-stationarity (SNS). We apply the\nhybrid spherical-planar wave model (HSPWM) as the channel model owing to the\nstructual feature of the modular array, and propose a 3-D localization\nalgorithm with relatively high accuracy and low complexity. Specifically, we\nfirst distinguish the visible sub-arrays (SAs) located in the VR and estimate\nthe angles-of-arrival (AoAs) from each UE to typical visible SAs with the\nlargest receive power via compressed sensing (CS) method. In addition, we apply\nthe weighted least square (WLS) method to obtain a coarse 3-D position\nestimation of each UE according to the AoA estimations. Then, we estimate the\nAoAs of the other SAs with a reduced dictionary (RD)-CS-based method for lower\ncomputational complexity, and utilize all the efficient AoA estimations to\nderive a fine position estimation. Simulation results indicate that the\nproposed positioning framework based on modular XL-array can achieve\nsatisfactory accuracy with evident reduction in complexity. Furthermore, the\ndeployment of SAs and the allocation of antenna elements need to be specially\ndesigned for better positioning performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work considers the three-dimensional (3-D) positioning problem in a\nTerahertz (THz) system enabled by a modular extra-large (XL) array with\nsub-connected architecture. Our purpose is to estimate the Cartesian\nCoordinates of multiple user equipments (UEs) with the received signal of the\nRF chains while considering the spatial non-stationarity (SNS). We apply the\nhybrid spherical-planar wave model (HSPWM) as the channel model owing to the\nstructual feature of the modular array, and propose a 3-D localization\nalgorithm with relatively high accuracy and low complexity. Specifically, we\nfirst distinguish the visible sub-arrays (SAs) located in the VR and estimate\nthe angles-of-arrival (AoAs) from each UE to typical visible SAs with the\nlargest receive power via compressed sensing (CS) method. In addition, we apply\nthe weighted least square (WLS) method to obtain a coarse 3-D position\nestimation of each UE according to the AoA estimations. Then, we estimate the\nAoAs of the other SAs with a reduced dictionary (RD)-CS-based method for lower\ncomputational complexity, and utilize all the efficient AoA estimations to\nderive a fine position estimation. Simulation results indicate that the\nproposed positioning framework based on modular XL-array can achieve\nsatisfactory accuracy with evident reduction in complexity. Furthermore, the\ndeployment of SAs and the allocation of antenna elements need to be specially\ndesigned for better positioning performance."
                },
                "authors": [
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Ruidong Li"
                    },
                    {
                        "name": "Cunhua Pan"
                    },
                    {
                        "name": "Hong Ren"
                    },
                    {
                        "name": "Tuo Wu"
                    },
                    {
                        "name": "Changhong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Changhong Wang"
                },
                "author": "Changhong Wang",
                "arxiv_comment": "13 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13455v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13455v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13444v1",
                "updated": "2025-04-18T03:50:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    3,
                    50,
                    56,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T03:50:56Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    3,
                    50,
                    56,
                    4,
                    108,
                    0
                ],
                "title": "Balancing Engagement and Polarization: Multi-Objective Alignment of News\n  Content Using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Engagement and Polarization: Multi-Objective Alignment of News\n  Content Using LLMs"
                },
                "summary": "We study how media firms can use LLMs to generate news content that aligns\nwith multiple objectives -- making content more engaging while maintaining a\npreferred level of polarization/slant consistent with the firm's editorial\npolicy. Using news articles from The New York Times, we first show that more\nengaging human-written content tends to be more polarizing. Further, naively\nemploying LLMs (with prompts or standard Direct Preference Optimization\napproaches) to generate more engaging content can also increase polarization.\nThis has an important managerial and policy implication: using LLMs without\nbuilding in controls for limiting slant can exacerbate news media polarization.\nWe present a constructive solution to this problem based on the Multi-Objective\nDirect Preference Optimization (MODPO) algorithm, a novel approach that\nintegrates Direct Preference Optimization with multi-objective optimization\ntechniques. We build on open-source LLMs and develop a new language model that\nsimultaneously makes content more engaging while maintaining a preferred\neditorial stance. Our model achieves this by modifying content characteristics\nstrongly associated with polarization but that have a relatively smaller impact\non engagement. Our approach and findings apply to other settings where firms\nseek to use LLMs for content creation to achieve multiple objectives, e.g.,\nadvertising and social media.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study how media firms can use LLMs to generate news content that aligns\nwith multiple objectives -- making content more engaging while maintaining a\npreferred level of polarization/slant consistent with the firm's editorial\npolicy. Using news articles from The New York Times, we first show that more\nengaging human-written content tends to be more polarizing. Further, naively\nemploying LLMs (with prompts or standard Direct Preference Optimization\napproaches) to generate more engaging content can also increase polarization.\nThis has an important managerial and policy implication: using LLMs without\nbuilding in controls for limiting slant can exacerbate news media polarization.\nWe present a constructive solution to this problem based on the Multi-Objective\nDirect Preference Optimization (MODPO) algorithm, a novel approach that\nintegrates Direct Preference Optimization with multi-objective optimization\ntechniques. We build on open-source LLMs and develop a new language model that\nsimultaneously makes content more engaging while maintaining a preferred\neditorial stance. Our model achieves this by modifying content characteristics\nstrongly associated with polarization but that have a relatively smaller impact\non engagement. Our approach and findings apply to other settings where firms\nseek to use LLMs for content creation to achieve multiple objectives, e.g.,\nadvertising and social media."
                },
                "authors": [
                    {
                        "name": "Mengjie"
                    },
                    {
                        "name": "Cheng"
                    },
                    {
                        "name": "Elie Ofek"
                    },
                    {
                        "name": "Hema Yoganarasimhan"
                    }
                ],
                "author_detail": {
                    "name": "Hema Yoganarasimhan"
                },
                "arxiv_affiliation": "Magie",
                "author": "Hema Yoganarasimhan",
                "arxiv_comment": "73 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13443v1",
                "updated": "2025-04-18T03:49:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    3,
                    49,
                    53,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T03:49:53Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    3,
                    49,
                    53,
                    4,
                    108,
                    0
                ],
                "title": "Trust, but verify",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trust, but verify"
                },
                "summary": "Decentralized AI agent networks, such as Gaia, allows individuals to run\ncustomized LLMs on their own computers and then provide services to the public.\nHowever, in order to maintain service quality, the network must verify that\nindividual nodes are running their designated LLMs. In this paper, we\ndemonstrate that in a cluster of mostly honest nodes, we can detect nodes that\nrun unauthorized or incorrect LLM through social consensus of its peers. We\nwill discuss the algorithm and experimental data from the Gaia network. We will\nalso discuss the intersubjective validation system, implemented as an\nEigenLayer AVS to introduce financial incentives and penalties to encourage\nhonest behavior from LLM nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized AI agent networks, such as Gaia, allows individuals to run\ncustomized LLMs on their own computers and then provide services to the public.\nHowever, in order to maintain service quality, the network must verify that\nindividual nodes are running their designated LLMs. In this paper, we\ndemonstrate that in a cluster of mostly honest nodes, we can detect nodes that\nrun unauthorized or incorrect LLM through social consensus of its peers. We\nwill discuss the algorithm and experimental data from the Gaia network. We will\nalso discuss the intersubjective validation system, implemented as an\nEigenLayer AVS to introduce financial incentives and penalties to encourage\nhonest behavior from LLM nodes."
                },
                "authors": [
                    {
                        "name": "Michael J. Yuan"
                    },
                    {
                        "name": "Carlos Campoy"
                    },
                    {
                        "name": "Sydney Lai"
                    },
                    {
                        "name": "James Snewin"
                    },
                    {
                        "name": "Ju Long"
                    }
                ],
                "author_detail": {
                    "name": "Ju Long"
                },
                "author": "Ju Long",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08041v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08041v3",
                "updated": "2025-04-18T03:23:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    3,
                    23,
                    33,
                    4,
                    108,
                    0
                ],
                "published": "2024-12-11T02:44:14Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    2,
                    44,
                    14,
                    2,
                    346,
                    0
                ],
                "title": "Quantifying the benefits of code hints for refactoring deprecated Java\n  APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying the benefits of code hints for refactoring deprecated Java\n  APIs"
                },
                "summary": "When done manually, refactoring legacy code in order to eliminate uses of\ndeprecated APIs is an error-prone and time-consuming process. In this paper, we\ninvestigate to which degree refactorings for deprecated Java APIs can be\nautomated, and quantify the benefit of Javadoc code hints for this task. To\nthis end, we build a symbolic and a neural engine for the automatic refactoring\nof deprecated APIs. The former is based on type-directed and component-based\nprogram synthesis, whereas the latter uses LLMs. We applied our engines to\nrefactor the deprecated methods in the Oracle JDK 15. Our experiments show that\ncode hints are enabling for the automation of this task: even the worst engine\ncorrectly refactors 71% of the tasks with code hints, which drops to at best\n14% on tasks without. Adding more code hints to Javadoc can hence boost the\nrefactoring of code that uses deprecated APIs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When done manually, refactoring legacy code in order to eliminate uses of\ndeprecated APIs is an error-prone and time-consuming process. In this paper, we\ninvestigate to which degree refactorings for deprecated Java APIs can be\nautomated, and quantify the benefit of Javadoc code hints for this task. To\nthis end, we build a symbolic and a neural engine for the automatic refactoring\nof deprecated APIs. The former is based on type-directed and component-based\nprogram synthesis, whereas the latter uses LLMs. We applied our engines to\nrefactor the deprecated methods in the Oracle JDK 15. Our experiments show that\ncode hints are enabling for the automation of this task: even the worst engine\ncorrectly refactors 71% of the tasks with code hints, which drops to at best\n14% on tasks without. Adding more code hints to Javadoc can hence boost the\nrefactoring of code that uses deprecated APIs."
                },
                "authors": [
                    {
                        "name": "Cristina David"
                    },
                    {
                        "name": "Pascal Kesseli"
                    },
                    {
                        "name": "Daniel Kroening"
                    },
                    {
                        "name": "Hanliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hanliang Zhang"
                },
                "author": "Hanliang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08041v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08041v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07137v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07137v3",
                "updated": "2025-04-18T02:53:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    2,
                    53,
                    1,
                    4,
                    108,
                    0
                ],
                "published": "2025-03-10T10:08:55Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    10,
                    8,
                    55,
                    0,
                    69,
                    0
                ],
                "title": "A Comprehensive Survey of Mixture-of-Experts: Algorithms, Theory, and\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey of Mixture-of-Experts: Algorithms, Theory, and\n  Applications"
                },
                "summary": "Artificial intelligence (AI) has achieved astonishing successes in many\ndomains, especially with the recent breakthroughs in the development of\nfoundational large models. These large models, leveraging their extensive\ntraining data, provide versatile solutions for a wide range of downstream\ntasks. However, as modern datasets become increasingly diverse and complex, the\ndevelopment of large AI models faces two major challenges: (1) the enormous\nconsumption of computational resources and deployment difficulties, and (2) the\ndifficulty in fitting heterogeneous and complex data, which limits the\nusability of the models. Mixture of Experts (MoE) models has recently attracted\nmuch attention in addressing these challenges, by dynamically selecting and\nactivating the most relevant sub-models to process input data. It has been\nshown that MoEs can significantly improve model performance and efficiency with\nfewer resources, particularly excelling in handling large-scale, multimodal\ndata. Given the tremendous potential MoE has demonstrated across various\ndomains, it is urgent to provide a comprehensive summary of recent advancements\nof MoEs in many important fields. Existing surveys on MoE have their\nlimitations, e.g., being outdated or lacking discussion on certain key areas,\nand we aim to address these gaps. In this paper, we first introduce the basic\ndesign of MoE, including gating functions, expert networks, routing mechanisms,\ntraining strategies, and system design. We then explore the algorithm design of\nMoE in important machine learning paradigms such as continual learning,\nmeta-learning, multi-task learning, and reinforcement learning. Additionally,\nwe summarize theoretical studies aimed at understanding MoE and review its\napplications in computer vision and natural language processing. Finally, we\ndiscuss promising future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) has achieved astonishing successes in many\ndomains, especially with the recent breakthroughs in the development of\nfoundational large models. These large models, leveraging their extensive\ntraining data, provide versatile solutions for a wide range of downstream\ntasks. However, as modern datasets become increasingly diverse and complex, the\ndevelopment of large AI models faces two major challenges: (1) the enormous\nconsumption of computational resources and deployment difficulties, and (2) the\ndifficulty in fitting heterogeneous and complex data, which limits the\nusability of the models. Mixture of Experts (MoE) models has recently attracted\nmuch attention in addressing these challenges, by dynamically selecting and\nactivating the most relevant sub-models to process input data. It has been\nshown that MoEs can significantly improve model performance and efficiency with\nfewer resources, particularly excelling in handling large-scale, multimodal\ndata. Given the tremendous potential MoE has demonstrated across various\ndomains, it is urgent to provide a comprehensive summary of recent advancements\nof MoEs in many important fields. Existing surveys on MoE have their\nlimitations, e.g., being outdated or lacking discussion on certain key areas,\nand we aim to address these gaps. In this paper, we first introduce the basic\ndesign of MoE, including gating functions, expert networks, routing mechanisms,\ntraining strategies, and system design. We then explore the algorithm design of\nMoE in important machine learning paradigms such as continual learning,\nmeta-learning, multi-task learning, and reinforcement learning. Additionally,\nwe summarize theoretical studies aimed at understanding MoE and review its\napplications in computer vision and natural language processing. Finally, we\ndiscuss promising future research directions."
                },
                "authors": [
                    {
                        "name": "Siyuan Mu"
                    },
                    {
                        "name": "Sen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Sen Lin"
                },
                "author": "Sen Lin",
                "arxiv_comment": "29 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07137v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07137v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13425v1",
                "updated": "2025-04-18T02:51:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    2,
                    51,
                    29,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T02:51:29Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    2,
                    51,
                    29,
                    4,
                    108,
                    0
                ],
                "title": "Secure Multifaceted-RAG for Enterprise: Hybrid Knowledge Retrieval with\n  Security Filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure Multifaceted-RAG for Enterprise: Hybrid Knowledge Retrieval with\n  Security Filtering"
                },
                "summary": "Existing Retrieval-Augmented Generation (RAG) systems face challenges in\nenterprise settings due to limited retrieval scope and data security risks.\nWhen relevant internal documents are unavailable, the system struggles to\ngenerate accurate and complete responses. Additionally, using closed-source\nLarge Language Models (LLMs) raises concerns about exposing proprietary\ninformation. To address these issues, we propose the Secure Multifaceted-RAG\n(SecMulti-RAG) framework, which retrieves not only from internal documents but\nalso from two supplementary sources: pre-generated expert knowledge for\nanticipated queries and on-demand external LLM-generated knowledge. To mitigate\nsecurity risks, we adopt a local open-source generator and selectively utilize\nexternal LLMs only when prompts are deemed safe by a filtering mechanism. This\napproach enhances completeness, prevents data leakage, and reduces costs. In\nour evaluation on a report generation task in the automotive industry,\nSecMulti-RAG significantly outperforms traditional RAG - achieving 79.3 to 91.9\npercent win rates across correctness, richness, and helpfulness in LLM-based\nevaluation, and 56.3 to 70.4 percent in human evaluation. This highlights\nSecMulti-RAG as a practical and secure solution for enterprise RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing Retrieval-Augmented Generation (RAG) systems face challenges in\nenterprise settings due to limited retrieval scope and data security risks.\nWhen relevant internal documents are unavailable, the system struggles to\ngenerate accurate and complete responses. Additionally, using closed-source\nLarge Language Models (LLMs) raises concerns about exposing proprietary\ninformation. To address these issues, we propose the Secure Multifaceted-RAG\n(SecMulti-RAG) framework, which retrieves not only from internal documents but\nalso from two supplementary sources: pre-generated expert knowledge for\nanticipated queries and on-demand external LLM-generated knowledge. To mitigate\nsecurity risks, we adopt a local open-source generator and selectively utilize\nexternal LLMs only when prompts are deemed safe by a filtering mechanism. This\napproach enhances completeness, prevents data leakage, and reduces costs. In\nour evaluation on a report generation task in the automotive industry,\nSecMulti-RAG significantly outperforms traditional RAG - achieving 79.3 to 91.9\npercent win rates across correctness, richness, and helpfulness in LLM-based\nevaluation, and 56.3 to 70.4 percent in human evaluation. This highlights\nSecMulti-RAG as a practical and secure solution for enterprise RAG."
                },
                "authors": [
                    {
                        "name": "Grace Byun"
                    },
                    {
                        "name": "Shinsun Lee"
                    },
                    {
                        "name": "Nayoung Choi"
                    },
                    {
                        "name": "Jinho Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jinho Choi"
                },
                "author": "Jinho Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15793v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15793v4",
                "updated": "2025-04-18T02:38:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    2,
                    38,
                    12,
                    4,
                    108,
                    0
                ],
                "published": "2025-03-20T02:19:14Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    2,
                    19,
                    14,
                    3,
                    79,
                    0
                ],
                "title": "DNR Bench: Benchmarking Over-Reasoning in Reasoning LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DNR Bench: Benchmarking Over-Reasoning in Reasoning LLMs"
                },
                "summary": "Test-time scaling has significantly improved large language model\nperformance, enabling deeper reasoning to solve complex problems. However, this\nincreased reasoning capability also leads to excessive token generation and\nunnecessary problem-solving attempts. We introduce Don\\'t Reason Bench (DNR\nBench), a new benchmark designed to evaluate LLMs ability to robustly\nunderstand the tricky reasoning triggers and avoiding unnecessary generation.\nDNR Bench consists of 150 adversarially designed prompts that are easy for\nhumans to understand and respond to, but surprisingly not for many of the\nrecent prominent LLMs. DNR Bench tests models abilities across different\ncapabilities, such as instruction adherence, hallucination avoidance,\nredundancy filtering, and unanswerable question recognition. We evaluate\nreasoning LLMs (RLMs), including DeepSeek-R1, OpenAI O3-mini, Claude-3.7-sonnet\nand compare them against a powerful non-reasoning model, e.g., GPT-4o. Our\nexperiments reveal that RLMs generate up to 70x more tokens than necessary,\noften failing at tasks that simpler non-reasoning models handle efficiently\nwith higher accuracy. Our findings underscore the need for more effective\ntraining and inference strategies in RLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling has significantly improved large language model\nperformance, enabling deeper reasoning to solve complex problems. However, this\nincreased reasoning capability also leads to excessive token generation and\nunnecessary problem-solving attempts. We introduce Don\\'t Reason Bench (DNR\nBench), a new benchmark designed to evaluate LLMs ability to robustly\nunderstand the tricky reasoning triggers and avoiding unnecessary generation.\nDNR Bench consists of 150 adversarially designed prompts that are easy for\nhumans to understand and respond to, but surprisingly not for many of the\nrecent prominent LLMs. DNR Bench tests models abilities across different\ncapabilities, such as instruction adherence, hallucination avoidance,\nredundancy filtering, and unanswerable question recognition. We evaluate\nreasoning LLMs (RLMs), including DeepSeek-R1, OpenAI O3-mini, Claude-3.7-sonnet\nand compare them against a powerful non-reasoning model, e.g., GPT-4o. Our\nexperiments reveal that RLMs generate up to 70x more tokens than necessary,\noften failing at tasks that simpler non-reasoning models handle efficiently\nwith higher accuracy. Our findings underscore the need for more effective\ntraining and inference strategies in RLMs."
                },
                "authors": [
                    {
                        "name": "Masoud Hashemi"
                    },
                    {
                        "name": "Oluwanifemi Bamgbose"
                    },
                    {
                        "name": "Sathwik Tejaswi Madhusudhan"
                    },
                    {
                        "name": "Jishnu Sethumadhavan Nair"
                    },
                    {
                        "name": "Aman Tiwari"
                    },
                    {
                        "name": "Vikas Yadav"
                    }
                ],
                "author_detail": {
                    "name": "Vikas Yadav"
                },
                "author": "Vikas Yadav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15793v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15793v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13416v1",
                "updated": "2025-04-18T02:25:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    2,
                    25,
                    8,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T02:25:08Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    2,
                    25,
                    8,
                    4,
                    108,
                    0
                ],
                "title": "STAMP Your Content: Proving Dataset Membership via Watermarked\n  Rephrasings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAMP Your Content: Proving Dataset Membership via Watermarked\n  Rephrasings"
                },
                "summary": "Given how large parts of publicly available text are crawled to pretrain\nlarge language models (LLMs), data creators increasingly worry about the\ninclusion of their proprietary data for model training without attribution or\nlicensing. Their concerns are also shared by benchmark curators whose test-sets\nmight be compromised. In this paper, we present STAMP, a framework for\ndetecting dataset membership-i.e., determining the inclusion of a dataset in\nthe pretraining corpora of LLMs. Given an original piece of content, our\nproposal involves first generating multiple rephrases, each embedding a\nwatermark with a unique secret key. One version is to be released publicly,\nwhile others are to be kept private. Subsequently, creators can compare model\nlikelihoods between public and private versions using paired statistical tests\nto prove membership. We show that our framework can successfully detect\ncontamination across four benchmarks which appear only once in the training\ndata and constitute less than 0.001% of the total tokens, outperforming several\ncontamination detection and dataset inference baselines. We verify that STAMP\npreserves both the semantic meaning and the utility of the original data in\ncomparing different models. We apply STAMP to two real-world scenarios to\nconfirm the inclusion of paper abstracts and blog articles in the pretraining\ncorpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given how large parts of publicly available text are crawled to pretrain\nlarge language models (LLMs), data creators increasingly worry about the\ninclusion of their proprietary data for model training without attribution or\nlicensing. Their concerns are also shared by benchmark curators whose test-sets\nmight be compromised. In this paper, we present STAMP, a framework for\ndetecting dataset membership-i.e., determining the inclusion of a dataset in\nthe pretraining corpora of LLMs. Given an original piece of content, our\nproposal involves first generating multiple rephrases, each embedding a\nwatermark with a unique secret key. One version is to be released publicly,\nwhile others are to be kept private. Subsequently, creators can compare model\nlikelihoods between public and private versions using paired statistical tests\nto prove membership. We show that our framework can successfully detect\ncontamination across four benchmarks which appear only once in the training\ndata and constitute less than 0.001% of the total tokens, outperforming several\ncontamination detection and dataset inference baselines. We verify that STAMP\npreserves both the semantic meaning and the utility of the original data in\ncomparing different models. We apply STAMP to two real-world scenarios to\nconfirm the inclusion of paper abstracts and blog articles in the pretraining\ncorpora."
                },
                "authors": [
                    {
                        "name": "Saksham Rastogi"
                    },
                    {
                        "name": "Pratyush Maini"
                    },
                    {
                        "name": "Danish Pruthi"
                    }
                ],
                "author_detail": {
                    "name": "Danish Pruthi"
                },
                "author": "Danish Pruthi",
                "arxiv_comment": "Accepted at DATA-FM, WMark @ ICLR 2025. Project page at see\n  https://codeboy5.github.io/stamp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13413v1",
                "updated": "2025-04-18T02:19:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    2,
                    19,
                    30,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T02:19:30Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    2,
                    19,
                    30,
                    4,
                    108,
                    0
                ],
                "title": "A Model-Based Approach to Imitation Learning through Multi-Step\n  Predictions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Model-Based Approach to Imitation Learning through Multi-Step\n  Predictions"
                },
                "summary": "Imitation learning is a widely used approach for training agents to replicate\nexpert behavior in complex decision-making tasks. However, existing methods\noften struggle with compounding errors and limited generalization, due to the\ninherent challenge of error correction and the distribution shift between\ntraining and deployment. In this paper, we present a novel model-based\nimitation learning framework inspired by model predictive control, which\naddresses these limitations by integrating predictive modeling through\nmulti-step state predictions. Our method outperforms traditional behavior\ncloning numerical benchmarks, demonstrating superior robustness to distribution\nshift and measurement noise both in available data and during execution.\nFurthermore, we provide theoretical guarantees on the sample complexity and\nerror bounds of our method, offering insights into its convergence properties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imitation learning is a widely used approach for training agents to replicate\nexpert behavior in complex decision-making tasks. However, existing methods\noften struggle with compounding errors and limited generalization, due to the\ninherent challenge of error correction and the distribution shift between\ntraining and deployment. In this paper, we present a novel model-based\nimitation learning framework inspired by model predictive control, which\naddresses these limitations by integrating predictive modeling through\nmulti-step state predictions. Our method outperforms traditional behavior\ncloning numerical benchmarks, demonstrating superior robustness to distribution\nshift and measurement noise both in available data and during execution.\nFurthermore, we provide theoretical guarantees on the sample complexity and\nerror bounds of our method, offering insights into its convergence properties."
                },
                "authors": [
                    {
                        "name": "Haldun Balim"
                    },
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Yuyang Zhang"
                    },
                    {
                        "name": "Na Li"
                    }
                ],
                "author_detail": {
                    "name": "Na Li"
                },
                "author": "Na Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12354v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12354v2",
                "updated": "2025-04-18T02:12:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    2,
                    12,
                    50,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-15T23:27:52Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    23,
                    27,
                    52,
                    1,
                    105,
                    0
                ],
                "title": "WaterFlow: Learning Fast & Robust Watermarks using Stable Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaterFlow: Learning Fast & Robust Watermarks using Stable Diffusion"
                },
                "summary": "The ability to embed watermarks in images is a fundamental problem of\ninterest for computer vision, and is exacerbated by the rapid rise of generated\nimagery in recent times. Current state-of-the-art techniques suffer from\ncomputational and statistical challenges such as the slow execution speed for\npractical deployments. In addition, other works trade off fast watermarking\nspeeds but suffer greatly in their robustness or perceptual quality. In this\nwork, we propose WaterFlow (WF), a fast and extremely robust approach for high\nfidelity visual watermarking based on a learned latent-dependent watermark. Our\napproach utilizes a pretrained latent diffusion model to encode an arbitrary\nimage into a latent space and produces a learned watermark that is then planted\ninto the Fourier Domain of the latent. The transformation is specified via\ninvertible flow layers that enhance the expressivity of the latent space of the\npre-trained model to better preserve image quality while permitting robust and\ntractable detection. Most notably, WaterFlow demonstrates state-of-the-art\nperformance on general robustness and is the first method capable of\neffectively defending against difficult combination attacks. We validate our\nfindings on three widely used real and generated datasets: MS-COCO,\nDiffusionDB, and WikiArt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to embed watermarks in images is a fundamental problem of\ninterest for computer vision, and is exacerbated by the rapid rise of generated\nimagery in recent times. Current state-of-the-art techniques suffer from\ncomputational and statistical challenges such as the slow execution speed for\npractical deployments. In addition, other works trade off fast watermarking\nspeeds but suffer greatly in their robustness or perceptual quality. In this\nwork, we propose WaterFlow (WF), a fast and extremely robust approach for high\nfidelity visual watermarking based on a learned latent-dependent watermark. Our\napproach utilizes a pretrained latent diffusion model to encode an arbitrary\nimage into a latent space and produces a learned watermark that is then planted\ninto the Fourier Domain of the latent. The transformation is specified via\ninvertible flow layers that enhance the expressivity of the latent space of the\npre-trained model to better preserve image quality while permitting robust and\ntractable detection. Most notably, WaterFlow demonstrates state-of-the-art\nperformance on general robustness and is the first method capable of\neffectively defending against difficult combination attacks. We validate our\nfindings on three widely used real and generated datasets: MS-COCO,\nDiffusionDB, and WikiArt."
                },
                "authors": [
                    {
                        "name": "Vinay Shukla"
                    },
                    {
                        "name": "Prachee Sharma"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "Sungchul Kim"
                    },
                    {
                        "name": "Tong Yu"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12354v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12354v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05050v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05050v2",
                "updated": "2025-04-18T02:10:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    2,
                    10,
                    21,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-07T13:20:17Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    20,
                    17,
                    0,
                    97,
                    0
                ],
                "title": "Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) are foundational explorations to artificial\ngeneral intelligence, yet their alignment with human values via instruction\ntuning and preference learning achieves only superficial compliance. Here, we\ndemonstrate that harmful knowledge embedded during pretraining persists as\nindelible \"dark patterns\" in LLMs' parametric memory, evading alignment\nsafeguards and resurfacing under adversarial inducement at distributional\nshifts. In this study, we first theoretically analyze the intrinsic ethical\nvulnerability of aligned LLMs by proving that current alignment methods yield\nonly local \"safety regions\" in the knowledge manifold. In contrast, pretrained\nknowledge remains globally connected to harmful concepts via high-likelihood\nadversarial trajectories. Building on this theoretical insight, we empirically\nvalidate our findings by employing semantic coherence inducement under\ndistributional shifts--a method that systematically bypasses alignment\nconstraints through optimized adversarial prompts. This combined theoretical\nand empirical approach achieves a 100% attack success rate across 19 out of 23\nstate-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing\ntheir universal vulnerabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are foundational explorations to artificial\ngeneral intelligence, yet their alignment with human values via instruction\ntuning and preference learning achieves only superficial compliance. Here, we\ndemonstrate that harmful knowledge embedded during pretraining persists as\nindelible \"dark patterns\" in LLMs' parametric memory, evading alignment\nsafeguards and resurfacing under adversarial inducement at distributional\nshifts. In this study, we first theoretically analyze the intrinsic ethical\nvulnerability of aligned LLMs by proving that current alignment methods yield\nonly local \"safety regions\" in the knowledge manifold. In contrast, pretrained\nknowledge remains globally connected to harmful concepts via high-likelihood\nadversarial trajectories. Building on this theoretical insight, we empirically\nvalidate our findings by employing semantic coherence inducement under\ndistributional shifts--a method that systematically bypasses alignment\nconstraints through optimized adversarial prompts. This combined theoretical\nand empirical approach achieves a 100% attack success rate across 19 out of 23\nstate-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing\ntheir universal vulnerabilities."
                },
                "authors": [
                    {
                        "name": "Jiawei Lian"
                    },
                    {
                        "name": "Jianhong Pan"
                    },
                    {
                        "name": "Lefan Wang"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Shaohui Mei"
                    },
                    {
                        "name": "Lap-Pui Chau"
                    }
                ],
                "author_detail": {
                    "name": "Lap-Pui Chau"
                },
                "author": "Lap-Pui Chau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05050v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05050v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22962v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22962v2",
                "updated": "2025-04-18T02:07:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    2,
                    7,
                    52,
                    4,
                    108,
                    0
                ],
                "published": "2025-03-29T03:48:11Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    3,
                    48,
                    11,
                    5,
                    88,
                    0
                ],
                "title": "Multimodal machine learning with large language embedding model for\n  polymer property prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal machine learning with large language embedding model for\n  polymer property prediction"
                },
                "summary": "Contemporary large language models (LLMs), such as GPT-4 and Llama, have\nharnessed extensive computational power and diverse text corpora to achieve\nremarkable proficiency in interpreting and generating domain-specific content,\nincluding materials science. To leverage the domain knowledge embedded within\nthese models, we propose a simple yet effective multimodal architecture,\nPolyLLMem, which integrates text embeddings generated by Llama 3 with molecular\nstructure embeddings derived from Uni-Mol, for polymer properties prediction\ntasks. In our model, Low-rank adaptation (LoRA) layers were also incorporated\nduring the property prediction tasks to refine the embeddings based on our\nlimited polymer dataset, thereby enhancing their chemical relevance for polymer\nSMILES representation. This balanced fusion of fine-tuned textual and\nstructural information enables PolyLLMem to accurately predict a variety of\npolymer properties despite the scarcity of training data. Its performance is\ncomparable to, and in some cases exceeds, that of graph-based models, as well\nas transformer-based models that typically require pretraining on millions of\npolymer samples. These findings demonstrate that LLM, such as Llama, can\neffectively capture chemical information encoded in polymer PSMILES, and\nunderscore the efficacy of multimodal fusion of LLM embeddings and molecular\nstructure embeddings in overcoming data scarcity and accelerating the discovery\nof advanced polymeric materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary large language models (LLMs), such as GPT-4 and Llama, have\nharnessed extensive computational power and diverse text corpora to achieve\nremarkable proficiency in interpreting and generating domain-specific content,\nincluding materials science. To leverage the domain knowledge embedded within\nthese models, we propose a simple yet effective multimodal architecture,\nPolyLLMem, which integrates text embeddings generated by Llama 3 with molecular\nstructure embeddings derived from Uni-Mol, for polymer properties prediction\ntasks. In our model, Low-rank adaptation (LoRA) layers were also incorporated\nduring the property prediction tasks to refine the embeddings based on our\nlimited polymer dataset, thereby enhancing their chemical relevance for polymer\nSMILES representation. This balanced fusion of fine-tuned textual and\nstructural information enables PolyLLMem to accurately predict a variety of\npolymer properties despite the scarcity of training data. Its performance is\ncomparable to, and in some cases exceeds, that of graph-based models, as well\nas transformer-based models that typically require pretraining on millions of\npolymer samples. These findings demonstrate that LLM, such as Llama, can\neffectively capture chemical information encoded in polymer PSMILES, and\nunderscore the efficacy of multimodal fusion of LLM embeddings and molecular\nstructure embeddings in overcoming data scarcity and accelerating the discovery\nof advanced polymeric materials."
                },
                "authors": [
                    {
                        "name": "Tianren Zhang"
                    },
                    {
                        "name": "Dai-Bei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Dai-Bei Yang"
                },
                "author": "Dai-Bei Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22962v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22962v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00824v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00824v4",
                "updated": "2025-04-18T02:06:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    2,
                    6,
                    6,
                    4,
                    108,
                    0
                ],
                "published": "2025-01-01T13:00:01Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    13,
                    0,
                    1,
                    2,
                    1,
                    0
                ],
                "title": "How Breakable Is Privacy: Probing and Resisting Model Inversion Attacks\n  in Collaborative Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Breakable Is Privacy: Probing and Resisting Model Inversion Attacks\n  in Collaborative Inference"
                },
                "summary": "Collaborative inference (CI) improves computational efficiency for edge\ndevices by transmitting intermediate features to cloud models. However, this\nprocess inevitably exposes feature representations to model inversion attacks\n(MIAs), enabling unauthorized data reconstruction. Despite extensive research,\nthere is no established criterion for assessing the difficulty of MIA\nimplementation, leaving a fundamental question unanswered: \\textit{What factors\ntruly and verifiably determine the attack's success in CI?} Moreover, existing\ndefenses lack the theoretical foundation described above, making it challenging\nto regulate feature information effectively while ensuring privacy and\nminimizing computational overhead. These shortcomings introduce three key\nchallenges: theoretical gap, methodological limitation, and practical\nconstraint.\n  To overcome these challenges, we propose the first theoretical criterion to\nassess MIA difficulty in CI, identifying mutual information, entropy, and\neffective information volume as key influencing factors. The validity of this\ncriterion is demonstrated by using the mutual information neural estimator.\nBuilding on this insight, we propose SiftFunnel, a privacy-preserving framework\nto resist MIA while maintaining usability. Specifically, we incorporate linear\nand non-linear correlation constraints alongside label smoothing to suppress\nredundant information transmission, effectively balancing privacy and\nusability. To enhance deployability, the edge model adopts a funnel-shaped\nstructure with attention mechanisms, strengthening privacy while reducing\ncomputational and storage burdens. Experiments show that, compared to\nstate-of-the-art defense, SiftFunnel increases reconstruction error by\n$\\sim$30\\%, lowers mutual and effective information metrics by $\\geq$50\\%, and\nreduces edge burdens by almost $20\\times$, while maintaining comparable\nusability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative inference (CI) improves computational efficiency for edge\ndevices by transmitting intermediate features to cloud models. However, this\nprocess inevitably exposes feature representations to model inversion attacks\n(MIAs), enabling unauthorized data reconstruction. Despite extensive research,\nthere is no established criterion for assessing the difficulty of MIA\nimplementation, leaving a fundamental question unanswered: \\textit{What factors\ntruly and verifiably determine the attack's success in CI?} Moreover, existing\ndefenses lack the theoretical foundation described above, making it challenging\nto regulate feature information effectively while ensuring privacy and\nminimizing computational overhead. These shortcomings introduce three key\nchallenges: theoretical gap, methodological limitation, and practical\nconstraint.\n  To overcome these challenges, we propose the first theoretical criterion to\nassess MIA difficulty in CI, identifying mutual information, entropy, and\neffective information volume as key influencing factors. The validity of this\ncriterion is demonstrated by using the mutual information neural estimator.\nBuilding on this insight, we propose SiftFunnel, a privacy-preserving framework\nto resist MIA while maintaining usability. Specifically, we incorporate linear\nand non-linear correlation constraints alongside label smoothing to suppress\nredundant information transmission, effectively balancing privacy and\nusability. To enhance deployability, the edge model adopts a funnel-shaped\nstructure with attention mechanisms, strengthening privacy while reducing\ncomputational and storage burdens. Experiments show that, compared to\nstate-of-the-art defense, SiftFunnel increases reconstruction error by\n$\\sim$30\\%, lowers mutual and effective information metrics by $\\geq$50\\%, and\nreduces edge burdens by almost $20\\times$, while maintaining comparable\nusability."
                },
                "authors": [
                    {
                        "name": "Rongke Liu"
                    }
                ],
                "author_detail": {
                    "name": "Rongke Liu"
                },
                "author": "Rongke Liu",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00824v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00824v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09946v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09946v2",
                "updated": "2025-04-18T02:05:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    2,
                    5,
                    38,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-14T07:14:27Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    7,
                    14,
                    27,
                    0,
                    104,
                    0
                ],
                "title": "Assessing Judging Bias in Large Reasoning Models: An Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Judging Bias in Large Reasoning Models: An Empirical Study"
                },
                "summary": "Large Reasoning Models (LRMs) like DeepSeek-R1 and OpenAI-o1 have\ndemonstrated remarkable reasoning capabilities, raising important questions\nabout their biases in LLM-as-a-judge settings. We present a comprehensive\nbenchmark comparing judging biases between LLMs and LRMs across both subjective\npreference-alignment datasets and objective fact-based datasets. Through\ninvestigation of bandwagon, authority, position, and distraction biases, we\nuncover four key findings: (1) despite their advanced reasoning capabilities,\nLRMs remain susceptible to the above biases; (2) LRMs demonstrate better\nrobustness than LLMs specifically on fact-related datasets; (3) LRMs exhibit\nnotable position bias, preferring options in later positions; and (4) we\nidentify a novel \"superficial reflection bias\" where phrases mimicking\nreasoning (e.g., \"wait, let me think...\") significantly influence model\njudgments. To address these biases, we design and evaluate three mitigation\nstrategies: specialized system prompts that reduce judging biases by up to 19\\%\nin preference alignment datasets and 14\\% in fact-related datasets, in-context\nlearning that provides up to 27\\% improvement on preference tasks but shows\ninconsistent results on factual tasks, and a self-reflection mechanism that\nreduces biases by up to 10\\% in preference datasets and 16\\% in fact-related\ndatasets, with self-reflection proving particularly effective for LRMs. Our\nwork provides crucial insights for developing more reliable LLM-as-a-Judge\nframeworks, especially as LRMs become increasingly deployed as automated\njudges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) like DeepSeek-R1 and OpenAI-o1 have\ndemonstrated remarkable reasoning capabilities, raising important questions\nabout their biases in LLM-as-a-judge settings. We present a comprehensive\nbenchmark comparing judging biases between LLMs and LRMs across both subjective\npreference-alignment datasets and objective fact-based datasets. Through\ninvestigation of bandwagon, authority, position, and distraction biases, we\nuncover four key findings: (1) despite their advanced reasoning capabilities,\nLRMs remain susceptible to the above biases; (2) LRMs demonstrate better\nrobustness than LLMs specifically on fact-related datasets; (3) LRMs exhibit\nnotable position bias, preferring options in later positions; and (4) we\nidentify a novel \"superficial reflection bias\" where phrases mimicking\nreasoning (e.g., \"wait, let me think...\") significantly influence model\njudgments. To address these biases, we design and evaluate three mitigation\nstrategies: specialized system prompts that reduce judging biases by up to 19\\%\nin preference alignment datasets and 14\\% in fact-related datasets, in-context\nlearning that provides up to 27\\% improvement on preference tasks but shows\ninconsistent results on factual tasks, and a self-reflection mechanism that\nreduces biases by up to 10\\% in preference datasets and 16\\% in fact-related\ndatasets, with self-reflection proving particularly effective for LRMs. Our\nwork provides crucial insights for developing more reliable LLM-as-a-Judge\nframeworks, especially as LRMs become increasingly deployed as automated\njudges."
                },
                "authors": [
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Zhanzhi Lou"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Xuandong Zhao"
                    },
                    {
                        "name": "Wenxuan Zhang"
                    },
                    {
                        "name": "Dawn Song"
                    },
                    {
                        "name": "Bingsheng He"
                    }
                ],
                "author_detail": {
                    "name": "Bingsheng He"
                },
                "author": "Bingsheng He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09946v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09946v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13399v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13399v1",
                "updated": "2025-04-18T01:25:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    1,
                    25,
                    2,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T01:25:02Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    1,
                    25,
                    2,
                    4,
                    108,
                    0
                ],
                "title": "Towards a Multi-Agent Vision-Language System for Zero-Shot Novel\n  Hazardous Object Detection for Autonomous Driving Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Multi-Agent Vision-Language System for Zero-Shot Novel\n  Hazardous Object Detection for Autonomous Driving Safety"
                },
                "summary": "Detecting anomalous hazards in visual data, particularly in video streams, is\na critical challenge in autonomous driving. Existing models often struggle with\nunpredictable, out-of-label hazards due to their reliance on predefined object\ncategories. In this paper, we propose a multimodal approach that integrates\nvision-language reasoning with zero-shot object detection to improve hazard\nidentification and explanation. Our pipeline consists of a Vision-Language\nModel (VLM), a Large Language Model (LLM), in order to detect hazardous objects\nwithin a traffic scene. We refine object detection by incorporating OpenAI's\nCLIP model to match predicted hazards with bounding box annotations, improving\nlocalization accuracy. To assess model performance, we create a ground truth\ndataset by denoising and extending the foundational COOOL\n(Challenge-of-Out-of-Label) anomaly detection benchmark dataset with complete\nnatural language descriptions for hazard annotations. We define a means of\nhazard detection and labeling evaluation on the extended dataset using cosine\nsimilarity. This evaluation considers the semantic similarity between the\npredicted hazard description and the annotated ground truth for each video.\nAdditionally, we release a set of tools for structuring and managing\nlarge-scale hazard detection datasets. Our findings highlight the strengths and\nlimitations of current vision-language-based approaches, offering insights into\nfuture improvements in autonomous hazard detection systems. Our models,\nscripts, and data can be found at https://github.com/mi3labucm/COOOLER.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting anomalous hazards in visual data, particularly in video streams, is\na critical challenge in autonomous driving. Existing models often struggle with\nunpredictable, out-of-label hazards due to their reliance on predefined object\ncategories. In this paper, we propose a multimodal approach that integrates\nvision-language reasoning with zero-shot object detection to improve hazard\nidentification and explanation. Our pipeline consists of a Vision-Language\nModel (VLM), a Large Language Model (LLM), in order to detect hazardous objects\nwithin a traffic scene. We refine object detection by incorporating OpenAI's\nCLIP model to match predicted hazards with bounding box annotations, improving\nlocalization accuracy. To assess model performance, we create a ground truth\ndataset by denoising and extending the foundational COOOL\n(Challenge-of-Out-of-Label) anomaly detection benchmark dataset with complete\nnatural language descriptions for hazard annotations. We define a means of\nhazard detection and labeling evaluation on the extended dataset using cosine\nsimilarity. This evaluation considers the semantic similarity between the\npredicted hazard description and the annotated ground truth for each video.\nAdditionally, we release a set of tools for structuring and managing\nlarge-scale hazard detection datasets. Our findings highlight the strengths and\nlimitations of current vision-language-based approaches, offering insights into\nfuture improvements in autonomous hazard detection systems. Our models,\nscripts, and data can be found at https://github.com/mi3labucm/COOOLER.git"
                },
                "authors": [
                    {
                        "name": "Shashank Shriram"
                    },
                    {
                        "name": "Srinivasa Perisetla"
                    },
                    {
                        "name": "Aryan Keskar"
                    },
                    {
                        "name": "Harsha Krishnaswamy"
                    },
                    {
                        "name": "Tonko Emil Westerhof Bossen"
                    },
                    {
                        "name": "Andreas Møgelmose"
                    },
                    {
                        "name": "Ross Greer"
                    }
                ],
                "author_detail": {
                    "name": "Ross Greer"
                },
                "author": "Ross Greer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13399v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12170v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12170v3",
                "updated": "2025-04-18T00:24:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    0,
                    24,
                    19,
                    4,
                    108,
                    0
                ],
                "published": "2024-02-16T06:29:16Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    6,
                    29,
                    16,
                    4,
                    47,
                    0
                ],
                "title": "Where is the answer? Investigating Positional Bias in Language Model\n  Knowledge Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where is the answer? Investigating Positional Bias in Language Model\n  Knowledge Extraction"
                },
                "summary": "Large language models require updates to remain up-to-date or adapt to new\ndomains by fine-tuning them with new documents. One key is memorizing the\nlatest information in a way that the memorized information is extractable with\na query prompt. However, LLMs suffer from a phenomenon called perplexity curse;\ndespite minimizing document perplexity during fine-tuning, LLMs struggle to\nextract information through a prompt sentence. In this new knowledge\nacquisition and extraction, we find a very intriguing fact that LLMs can\naccurately answer questions about the first sentence, but they struggle to\nextract information described in the middle or end of the documents used for\nfine-tuning. Our study suggests that the auto-regressive training causes this\nissue; each token is prompted by reliance on all previous tokens, which hinders\nthe model from recalling information from training documents by question\nprompts. To conduct the in-depth study, we publish both synthetic and real\ndatasets, enabling the evaluation of the QA performance w.r.t. the position of\nthe corresponding answer in a document. Our investigation shows that even a\nlarge model suffers from the perplexity curse, but regularization such as\ndenoising auto-regressive loss can enhance the information extraction from\ndiverse positions. These findings will be (i) a key to improving knowledge\nextraction from LLMs and (ii) new elements to discuss the trade-off between RAG\nand fine-tuning in adapting LLMs to a new domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models require updates to remain up-to-date or adapt to new\ndomains by fine-tuning them with new documents. One key is memorizing the\nlatest information in a way that the memorized information is extractable with\na query prompt. However, LLMs suffer from a phenomenon called perplexity curse;\ndespite minimizing document perplexity during fine-tuning, LLMs struggle to\nextract information through a prompt sentence. In this new knowledge\nacquisition and extraction, we find a very intriguing fact that LLMs can\naccurately answer questions about the first sentence, but they struggle to\nextract information described in the middle or end of the documents used for\nfine-tuning. Our study suggests that the auto-regressive training causes this\nissue; each token is prompted by reliance on all previous tokens, which hinders\nthe model from recalling information from training documents by question\nprompts. To conduct the in-depth study, we publish both synthetic and real\ndatasets, enabling the evaluation of the QA performance w.r.t. the position of\nthe corresponding answer in a document. Our investigation shows that even a\nlarge model suffers from the perplexity curse, but regularization such as\ndenoising auto-regressive loss can enhance the information extraction from\ndiverse positions. These findings will be (i) a key to improving knowledge\nextraction from LLMs and (ii) new elements to discuss the trade-off between RAG\nand fine-tuning in adapting LLMs to a new domain."
                },
                "authors": [
                    {
                        "name": "Kuniaki Saito"
                    },
                    {
                        "name": "Kihyuk Sohn"
                    },
                    {
                        "name": "Chen-Yu Lee"
                    },
                    {
                        "name": "Yoshitaka Ushiku"
                    }
                ],
                "author_detail": {
                    "name": "Yoshitaka Ushiku"
                },
                "author": "Yoshitaka Ushiku",
                "arxiv_comment": "Code is published at https://github.com/omron-sinicx/WhereIsTheAnswer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12170v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12170v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14570v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14570v2",
                "updated": "2025-04-17T23:26:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    23,
                    26,
                    11,
                    3,
                    107,
                    0
                ],
                "published": "2024-10-18T16:16:52Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    16,
                    52,
                    4,
                    292,
                    0
                ],
                "title": "Understanding the Difficulty of Low-Precision Post-Training Quantization\n  for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Difficulty of Low-Precision Post-Training Quantization\n  for LLMs"
                },
                "summary": "Large language models of high parameter counts are computationally expensive,\nyet can be made much more efficient by compressing their weights to very low\nnumerical precision. This can be achieved either through post-training\nquantization by minimizing local, layer-wise quantization errors, or through\nquantization-aware fine-tuning by minimizing the global loss function. In this\nstudy, we discovered that, under the same data constraint, the former approach\nnearly always fared worse than the latter, a phenomenon particularly prominent\nwhen the numerical precision is very low. We further showed that this\ndifficulty of post-training quantization arose from stark misalignment between\noptimization of the local and global objective functions. Our findings explains\nlimited utility in minimization of local quantization error and the importance\nof direct quantization-aware fine-tuning, in the regime of large models at very\nlow precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models of high parameter counts are computationally expensive,\nyet can be made much more efficient by compressing their weights to very low\nnumerical precision. This can be achieved either through post-training\nquantization by minimizing local, layer-wise quantization errors, or through\nquantization-aware fine-tuning by minimizing the global loss function. In this\nstudy, we discovered that, under the same data constraint, the former approach\nnearly always fared worse than the latter, a phenomenon particularly prominent\nwhen the numerical precision is very low. We further showed that this\ndifficulty of post-training quantization arose from stark misalignment between\noptimization of the local and global objective functions. Our findings explains\nlimited utility in minimization of local quantization error and the importance\nof direct quantization-aware fine-tuning, in the regime of large models at very\nlow precision."
                },
                "authors": [
                    {
                        "name": "Zifei Xu"
                    },
                    {
                        "name": "Sayeh Sharify"
                    },
                    {
                        "name": "Wanzin Yazar"
                    },
                    {
                        "name": "Tristan Webb"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14570v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14570v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.10776v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.10776v6",
                "updated": "2025-04-17T22:42:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    22,
                    42,
                    4,
                    3,
                    107,
                    0
                ],
                "published": "2023-11-16T01:21:33Z",
                "published_parsed": [
                    2023,
                    11,
                    16,
                    1,
                    21,
                    33,
                    3,
                    320,
                    0
                ],
                "title": "Chemist-X: Large Language Model-empowered Agent for Reaction Condition\n  Recommendation in Chemical Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chemist-X: Large Language Model-empowered Agent for Reaction Condition\n  Recommendation in Chemical Synthesis"
                },
                "summary": "Recent AI research plots a promising future of automatic chemical reactions\nwithin the chemistry society. This study proposes Chemist-X, a comprehensive AI\nagent that automates the reaction condition optimization (RCO) task in chemical\nsynthesis with retrieval-augmented generation (RAG) technology and\nAI-controlled wet-lab experiment executions. To begin with, as an emulation on\nhow chemical experts solve the RCO task, Chemist-X utilizes a novel RAG scheme\nto interrogate available molecular and literature databases to narrow the\nsearching space for later processing. The agent then leverages a computer-aided\ndesign (CAD) tool we have developed through a large language model (LLM)\nsupervised programming interface. With updated chemical knowledge obtained via\nRAG, as well as the ability in using CAD tools, our agent significantly\noutperforms conventional RCO AIs confined to the fixed knowledge within its\ntraining data. Finally, Chemist-X interacts with the physical world through an\nautomated robotic system, which can validate the suggested chemical reaction\ncondition without human interventions. The control of the robotic system was\nachieved with a novel algorithm we have developed for the equipment, which\nrelies on LLMs for reliable script generation. Results of our automatic wet-lab\nexperiments, achieved by fully LLM-supervised end-to-end operation with no\nhuman in the lope, prove Chemist-X's ability in self-driving laboratories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent AI research plots a promising future of automatic chemical reactions\nwithin the chemistry society. This study proposes Chemist-X, a comprehensive AI\nagent that automates the reaction condition optimization (RCO) task in chemical\nsynthesis with retrieval-augmented generation (RAG) technology and\nAI-controlled wet-lab experiment executions. To begin with, as an emulation on\nhow chemical experts solve the RCO task, Chemist-X utilizes a novel RAG scheme\nto interrogate available molecular and literature databases to narrow the\nsearching space for later processing. The agent then leverages a computer-aided\ndesign (CAD) tool we have developed through a large language model (LLM)\nsupervised programming interface. With updated chemical knowledge obtained via\nRAG, as well as the ability in using CAD tools, our agent significantly\noutperforms conventional RCO AIs confined to the fixed knowledge within its\ntraining data. Finally, Chemist-X interacts with the physical world through an\nautomated robotic system, which can validate the suggested chemical reaction\ncondition without human interventions. The control of the robotic system was\nachieved with a novel algorithm we have developed for the equipment, which\nrelies on LLMs for reliable script generation. Results of our automatic wet-lab\nexperiments, achieved by fully LLM-supervised end-to-end operation with no\nhuman in the lope, prove Chemist-X's ability in self-driving laboratories."
                },
                "authors": [
                    {
                        "name": "Kexin Chen"
                    },
                    {
                        "name": "Jiamin Lu"
                    },
                    {
                        "name": "Junyou Li"
                    },
                    {
                        "name": "Xiaoran Yang"
                    },
                    {
                        "name": "Yuyang Du"
                    },
                    {
                        "name": "Kunyi Wang"
                    },
                    {
                        "name": "Qiannuan Shi"
                    },
                    {
                        "name": "Jiahui Yu"
                    },
                    {
                        "name": "Lanqing Li"
                    },
                    {
                        "name": "Jiezhong Qiu"
                    },
                    {
                        "name": "Jianzhang Pan"
                    },
                    {
                        "name": "Yi Huang"
                    },
                    {
                        "name": "Qun Fang"
                    },
                    {
                        "name": "Pheng Ann Heng"
                    },
                    {
                        "name": "Guangyong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guangyong Chen"
                },
                "author": "Guangyong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.10776v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.10776v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23798v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23798v2",
                "updated": "2025-04-17T22:26:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    22,
                    26,
                    31,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-31T07:20:58Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    7,
                    20,
                    58,
                    0,
                    90,
                    0
                ],
                "title": "Adaptive Layer-skipping in Pre-trained LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Layer-skipping in Pre-trained LLMs"
                },
                "summary": "Various layer-skipping methods have been proposed to accelerate token\ngeneration in large language models (LLMs). However, they have overlooked a\nfundamental question: How do computational demands vary across the generation\nof different tokens? In this work, we introduce FlexiDepth, a method that\ndynamically adjusts the number of Transformer layers used in text generation.\nBy incorporating a plug-in router and adapter, FlexiDepth enables adaptive\nlayer-skipping in LLMs without modifying their original parameters. Introducing\nFlexiDepth to Llama-3-8B model achieves layer skipping of 8 layers out of 32,\nand meanwhile maintains the full 100\\% benchmark performance. Experimental\nresults with FlexiDepth demonstrate that computational demands in LLMs\nsignificantly vary based on token type. Specifically, generating repetitive\ntokens or fixed phrases requires fewer layers, whereas producing tokens\ninvolving computation or high uncertainty requires more layers. Interestingly,\nthis adaptive allocation pattern aligns with human intuition. To advance\nresearch in this area, we open sourced FlexiDepth and a dataset documenting\nFlexiDepth's layer allocation patterns for future exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various layer-skipping methods have been proposed to accelerate token\ngeneration in large language models (LLMs). However, they have overlooked a\nfundamental question: How do computational demands vary across the generation\nof different tokens? In this work, we introduce FlexiDepth, a method that\ndynamically adjusts the number of Transformer layers used in text generation.\nBy incorporating a plug-in router and adapter, FlexiDepth enables adaptive\nlayer-skipping in LLMs without modifying their original parameters. Introducing\nFlexiDepth to Llama-3-8B model achieves layer skipping of 8 layers out of 32,\nand meanwhile maintains the full 100\\% benchmark performance. Experimental\nresults with FlexiDepth demonstrate that computational demands in LLMs\nsignificantly vary based on token type. Specifically, generating repetitive\ntokens or fixed phrases requires fewer layers, whereas producing tokens\ninvolving computation or high uncertainty requires more layers. Interestingly,\nthis adaptive allocation pattern aligns with human intuition. To advance\nresearch in this area, we open sourced FlexiDepth and a dataset documenting\nFlexiDepth's layer allocation patterns for future exploration."
                },
                "authors": [
                    {
                        "name": "Xuan Luo"
                    },
                    {
                        "name": "Weizhi Wang"
                    },
                    {
                        "name": "Xifeng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Xifeng Yan"
                },
                "author": "Xifeng Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23798v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23798v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]