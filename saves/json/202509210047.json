[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.15038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15038v1",
                "updated": "2025-09-18T15:04:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    4,
                    6,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:04:06Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    4,
                    6,
                    3,
                    261,
                    0
                ],
                "title": "Value-Guided KV Compression for LLMs via Approximated CUR Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-Guided KV Compression for LLMs via Approximated CUR Decomposition"
                },
                "summary": "Key-value (KV) cache compression has emerged as a critical technique for\nreducing the memory and latency overhead of autoregressive language models\nduring inference. Prior approaches predominantly rely on query-key attention\nscores to rank and evict cached tokens, assuming that attention intensity\ncorrelates with semantic importance. However, this heuristic overlooks the\ncontribution of value vectors, which directly influence the attention output.\nIn this paper, we propose CurDKV, a novel, value-centric KV compression method\nthat selects keys and values based on leverage scores computed from CUR matrix\ndecomposition. Our approach approximates the dominant subspace of the attention\noutput $softmax(QK^T)V$, ensuring that the retained tokens best preserve the\nmodel's predictive behavior. Theoretically, we show that attention score\napproximation does not guarantee output preservation, and demonstrate that\nCUR-based selection minimizes end-to-end attention reconstruction loss.\nEmpirically, CurDKV achieves up to 9.6% higher accuracy than state-of-the-art\nmethods like SnapKV and ChunkKV under aggressive compression budgets on LLaMA\nand Mistral, while maintaining compatibility with FlashAttention and Grouped\nQuery Attention. In addition to improved accuracy, CurDKV reduces generation\nlatency by up to 40% at high compression, offering a practical speed-accuracy\ntradeoff.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) cache compression has emerged as a critical technique for\nreducing the memory and latency overhead of autoregressive language models\nduring inference. Prior approaches predominantly rely on query-key attention\nscores to rank and evict cached tokens, assuming that attention intensity\ncorrelates with semantic importance. However, this heuristic overlooks the\ncontribution of value vectors, which directly influence the attention output.\nIn this paper, we propose CurDKV, a novel, value-centric KV compression method\nthat selects keys and values based on leverage scores computed from CUR matrix\ndecomposition. Our approach approximates the dominant subspace of the attention\noutput $softmax(QK^T)V$, ensuring that the retained tokens best preserve the\nmodel's predictive behavior. Theoretically, we show that attention score\napproximation does not guarantee output preservation, and demonstrate that\nCUR-based selection minimizes end-to-end attention reconstruction loss.\nEmpirically, CurDKV achieves up to 9.6% higher accuracy than state-of-the-art\nmethods like SnapKV and ChunkKV under aggressive compression budgets on LLaMA\nand Mistral, while maintaining compatibility with FlashAttention and Grouped\nQuery Attention. In addition to improved accuracy, CurDKV reduces generation\nlatency by up to 40% at high compression, offering a practical speed-accuracy\ntradeoff."
                },
                "authors": [
                    {
                        "name": "Ayan Sengupta"
                    },
                    {
                        "name": "Siddhant Chaudhary"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15024v1",
                "updated": "2025-09-18T14:51:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    51,
                    13,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T14:51:13Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    51,
                    13,
                    3,
                    261,
                    0
                ],
                "title": "Attention Beyond Neighborhoods: Reviving Transformer for Graph\n  Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Beyond Neighborhoods: Reviving Transformer for Graph\n  Clustering"
                },
                "summary": "Attention mechanisms have become a cornerstone in modern neural networks,\ndriving breakthroughs across diverse domains. However, their application to\ngraph structured data, where capturing topological connections is essential,\nremains underexplored and underperforming compared to Graph Neural Networks\n(GNNs), particularly in the graph clustering task. GNN tends to overemphasize\nneighborhood aggregation, leading to a homogenization of node representations.\nConversely, Transformer tends to over globalize, highlighting distant nodes at\nthe expense of meaningful local patterns. This dichotomy raises a key question:\nIs attention inherently redundant for unsupervised graph learning? To address\nthis, we conduct a comprehensive empirical analysis, uncovering the\ncomplementary weaknesses of GNN and Transformer in graph clustering. Motivated\nby these insights, we propose the Attentive Graph Clustering Network (AGCN) a\nnovel architecture that reinterprets the notion that graph is attention. AGCN\ndirectly embeds the attention mechanism into the graph structure, enabling\neffective global information extraction while maintaining sensitivity to local\ntopological cues. Our framework incorporates theoretical analysis to contrast\nAGCN behavior with GNN and Transformer and introduces two innovations: (1) a KV\ncache mechanism to improve computational efficiency, and (2) a pairwise margin\ncontrastive loss to boost the discriminative capacity of the attention space.\nExtensive experimental results demonstrate that AGCN outperforms\nstate-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms have become a cornerstone in modern neural networks,\ndriving breakthroughs across diverse domains. However, their application to\ngraph structured data, where capturing topological connections is essential,\nremains underexplored and underperforming compared to Graph Neural Networks\n(GNNs), particularly in the graph clustering task. GNN tends to overemphasize\nneighborhood aggregation, leading to a homogenization of node representations.\nConversely, Transformer tends to over globalize, highlighting distant nodes at\nthe expense of meaningful local patterns. This dichotomy raises a key question:\nIs attention inherently redundant for unsupervised graph learning? To address\nthis, we conduct a comprehensive empirical analysis, uncovering the\ncomplementary weaknesses of GNN and Transformer in graph clustering. Motivated\nby these insights, we propose the Attentive Graph Clustering Network (AGCN) a\nnovel architecture that reinterprets the notion that graph is attention. AGCN\ndirectly embeds the attention mechanism into the graph structure, enabling\neffective global information extraction while maintaining sensitivity to local\ntopological cues. Our framework incorporates theoretical analysis to contrast\nAGCN behavior with GNN and Transformer and introduces two innovations: (1) a KV\ncache mechanism to improve computational efficiency, and (2) a pairwise margin\ncontrastive loss to boost the discriminative capacity of the attention space.\nExtensive experimental results demonstrate that AGCN outperforms\nstate-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Xuanting Xie"
                    },
                    {
                        "name": "Bingheng Li"
                    },
                    {
                        "name": "Erlin Pan"
                    },
                    {
                        "name": "Rui Hou"
                    },
                    {
                        "name": "Wenyu Chen"
                    },
                    {
                        "name": "Zhao Kang"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Kang"
                },
                "author": "Zhao Kang",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13789v2",
                "updated": "2025-09-18T04:57:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    4,
                    57,
                    32,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-17T07:58:36Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    58,
                    36,
                    2,
                    260,
                    0
                ],
                "title": "BWCache: Accelerating Video Diffusion Transformers through Block-Wise\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BWCache: Accelerating Video Diffusion Transformers through Block-Wise\n  Caching"
                },
                "summary": "Recent advancements in Diffusion Transformers (DiTs) have established them as\nthe state-of-the-art method for video generation. However, their inherently\nsequential denoising process results in inevitable latency, limiting real-world\napplicability. Existing acceleration methods either compromise visual quality\ndue to architectural modifications or fail to reuse intermediate features at\nproper granularity. Our analysis reveals that DiT blocks are the primary\ncontributors to inference latency. Across diffusion timesteps, the feature\nvariations of DiT blocks exhibit a U-shaped pattern with high similarity during\nintermediate timesteps, which suggests substantial computational redundancy. In\nthis paper, we propose Block-Wise Caching (BWCache), a training-free method to\naccelerate DiT-based video generation. BWCache dynamically caches and reuses\nfeatures from DiT blocks across diffusion timesteps. Furthermore, we introduce\na similarity indicator that triggers feature reuse only when the differences\nbetween block features at adjacent timesteps fall below a threshold, thereby\nminimizing redundant computations while maintaining visual fidelity. Extensive\nexperiments on several video diffusion models demonstrate that BWCache achieves\nup to 2.24$\\times$ speedup with comparable visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Diffusion Transformers (DiTs) have established them as\nthe state-of-the-art method for video generation. However, their inherently\nsequential denoising process results in inevitable latency, limiting real-world\napplicability. Existing acceleration methods either compromise visual quality\ndue to architectural modifications or fail to reuse intermediate features at\nproper granularity. Our analysis reveals that DiT blocks are the primary\ncontributors to inference latency. Across diffusion timesteps, the feature\nvariations of DiT blocks exhibit a U-shaped pattern with high similarity during\nintermediate timesteps, which suggests substantial computational redundancy. In\nthis paper, we propose Block-Wise Caching (BWCache), a training-free method to\naccelerate DiT-based video generation. BWCache dynamically caches and reuses\nfeatures from DiT blocks across diffusion timesteps. Furthermore, we introduce\na similarity indicator that triggers feature reuse only when the differences\nbetween block features at adjacent timesteps fall below a threshold, thereby\nminimizing redundant computations while maintaining visual fidelity. Extensive\nexperiments on several video diffusion models demonstrate that BWCache achieves\nup to 2.24$\\times$ speedup with comparable visual quality."
                },
                "authors": [
                    {
                        "name": "Hanshuai Cui"
                    },
                    {
                        "name": "Zhiqing Tang"
                    },
                    {
                        "name": "Zhifei Xu"
                    },
                    {
                        "name": "Zhi Yao"
                    },
                    {
                        "name": "Wenyi Zeng"
                    },
                    {
                        "name": "Weijia Jia"
                    }
                ],
                "author_detail": {
                    "name": "Weijia Jia"
                },
                "author": "Weijia Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14403v1",
                "updated": "2025-09-17T20:08:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    20,
                    8,
                    53,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T20:08:53Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    20,
                    8,
                    53,
                    2,
                    260,
                    0
                ],
                "title": "Kilovolt-Class $β-Ga_2O_3$ Field-Plated Schottky Barrier Diodes with\n  MOCVD-Grown Intentionally $10^{15}$ $cm^{-3}$ Doped Drift Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kilovolt-Class $β-Ga_2O_3$ Field-Plated Schottky Barrier Diodes with\n  MOCVD-Grown Intentionally $10^{15}$ $cm^{-3}$ Doped Drift Layers"
                },
                "summary": "We report on the growth optimization of intentionally low-doped ($10^{15}$\n$cm^{-3}$) high-quality $\\beta-Ga_2O_3$ drift layers up to 10 $\\mu m$ thick via\nMOCVD and the fabrication of kilovolt-class field plated Schottky barrier\ndiodes on these thick drift layers. Homoepitaxial growth was performed on (010)\n$10^{15}$ $cm^{-3}$ substrates using TMGa as the Ga precursor. Growth\nparameters were systematically optimized to determine the best conditions for\nhigh quality thick growths with the given reactor geometry. Chamber pressure\nwas found to improve the growth rate, mobility, and roughness of the samples.\nGrowth rates of up to 7.2 $\\mu m$/hr., thicknesses of up to 10 $\\mu m$, Hall\nmobilities of up to 176 $cm^2$/Vs, RMS roughness down to 5.45 nm, UID\nconcentrations as low as $2 \\times$ $10^{15}$ $cm^{-3}$, and controllable\nintentional doping down to $3 \\times$ $10^{15}$ $cm^{-3}$ were achieved. Field\nplated Schottky barrier diodes (FP-SBDs) were fabricated on a $6.5 \\times$\n$10^{15}$ $cm^{-3}$ intentionally doped 10 $\\mu m$ thick film to determine the\nelectrical performance of the MOCVD-grown material. The FP-SBD was found to\nhave current density $>$100 A/$cm^2$ at 3 V forward bias with a specific\ndifferential on resistance ($R_{on,sp}$) of 16.22 m$\\Omega$.$cm^2$ and a turn\non voltage of 1 V. The diodes were found to have high quality anode\nmetal/semiconductor interfaces with an ideality factor of 1.04, close to unity.\nDiodes had a maximum breakdown voltage of 1.50 kV, leading to a punch-through\nmaximum field of 2.04 MV/cm under the anode metal, which is a state-of-the-art\nresult for SBDs on MOCVD-grown (010) drift layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on the growth optimization of intentionally low-doped ($10^{15}$\n$cm^{-3}$) high-quality $\\beta-Ga_2O_3$ drift layers up to 10 $\\mu m$ thick via\nMOCVD and the fabrication of kilovolt-class field plated Schottky barrier\ndiodes on these thick drift layers. Homoepitaxial growth was performed on (010)\n$10^{15}$ $cm^{-3}$ substrates using TMGa as the Ga precursor. Growth\nparameters were systematically optimized to determine the best conditions for\nhigh quality thick growths with the given reactor geometry. Chamber pressure\nwas found to improve the growth rate, mobility, and roughness of the samples.\nGrowth rates of up to 7.2 $\\mu m$/hr., thicknesses of up to 10 $\\mu m$, Hall\nmobilities of up to 176 $cm^2$/Vs, RMS roughness down to 5.45 nm, UID\nconcentrations as low as $2 \\times$ $10^{15}$ $cm^{-3}$, and controllable\nintentional doping down to $3 \\times$ $10^{15}$ $cm^{-3}$ were achieved. Field\nplated Schottky barrier diodes (FP-SBDs) were fabricated on a $6.5 \\times$\n$10^{15}$ $cm^{-3}$ intentionally doped 10 $\\mu m$ thick film to determine the\nelectrical performance of the MOCVD-grown material. The FP-SBD was found to\nhave current density $>$100 A/$cm^2$ at 3 V forward bias with a specific\ndifferential on resistance ($R_{on,sp}$) of 16.22 m$\\Omega$.$cm^2$ and a turn\non voltage of 1 V. The diodes were found to have high quality anode\nmetal/semiconductor interfaces with an ideality factor of 1.04, close to unity.\nDiodes had a maximum breakdown voltage of 1.50 kV, leading to a punch-through\nmaximum field of 2.04 MV/cm under the anode metal, which is a state-of-the-art\nresult for SBDs on MOCVD-grown (010) drift layers."
                },
                "authors": [
                    {
                        "name": "Carl Peterson"
                    },
                    {
                        "name": "Chinmoy Nath Saha"
                    },
                    {
                        "name": "Rachel Kahler"
                    },
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Akhila Mattapalli"
                    },
                    {
                        "name": "Saurav Roy"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14347v1",
                "updated": "2025-09-17T18:26:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    18,
                    26,
                    29,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T18:26:29Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    18,
                    26,
                    29,
                    2,
                    260,
                    0
                ],
                "title": "On the Illusion of Success: An Empirical Study of Build Reruns and\n  Silent Failures in Industrial CI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Illusion of Success: An Empirical Study of Build Reruns and\n  Silent Failures in Industrial CI"
                },
                "summary": "Reliability of build outcomes is a cornerstone of effective Continuous\nIntegration (CI). Yet in practice, developers often struggle with\nnon-deterministic issues in the code or CI infrastructure, which undermine\ntrust in build results. When faced with such unexpected outcomes, developers\noften repeatedly rerun jobs hoping for true success, but this practice is known\nto increase CI costs and reduce productivity. While recent studies have focused\non intermittent job failures, no prior work has investigated silent failures,\nwhere build jobs are marked as successful but fail to complete all or part of\ntheir tasks. Such silent failures often go unnoticed, creating an illusion of\nsuccess with detrimental consequences such as bugs escaping into production.\nThis paper presents the first empirical study of silent failures through the\npractice of rerunning successful jobs. An analysis of 142,387 jobs across 81\nindustrial projects shows that 11% of successful jobs are rerun, with 35% of\nthese reruns occurring after more than 24 hours. Using mixed-effects models on\n32 independent variables (AUC of 85%), we identified key factors associated\nwith reruns of successful jobs, notably testing and static analysis tasks,\nscripting languages like Shell, and developers prior rerun tendencies. A\nfurther analysis of 92 public issues revealed 11 categories of silent failures\naligning with these factors, the most frequent being artifact operation errors,\ncaching errors, and ignored exit codes. Overall, our findings provide valuable\ninsights into the circumstances and causes of silent failures to raise\nawareness among teams, and present solutions to improve CI reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliability of build outcomes is a cornerstone of effective Continuous\nIntegration (CI). Yet in practice, developers often struggle with\nnon-deterministic issues in the code or CI infrastructure, which undermine\ntrust in build results. When faced with such unexpected outcomes, developers\noften repeatedly rerun jobs hoping for true success, but this practice is known\nto increase CI costs and reduce productivity. While recent studies have focused\non intermittent job failures, no prior work has investigated silent failures,\nwhere build jobs are marked as successful but fail to complete all or part of\ntheir tasks. Such silent failures often go unnoticed, creating an illusion of\nsuccess with detrimental consequences such as bugs escaping into production.\nThis paper presents the first empirical study of silent failures through the\npractice of rerunning successful jobs. An analysis of 142,387 jobs across 81\nindustrial projects shows that 11% of successful jobs are rerun, with 35% of\nthese reruns occurring after more than 24 hours. Using mixed-effects models on\n32 independent variables (AUC of 85%), we identified key factors associated\nwith reruns of successful jobs, notably testing and static analysis tasks,\nscripting languages like Shell, and developers prior rerun tendencies. A\nfurther analysis of 92 public issues revealed 11 categories of silent failures\naligning with these factors, the most frequent being artifact operation errors,\ncaching errors, and ignored exit codes. Overall, our findings provide valuable\ninsights into the circumstances and causes of silent failures to raise\nawareness among teams, and present solutions to improve CI reliability."
                },
                "authors": [
                    {
                        "name": "Henri Aïdasso"
                    },
                    {
                        "name": "Francis Bordeleau"
                    },
                    {
                        "name": "Ali Tizghadam"
                    }
                ],
                "author_detail": {
                    "name": "Ali Tizghadam"
                },
                "author": "Ali Tizghadam",
                "arxiv_comment": "17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14093v1",
                "updated": "2025-09-17T15:33:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    33,
                    44,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T15:33:44Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    33,
                    44,
                    2,
                    260,
                    0
                ],
                "title": "Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A\n  Self-Optimizing Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A\n  Self-Optimizing Framework"
                },
                "summary": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by\nprompting intermediate steps, improving accuracy and robustness in arithmetic,\nlogic, and commonsense tasks. However, this benefit comes with high\ncomputational costs: longer outputs increase latency, memory usage, and\nKV-cache demands. These issues are especially critical in software engineering\ntasks where concise and deterministic outputs are required. To investigate\nthese trade-offs, we conduct an empirical study based on code generation\nbenchmarks. The results reveal that longer CoT does not always help. Excessive\nreasoning often causes truncation, accuracy drops, and latency up to five times\nhigher, with failed outputs consistently longer than successful ones. These\nfindings challenge the assumption that longer reasoning is inherently better\nand highlight the need for adaptive CoT control. Motivated by this, we propose\nSEER (Self-Enhancing Efficient Reasoning), an adaptive framework that\ncompresses CoT while preserving accuracy. SEER combines Best-of-N sampling with\ntask-aware adaptive filtering, dynamically adjusting thresholds based on\npre-inference outputs to reduce verbosity and computational overhead. We then\nevaluate SEER on three software engineering tasks and one math task. On\naverage, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,\nand eliminates most infinite loops. These results demonstrate SEER as a\npractical method to make CoT-enhanced LLMs more efficient and robust, even\nunder resource constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by\nprompting intermediate steps, improving accuracy and robustness in arithmetic,\nlogic, and commonsense tasks. However, this benefit comes with high\ncomputational costs: longer outputs increase latency, memory usage, and\nKV-cache demands. These issues are especially critical in software engineering\ntasks where concise and deterministic outputs are required. To investigate\nthese trade-offs, we conduct an empirical study based on code generation\nbenchmarks. The results reveal that longer CoT does not always help. Excessive\nreasoning often causes truncation, accuracy drops, and latency up to five times\nhigher, with failed outputs consistently longer than successful ones. These\nfindings challenge the assumption that longer reasoning is inherently better\nand highlight the need for adaptive CoT control. Motivated by this, we propose\nSEER (Self-Enhancing Efficient Reasoning), an adaptive framework that\ncompresses CoT while preserving accuracy. SEER combines Best-of-N sampling with\ntask-aware adaptive filtering, dynamically adjusting thresholds based on\npre-inference outputs to reduce verbosity and computational overhead. We then\nevaluate SEER on three software engineering tasks and one math task. On\naverage, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,\nand eliminates most infinite loops. These results demonstrate SEER as a\npractical method to make CoT-enhanced LLMs more efficient and robust, even\nunder resource constraints."
                },
                "authors": [
                    {
                        "name": "Kerui Huang"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Tongtong Xu"
                    },
                    {
                        "name": "Lingfeng Bao"
                    },
                    {
                        "name": "Xin Xia"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xia"
                },
                "author": "Xin Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14041v1",
                "updated": "2025-09-17T14:42:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    42,
                    38,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T14:42:38Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    42,
                    38,
                    2,
                    260,
                    0
                ],
                "title": "A TRRIP Down Memory Lane: Temperature-Based Re-Reference Interval\n  Prediction For Instruction Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A TRRIP Down Memory Lane: Temperature-Based Re-Reference Interval\n  Prediction For Instruction Caching"
                },
                "summary": "Modern mobile CPU software pose challenges for conventional instruction cache\nreplacement policies due to their complex runtime behavior causing high reuse\ndistance between executions of the same instruction. Mobile code commonly\nsuffers from large amounts of stalls in the CPU frontend and thus starvation of\nthe rest of the CPU resources. Complexity of these applications and their code\nfootprint are projected to grow at a rate faster than available on-chip memory\ndue to power and area constraints, making conventional hardware-centric methods\nfor managing instruction caches to be inadequate. We present a novel\nsoftware-hardware co-design approach called TRRIP (Temperature-based\nRe-Reference Interval Prediction) that enables the compiler to analyze,\nclassify, and transform code based on \"temperature\" (hot/cold), and to provide\nthe hardware with a summary of code temperature information through a\nwell-defined OS interface based on using code page attributes. TRRIP's\nlightweight hardware extension employs code temperature attributes to optimize\nthe instruction cache replacement policy resulting in the eviction rate\nreduction of hot code. TRRIP is designed to be practical and adoptable in real\nmobile systems that have strict feature requirements on both the software and\nhardware components. TRRIP can reduce the L2 MPKI for instructions by 26.5%\nresulting in geomean speedup of 3.9%, on top of RRIP cache replacement running\nmobile code already optimized using PGO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern mobile CPU software pose challenges for conventional instruction cache\nreplacement policies due to their complex runtime behavior causing high reuse\ndistance between executions of the same instruction. Mobile code commonly\nsuffers from large amounts of stalls in the CPU frontend and thus starvation of\nthe rest of the CPU resources. Complexity of these applications and their code\nfootprint are projected to grow at a rate faster than available on-chip memory\ndue to power and area constraints, making conventional hardware-centric methods\nfor managing instruction caches to be inadequate. We present a novel\nsoftware-hardware co-design approach called TRRIP (Temperature-based\nRe-Reference Interval Prediction) that enables the compiler to analyze,\nclassify, and transform code based on \"temperature\" (hot/cold), and to provide\nthe hardware with a summary of code temperature information through a\nwell-defined OS interface based on using code page attributes. TRRIP's\nlightweight hardware extension employs code temperature attributes to optimize\nthe instruction cache replacement policy resulting in the eviction rate\nreduction of hot code. TRRIP is designed to be practical and adoptable in real\nmobile systems that have strict feature requirements on both the software and\nhardware components. TRRIP can reduce the L2 MPKI for instructions by 26.5%\nresulting in geomean speedup of 3.9%, on top of RRIP cache replacement running\nmobile code already optimized using PGO."
                },
                "authors": [
                    {
                        "name": "Henry Kao"
                    },
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Prabhdeep Singh Soni"
                    },
                    {
                        "name": "Ali Sedaghati"
                    },
                    {
                        "name": "Fang Su"
                    },
                    {
                        "name": "Bryan Chan"
                    },
                    {
                        "name": "Maziar Goudarzi"
                    },
                    {
                        "name": "Reza Azimi"
                    }
                ],
                "author_detail": {
                    "name": "Reza Azimi"
                },
                "author": "Reza Azimi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13848v1",
                "updated": "2025-09-17T09:24:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    24,
                    40,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T09:24:40Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    24,
                    40,
                    2,
                    260,
                    0
                ],
                "title": "SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation"
                },
                "summary": "Feature caching has recently emerged as a promising method for diffusion\nmodel acceleration. It effectively alleviates the inefficiency problem caused\nby high computational requirements by caching similar features in the inference\nprocess of the diffusion model. In this paper, we analyze existing feature\ncaching methods from the perspective of information utilization, and point out\nthat relying solely on historical information will lead to constrained accuracy\nand speed performance. And we propose a novel paradigm that introduces future\ninformation via self-speculation based on the information similarity at the\nsame time step across different iteration times. Based on this paradigm, we\npresent \\textit{SpecDiff}, a training-free multi-level feature caching strategy\nincluding a cached feature selection algorithm and a multi-level feature\nclassification algorithm. (1) Feature selection algorithm based on\nself-speculative information. \\textit{SpecDiff} determines a dynamic importance\nscore for each token based on self-speculative information and historical\ninformation, and performs cached feature selection through the importance\nscore. (2) Multi-level feature classification algorithm based on feature\nimportance scores. \\textit{SpecDiff} classifies tokens by leveraging the\ndifferences in feature importance scores and introduces a multi-level feature\ncalculation strategy. Extensive experiments show that \\textit{SpecDiff}\nachieves average 2.80 \\times, 2.74 \\times , and 3.17\\times speedup with\nnegligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow\non NVIDIA A800-80GB GPU. By merging speculative and historical information,\n\\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing\nthe Pareto frontier of speedup and accuracy in the efficient diffusion model\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature caching has recently emerged as a promising method for diffusion\nmodel acceleration. It effectively alleviates the inefficiency problem caused\nby high computational requirements by caching similar features in the inference\nprocess of the diffusion model. In this paper, we analyze existing feature\ncaching methods from the perspective of information utilization, and point out\nthat relying solely on historical information will lead to constrained accuracy\nand speed performance. And we propose a novel paradigm that introduces future\ninformation via self-speculation based on the information similarity at the\nsame time step across different iteration times. Based on this paradigm, we\npresent \\textit{SpecDiff}, a training-free multi-level feature caching strategy\nincluding a cached feature selection algorithm and a multi-level feature\nclassification algorithm. (1) Feature selection algorithm based on\nself-speculative information. \\textit{SpecDiff} determines a dynamic importance\nscore for each token based on self-speculative information and historical\ninformation, and performs cached feature selection through the importance\nscore. (2) Multi-level feature classification algorithm based on feature\nimportance scores. \\textit{SpecDiff} classifies tokens by leveraging the\ndifferences in feature importance scores and introduces a multi-level feature\ncalculation strategy. Extensive experiments show that \\textit{SpecDiff}\nachieves average 2.80 \\times, 2.74 \\times , and 3.17\\times speedup with\nnegligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow\non NVIDIA A800-80GB GPU. By merging speculative and historical information,\n\\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing\nthe Pareto frontier of speedup and accuracy in the efficient diffusion model\ninference."
                },
                "authors": [
                    {
                        "name": "Jiayi Pan"
                    },
                    {
                        "name": "Jiaming Xu"
                    },
                    {
                        "name": "Yongkang Zhou"
                    },
                    {
                        "name": "Guohao Dai"
                    }
                ],
                "author_detail": {
                    "name": "Guohao Dai"
                },
                "author": "Guohao Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13604v1",
                "updated": "2025-09-17T00:28:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    0,
                    28,
                    49,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T00:28:49Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    0,
                    28,
                    49,
                    2,
                    260,
                    0
                ],
                "title": "A Framework for Multi-source Prefetching Through Adaptive Weight",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Multi-source Prefetching Through Adaptive Weight"
                },
                "summary": "The World Wide Web has come to be a great part of our daily life, yet user\nobserved latency is still a problem that needs a proper means of handling. Even\nthough earlier attempts focused on caching as the chief solution to tackling\nthis issue, its success was extremely limited. Prefetching has come to be the\nprimary technique in supplementing caching towards soothing the latency problem\nassociated with the contemporary Internet. However, existing approaches in\nprefetching are extremely limited in their ability to employ application level\nweb document relationship which is often visible only to the content developer.\nThis is because most approaches are access history based schemes that make\nfuture users' access prediction only based on past user access. Attempts to\nincorporate prefetching schemes that utilize semantic information with those\nthat use users past access history are extremely limited in their\nextensibility. In this work we present a novel framework that enables\nintegration of schemes from both worlds of prefetching without the need for a\nmajor modification to the algorithms. When there is a need/possibility to\ncapture new application level context, a new algorithm could be developed to do\nso and then it can be integrated into the framework. Since each participating\nscheme is merely viewed as an algorithm that produces a list of candidate\nobjects that are likely to be accessed in the near future, the framework can\nentertain any one of the existing prefetching schemes. With its adaptive weight\nmanagement technique the framework adjusts the effect of each algorithm in the\noverall prediction to parallel with its observed performance so far. We have\nfound this formwork to be less aggressive than its contemporary counterparts\nwhich is extremely important for resource constrained mobile devices that have\ncome to be the major means of access by users of the current web.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The World Wide Web has come to be a great part of our daily life, yet user\nobserved latency is still a problem that needs a proper means of handling. Even\nthough earlier attempts focused on caching as the chief solution to tackling\nthis issue, its success was extremely limited. Prefetching has come to be the\nprimary technique in supplementing caching towards soothing the latency problem\nassociated with the contemporary Internet. However, existing approaches in\nprefetching are extremely limited in their ability to employ application level\nweb document relationship which is often visible only to the content developer.\nThis is because most approaches are access history based schemes that make\nfuture users' access prediction only based on past user access. Attempts to\nincorporate prefetching schemes that utilize semantic information with those\nthat use users past access history are extremely limited in their\nextensibility. In this work we present a novel framework that enables\nintegration of schemes from both worlds of prefetching without the need for a\nmajor modification to the algorithms. When there is a need/possibility to\ncapture new application level context, a new algorithm could be developed to do\nso and then it can be integrated into the framework. Since each participating\nscheme is merely viewed as an algorithm that produces a list of candidate\nobjects that are likely to be accessed in the near future, the framework can\nentertain any one of the existing prefetching schemes. With its adaptive weight\nmanagement technique the framework adjusts the effect of each algorithm in the\noverall prediction to parallel with its observed performance so far. We have\nfound this formwork to be less aggressive than its contemporary counterparts\nwhich is extremely important for resource constrained mobile devices that have\ncome to be the major means of access by users of the current web."
                },
                "authors": [
                    {
                        "name": "Yoseph Berhanu Alebachew"
                    },
                    {
                        "name": "Mulugeta Libsie"
                    }
                ],
                "author_detail": {
                    "name": "Mulugeta Libsie"
                },
                "author": "Mulugeta Libsie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21492v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21492v3",
                "updated": "2025-09-16T23:56:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    23,
                    56,
                    55,
                    1,
                    259,
                    0
                ],
                "published": "2025-07-29T04:21:11Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    4,
                    21,
                    11,
                    1,
                    210,
                    0
                ],
                "title": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist"
                },
                "summary": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts."
                },
                "authors": [
                    {
                        "name": "Yicong Luo"
                    },
                    {
                        "name": "Senhe Hao"
                    },
                    {
                        "name": "Brian Wheatman"
                    },
                    {
                        "name": "Prashant Pandey"
                    },
                    {
                        "name": "Helen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Helen Xu"
                },
                "author": "Helen Xu",
                "arxiv_doi": "10.1145/3754598.3754655",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754655",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.21492v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21492v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Original paper was accepted into ICPP 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08256v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08256v2",
                "updated": "2025-09-16T23:15:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    23,
                    15,
                    44,
                    1,
                    259,
                    0
                ],
                "published": "2025-05-28T05:22:44Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    5,
                    22,
                    44,
                    2,
                    148,
                    0
                ],
                "title": "FIER: Fine-Grained and Efficient KV Cache Retrieval for Long-context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FIER: Fine-Grained and Efficient KV Cache Retrieval for Long-context LLM\n  Inference"
                },
                "summary": "The Key-Value (KV) cache reading latency increases significantly with context\nlengths, hindering the efficiency of long-context LLM inference. To address\nthis, previous works propose retaining a small fraction of KV cache based on\ntoken importance. For example, KV eviction uses static heuristics to retain\ntokens, while KV retrieval dynamically selects query-relevant tokens for more\nadaptive cache management. However, we observe that important tokens are often\nsparsely distributed across the long context. This sparsity makes existing\npage-level KV retrieval inaccurate, as each page may include irrelevant tokens\nand miss critical ones. In this work, we propose Fier, a\n\\underline{Fi}ne-Grained and \\underline{E}fficient KV cache\n\\underline{R}etrieval method. Fier uses 1-bit quantized keys to estimate the\nimportance of each token, resulting in efficient and precise retrieval.\nExperiments show that Fier matches full KV performance using only 11\\% of the\ncache budget across various long-context tasks, reducing decoding latency by\n1.2$\\times$ to 1.5$\\times$.Code is available at\nhttps://github.com/SimWangArizona/FIER",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache reading latency increases significantly with context\nlengths, hindering the efficiency of long-context LLM inference. To address\nthis, previous works propose retaining a small fraction of KV cache based on\ntoken importance. For example, KV eviction uses static heuristics to retain\ntokens, while KV retrieval dynamically selects query-relevant tokens for more\nadaptive cache management. However, we observe that important tokens are often\nsparsely distributed across the long context. This sparsity makes existing\npage-level KV retrieval inaccurate, as each page may include irrelevant tokens\nand miss critical ones. In this work, we propose Fier, a\n\\underline{Fi}ne-Grained and \\underline{E}fficient KV cache\n\\underline{R}etrieval method. Fier uses 1-bit quantized keys to estimate the\nimportance of each token, resulting in efficient and precise retrieval.\nExperiments show that Fier matches full KV performance using only 11\\% of the\ncache budget across various long-context tasks, reducing decoding latency by\n1.2$\\times$ to 1.5$\\times$.Code is available at\nhttps://github.com/SimWangArizona/FIER"
                },
                "authors": [
                    {
                        "name": "Dongwei Wang"
                    },
                    {
                        "name": "Zijie Liu"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Yuxin Ren"
                    },
                    {
                        "name": "Jianing Deng"
                    },
                    {
                        "name": "Jingtong Hu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Huanrui Yang"
                    }
                ],
                "author_detail": {
                    "name": "Huanrui Yang"
                },
                "author": "Huanrui Yang",
                "arxiv_comment": "EMNLP2025 Camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08256v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08256v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v3",
                "updated": "2025-09-16T10:33:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    33,
                    29,
                    1,
                    259,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_doi": "10.1145/3744916.3764523",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3744916.3764523",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.08523v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ICSE '26 (The 48th IEEE/ACM International Conference on\n  Software Engineering)",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12900v1",
                "updated": "2025-09-16T09:54:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    54,
                    58,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T09:54:58Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    54,
                    58,
                    1,
                    259,
                    0
                ],
                "title": "Topology and Fragility of European High-Voltage Networks: A\n  Cross-Country Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topology and Fragility of European High-Voltage Networks: A\n  Cross-Country Comparative Analysis"
                },
                "summary": "Reliable electricity supply depends on the seamless operation of high-voltage\ngrid infrastructure spanning both transmission and sub-transmission levels.\nBeneath this apparent uniformity lies a striking structural diversity, which\nleaves a clear imprint on system vulnerability. In this paper, we present\nharmonized topological models of the high-voltage grids of 15 European\ncountries, integrating all elements at voltage levels above 110 kV. Topological\nanalysis of these networks reveals a simple yet robust pattern: node degree\ndistributions consistently follow an exponential decay, but the rate of decay\nvaries significantly across countries. Through a detailed and systematic\nevaluation of network tolerance to node and edge removals, we show that the\ndecay rate delineates the boundary between systems that are more resilient to\nfailures and those that are prone to large-scale disruptions. Furthermore, we\ndemonstrate that this numerical boundary is highly sensitive to which layers of\nthe infrastructure are included in the models. To our knowledge, this study\nprovides the first quantitative cross-country comparison of 15 European\nhigh-voltage networks, linking topological properties with vulnerability\ncharacteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable electricity supply depends on the seamless operation of high-voltage\ngrid infrastructure spanning both transmission and sub-transmission levels.\nBeneath this apparent uniformity lies a striking structural diversity, which\nleaves a clear imprint on system vulnerability. In this paper, we present\nharmonized topological models of the high-voltage grids of 15 European\ncountries, integrating all elements at voltage levels above 110 kV. Topological\nanalysis of these networks reveals a simple yet robust pattern: node degree\ndistributions consistently follow an exponential decay, but the rate of decay\nvaries significantly across countries. Through a detailed and systematic\nevaluation of network tolerance to node and edge removals, we show that the\ndecay rate delineates the boundary between systems that are more resilient to\nfailures and those that are prone to large-scale disruptions. Furthermore, we\ndemonstrate that this numerical boundary is highly sensitive to which layers of\nthe infrastructure are included in the models. To our knowledge, this study\nprovides the first quantitative cross-country comparison of 15 European\nhigh-voltage networks, linking topological properties with vulnerability\ncharacteristics."
                },
                "authors": [
                    {
                        "name": "Bálint Hartmann"
                    },
                    {
                        "name": "Michelle T. Cirunay"
                    }
                ],
                "author_detail": {
                    "name": "Michelle T. Cirunay"
                },
                "author": "Michelle T. Cirunay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12867v1",
                "updated": "2025-09-16T09:22:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    22,
                    21,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T09:22:21Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    22,
                    21,
                    1,
                    259,
                    0
                ],
                "title": "Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use"
                },
                "summary": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding and reasoning, yet they remain limited when tackling\nreal-world tasks that require up-to-date knowledge, precise operations, or\nspecialized tool use. To address this, we propose Tool-R1, a reinforcement\nlearning framework that enables LLMs to perform general, compositional, and\nmulti-step tool use by generating executable Python code. Tool-R1 supports\nintegration of user-defined tools and standard libraries, with variable sharing\nacross steps to construct coherent workflows. An outcome-based reward function,\ncombining LLM-based answer judgment and code execution success, guides policy\noptimization. To improve training efficiency, we maintain a dynamic sample\nqueue to cache and reuse high-quality trajectories, reducing the overhead of\ncostly online sampling. Experiments on the GAIA benchmark show that Tool-R1\nsubstantially improves both accuracy and robustness, achieving about 10\\% gain\nover strong baselines, with larger improvements on complex multi-step tasks.\nThese results highlight the potential of Tool-R1 for enabling reliable and\nefficient tool-augmented reasoning in real-world applications. Our code will be\navailable at https://github.com/YBYBZhang/Tool-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding and reasoning, yet they remain limited when tackling\nreal-world tasks that require up-to-date knowledge, precise operations, or\nspecialized tool use. To address this, we propose Tool-R1, a reinforcement\nlearning framework that enables LLMs to perform general, compositional, and\nmulti-step tool use by generating executable Python code. Tool-R1 supports\nintegration of user-defined tools and standard libraries, with variable sharing\nacross steps to construct coherent workflows. An outcome-based reward function,\ncombining LLM-based answer judgment and code execution success, guides policy\noptimization. To improve training efficiency, we maintain a dynamic sample\nqueue to cache and reuse high-quality trajectories, reducing the overhead of\ncostly online sampling. Experiments on the GAIA benchmark show that Tool-R1\nsubstantially improves both accuracy and robustness, achieving about 10\\% gain\nover strong baselines, with larger improvements on complex multi-step tasks.\nThese results highlight the potential of Tool-R1 for enabling reliable and\nefficient tool-augmented reasoning in real-world applications. Our code will be\navailable at https://github.com/YBYBZhang/Tool-R1."
                },
                "authors": [
                    {
                        "name": "Yabo Zhang"
                    },
                    {
                        "name": "Yihan Zeng"
                    },
                    {
                        "name": "Qingyun Li"
                    },
                    {
                        "name": "Zhen Hu"
                    },
                    {
                        "name": "Kavin Han"
                    },
                    {
                        "name": "Wangmeng Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Wangmeng Zuo"
                },
                "author": "Wangmeng Zuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12817v1",
                "updated": "2025-09-16T08:36:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    8,
                    36,
                    5,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T08:36:05Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    8,
                    36,
                    5,
                    1,
                    259,
                    0
                ],
                "title": "SAGA: Selective Adaptive Gating for Efficient and Expressive Linear\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAGA: Selective Adaptive Gating for Efficient and Expressive Linear\n  Attention"
                },
                "summary": "While Transformer architecture excel at modeling long-range dependencies\ncontributing to its widespread adoption in vision tasks the quadratic\ncomplexity of softmax-based attention mechanisms imposes a major bottleneck,\nparticularly when processing high-resolution images. Linear attention presents\na promising alternative by reformulating the attention computation from $(QK)V$\nto $Q(KV)$, thereby reducing the complexity from $\\mathcal{O}(N^2)$ to\n$\\mathcal{O}(N)$ while preserving the global receptive field. However, most\nexisting methods compress historical key-value (KV) information uniformly,\nwhich can lead to feature redundancy and the loss of directional alignment with\nthe query (Q). This uniform compression results in low-rank $KV$ feature maps,\ncontributing to a performance gap compared to softmax attention. To mitigate\nthis limitation, we propose \\textbf{S}elective \\textbf{A}daptive\n\\textbf{GA}ting for Efficient and Expressive Linear Attention (SAGA) , which\nintroduces input-adaptive learnable gates to selectively modulate information\naggregation into the $KV$ feature map. These gates enhance semantic diversity\nand alleviate the low-rank constraint inherent in conventional linear\nattention. Additionally, we propose an efficient Hadamard-product decomposition\nmethod for gate computation, which introduces no additional memory overhead.\nExperiments demonstrate that SAGA achieves a 1.76$\\times$ improvement in\nthroughput and a 2.69$\\times$ reduction in peak GPU memory compared to PVT-T at\na resolution of $1280 \\times 1280$. Moreover, it improves top-1 accuracy by up\nto 4.4\\% on the ImageNet dataset, demonstrating both computational efficiency\nand model effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer architecture excel at modeling long-range dependencies\ncontributing to its widespread adoption in vision tasks the quadratic\ncomplexity of softmax-based attention mechanisms imposes a major bottleneck,\nparticularly when processing high-resolution images. Linear attention presents\na promising alternative by reformulating the attention computation from $(QK)V$\nto $Q(KV)$, thereby reducing the complexity from $\\mathcal{O}(N^2)$ to\n$\\mathcal{O}(N)$ while preserving the global receptive field. However, most\nexisting methods compress historical key-value (KV) information uniformly,\nwhich can lead to feature redundancy and the loss of directional alignment with\nthe query (Q). This uniform compression results in low-rank $KV$ feature maps,\ncontributing to a performance gap compared to softmax attention. To mitigate\nthis limitation, we propose \\textbf{S}elective \\textbf{A}daptive\n\\textbf{GA}ting for Efficient and Expressive Linear Attention (SAGA) , which\nintroduces input-adaptive learnable gates to selectively modulate information\naggregation into the $KV$ feature map. These gates enhance semantic diversity\nand alleviate the low-rank constraint inherent in conventional linear\nattention. Additionally, we propose an efficient Hadamard-product decomposition\nmethod for gate computation, which introduces no additional memory overhead.\nExperiments demonstrate that SAGA achieves a 1.76$\\times$ improvement in\nthroughput and a 2.69$\\times$ reduction in peak GPU memory compared to PVT-T at\na resolution of $1280 \\times 1280$. Moreover, it improves top-1 accuracy by up\nto 4.4\\% on the ImageNet dataset, demonstrating both computational efficiency\nand model effectiveness."
                },
                "authors": [
                    {
                        "name": "Yuan Cao"
                    },
                    {
                        "name": "Dong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dong Wang"
                },
                "author": "Dong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11156v2",
                "updated": "2025-09-16T07:49:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    7,
                    49,
                    41,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-14T08:22:37Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    8,
                    22,
                    37,
                    6,
                    257,
                    0
                ],
                "title": "Adaptive K-PackCache: Cost-Centric Data Caching in Cloud",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive K-PackCache: Cost-Centric Data Caching in Cloud"
                },
                "summary": "Recent advances in data analytics have enabled the accurate prediction of\nuser access patterns, giving rise to the idea of packed caching delivering\nmultiple co accessed data items together as a bundle. This improves caching\nefficiency, as accessing one item often implies the need for others. Prior work\nhas explored only 2 item pairwise packing. In this paper, we extend the concept\nto general K packing, allowing variable size bundles for improved flexibility\nand performance. We formulate the K PackCache problem from a content delivery\nnetwork CDN operator perspective, aiming to minimize total cost comprising two\ncomponents: transfer cost modeled as a base cost plus a linearly increasing\nterm with the number of items packed, and memory rental cost for caching, which\ndepends on how long and how much is stored. Overpacking increases cost due to\nlow utility, underpacking leads to missed sharing opportunities. We propose an\nonline algorithm, Adaptive K PackCache AKPC, which dynamically forms, merges,\nand splits data cliques based on user access patterns and content correlation.\nOur approach supports batch requests, enables approximate clique merging, and\noffers a formal competitive guarantee. Through extensive evaluation on the\nNetflix and Spotify datasets, AKPC reduces total cost by up to 63 and 55\npercentage over online baselines, respectively, and achieves performance within\n15 and 13 percentage of the optimal. This demonstrates its scalability and\neffectiveness for real world caching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in data analytics have enabled the accurate prediction of\nuser access patterns, giving rise to the idea of packed caching delivering\nmultiple co accessed data items together as a bundle. This improves caching\nefficiency, as accessing one item often implies the need for others. Prior work\nhas explored only 2 item pairwise packing. In this paper, we extend the concept\nto general K packing, allowing variable size bundles for improved flexibility\nand performance. We formulate the K PackCache problem from a content delivery\nnetwork CDN operator perspective, aiming to minimize total cost comprising two\ncomponents: transfer cost modeled as a base cost plus a linearly increasing\nterm with the number of items packed, and memory rental cost for caching, which\ndepends on how long and how much is stored. Overpacking increases cost due to\nlow utility, underpacking leads to missed sharing opportunities. We propose an\nonline algorithm, Adaptive K PackCache AKPC, which dynamically forms, merges,\nand splits data cliques based on user access patterns and content correlation.\nOur approach supports batch requests, enables approximate clique merging, and\noffers a formal competitive guarantee. Through extensive evaluation on the\nNetflix and Spotify datasets, AKPC reduces total cost by up to 63 and 55\npercentage over online baselines, respectively, and achieves performance within\n15 and 13 percentage of the optimal. This demonstrates its scalability and\neffectiveness for real world caching systems."
                },
                "authors": [
                    {
                        "name": "Suvarthi Sarkar"
                    },
                    {
                        "name": "Aadarshraj Sah"
                    },
                    {
                        "name": "Poddutoori Sweeya Reddy"
                    },
                    {
                        "name": "Aryabartta Sahu"
                    }
                ],
                "author_detail": {
                    "name": "Aryabartta Sahu"
                },
                "author": "Aryabartta Sahu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13231v2",
                "updated": "2025-09-15T14:40:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    40,
                    16,
                    0,
                    258,
                    0
                ],
                "published": "2025-08-17T19:07:08Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    19,
                    7,
                    8,
                    6,
                    229,
                    0
                ],
                "title": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System"
                },
                "summary": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference."
                },
                "authors": [
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Kaoutar El Maghraoui"
                    },
                    {
                        "name": "Naigang Wang"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "IEEE Computer Architecture Letter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11815v1",
                "updated": "2025-09-15T11:53:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    53,
                    56,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T11:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    53,
                    56,
                    0,
                    258,
                    0
                ],
                "title": "SpecVLM: Fast Speculative Decoding in Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecVLM: Fast Speculative Decoding in Vision-Language Models"
                },
                "summary": "Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM."
                },
                "authors": [
                    {
                        "name": "Haiduo Huang"
                    },
                    {
                        "name": "Fuwei Yang"
                    },
                    {
                        "name": "Zhenhua Liu"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Pengju Ren"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11628v1",
                "updated": "2025-09-15T06:46:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    6,
                    46,
                    22,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T06:46:22Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    6,
                    46,
                    22,
                    0,
                    258,
                    0
                ],
                "title": "SpeCa: Accelerating Diffusion Transformers with Speculative Feature\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeCa: Accelerating Diffusion Transformers with Speculative Feature\n  Caching"
                },
                "summary": "Diffusion models have revolutionized high-fidelity image and video synthesis,\nyet their computational demands remain prohibitive for real-time applications.\nThese models face two fundamental challenges: strict temporal dependencies\npreventing parallelization, and computationally intensive forward passes\nrequired at each denoising step. Drawing inspiration from speculative decoding\nin large language models, we present SpeCa, a novel 'Forecast-then-verify'\nacceleration framework that effectively addresses both limitations. SpeCa's\ncore innovation lies in introducing Speculative Sampling to diffusion models,\npredicting intermediate features for subsequent timesteps based on fully\ncomputed reference timesteps. Our approach implements a parameter-free\nverification mechanism that efficiently evaluates prediction reliability,\nenabling real-time decisions to accept or reject each prediction while\nincurring negligible computational overhead. Furthermore, SpeCa introduces\nsample-adaptive computation allocation that dynamically modulates resources\nbased on generation complexity, allocating reduced computation for simpler\nsamples while preserving intensive processing for complex instances.\nExperiments demonstrate 6.34x acceleration on FLUX with minimal quality\ndegradation (5.5% drop), 7.3x speedup on DiT while preserving generation\nfidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The\nverification mechanism incurs minimal overhead (1.67%-3.5% of full inference\ncosts), establishing a new paradigm for efficient diffusion model inference\nwhile maintaining generation quality even at aggressive acceleration ratios.\nOur codes have been released in Github:\n\\textbf{https://github.com/Shenyi-Z/Cache4Diffusion}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have revolutionized high-fidelity image and video synthesis,\nyet their computational demands remain prohibitive for real-time applications.\nThese models face two fundamental challenges: strict temporal dependencies\npreventing parallelization, and computationally intensive forward passes\nrequired at each denoising step. Drawing inspiration from speculative decoding\nin large language models, we present SpeCa, a novel 'Forecast-then-verify'\nacceleration framework that effectively addresses both limitations. SpeCa's\ncore innovation lies in introducing Speculative Sampling to diffusion models,\npredicting intermediate features for subsequent timesteps based on fully\ncomputed reference timesteps. Our approach implements a parameter-free\nverification mechanism that efficiently evaluates prediction reliability,\nenabling real-time decisions to accept or reject each prediction while\nincurring negligible computational overhead. Furthermore, SpeCa introduces\nsample-adaptive computation allocation that dynamically modulates resources\nbased on generation complexity, allocating reduced computation for simpler\nsamples while preserving intensive processing for complex instances.\nExperiments demonstrate 6.34x acceleration on FLUX with minimal quality\ndegradation (5.5% drop), 7.3x speedup on DiT while preserving generation\nfidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The\nverification mechanism incurs minimal overhead (1.67%-3.5% of full inference\ncosts), establishing a new paradigm for efficient diffusion model inference\nwhile maintaining generation quality even at aggressive acceleration ratios.\nOur codes have been released in Github:\n\\textbf{https://github.com/Shenyi-Z/Cache4Diffusion}"
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Fei Ren"
                    },
                    {
                        "name": "Shaobo Wang"
                    },
                    {
                        "name": "Kaixin Li"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_doi": "10.1145/3746027.3755331",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746027.3755331",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.11628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 9 figures, ACM Multimedia 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14089v2",
                "updated": "2025-09-15T01:15:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    1,
                    15,
                    50,
                    0,
                    258,
                    0
                ],
                "published": "2025-04-18T22:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models"
                },
                "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average."
                },
                "authors": [
                    {
                        "name": "Kang He"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06261v2",
                "updated": "2025-09-15T00:51:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    0,
                    51,
                    47,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-08T00:57:50Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    0,
                    57,
                    50,
                    0,
                    251,
                    0
                ],
                "title": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\n  Heterogeneous Precision LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\n  Heterogeneous Precision LLM Serving"
                },
                "summary": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems."
                },
                "authors": [
                    {
                        "name": "Kyungmin Bin"
                    },
                    {
                        "name": "Seungbeom Choi"
                    },
                    {
                        "name": "Jimyoung Son"
                    },
                    {
                        "name": "Jieun Choi"
                    },
                    {
                        "name": "Daseul Bae"
                    },
                    {
                        "name": "Daehyeon Baek"
                    },
                    {
                        "name": "Kihyo Moon"
                    },
                    {
                        "name": "Minsung Jang"
                    },
                    {
                        "name": "Hyojung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hyojung Lee"
                },
                "author": "Hyojung Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11239v1",
                "updated": "2025-09-14T12:29:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    14,
                    12,
                    29,
                    49,
                    6,
                    257,
                    0
                ],
                "published": "2025-09-14T12:29:49Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    12,
                    29,
                    49,
                    6,
                    257,
                    0
                ],
                "title": "Multi-Layer Perceptron-Based Relay Node Selection for Next-Generation\n  Intelligent Delay-Tolerant Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Layer Perceptron-Based Relay Node Selection for Next-Generation\n  Intelligent Delay-Tolerant Networks"
                },
                "summary": "Delay Tolerant Networks (DTNs) are critical for emergency communication in\nhighly dynamic and challenging scenarios characterized by intermittent\nconnectivity, frequent disruptions, and unpredictable node mobility. While some\nprotocols are widely adopted for simplicity and low overhead, their static\nreplication strategy lacks the ability to adaptively distinguish high-quality\nrelay nodes, often leading to inefficient and suboptimal message dissemination.\nTo address this challenge, we propose a novel intelligent routing enhancement\nthat integrates machine learning-based node evaluation into the Spray and Wait\nframework. Several dynamic, core features are extracted from simulation logs\nand are used to train multiple classifiers - Multi-Layer Perceptron (MLP),\nSupport Vector Machine (SVM), and Random Forest (RF) - to predict whether a\nnode is suitable as a relay under dynamic conditions. The trained models are\ndeployed via a lightweight Flask-based RESTful API, enabling real-time,\nadaptive predictions. We implement the enhanced router MLPBasedSprayRouter,\nwhich selectively forwards messages based on the predicted relay quality. A\ncaching mechanism is incorporated to reduce computational overhead and ensure\nstable, low-latency inference. Extensive experiments under realistic emergency\nmobility scenarios demonstrate that the proposed framework significantly\nimproves delivery ratio while reducing average latency compared to the baseline\nprotocols. Among all evaluated classifiers, MLP achieved the most robust\nperformance, consistently outperforming both SVM and RF in terms of accuracy,\nadaptability, and inference speed. These results confirm the novelty and\npracticality of integrating machine learning into DTN routing, paving the way\nfor resilient and intelligent communication systems in smart cities, disaster\nrecovery, and other dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delay Tolerant Networks (DTNs) are critical for emergency communication in\nhighly dynamic and challenging scenarios characterized by intermittent\nconnectivity, frequent disruptions, and unpredictable node mobility. While some\nprotocols are widely adopted for simplicity and low overhead, their static\nreplication strategy lacks the ability to adaptively distinguish high-quality\nrelay nodes, often leading to inefficient and suboptimal message dissemination.\nTo address this challenge, we propose a novel intelligent routing enhancement\nthat integrates machine learning-based node evaluation into the Spray and Wait\nframework. Several dynamic, core features are extracted from simulation logs\nand are used to train multiple classifiers - Multi-Layer Perceptron (MLP),\nSupport Vector Machine (SVM), and Random Forest (RF) - to predict whether a\nnode is suitable as a relay under dynamic conditions. The trained models are\ndeployed via a lightweight Flask-based RESTful API, enabling real-time,\nadaptive predictions. We implement the enhanced router MLPBasedSprayRouter,\nwhich selectively forwards messages based on the predicted relay quality. A\ncaching mechanism is incorporated to reduce computational overhead and ensure\nstable, low-latency inference. Extensive experiments under realistic emergency\nmobility scenarios demonstrate that the proposed framework significantly\nimproves delivery ratio while reducing average latency compared to the baseline\nprotocols. Among all evaluated classifiers, MLP achieved the most robust\nperformance, consistently outperforming both SVM and RF in terms of accuracy,\nadaptability, and inference speed. These results confirm the novelty and\npracticality of integrating machine learning into DTN routing, paving the way\nfor resilient and intelligent communication systems in smart cities, disaster\nrecovery, and other dynamic environments."
                },
                "authors": [
                    {
                        "name": "Zhekun Huang"
                    },
                    {
                        "name": "Milena Radenkovic"
                    }
                ],
                "author_detail": {
                    "name": "Milena Radenkovic"
                },
                "author": "Milena Radenkovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11181v1",
                "updated": "2025-09-14T09:26:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    14,
                    9,
                    26,
                    44,
                    6,
                    257,
                    0
                ],
                "published": "2025-09-14T09:26:44Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    9,
                    26,
                    44,
                    6,
                    257,
                    0
                ],
                "title": "Dislocation response to electric fields in strontium titanate: A\n  mesoscale indentation study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dislocation response to electric fields in strontium titanate: A\n  mesoscale indentation study"
                },
                "summary": "Dislocations in perovskite oxides have drawn increasing research interest due\nto their potential of tuning functional properties of electroceramics. Open\nquestions remain regarding the behavior of dislocations concerning their\nstability under strong externally applied electric fields. In this study, we\ninvestigate the dielectric breakdown strength of nominally undoped SrTiO3\ncrystals after the introduction of high-density dislocations. The\ndislocation-rich samples are prepared using the Brinell scratching method, and\nthey consistently exhibit lower dielectric breakdown strength as well as a\nlarger scatter in the breakdown probability. We also study the impact of\nelectric field on the introduction and movement of dislocations in SrTiO3\ncrystals using Brinell indentation coupled with an electric field of 2 kV/mm.\nNo changes on the dislocation plastic zone size, depth, and dislocation\ndistribution are observed under this electric field. Based on the charge state\nof the dislocations in SrTiO3 as well as the electrical and thermal\nconductivity modified by dislocations, we discuss the forces induced by the\nelectric field to act on the dislocations to underline the possible mechanisms\nfor such dislocation behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dislocations in perovskite oxides have drawn increasing research interest due\nto their potential of tuning functional properties of electroceramics. Open\nquestions remain regarding the behavior of dislocations concerning their\nstability under strong externally applied electric fields. In this study, we\ninvestigate the dielectric breakdown strength of nominally undoped SrTiO3\ncrystals after the introduction of high-density dislocations. The\ndislocation-rich samples are prepared using the Brinell scratching method, and\nthey consistently exhibit lower dielectric breakdown strength as well as a\nlarger scatter in the breakdown probability. We also study the impact of\nelectric field on the introduction and movement of dislocations in SrTiO3\ncrystals using Brinell indentation coupled with an electric field of 2 kV/mm.\nNo changes on the dislocation plastic zone size, depth, and dislocation\ndistribution are observed under this electric field. Based on the charge state\nof the dislocations in SrTiO3 as well as the electrical and thermal\nconductivity modified by dislocations, we discuss the forces induced by the\nelectric field to act on the dislocations to underline the possible mechanisms\nfor such dislocation behavior."
                },
                "authors": [
                    {
                        "name": "Alexander Frisch"
                    },
                    {
                        "name": "Daniel Isaia"
                    },
                    {
                        "name": "Oliver Preuß"
                    },
                    {
                        "name": "Xufei Fang"
                    }
                ],
                "author_detail": {
                    "name": "Xufei Fang"
                },
                "author": "Xufei Fang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11155v1",
                "updated": "2025-09-14T08:20:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    14,
                    8,
                    20,
                    48,
                    6,
                    257,
                    0
                ],
                "published": "2025-09-14T08:20:48Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    8,
                    20,
                    48,
                    6,
                    257,
                    0
                ],
                "title": "AQUA: Attention via QUery mAgnitudes for Memory and Compute Efficient\n  Inference in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AQUA: Attention via QUery mAgnitudes for Memory and Compute Efficient\n  Inference in LLMs"
                },
                "summary": "The quadratic complexity of the attention mechanism remains a fundamental\nbarrier to scaling Large Language Models (LLMs) to longer contexts, creating a\ncritical bottleneck in both computation and memory. To address this, we\nintroduce AQUA (Attention via QUery mAgnitudes) a novel and versatile\napproximation strategy that significantly reduces the cost of attention with a\ngraceful performance trade-off. Our method operates in two phases: an efficient\noffline step where we compute a universal, language agnostic projection matrix\nvia SVD on a calibration dataset, and an online inference step where we project\nquery and key vectors and dynamically select a sparse subset of dimensions\nbased on the query's magnitude. We provide a formal theoretical analysis of\nAQUA, establishing the break-even point at which it becomes more\ncomputationally efficient than standard attention. Our empirical evaluations on\nstate-of-the-art models like Llama-3.1-8B demonstrate that a 25% reduction in\nthe attention dot-product computation can be achieved with a statistically\ninsignificant impact on performance across a wide range of benchmarks. We\nfurther showcase the versatility of AQUA by demonstrating its ability to\nsynergistically accelerate existing token eviction methods like H2O and to\ndirectly reduce KV-cache memory size. By offering a controllable knob to\nbalance efficiency and accuracy, AQUA provides a practical and powerful tool\nfor making large-scale LLM inference more accessible and sustainable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic complexity of the attention mechanism remains a fundamental\nbarrier to scaling Large Language Models (LLMs) to longer contexts, creating a\ncritical bottleneck in both computation and memory. To address this, we\nintroduce AQUA (Attention via QUery mAgnitudes) a novel and versatile\napproximation strategy that significantly reduces the cost of attention with a\ngraceful performance trade-off. Our method operates in two phases: an efficient\noffline step where we compute a universal, language agnostic projection matrix\nvia SVD on a calibration dataset, and an online inference step where we project\nquery and key vectors and dynamically select a sparse subset of dimensions\nbased on the query's magnitude. We provide a formal theoretical analysis of\nAQUA, establishing the break-even point at which it becomes more\ncomputationally efficient than standard attention. Our empirical evaluations on\nstate-of-the-art models like Llama-3.1-8B demonstrate that a 25% reduction in\nthe attention dot-product computation can be achieved with a statistically\ninsignificant impact on performance across a wide range of benchmarks. We\nfurther showcase the versatility of AQUA by demonstrating its ability to\nsynergistically accelerate existing token eviction methods like H2O and to\ndirectly reduce KV-cache memory size. By offering a controllable knob to\nbalance efficiency and accuracy, AQUA provides a practical and powerful tool\nfor making large-scale LLM inference more accessible and sustainable."
                },
                "authors": [
                    {
                        "name": "Santhosh G S"
                    },
                    {
                        "name": "Saurav Prakash"
                    },
                    {
                        "name": "Balaraman Ravindran"
                    }
                ],
                "author_detail": {
                    "name": "Balaraman Ravindran"
                },
                "author": "Balaraman Ravindran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10798v1",
                "updated": "2025-09-13T03:34:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    13,
                    3,
                    34,
                    12,
                    5,
                    256,
                    0
                ],
                "published": "2025-09-13T03:34:12Z",
                "published_parsed": [
                    2025,
                    9,
                    13,
                    3,
                    34,
                    12,
                    5,
                    256,
                    0
                ],
                "title": "Judge Q: Trainable Queries for Optimized Information Retention in KV\n  Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judge Q: Trainable Queries for Optimized Information Retention in KV\n  Cache Eviction"
                },
                "summary": "Large language models (LLMs) utilize key-value (KV) cache to store historical\ninformation during sequence processing. The size of KV cache grows linearly as\nthe length of the sequence extends, which seriously affects memory usage and\ndecoding efficiency. Current methods for KV cache eviction typically utilize\nthe last window from the pre-filling phase as queries to compute the KV\nimportance scores for eviction. Although this scheme is simple to implement, it\ntends to overly focus on local information, potentially leading to the neglect\nor omission of crucial global information. To mitigate this issue, we propose\nJudge Q, a novel training method which incorporates a soft token list. This\nmethod only tunes the model's embedding layer at a low training cost. By\nconcatenating the soft token list at the end of the input sequence, we train\nthese tokens' attention map to the original input sequence to align with that\nof the actual decoded tokens. In this way, the queries corresponding to the\nsoft tokens can effectively capture global information and better evaluate the\nimportance of the keys and values within the KV cache, thus maintaining\ndecoding quality when KV cache is evicted. Under the same eviction budget, our\nmethod exhibits less performance degradation compared to existing eviction\napproaches. We validate our approach through experiments conducted on models\nsuch as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks\nincluding LongBench, RULER, and Needle-in-a-Haystack. Results indicate an\nimprovement of approximately 1 point on the LongBench and over 3 points on\nRULER. This proposed methodology can be seamlessly integrated into existing\nopen-source models with minimal training overhead, thereby enhancing\nperformance in KV cache eviction scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) utilize key-value (KV) cache to store historical\ninformation during sequence processing. The size of KV cache grows linearly as\nthe length of the sequence extends, which seriously affects memory usage and\ndecoding efficiency. Current methods for KV cache eviction typically utilize\nthe last window from the pre-filling phase as queries to compute the KV\nimportance scores for eviction. Although this scheme is simple to implement, it\ntends to overly focus on local information, potentially leading to the neglect\nor omission of crucial global information. To mitigate this issue, we propose\nJudge Q, a novel training method which incorporates a soft token list. This\nmethod only tunes the model's embedding layer at a low training cost. By\nconcatenating the soft token list at the end of the input sequence, we train\nthese tokens' attention map to the original input sequence to align with that\nof the actual decoded tokens. In this way, the queries corresponding to the\nsoft tokens can effectively capture global information and better evaluate the\nimportance of the keys and values within the KV cache, thus maintaining\ndecoding quality when KV cache is evicted. Under the same eviction budget, our\nmethod exhibits less performance degradation compared to existing eviction\napproaches. We validate our approach through experiments conducted on models\nsuch as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks\nincluding LongBench, RULER, and Needle-in-a-Haystack. Results indicate an\nimprovement of approximately 1 point on the LongBench and over 3 points on\nRULER. This proposed methodology can be seamlessly integrated into existing\nopen-source models with minimal training overhead, thereby enhancing\nperformance in KV cache eviction scenarios."
                },
                "authors": [
                    {
                        "name": "Yijun Liu"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Shiyu Ji"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10372v1",
                "updated": "2025-09-12T16:05:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    5,
                    27,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T16:05:27Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    5,
                    27,
                    4,
                    255,
                    0
                ],
                "title": "MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging\n  Bit-Slice-enabled Sparsity and Repetitiveness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging\n  Bit-Slice-enabled Sparsity and Repetitiveness"
                },
                "summary": "Large language models (LLMs) face significant inference latency due to\ninefficiencies in GEMM operations, weight access, and KV cache access,\nespecially in real-time scenarios. This highlights the need for a versatile\ncompute-memory efficient accelerator. Unfortunately, existing Transformer\naccelerators struggle to address both aspects simultaneously, as they focus on\nvalue-level processing, missing fine-grained opportunities to optimize\ncomputation and memory collaboratively. This paper introduces MCBP, a\nbit-grained compute-memory efficient algorithm-hardware co-design that\nleverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM\ninference. MCBP features three key innovations: 1) BS-repetitiveness-enabled\ncomputation reduction (BRCR), which eliminates redundant GEMM computations via\nleveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state\ncoding (BSTC), which reduces weight access via exploiting significant sparsity\nin high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP),\nwhich reduces KV cache access by leveraging early-termination-based bit-grained\nprediction. These techniques, supported by custom accelerator designs,\neffectively alleviate the burden in GEMM, weight access, and KV cache access.\nExtensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up\nand 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA\nTransformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than\nSpatten, FACT and SOFA, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face significant inference latency due to\ninefficiencies in GEMM operations, weight access, and KV cache access,\nespecially in real-time scenarios. This highlights the need for a versatile\ncompute-memory efficient accelerator. Unfortunately, existing Transformer\naccelerators struggle to address both aspects simultaneously, as they focus on\nvalue-level processing, missing fine-grained opportunities to optimize\ncomputation and memory collaboratively. This paper introduces MCBP, a\nbit-grained compute-memory efficient algorithm-hardware co-design that\nleverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM\ninference. MCBP features three key innovations: 1) BS-repetitiveness-enabled\ncomputation reduction (BRCR), which eliminates redundant GEMM computations via\nleveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state\ncoding (BSTC), which reduces weight access via exploiting significant sparsity\nin high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP),\nwhich reduces KV cache access by leveraging early-termination-based bit-grained\nprediction. These techniques, supported by custom accelerator designs,\neffectively alleviate the burden in GEMM, weight access, and KV cache access.\nExtensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up\nand 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA\nTransformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than\nSpatten, FACT and SOFA, respectively."
                },
                "authors": [
                    {
                        "name": "Huizheng Wang"
                    },
                    {
                        "name": "Zichuan Wang"
                    },
                    {
                        "name": "Zhiheng Yue"
                    },
                    {
                        "name": "Yousheng Long"
                    },
                    {
                        "name": "Taiquan Wei"
                    },
                    {
                        "name": "Jianxun Yang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Shaojun Wei"
                    },
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Shouyi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Shouyi Yin"
                },
                "author": "Shouyi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10312v1",
                "updated": "2025-09-12T14:53:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    53,
                    45,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T14:53:45Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    53,
                    45,
                    4,
                    255,
                    0
                ],
                "title": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion\n  Transformers with Cluster-Driven Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion\n  Transformers with Cluster-Driven Feature Caching"
                },
                "summary": "Diffusion transformers have gained significant attention in recent years for\ntheir ability to generate high-quality images and videos, yet still suffer from\na huge computational cost due to their iterative denoising process. Recently,\nfeature caching has been introduced to accelerate diffusion transformers by\ncaching the feature computation in previous timesteps and reusing it in the\nfollowing timesteps, which leverage the temporal similarity of diffusion models\nwhile ignoring the similarity in the spatial dimension. In this paper, we\nintroduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and\ncomplementary perspective for previous feature caching. Specifically, ClusCa\nperforms spatial clustering on tokens in each timestep, computes only one token\nin each cluster and propagates their information to all the other tokens, which\nis able to reduce the number of tokens by over 90%. Extensive experiments on\nDiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image\nand text-to-video generation. Besides, it can be directly applied to any\ndiffusion transformer without requirements for training. For instance, ClusCa\nachieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing\nthe original model by 0.51%. The code is available at\nhttps://github.com/Shenyi-Z/Cache4Diffusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have gained significant attention in recent years for\ntheir ability to generate high-quality images and videos, yet still suffer from\na huge computational cost due to their iterative denoising process. Recently,\nfeature caching has been introduced to accelerate diffusion transformers by\ncaching the feature computation in previous timesteps and reusing it in the\nfollowing timesteps, which leverage the temporal similarity of diffusion models\nwhile ignoring the similarity in the spatial dimension. In this paper, we\nintroduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and\ncomplementary perspective for previous feature caching. Specifically, ClusCa\nperforms spatial clustering on tokens in each timestep, computes only one token\nin each cluster and propagates their information to all the other tokens, which\nis able to reduce the number of tokens by over 90%. Extensive experiments on\nDiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image\nand text-to-video generation. Besides, it can be directly applied to any\ndiffusion transformer without requirements for training. For instance, ClusCa\nachieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing\nthe original model by 0.51%. The code is available at\nhttps://github.com/Shenyi-Z/Cache4Diffusion."
                },
                "authors": [
                    {
                        "name": "Zhixin Zheng"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Shaobo Wang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "11 pages, 11 figures; Accepted by ACM MM2025; Mainly focus on feature\n  caching for diffusion transformers acceleration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10251v1",
                "updated": "2025-09-12T13:49:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    49,
                    27,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T13:49:27Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    49,
                    27,
                    4,
                    255,
                    0
                ],
                "title": "XBOF: A Cost-Efficient CXL JBOF with Inter-SSD Compute Resource Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XBOF: A Cost-Efficient CXL JBOF with Inter-SSD Compute Resource Sharing"
                },
                "summary": "Enterprise SSDs integrate numerous computing resources (e.g., ARM processor\nand onboard DRAM) to satisfy the ever-increasing performance requirements of\nI/O bursts. While these resources substantially elevate the monetary costs of\nSSDs, the sporadic nature of I/O bursts causes severe SSD resource\nunderutilization in just a bunch of flash (JBOF) level. Tackling this\nchallenge, we propose XBOF, a cost-efficient JBOF design, which only reserves\nmoderate computing resources in SSDs at low monetary cost, while achieving\ndemanded I/O performance through efficient inter-SSD resource sharing.\nSpecifically, XBOF first disaggregates SSD architecture into multiple disjoint\nparts based on their functionality, enabling fine-grained SSD internal resource\nmanagement. XBOF then employs a decentralized scheme to manage these\ndisaggregated resources and harvests the computing resources of idle SSDs to\nassist busy SSDs in handling I/O bursts. This idea is facilitated by the\ncache-coherent capability of Compute eXpress Link (CXL), with which the busy\nSSDs can directly utilize the harvested computing resources to accelerate\nmetadata processing. The evaluation results show that XBOF improves SSD\nresource utilization by 50.4% and saves 19.0% monetary costs with a negligible\nperformance loss, compared to existing JBOF designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprise SSDs integrate numerous computing resources (e.g., ARM processor\nand onboard DRAM) to satisfy the ever-increasing performance requirements of\nI/O bursts. While these resources substantially elevate the monetary costs of\nSSDs, the sporadic nature of I/O bursts causes severe SSD resource\nunderutilization in just a bunch of flash (JBOF) level. Tackling this\nchallenge, we propose XBOF, a cost-efficient JBOF design, which only reserves\nmoderate computing resources in SSDs at low monetary cost, while achieving\ndemanded I/O performance through efficient inter-SSD resource sharing.\nSpecifically, XBOF first disaggregates SSD architecture into multiple disjoint\nparts based on their functionality, enabling fine-grained SSD internal resource\nmanagement. XBOF then employs a decentralized scheme to manage these\ndisaggregated resources and harvests the computing resources of idle SSDs to\nassist busy SSDs in handling I/O bursts. This idea is facilitated by the\ncache-coherent capability of Compute eXpress Link (CXL), with which the busy\nSSDs can directly utilize the harvested computing resources to accelerate\nmetadata processing. The evaluation results show that XBOF improves SSD\nresource utilization by 50.4% and saves 19.0% monetary costs with a negligible\nperformance loss, compared to existing JBOF designs."
                },
                "authors": [
                    {
                        "name": "Shushu Yi"
                    },
                    {
                        "name": "Yuda An"
                    },
                    {
                        "name": "Li Peng"
                    },
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Jieming Yin"
                    },
                    {
                        "name": "Guangyan Zhang"
                    },
                    {
                        "name": "Wenfei Wu"
                    },
                    {
                        "name": "Diyu Zhou"
                    },
                    {
                        "name": "Zhenlin Wang"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10016v1",
                "updated": "2025-09-12T07:20:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    20,
                    53,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T07:20:53Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    20,
                    53,
                    4,
                    255,
                    0
                ],
                "title": "SvalMIZ-25 Svalbard Marginal Ice Zone Campaign 2025 -- Cruise Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SvalMIZ-25 Svalbard Marginal Ice Zone Campaign 2025 -- Cruise Report"
                },
                "summary": "The coupling of weather, sea-ice, ocean, and wave forecasting systems has\nbeen a long-standing research focus to improve Arctic forecasting systems and\ntheir realism and is also a priority of international initiatives such as the\nWMO research project PCAPS. The goal of the Svalbard Marginal Ice Zone 2025\nCampaign (SvalMIZ-25) was to observe and better understand the complex\ninterplay between atmosphere, waves, and sea-ice in the winter Marginal Ice\nZone (MIZ) in order to advance predictive skill of coupled Arctic forecasting\nsystems. The main objective has been to set up a network of observations with a\nspatial distribution that allows for a representative comparison between in\nsitu observations and gridded model data. The observed variables include air\nand surface temperature, sea-ice drift, and wave energy spectra. With the\nsupport of the Norwegian Coast Guard, we participated in the research cruise\nwith KV Svalbard from 22.April - 11.May 2025. In total 21 buoys were deployed\nin the Marginal Ice Zone north of the Svalbard Archipelago.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The coupling of weather, sea-ice, ocean, and wave forecasting systems has\nbeen a long-standing research focus to improve Arctic forecasting systems and\ntheir realism and is also a priority of international initiatives such as the\nWMO research project PCAPS. The goal of the Svalbard Marginal Ice Zone 2025\nCampaign (SvalMIZ-25) was to observe and better understand the complex\ninterplay between atmosphere, waves, and sea-ice in the winter Marginal Ice\nZone (MIZ) in order to advance predictive skill of coupled Arctic forecasting\nsystems. The main objective has been to set up a network of observations with a\nspatial distribution that allows for a representative comparison between in\nsitu observations and gridded model data. The observed variables include air\nand surface temperature, sea-ice drift, and wave energy spectra. With the\nsupport of the Norwegian Coast Guard, we participated in the research cruise\nwith KV Svalbard from 22.April - 11.May 2025. In total 21 buoys were deployed\nin the Marginal Ice Zone north of the Svalbard Archipelago."
                },
                "authors": [
                    {
                        "name": "M. Müller"
                    },
                    {
                        "name": "J. Rabault"
                    },
                    {
                        "name": "C. Palerme"
                    },
                    {
                        "name": "J. Tjernström"
                    }
                ],
                "author_detail": {
                    "name": "J. Tjernström"
                },
                "author": "J. Tjernström",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09754v1",
                "updated": "2025-09-11T16:48:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    48,
                    24,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T16:48:24Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    48,
                    24,
                    3,
                    254,
                    0
                ],
                "title": "LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation"
                },
                "summary": "KV Cache is commonly used to accelerate LLM inference with long contexts, yet\nits high memory demand drives the need for cache compression. Existing\ncompression methods, however, are largely heuristic and lack dynamic budget\nallocation. To address this limitation, we introduce a unified framework for\ncache compression by minimizing information loss in Transformer residual\nstreams. Building on it, we analyze the layer attention output loss and derive\na new metric to compare cache entries across heads, enabling layer-wise\ncompression with dynamic head budgets. Additionally, by contrasting cross-layer\ninformation, we also achieve dynamic layer budgets. LAVa is the first unified\nstrategy for cache eviction and dynamic budget allocation that, unlike prior\nmethods, does not rely on training or the combination of multiple strategies.\nExperiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and\nInfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a\nnew insight: dynamic layer budgets are crucial for generation tasks (e.g., code\ncompletion), while dynamic head budgets play a key role in extraction tasks\n(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently\nmaintains top performance across task types. Our code is available at\nhttps://github.com/MGDDestiny/Lava.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache is commonly used to accelerate LLM inference with long contexts, yet\nits high memory demand drives the need for cache compression. Existing\ncompression methods, however, are largely heuristic and lack dynamic budget\nallocation. To address this limitation, we introduce a unified framework for\ncache compression by minimizing information loss in Transformer residual\nstreams. Building on it, we analyze the layer attention output loss and derive\na new metric to compare cache entries across heads, enabling layer-wise\ncompression with dynamic head budgets. Additionally, by contrasting cross-layer\ninformation, we also achieve dynamic layer budgets. LAVa is the first unified\nstrategy for cache eviction and dynamic budget allocation that, unlike prior\nmethods, does not rely on training or the combination of multiple strategies.\nExperiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and\nInfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a\nnew insight: dynamic layer budgets are crucial for generation tasks (e.g., code\ncompletion), while dynamic head budgets play a key role in extraction tasks\n(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently\nmaintains top performance across task types. Our code is available at\nhttps://github.com/MGDDestiny/Lava."
                },
                "authors": [
                    {
                        "name": "Yiqun Shen"
                    },
                    {
                        "name": "Song Yuan"
                    },
                    {
                        "name": "Zhengze Zhang"
                    },
                    {
                        "name": "Xiaoliang Wang"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Nguyen Cam-Tu"
                    }
                ],
                "author_detail": {
                    "name": "Nguyen Cam-Tu"
                },
                "author": "Nguyen Cam-Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09525v1",
                "updated": "2025-09-11T15:06:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    6,
                    3,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T15:06:03Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    6,
                    3,
                    3,
                    254,
                    0
                ],
                "title": "TrEnv: Transparently Share Serverless Execution Environments Across\n  Different Functions and Nodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrEnv: Transparently Share Serverless Execution Environments Across\n  Different Functions and Nodes"
                },
                "summary": "Serverless computing provides dynamic scalability, but its infrastructure\noverhead becomes a bottleneck for emerging workloads such as LLM agents, which\nexhibit unpredictable invocation patterns and variable resource demands. Our\nanalysis shows that for these agents, the cost of running on serverless\nplatforms can reach up to 70% of the cost of LLM API calls. This finding\nmotivates the need for a more efficient, high-density serverless platform. We\npresent TrEnv, a co-designed serverless platform that supports both container-\nand VM-based environments, optimized for the unique demands of LLM agents.\nTrEnv reduces startup latency and memory usage through repurposable sandboxes\nand memory templates, which enable fast reuse and restoration of execution\nenvironments. To further reduce overhead in VM-based agent workloads, TrEnv\nleverages browser sharing and a page cache bypassing mechanism. Evaluations\nshow that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in\ncontainer-based settings, and achieves up to 58% lower P99 latency and 61%\nmemory savings for VM-based agents compared to state-of-the-art systems like\nE2B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing provides dynamic scalability, but its infrastructure\noverhead becomes a bottleneck for emerging workloads such as LLM agents, which\nexhibit unpredictable invocation patterns and variable resource demands. Our\nanalysis shows that for these agents, the cost of running on serverless\nplatforms can reach up to 70% of the cost of LLM API calls. This finding\nmotivates the need for a more efficient, high-density serverless platform. We\npresent TrEnv, a co-designed serverless platform that supports both container-\nand VM-based environments, optimized for the unique demands of LLM agents.\nTrEnv reduces startup latency and memory usage through repurposable sandboxes\nand memory templates, which enable fast reuse and restoration of execution\nenvironments. To further reduce overhead in VM-based agent workloads, TrEnv\nleverages browser sharing and a page cache bypassing mechanism. Evaluations\nshow that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in\ncontainer-based settings, and achieves up to 58% lower P99 latency and 61%\nmemory savings for VM-based agents compared to state-of-the-art systems like\nE2B."
                },
                "authors": [
                    {
                        "name": "Jialiang Huang"
                    },
                    {
                        "name": "Teng Ma"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Sixing Lin"
                    },
                    {
                        "name": "Kang Chen"
                    },
                    {
                        "name": "Jinlei Jiang"
                    },
                    {
                        "name": "Xia Liao"
                    },
                    {
                        "name": "Yingdi Shan"
                    },
                    {
                        "name": "Yongwei Wu"
                    },
                    {
                        "name": "Ning Zhang"
                    },
                    {
                        "name": "Mengting Lu"
                    },
                    {
                        "name": "Tao Ma"
                    },
                    {
                        "name": "Haifeng Gong"
                    },
                    {
                        "name": "Mingxing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mingxing Zhang"
                },
                "author": "Mingxing Zhang",
                "arxiv_comment": "38 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09494v1",
                "updated": "2025-09-11T14:34:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    34,
                    1,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T14:34:01Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    34,
                    1,
                    3,
                    254,
                    0
                ],
                "title": "In-Loop Filtering Using Learned Look-Up Tables for Video Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Loop Filtering Using Learned Look-Up Tables for Video Coding"
                },
                "summary": "In-loop filtering (ILF) is a key technology in video coding standards to\nreduce artifacts and enhance visual quality. Recently, neural network-based ILF\nschemes have achieved remarkable coding gains, emerging as a powerful candidate\nfor next-generation video coding standards. However, the use of deep neural\nnetworks (DNN) brings significant computational and time complexity or high\ndemands for dedicated hardware, making it challenging for general use. To\naddress this limitation, we study a practical ILF solution by adopting look-up\ntables (LUTs). After training a DNN with a restricted reference range for ILF,\nall possible inputs are traversed, and the output values of the DNN are cached\ninto LUTs. During the coding process, the filtering process is performed by\nsimply retrieving the filtered pixel through locating the input pixels and\ninterpolating between the cached values, instead of relying on heavy inference\ncomputations. In this paper, we propose a universal LUT-based ILF framework,\ntermed LUT-ILF++. First, we introduce the cooperation of multiple kinds of\nfiltering LUTs and propose a series of customized indexing mechanisms to enable\nbetter filtering reference perception with limited storage consumption. Second,\nwe propose the cross-component indexing mechanism to enable the filtering of\ndifferent color components jointly. Third, in order to make our solution\npractical for coding uses, we propose the LUT compaction scheme to enable the\nLUT pruning, achieving a lower storage cost of the entire solution. The\nproposed framework is implemented in the VVC reference software. Experimental\nresults show that the proposed framework achieves on average 0.82%/2.97%/1.63%\nand 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI\nand RA configurations, respectively. Compared to DNN-based solutions, our\nproposed solution has much lower time complexity and storage cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-loop filtering (ILF) is a key technology in video coding standards to\nreduce artifacts and enhance visual quality. Recently, neural network-based ILF\nschemes have achieved remarkable coding gains, emerging as a powerful candidate\nfor next-generation video coding standards. However, the use of deep neural\nnetworks (DNN) brings significant computational and time complexity or high\ndemands for dedicated hardware, making it challenging for general use. To\naddress this limitation, we study a practical ILF solution by adopting look-up\ntables (LUTs). After training a DNN with a restricted reference range for ILF,\nall possible inputs are traversed, and the output values of the DNN are cached\ninto LUTs. During the coding process, the filtering process is performed by\nsimply retrieving the filtered pixel through locating the input pixels and\ninterpolating between the cached values, instead of relying on heavy inference\ncomputations. In this paper, we propose a universal LUT-based ILF framework,\ntermed LUT-ILF++. First, we introduce the cooperation of multiple kinds of\nfiltering LUTs and propose a series of customized indexing mechanisms to enable\nbetter filtering reference perception with limited storage consumption. Second,\nwe propose the cross-component indexing mechanism to enable the filtering of\ndifferent color components jointly. Third, in order to make our solution\npractical for coding uses, we propose the LUT compaction scheme to enable the\nLUT pruning, achieving a lower storage cost of the entire solution. The\nproposed framework is implemented in the VVC reference software. Experimental\nresults show that the proposed framework achieves on average 0.82%/2.97%/1.63%\nand 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI\nand RA configurations, respectively. Compared to DNN-based solutions, our\nproposed solution has much lower time complexity and storage cost."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Jialin Li"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05211v2",
                "updated": "2025-09-11T12:06:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    6,
                    49,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-07T09:47:21Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    47,
                    21,
                    3,
                    219,
                    0
                ],
                "title": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization"
                },
                "summary": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference."
                },
                "authors": [
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Runsen Xu"
                    },
                    {
                        "name": "Chenhang Cui"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19880v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19880v2",
                "updated": "2025-09-11T10:20:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    10,
                    20,
                    20,
                    3,
                    254,
                    0
                ],
                "published": "2025-05-26T12:06:12Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    12,
                    6,
                    12,
                    0,
                    146,
                    0
                ],
                "title": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing"
                },
                "summary": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead."
                },
                "authors": [
                    {
                        "name": "Saman Akbari"
                    },
                    {
                        "name": "Manfred Hauswirth"
                    }
                ],
                "author_detail": {
                    "name": "Manfred Hauswirth"
                },
                "author": "Manfred Hauswirth",
                "arxiv_doi": "10.1109/CLOUD67622.2025.00051",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/CLOUD67622.2025.00051",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.19880v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19880v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the 2025 IEEE 18th International Conference on Cloud\n  Computing (CLOUD)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19740v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19740v3",
                "updated": "2025-09-11T06:45:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    45,
                    58,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-27T10:11:27Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    11,
                    27,
                    2,
                    239,
                    0
                ],
                "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval"
                },
                "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19740v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19740v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01085v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01085v3",
                "updated": "2025-09-11T06:16:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    16,
                    31,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-01T03:16:52Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    16,
                    52,
                    0,
                    244,
                    0
                ],
                "title": "Bidirectional Sparse Attention for Faster Video Diffusion Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional Sparse Attention for Faster Video Diffusion Training"
                },
                "summary": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention."
                },
                "authors": [
                    {
                        "name": "Chenlu Zhan"
                    },
                    {
                        "name": "Wen Li"
                    },
                    {
                        "name": "Chuyu Shen"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Suhui Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01085v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01085v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09094v1",
                "updated": "2025-09-11T02:00:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    2,
                    0,
                    27,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T02:00:27Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    2,
                    0,
                    27,
                    3,
                    254,
                    0
                ],
                "title": "Coherence-Aware Task Graph Modeling for Realistic Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherence-Aware Task Graph Modeling for Realistic Application"
                },
                "summary": "As multicore systems continue to scale, cache coherence has emerged as a\ncritical determinant of system performance, with coherence behavior and task\nexecution closely intertwined, reshaping inter-task dependencies. Task graph\nmodeling provides a structured way to capture such dependencies and serves as\nthe foundation for many system-level design strategies. However, these\nstrategies typically rely on predefined task graphs, while many real-world\napplications lack explicit graphs and exhibit dynamic, data-dependent behavior,\nlimiting the effectiveness of static approaches. To address this, several task\ngraph modeling methods for realistic workloads have been developed. Yet, they\neither rely on implicit techniques that use application-specific features\nwithout producing explicit graphs, or they generate graphs tailored to fixed\nscheduling models, which limits generality. More importantly, they often\noverlook coherence interactions, creating a gap between design assumptions and\nactual runtime behavior. To overcome these limitations, we propose CoTAM, a\nCoherence-Aware Task Graph Modeling framework for realistic workloads that\nconstructs a unified task graph reflecting runtime behavior. CoTAM analyzes the\nimpact of coherence by decoupling its effects from overall execution,\nquantifies its influence through a learned weighting scheme, and infers\ninter-task dependencies for coherence-aware graph generation. Extensive\nexperiments show that CoTAM outperforms implicit methods, bridging the gap\nbetween dynamic workload behavior and existing designs while demonstrating the\nimportance of incorporating cache coherence into task graph modeling for\naccurate and generalizable system-level analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multicore systems continue to scale, cache coherence has emerged as a\ncritical determinant of system performance, with coherence behavior and task\nexecution closely intertwined, reshaping inter-task dependencies. Task graph\nmodeling provides a structured way to capture such dependencies and serves as\nthe foundation for many system-level design strategies. However, these\nstrategies typically rely on predefined task graphs, while many real-world\napplications lack explicit graphs and exhibit dynamic, data-dependent behavior,\nlimiting the effectiveness of static approaches. To address this, several task\ngraph modeling methods for realistic workloads have been developed. Yet, they\neither rely on implicit techniques that use application-specific features\nwithout producing explicit graphs, or they generate graphs tailored to fixed\nscheduling models, which limits generality. More importantly, they often\noverlook coherence interactions, creating a gap between design assumptions and\nactual runtime behavior. To overcome these limitations, we propose CoTAM, a\nCoherence-Aware Task Graph Modeling framework for realistic workloads that\nconstructs a unified task graph reflecting runtime behavior. CoTAM analyzes the\nimpact of coherence by decoupling its effects from overall execution,\nquantifies its influence through a learned weighting scheme, and infers\ninter-task dependencies for coherence-aware graph generation. Extensive\nexperiments show that CoTAM outperforms implicit methods, bridging the gap\nbetween dynamic workload behavior and existing designs while demonstrating the\nimportance of incorporating cache coherence into task graph modeling for\naccurate and generalizable system-level analysis."
                },
                "authors": [
                    {
                        "name": "Guochu Xiong"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Weichen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weichen Liu"
                },
                "author": "Weichen Liu",
                "arxiv_doi": "10.1145/3742875.3754678",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3742875.3754678",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.09094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by MEMOCODE'25, 10 pages",
                "arxiv_journal_ref": "International Symposium on Formal Methods and Models for System\n  Design (MEMOCODE '25), September 28-October 3, 2025, Taipei, Taiwan",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23674v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23674v2",
                "updated": "2025-09-10T17:59:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    59,
                    8,
                    2,
                    253,
                    0
                ],
                "published": "2025-07-31T15:50:57Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses"
                },
                "summary": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience."
                },
                "authors": [
                    {
                        "name": "Muhammad Taha Cheema"
                    },
                    {
                        "name": "Abeer Aamir"
                    },
                    {
                        "name": "Khawaja Gul Muhammad"
                    },
                    {
                        "name": "Naveed Anwar Bhatti"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23674v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23674v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08696v1",
                "updated": "2025-09-10T15:41:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    41,
                    15,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T15:41:15Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    41,
                    15,
                    2,
                    253,
                    0
                ],
                "title": "Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer\n  Layer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer\n  Layer Caching"
                },
                "summary": "This paper presents a method to accelerate the inference process of diffusion\ntransformer (DiT)-based text-to-speech (TTS) models by applying a selective\ncaching mechanism to transformer layers. Specifically, I integrate SmoothCache\ninto the F5-TTS architecture, focusing on caching outputs of self-attention and\nfeed-forward network layers to reduce redundant computations during the\ndenoising process. A calibration phase is introduced to analyze L1 relative\nerrors between timesteps, guiding the selection of cache schedules that\nminimize quality degradation. To address the problem of inter-layer dependency,\na unified caching schedule is adopted, applying the cache pattern derived from\nself-attention layers to both layer types. Experiments on LibriSpeech-PC and\nSeed-TTS datasets evaluate various cache thresholds and denoising step\nconfigurations. Results show that caching at higher denoising steps reduces\ninference time without compromising output quality, whereas caching at lower\nsteps can negatively impact synthesis quality similarly to reducing the total\nnumber of denoising steps. Objective and subjective metrics confirm the\neffectiveness of SmoothCache in maintaining performance while improving\ncomputational efficiency. Comparisons between cached inference and reduced-step\ninference further highlight the benefits of selective caching, especially under\nhigh-step configurations. This work demonstrates that transformer layer caching\nis a practical solution for optimizing diffusion transformer-based TTS models\nwithout requiring architectural changes or retraining. Example inference\nresults can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a method to accelerate the inference process of diffusion\ntransformer (DiT)-based text-to-speech (TTS) models by applying a selective\ncaching mechanism to transformer layers. Specifically, I integrate SmoothCache\ninto the F5-TTS architecture, focusing on caching outputs of self-attention and\nfeed-forward network layers to reduce redundant computations during the\ndenoising process. A calibration phase is introduced to analyze L1 relative\nerrors between timesteps, guiding the selection of cache schedules that\nminimize quality degradation. To address the problem of inter-layer dependency,\na unified caching schedule is adopted, applying the cache pattern derived from\nself-attention layers to both layer types. Experiments on LibriSpeech-PC and\nSeed-TTS datasets evaluate various cache thresholds and denoising step\nconfigurations. Results show that caching at higher denoising steps reduces\ninference time without compromising output quality, whereas caching at lower\nsteps can negatively impact synthesis quality similarly to reducing the total\nnumber of denoising steps. Objective and subjective metrics confirm the\neffectiveness of SmoothCache in maintaining performance while improving\ncomputational efficiency. Comparisons between cached inference and reduced-step\ninference further highlight the benefits of selective caching, especially under\nhigh-step configurations. This work demonstrates that transformer layer caching\nis a practical solution for optimizing diffusion transformer-based TTS models\nwithout requiring architectural changes or retraining. Example inference\nresults can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ ."
                },
                "authors": [
                    {
                        "name": "Siratish Sakpiboonchit"
                    }
                ],
                "author_detail": {
                    "name": "Siratish Sakpiboonchit"
                },
                "author": "Siratish Sakpiboonchit",
                "arxiv_comment": "9 pages, 2 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08542v1",
                "updated": "2025-09-10T12:46:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    46,
                    29,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T12:46:29Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    46,
                    29,
                    2,
                    253,
                    0
                ],
                "title": "BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter\n  1.58-bit LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter\n  1.58-bit LLM Inference"
                },
                "summary": "Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy\nefficiency for CNNs by eliminating runtime weight updates. However, their\nscalability to Large Language Models (LLMs) is fundamentally constrained by\ntheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA\nseries - demands more than 1,000 cm2 of silicon area even in advanced CMOS\nnodes. This paper presents BitROM, the first CiROM-based accelerator that\novercomes this limitation through co-design with BitNet's 1.58-bit quantization\nmodel, enabling practical and efficient LLM inference at the edge. BitROM\nintroduces three key innovations: 1) a novel Bidirectional ROM Array that\nstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator\noptimized for ternary-weight computations; and 3) an integrated Decode-Refresh\n(DR) eDRAM that supports on-die KV-cache management, significantly reducing\nexternal memory access during decoding. In addition, BitROM integrates\nLoRA-based adapters to enable efficient transfer learning across various\ndownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit\ndensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over\nprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%\nreduction in external DRAM access, further enhancing deployment efficiency for\nLLMs in edge applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy\nefficiency for CNNs by eliminating runtime weight updates. However, their\nscalability to Large Language Models (LLMs) is fundamentally constrained by\ntheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA\nseries - demands more than 1,000 cm2 of silicon area even in advanced CMOS\nnodes. This paper presents BitROM, the first CiROM-based accelerator that\novercomes this limitation through co-design with BitNet's 1.58-bit quantization\nmodel, enabling practical and efficient LLM inference at the edge. BitROM\nintroduces three key innovations: 1) a novel Bidirectional ROM Array that\nstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator\noptimized for ternary-weight computations; and 3) an integrated Decode-Refresh\n(DR) eDRAM that supports on-die KV-cache management, significantly reducing\nexternal memory access during decoding. In addition, BitROM integrates\nLoRA-based adapters to enable efficient transfer learning across various\ndownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit\ndensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over\nprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%\nreduction in external DRAM access, further enhancing deployment efficiency for\nLLMs in edge applications."
                },
                "authors": [
                    {
                        "name": "Wenlun Zhang"
                    },
                    {
                        "name": "Xinyu Li"
                    },
                    {
                        "name": "Shimpei Ando"
                    },
                    {
                        "name": "Kentaro Yoshioka"
                    }
                ],
                "author_detail": {
                    "name": "Kentaro Yoshioka"
                },
                "author": "Kentaro Yoshioka",
                "arxiv_comment": "Accepted to ASP-DAC 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08342v1",
                "updated": "2025-09-10T07:28:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    28,
                    24,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T07:28:24Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    28,
                    24,
                    2,
                    253,
                    0
                ],
                "title": "Accelerating Mixture-of-Expert Inference with Adaptive Expert Split\n  Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Mixture-of-Expert Inference with Adaptive Expert Split\n  Mechanism"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a promising architecture for modern\nlarge language models (LLMs). However, massive parameters impose heavy GPU\nmemory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.\nOffloading the expert parameters to CPU RAM offers an effective way to\nalleviate the VRAM requirements for MoE inference. Existing approaches\ntypically cache a small subset of experts in VRAM and dynamically prefetch\nexperts from RAM during inference, leading to significant degradation in\ninference speed due to the poor cache hit rate and substantial expert loading\nlatency. In this work, we propose MoEpic, an efficient MoE inference system\nwith a novel expert split mechanism. Specifically, each expert is vertically\ndivided into two segments: top and bottom. MoEpic caches the top segment of hot\nexperts, so that more experts will be stored under the limited VRAM budget,\nthereby improving the cache hit rate. During each layer's inference, MoEpic\npredicts and prefetches the activated experts for the next layer. Since the top\nsegments of cached experts are exempt from fetching, the loading time is\nreduced, which allows efficient transfer-computation overlap. Nevertheless, the\nperformance of MoEpic critically depends on the cache configuration (i.e., each\nlayer's VRAM budget and expert split ratio). To this end, we propose a\ndivide-and-conquer algorithm based on fixed-point iteration for adaptive cache\nconfiguration. Extensive experiments on popular MoE LLMs demonstrate that\nMoEpic can save about half of the GPU cost, while lowering the inference\nlatency by about 37.51%-65.73% compared to the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a promising architecture for modern\nlarge language models (LLMs). However, massive parameters impose heavy GPU\nmemory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.\nOffloading the expert parameters to CPU RAM offers an effective way to\nalleviate the VRAM requirements for MoE inference. Existing approaches\ntypically cache a small subset of experts in VRAM and dynamically prefetch\nexperts from RAM during inference, leading to significant degradation in\ninference speed due to the poor cache hit rate and substantial expert loading\nlatency. In this work, we propose MoEpic, an efficient MoE inference system\nwith a novel expert split mechanism. Specifically, each expert is vertically\ndivided into two segments: top and bottom. MoEpic caches the top segment of hot\nexperts, so that more experts will be stored under the limited VRAM budget,\nthereby improving the cache hit rate. During each layer's inference, MoEpic\npredicts and prefetches the activated experts for the next layer. Since the top\nsegments of cached experts are exempt from fetching, the loading time is\nreduced, which allows efficient transfer-computation overlap. Nevertheless, the\nperformance of MoEpic critically depends on the cache configuration (i.e., each\nlayer's VRAM budget and expert split ratio). To this end, we propose a\ndivide-and-conquer algorithm based on fixed-point iteration for adaptive cache\nconfiguration. Extensive experiments on popular MoE LLMs demonstrate that\nMoEpic can save about half of the GPU cost, while lowering the inference\nlatency by about 37.51%-65.73% compared to the baselines."
                },
                "authors": [
                    {
                        "name": "Jiaming Yan"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08315v1",
                "updated": "2025-09-10T06:32:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    6,
                    32,
                    49,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T06:32:49Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    6,
                    32,
                    49,
                    2,
                    253,
                    0
                ],
                "title": "EvolKV: Evolutionary KV Cache Compression for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvolKV: Evolutionary KV Cache Compression for LLM Inference"
                },
                "summary": "Existing key-value (KV) cache compression methods typically rely on\nheuristics, such as uniform cache allocation across layers or static eviction\npolicies, however, they ignore the critical interplays among layer-specific\nfeature patterns and task performance, which can lead to degraded\ngeneralization. In this paper, we propose EvolKV, an adaptive framework for\nlayer-wise, task-driven KV cache compression that jointly optimizes the memory\nefficiency and task performance. By reformulating cache allocation as a\nmulti-objective optimization problem, EvolKV leverages evolutionary search to\ndynamically configure layer budgets while directly maximizing downstream\nperformance. Extensive experiments on 11 tasks demonstrate that our approach\noutperforms all baseline methods across a wide range of KV cache budgets on\nlong-context tasks and surpasses heuristic baselines by up to 7 percentage\npoints on GSM8K. Notably, EvolKV achieves superior performance over the full KV\ncache setting on code completion while utilizing only 1.5% of the original\nbudget, suggesting the untapped potential in learned compression strategies for\nKV cache budget allocation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing key-value (KV) cache compression methods typically rely on\nheuristics, such as uniform cache allocation across layers or static eviction\npolicies, however, they ignore the critical interplays among layer-specific\nfeature patterns and task performance, which can lead to degraded\ngeneralization. In this paper, we propose EvolKV, an adaptive framework for\nlayer-wise, task-driven KV cache compression that jointly optimizes the memory\nefficiency and task performance. By reformulating cache allocation as a\nmulti-objective optimization problem, EvolKV leverages evolutionary search to\ndynamically configure layer budgets while directly maximizing downstream\nperformance. Extensive experiments on 11 tasks demonstrate that our approach\noutperforms all baseline methods across a wide range of KV cache budgets on\nlong-context tasks and surpasses heuristic baselines by up to 7 percentage\npoints on GSM8K. Notably, EvolKV achieves superior performance over the full KV\ncache setting on code completion while utilizing only 1.5% of the original\nbudget, suggesting the untapped potential in learned compression strategies for\nKV cache budget allocation."
                },
                "authors": [
                    {
                        "name": "Bohan Yu"
                    },
                    {
                        "name": "Yekun Chai"
                    }
                ],
                "author_detail": {
                    "name": "Yekun Chai"
                },
                "author": "Yekun Chai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v3",
                "updated": "2025-09-09T13:30:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    30,
                    17,
                    1,
                    252,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "arxiv_comment": "Accepted by EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07379v1",
                "updated": "2025-09-09T04:00:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    4,
                    0,
                    43,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T04:00:43Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    4,
                    0,
                    43,
                    1,
                    252,
                    0
                ],
                "title": "DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for\n  Efficient MoE LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for\n  Efficient MoE LLM Inference"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of deep learning tasks. Mixture of Experts (MoE) further enhances\ntheir capabilities by increasing model width through sparsely activated expert\nbranches, which keeps inference computation efficient. However, the large\nnumber of expert weights introduces significant GPU memory pressure, especially\nin resource-constrained environments such as single-GPU servers. More\nimportantly, MoE inference consists of two fundamentally different stages: a\nprefill stage where most experts are activated densely, and a decode stage\nwhere only a few experts are triggered sparsely. Treating these stages with a\nuniform scheduling strategy often leads to suboptimal latency and memory usage.\nTo address this, we propose DuoServe-MoE, an inference serving system that\nexplicitly separates prefill and decode stages and applies tailored expert\nscheduling strategies to each. In the prefill stage, DuoServe-MoE uses a\ntwo-stream CUDA pipeline that overlaps expert weight prefetching with the\ncomputation of non-MoE layers, limiting expert residency in GPU memory. In the\ndecode stage, a lightweight layer-level predictor trained offline from\nactivation traces is used to prefetch only the most likely activated experts,\nwithout requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B\nand 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to\n7.54 times while keeping peak memory usage at only 15 percent of the full model\nsize.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of deep learning tasks. Mixture of Experts (MoE) further enhances\ntheir capabilities by increasing model width through sparsely activated expert\nbranches, which keeps inference computation efficient. However, the large\nnumber of expert weights introduces significant GPU memory pressure, especially\nin resource-constrained environments such as single-GPU servers. More\nimportantly, MoE inference consists of two fundamentally different stages: a\nprefill stage where most experts are activated densely, and a decode stage\nwhere only a few experts are triggered sparsely. Treating these stages with a\nuniform scheduling strategy often leads to suboptimal latency and memory usage.\nTo address this, we propose DuoServe-MoE, an inference serving system that\nexplicitly separates prefill and decode stages and applies tailored expert\nscheduling strategies to each. In the prefill stage, DuoServe-MoE uses a\ntwo-stream CUDA pipeline that overlaps expert weight prefetching with the\ncomputation of non-MoE layers, limiting expert residency in GPU memory. In the\ndecode stage, a lightweight layer-level predictor trained offline from\nactivation traces is used to prefetch only the most likely activated experts,\nwithout requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B\nand 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to\n7.54 times while keeping peak memory usage at only 15 percent of the full model\nsize."
                },
                "authors": [
                    {
                        "name": "Yuning Zhang"
                    },
                    {
                        "name": "Grant Pinkert"
                    },
                    {
                        "name": "Nan Yang"
                    },
                    {
                        "name": "Yanli Li"
                    },
                    {
                        "name": "Dong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yuan"
                },
                "author": "Dong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01742v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01742v2",
                "updated": "2025-09-09T00:15:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    0,
                    15,
                    5,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-01T19:49:21Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    19,
                    49,
                    21,
                    0,
                    244,
                    0
                ],
                "title": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators"
                },
                "summary": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO(log_2(log_2 (N))) bandwidth overhead. BOLT introduces three key innovations:\n(i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache\nto accelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO(log_2(log_2 (N))) bandwidth overhead. BOLT introduces three key innovations:\n(i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache\nto accelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook."
                },
                "authors": [
                    {
                        "name": "Yitong Guo"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Haobin Hiroki Chen"
                    },
                    {
                        "name": "Yukui Luo"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Chenghong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chenghong Wang"
                },
                "author": "Chenghong Wang",
                "arxiv_comment": "Accepted by CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01742v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01742v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06949v1",
                "updated": "2025-09-08T17:58:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    6,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:58:06Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    6,
                    0,
                    251,
                    0
                ],
                "title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models"
                },
                "summary": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL"
                },
                "authors": [
                    {
                        "name": "Yinjie Wang"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Ke Shen"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "arxiv_comment": "Code and Models: https://github.com/Gen-Verse/dLLM-RL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03377v2",
                "updated": "2025-09-08T17:22:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    22,
                    17,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-03T14:53:45Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    53,
                    45,
                    2,
                    246,
                    0
                ],
                "title": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing"
                },
                "summary": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v4",
                "updated": "2025-09-08T13:34:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    34,
                    54,
                    0,
                    251,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AGI/AMD-Hybrid-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AGI/AMD-Hybrid-Models."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06579v1",
                "updated": "2025-09-08T11:49:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    49,
                    51,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T11:49:51Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    49,
                    51,
                    0,
                    251,
                    0
                ],
                "title": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View\n  Synthesis"
                },
                "summary": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html."
                },
                "authors": [
                    {
                        "name": "Xin Kong"
                    },
                    {
                        "name": "Daniel Watson"
                    },
                    {
                        "name": "Yannick Strümpler"
                    },
                    {
                        "name": "Michael Niemeyer"
                    },
                    {
                        "name": "Federico Tombari"
                    }
                ],
                "author_detail": {
                    "name": "Federico Tombari"
                },
                "author": "Federico Tombari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06493v1",
                "updated": "2025-09-08T09:54:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T09:54:18Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers"
                },
                "summary": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search."
                },
                "authors": [
                    {
                        "name": "Ran Xin"
                    },
                    {
                        "name": "Zeyu Zheng"
                    },
                    {
                        "name": "Yanchen Nie"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Xia Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xia Xiao"
                },
                "author": "Xia Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09822v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09822v4",
                "updated": "2025-09-08T09:09:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    9,
                    36,
                    0,
                    251,
                    0
                ],
                "published": "2025-08-13T13:54:51Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "title": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining"
                },
                "summary": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/"
                },
                "authors": [
                    {
                        "name": "Zijian Song"
                    },
                    {
                        "name": "Sihan Qin"
                    },
                    {
                        "name": "Tianshui Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Guangrun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangrun Wang"
                },
                "author": "Guangrun Wang",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09822v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09822v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06444v1",
                "updated": "2025-09-08T08:44:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    44,
                    24,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T08:44:24Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    44,
                    24,
                    0,
                    251,
                    0
                ],
                "title": "HyFedRAG: A Federated Retrieval-Augmented Generation Framework for\n  Heterogeneous and Privacy-Sensitive Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyFedRAG: A Federated Retrieval-Augmented Generation Framework for\n  Heterogeneous and Privacy-Sensitive Data"
                },
                "summary": "Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive\ndata, especially in distributed healthcare settings where patient data spans\nSQL, knowledge graphs, and clinical notes. Clinicians face difficulties\nretrieving rare disease cases due to privacy constraints and the limitations of\ntraditional cloud-based RAG systems in handling diverse formats and edge\ndevices. To address this, we introduce HyFedRAG, a unified and efficient\nFederated RAG framework tailored for Hybrid data modalities. By leveraging an\nedge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across\ndiverse data sources while preserving data privacy. Our key contributions are:\n(1) We design an edge-cloud collaborative RAG framework built on Flower, which\nsupports querying structured SQL data, semi-structured knowledge graphs, and\nunstructured documents. The edge-side LLMs convert diverse data into\nstandardized privacy-preserving representations, and the server-side LLMs\nintegrates them for global reasoning and generation. (2) We integrate\nlightweight local retrievers with privacy-aware LLMs and provide three\nanonymization tools that enable each client to produce semantically rich,\nde-identified summaries for global inference across devices. (3) To optimize\nresponse latency and reduce redundant computation, we design a three-tier\ncaching strategy consisting of local cache, intermediate representation cache,\nand cloud inference cache. Experimental results on PMC-Patients demonstrate\nthat HyFedRAG outperforms existing baselines in terms of retrieval quality,\ngeneration consistency, and system efficiency. Our framework offers a scalable\nand privacy-compliant solution for RAG over structural-heterogeneous data,\nunlocking the potential of LLMs in sensitive and diverse data environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive\ndata, especially in distributed healthcare settings where patient data spans\nSQL, knowledge graphs, and clinical notes. Clinicians face difficulties\nretrieving rare disease cases due to privacy constraints and the limitations of\ntraditional cloud-based RAG systems in handling diverse formats and edge\ndevices. To address this, we introduce HyFedRAG, a unified and efficient\nFederated RAG framework tailored for Hybrid data modalities. By leveraging an\nedge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across\ndiverse data sources while preserving data privacy. Our key contributions are:\n(1) We design an edge-cloud collaborative RAG framework built on Flower, which\nsupports querying structured SQL data, semi-structured knowledge graphs, and\nunstructured documents. The edge-side LLMs convert diverse data into\nstandardized privacy-preserving representations, and the server-side LLMs\nintegrates them for global reasoning and generation. (2) We integrate\nlightweight local retrievers with privacy-aware LLMs and provide three\nanonymization tools that enable each client to produce semantically rich,\nde-identified summaries for global inference across devices. (3) To optimize\nresponse latency and reduce redundant computation, we design a three-tier\ncaching strategy consisting of local cache, intermediate representation cache,\nand cloud inference cache. Experimental results on PMC-Patients demonstrate\nthat HyFedRAG outperforms existing baselines in terms of retrieval quality,\ngeneration consistency, and system efficiency. Our framework offers a scalable\nand privacy-compliant solution for RAG over structural-heterogeneous data,\nunlocking the potential of LLMs in sensitive and diverse data environments."
                },
                "authors": [
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Hainan Zhang"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Hong-Wei Zheng"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "arxiv_comment": "9 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06436v1",
                "updated": "2025-09-08T08:34:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T08:34:02Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "title": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning"
                },
                "summary": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Xiaofei Xu"
                    },
                    {
                        "name": "Ke Deng"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Lin Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lin Tian"
                },
                "author": "Lin Tian",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06047v1",
                "updated": "2025-09-07T13:15:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    7,
                    13,
                    15,
                    17,
                    6,
                    250,
                    0
                ],
                "published": "2025-09-07T13:15:17Z",
                "published_parsed": [
                    2025,
                    9,
                    7,
                    13,
                    15,
                    17,
                    6,
                    250,
                    0
                ],
                "title": "A facile vector substrate platform via BaTiO3 membrane transfer enables\n  high quality solution processed epitaxial PZT on silicon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A facile vector substrate platform via BaTiO3 membrane transfer enables\n  high quality solution processed epitaxial PZT on silicon"
                },
                "summary": "The direct integration of high-performance ferroelectric oxides with silicon\nremains challenging due to lattice mismatch, thermal incompatibility, and the\nneed for high-temperature epitaxial growth. Here, a hybrid integration approach\nis demonstrated in which crystalline BaTiO3 (BTO) membranes are first\ntransferred onto Pt coated Si substrates and subsequently used as vector\nsubstrates (VS) for the growth of epitaxial (001) Pb(Zr0.52Ti0.48)O3 (PZT) thin\nfilms via chemical solution deposition (CSD). A KI and HCl based etchant\nenables rapid and complete dissolution of the SrVO3 sacrificial layer in about\n30 minutes, reducing the release time from days to minutes compared with\nconventional water based approaches to dissolve AVO3 and AMoO3 (A is Ca, Sr,\nBa). The BTO VS imposes dominant (00l) out of plane orientation and in plane\ncube on cube epitaxy in the overlying PZT. Devices exhibit remnant polarization\n10 to 12 micro coulomb/cm2 and coercive field of 100 kV/cm, with stable\nswitching to 10^8 cycles on the VS. From piezoelectric butterfly loops, we\nextract effective d33 of 70 pm/V for PZT on VS, and 54 pm/V for PZT grown on\nconventional Pt Si substrates. This approach demonstrates a scalable and cost\neffective route for integrating functional ferroelectric materials onto silicon\nand offers a promising platform for future CMOS compatible oxide electronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The direct integration of high-performance ferroelectric oxides with silicon\nremains challenging due to lattice mismatch, thermal incompatibility, and the\nneed for high-temperature epitaxial growth. Here, a hybrid integration approach\nis demonstrated in which crystalline BaTiO3 (BTO) membranes are first\ntransferred onto Pt coated Si substrates and subsequently used as vector\nsubstrates (VS) for the growth of epitaxial (001) Pb(Zr0.52Ti0.48)O3 (PZT) thin\nfilms via chemical solution deposition (CSD). A KI and HCl based etchant\nenables rapid and complete dissolution of the SrVO3 sacrificial layer in about\n30 minutes, reducing the release time from days to minutes compared with\nconventional water based approaches to dissolve AVO3 and AMoO3 (A is Ca, Sr,\nBa). The BTO VS imposes dominant (00l) out of plane orientation and in plane\ncube on cube epitaxy in the overlying PZT. Devices exhibit remnant polarization\n10 to 12 micro coulomb/cm2 and coercive field of 100 kV/cm, with stable\nswitching to 10^8 cycles on the VS. From piezoelectric butterfly loops, we\nextract effective d33 of 70 pm/V for PZT on VS, and 54 pm/V for PZT grown on\nconventional Pt Si substrates. This approach demonstrates a scalable and cost\neffective route for integrating functional ferroelectric materials onto silicon\nand offers a promising platform for future CMOS compatible oxide electronics."
                },
                "authors": [
                    {
                        "name": "Asraful Haque"
                    },
                    {
                        "name": "Antony Jeyaseelan"
                    },
                    {
                        "name": "Shubham Kumar Parate"
                    },
                    {
                        "name": "Srinivasan Raghavan"
                    },
                    {
                        "name": "Pavan Nukala"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Nukala"
                },
                "arxiv_affiliation": "Centre for Nanoscience and Engineering, Indian Institute of Science, Bengaluru, India",
                "author": "Pavan Nukala",
                "arxiv_comment": "17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13863v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13863v2",
                "updated": "2025-09-06T05:58:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    6,
                    5,
                    58,
                    51,
                    5,
                    249,
                    0
                ],
                "published": "2025-08-19T14:30:41Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    30,
                    41,
                    1,
                    231,
                    0
                ],
                "title": "Tight Cache Contention Analysis for WCET Estimation on Multicore Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tight Cache Contention Analysis for WCET Estimation on Multicore Systems"
                },
                "summary": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Shenlin Cai"
                    },
                    {
                        "name": "Yaowei Liang"
                    },
                    {
                        "name": "Chen Jie"
                    },
                    {
                        "name": "Yinjie Fang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Guoquan Zhang"
                    },
                    {
                        "name": "Yaoyao Gu"
                    },
                    {
                        "name": "Xiang Xiao"
                    },
                    {
                        "name": "Wei Qin"
                    },
                    {
                        "name": "Xiangzhen Ouyang"
                    },
                    {
                        "name": "Wanli Chang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Chang"
                },
                "author": "Wanli Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13863v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13863v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05207v1",
                "updated": "2025-09-05T16:10:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    10,
                    20,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T16:10:20Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    10,
                    20,
                    4,
                    248,
                    0
                ],
                "title": "RapidGNN: Energy and Communication-Efficient Distributed Training on\n  Large-Scale Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RapidGNN: Energy and Communication-Efficient Distributed Training on\n  Large-Scale Graph Neural Networks"
                },
                "summary": "Graph Neural Networks (GNNs) have become popular across a diverse set of\ntasks in exploring structural relationships between entities. However, due to\nthe highly connected structure of the datasets, distributed training of GNNs on\nlarge-scale graphs poses significant challenges. Traditional sampling-based\napproaches mitigate the computational loads, yet the communication overhead\nremains a challenge. This paper presents RapidGNN, a distributed GNN training\nframework with deterministic sampling-based scheduling to enable efficient\ncache construction and prefetching of remote features. Evaluation on benchmark\ngraph datasets demonstrates RapidGNN's effectiveness across different scales\nand topologies. RapidGNN improves end-to-end training throughput by 2.46x to\n3.00x on average over baseline methods across the benchmark datasets, while\ncutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further\ndemonstrates near-linear scalability with an increasing number of computing\nunits efficiently. Furthermore, it achieves increased energy efficiency over\nthe baseline methods for both CPU and GPU by 44% and 32%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have become popular across a diverse set of\ntasks in exploring structural relationships between entities. However, due to\nthe highly connected structure of the datasets, distributed training of GNNs on\nlarge-scale graphs poses significant challenges. Traditional sampling-based\napproaches mitigate the computational loads, yet the communication overhead\nremains a challenge. This paper presents RapidGNN, a distributed GNN training\nframework with deterministic sampling-based scheduling to enable efficient\ncache construction and prefetching of remote features. Evaluation on benchmark\ngraph datasets demonstrates RapidGNN's effectiveness across different scales\nand topologies. RapidGNN improves end-to-end training throughput by 2.46x to\n3.00x on average over baseline methods across the benchmark datasets, while\ncutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further\ndemonstrates near-linear scalability with an increasing number of computing\nunits efficiently. Furthermore, it achieves increased energy efficiency over\nthe baseline methods for both CPU and GPU by 44% and 32%, respectively."
                },
                "authors": [
                    {
                        "name": "Arefin Niam"
                    },
                    {
                        "name": "Tevfik Kosar"
                    },
                    {
                        "name": "M S Q Zulkar Nine"
                    }
                ],
                "author_detail": {
                    "name": "M S Q Zulkar Nine"
                },
                "author": "M S Q Zulkar Nine",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2505.10806",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05165v1",
                "updated": "2025-09-05T14:58:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T14:58:24Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "title": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens"
                },
                "summary": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment."
                },
                "authors": [
                    {
                        "name": "Dmitry Akulov"
                    },
                    {
                        "name": "Mohamed Sana"
                    },
                    {
                        "name": "Antonio De Domenico"
                    },
                    {
                        "name": "Tareq Si Salem"
                    },
                    {
                        "name": "Nicola Piovesan"
                    },
                    {
                        "name": "Fadhel Ayed"
                    }
                ],
                "author_detail": {
                    "name": "Fadhel Ayed"
                },
                "author": "Fadhel Ayed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09758v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09758v2",
                "updated": "2025-09-05T10:39:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    39,
                    3,
                    4,
                    248,
                    0
                ],
                "published": "2025-06-11T14:03:13Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "title": "Mainframe-Style Channel Controllers for Modern Disaggregated Memory\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainframe-Style Channel Controllers for Modern Disaggregated Memory\n  Systems"
                },
                "summary": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs."
                },
                "authors": [
                    {
                        "name": "Zikai Liu"
                    },
                    {
                        "name": "Jasmin Schult"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "arxiv_doi": "10.1145/3725783.3764403",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725783.3764403",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.09758v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09758v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Camera-ready authors' version for APSys'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04377v1",
                "updated": "2025-09-04T16:40:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    40,
                    1,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T16:40:01Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    40,
                    1,
                    3,
                    247,
                    0
                ],
                "title": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference"
                },
                "summary": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks."
                },
                "authors": [
                    {
                        "name": "Krishna Teja Chitty-Venkata"
                    },
                    {
                        "name": "Jie Ye"
                    },
                    {
                        "name": "Xian-He Sun"
                    },
                    {
                        "name": "Anthony Kougkas"
                    },
                    {
                        "name": "Murali Emani"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Nicolae"
                },
                "author": "Bogdan Nicolae",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12084v2",
                "updated": "2025-09-04T15:21:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    21,
                    11,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-21T12:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "title": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis"
                },
                "summary": "This study presents a comprehensive multi-level analysis of the NVIDIA Hopper\nGPU architecture, focusing on its performance characteristics and novel\nfeatures. We benchmark Hopper's memory subsystem, highlighting improvements in\nthe L2 partitioned cache and global memory access compared to Ampere and Ada\nLovelace. The evaluation of Hopper's fourth-generation tensor cores reveals the\nbenefits of FP8 precision and asynchronous wgmma instructions for matrix\noperations. Additionally, we investigate the performance of DPX instructions\nfor dynamic programming, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. Through multi-level evaluation, we discover that the Hopper\narchitecture demonstrates significant acceleration potential in real-world\napplications. For instance, the asynchronous programming model supported by TMA\nachieves a 1.5x speedup in matrix multiplication, FP8 delivers nearly double\nthe performance of FP16, and DPX instructions accelerate a computational\nbiology algorithm by at least 4.75x. Our findings provide actionable insights\nfor optimizing compute-intensive workloads, from AI training to bioinformatics,\non Hopper GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a comprehensive multi-level analysis of the NVIDIA Hopper\nGPU architecture, focusing on its performance characteristics and novel\nfeatures. We benchmark Hopper's memory subsystem, highlighting improvements in\nthe L2 partitioned cache and global memory access compared to Ampere and Ada\nLovelace. The evaluation of Hopper's fourth-generation tensor cores reveals the\nbenefits of FP8 precision and asynchronous wgmma instructions for matrix\noperations. Additionally, we investigate the performance of DPX instructions\nfor dynamic programming, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. Through multi-level evaluation, we discover that the Hopper\narchitecture demonstrates significant acceleration potential in real-world\napplications. For instance, the asynchronous programming model supported by TMA\nachieves a 1.5x speedup in matrix multiplication, FP8 delivers nearly double\nthe performance of FP16, and DPX instructions accelerate a computational\nbiology algorithm by at least 4.75x. Our findings provide actionable insights\nfor optimizing compute-intensive workloads, from AI training to bioinformatics,\non Hopper GPUs."
                },
                "authors": [
                    {
                        "name": "Weile Luo"
                    },
                    {
                        "name": "Ruibo Fan"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Hongyuan Liu"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2402.13499",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04185v1",
                "updated": "2025-09-04T13:02:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    2,
                    39,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T13:02:39Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    2,
                    39,
                    3,
                    247,
                    0
                ],
                "title": "Set Block Decoding is a Language Model Inference Accelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Set Block Decoding is a Language Model Inference Accelerator"
                },
                "summary": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training."
                },
                "authors": [
                    {
                        "name": "Itai Gat"
                    },
                    {
                        "name": "Heli Ben-Hamu"
                    },
                    {
                        "name": "Marton Havasi"
                    },
                    {
                        "name": "Daniel Haziza"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Gabriel Synnaeve"
                    },
                    {
                        "name": "David Lopez-Paz"
                    },
                    {
                        "name": "Brian Karrer"
                    },
                    {
                        "name": "Yaron Lipman"
                    }
                ],
                "author_detail": {
                    "name": "Yaron Lipman"
                },
                "author": "Yaron Lipman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04180v1",
                "updated": "2025-09-04T12:54:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    54,
                    32,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T12:54:32Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    54,
                    32,
                    3,
                    247,
                    0
                ],
                "title": "VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer\n  Vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer\n  Vision"
                },
                "summary": "AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}."
                },
                "authors": [
                    {
                        "name": "Safouane El Ghazouali"
                    },
                    {
                        "name": "Umberto Michelucci"
                    }
                ],
                "author_detail": {
                    "name": "Umberto Michelucci"
                },
                "author": "Umberto Michelucci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04010v1",
                "updated": "2025-09-04T08:41:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    41,
                    6,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T08:41:06Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    41,
                    6,
                    3,
                    247,
                    0
                ],
                "title": "Systematic Timing Leakage Analysis of NIST PQDSS Candidates: Tooling and\n  Lessons Learned",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Timing Leakage Analysis of NIST PQDSS Candidates: Tooling and\n  Lessons Learned"
                },
                "summary": "The PQDSS standardization process requires cryptographic primitives to be\nfree from vulnerabilities, including timing and cache side-channels. Resistance\nto timing leakage is therefore an essential property, and achieving this\ntypically relies on software implementations that follow constant-time\nprinciples. Moreover, ensuring that all implementations are constant-time is\ncrucial for fair performance comparisons, as secure implementations often incur\nadditional overhead. Such analysis also helps identify scheme proposals that\nare inherently difficult to implement in constant time. Because constant-time\nproperties can be broken during compilation, it is often necessary to analyze\nthe compiled binary directly. Since manual binary analysis is extremely\nchallenging, automated analysis becomes highly important. Although several\ntools exist to assist with such analysis, they often have usability limitations\nand are difficult to set up correctly. To support the developers besides the\nNIST committee in verifying candidates, we developed a toolchain that automates\nconfiguration, execution, and result analysis for several widely used\nconstant-time analysis tools. We selected TIMECOP and Binsec/Rel2 to verify\nconstant-time policy compliance at the binary level, and dudect and RTLF to\ndetect side-channel vulnerabilities through statistical analysis of execution\ntime behavior. We demonstrate its effectiveness and practicability by\nevaluating the NIST PQDSS round 1 and round 2 implementations. We reported 26\nissues in total to the respective developers, and 5 of them have already been\nfixed. We also discuss our different findings, as well as the benefits of\nshortcomings of the different tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The PQDSS standardization process requires cryptographic primitives to be\nfree from vulnerabilities, including timing and cache side-channels. Resistance\nto timing leakage is therefore an essential property, and achieving this\ntypically relies on software implementations that follow constant-time\nprinciples. Moreover, ensuring that all implementations are constant-time is\ncrucial for fair performance comparisons, as secure implementations often incur\nadditional overhead. Such analysis also helps identify scheme proposals that\nare inherently difficult to implement in constant time. Because constant-time\nproperties can be broken during compilation, it is often necessary to analyze\nthe compiled binary directly. Since manual binary analysis is extremely\nchallenging, automated analysis becomes highly important. Although several\ntools exist to assist with such analysis, they often have usability limitations\nand are difficult to set up correctly. To support the developers besides the\nNIST committee in verifying candidates, we developed a toolchain that automates\nconfiguration, execution, and result analysis for several widely used\nconstant-time analysis tools. We selected TIMECOP and Binsec/Rel2 to verify\nconstant-time policy compliance at the binary level, and dudect and RTLF to\ndetect side-channel vulnerabilities through statistical analysis of execution\ntime behavior. We demonstrate its effectiveness and practicability by\nevaluating the NIST PQDSS round 1 and round 2 implementations. We reported 26\nissues in total to the respective developers, and 5 of them have already been\nfixed. We also discuss our different findings, as well as the benefits of\nshortcomings of the different tools."
                },
                "authors": [
                    {
                        "name": "Olivier Adjonyo"
                    },
                    {
                        "name": "Sebastien Bardin"
                    },
                    {
                        "name": "Emanuele Bellini"
                    },
                    {
                        "name": "Gilbert Ndollane Dione"
                    },
                    {
                        "name": "Mahmudul Faisal Al Ameen"
                    },
                    {
                        "name": "Robert Merget"
                    },
                    {
                        "name": "Frederic Recoules"
                    },
                    {
                        "name": "Yanis Sellami"
                    }
                ],
                "author_detail": {
                    "name": "Yanis Sellami"
                },
                "author": "Yanis Sellami",
                "arxiv_comment": "20 pages, 1 figure, to be published and presented at Sixth PQC\n  Standardization Conference by NIST, partially supported by the \"France 2030\"\n  government investment plan managed by the French National Research Agency,\n  under the reference ANR-22-PECY-0005",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12689v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12689v3",
                "updated": "2025-09-04T06:20:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    6,
                    20,
                    55,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-22T07:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "IC-Cache: Efficient Large Language Model Serving via In-context Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IC-Cache: Efficient Large Language Model Serving via In-context Caching"
                },
                "summary": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 70% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge transfer among requests. However, naively caching and reusing past\nresponses leads to a big quality drop. In this paper, we introduce IC-Cache, a\ncaching system that enables live LLM capability augmentation to improve serving\nefficiency: by leveraging historical request-response pairs from larger models\nas in-context examples, IC-Cache empowers small LLMs to imitate and even exceed\nthe compositional abilities (e.g., reasoning) of their larger counterparts,\nenabling selective offloading of requests to reduce cost and latency. Achieving\nthis live augmentation at scale introduces intricate trade-offs between\nresponse quality, latency, and system throughput. For a new request, IC-Cache\nefficiently selects similar, high-utility examples to prepend them to the new\nrequest's input. At scale, it adaptively routes requests across LLMs of varying\ncapabilities, accounting for response quality and serving loads. IC-Cache\nemploys a cost-aware cache replay mechanism that refines example quality\noffline to maximize online cache utility and efficiency. Evaluations on\nmillions of realistic requests demonstrate that IC-Cache improves LLM serving\nthroughput by 1.4-5.9x and reduces latency by 28-71% without hurting response\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 70% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge transfer among requests. However, naively caching and reusing past\nresponses leads to a big quality drop. In this paper, we introduce IC-Cache, a\ncaching system that enables live LLM capability augmentation to improve serving\nefficiency: by leveraging historical request-response pairs from larger models\nas in-context examples, IC-Cache empowers small LLMs to imitate and even exceed\nthe compositional abilities (e.g., reasoning) of their larger counterparts,\nenabling selective offloading of requests to reduce cost and latency. Achieving\nthis live augmentation at scale introduces intricate trade-offs between\nresponse quality, latency, and system throughput. For a new request, IC-Cache\nefficiently selects similar, high-utility examples to prepend them to the new\nrequest's input. At scale, it adaptively routes requests across LLMs of varying\ncapabilities, accounting for response quality and serving loads. IC-Cache\nemploys a cost-aware cache replay mechanism that refines example quality\noffline to maximize online cache utility and efficiency. Evaluations on\nmillions of realistic requests demonstrate that IC-Cache improves LLM serving\nthroughput by 1.4-5.9x and reduces latency by 28-71% without hurting response\nquality."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Nikhil Sarda"
                    },
                    {
                        "name": "Lillian Tsai"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Yanqi Zhou"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Henry M. Levy"
                    },
                    {
                        "name": "David Culler"
                    }
                ],
                "author_detail": {
                    "name": "David Culler"
                },
                "author": "David Culler",
                "arxiv_doi": "10.1145/3731569.3764829",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731569.3764829",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.12689v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12689v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01228v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01228v2",
                "updated": "2025-09-03T20:54:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    20,
                    54,
                    57,
                    2,
                    246,
                    0
                ],
                "published": "2024-10-02T04:12:13Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    4,
                    12,
                    13,
                    2,
                    276,
                    0
                ],
                "title": "ConServe: Fine-Grained GPU Harvesting for LLM Online and Offline\n  Co-Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConServe: Fine-Grained GPU Harvesting for LLM Online and Offline\n  Co-Serving"
                },
                "summary": "Large language model (LLM) serving demands low latency and high throughput,\nbut high load variability makes it challenging to achieve high GPU utilization.\nIn this paper, we identify a synergetic but overlooked opportunity to co-serve\nlatency-critical online requests alongside latency-tolerant offline tasks such\nas model benchmarking. While promising, existing serving systems fail to\nco-serve them efficiently, as their coarse-grained resource management at the\nrequest or iteration level cannot harvest millisecond-level GPU idle cycles\nwithout introducing interference that violates online latency objectives.\nConServe is a new LLM co-serving system that achieves high throughput and\nstrong online latency guarantees by managing resources at finer granularities.\nConServe introduces three techniques: (1) a latency-aware token-level scheduler\nthat precisely sizes offline batches and tokens to fit within online latency\nobjectives; (2) sub-iteration, layer-wise preemption that allows offline tasks\nto yield to online load spikes; and (3) incremental KV cache management that\nenables preempting and resuming offline requests at near-zero cost. Evaluations\nwith Llama-3.1 and Qwen-2.5 models on real-world workloads show that ConServe\ndelivers an average of 2.2$\\times$ higher throughput and reduces online serving\ntail latency by 2.9$\\times$ on average compared to state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving demands low latency and high throughput,\nbut high load variability makes it challenging to achieve high GPU utilization.\nIn this paper, we identify a synergetic but overlooked opportunity to co-serve\nlatency-critical online requests alongside latency-tolerant offline tasks such\nas model benchmarking. While promising, existing serving systems fail to\nco-serve them efficiently, as their coarse-grained resource management at the\nrequest or iteration level cannot harvest millisecond-level GPU idle cycles\nwithout introducing interference that violates online latency objectives.\nConServe is a new LLM co-serving system that achieves high throughput and\nstrong online latency guarantees by managing resources at finer granularities.\nConServe introduces three techniques: (1) a latency-aware token-level scheduler\nthat precisely sizes offline batches and tokens to fit within online latency\nobjectives; (2) sub-iteration, layer-wise preemption that allows offline tasks\nto yield to online load spikes; and (3) incremental KV cache management that\nenables preempting and resuming offline requests at near-zero cost. Evaluations\nwith Llama-3.1 and Qwen-2.5 models on real-world workloads show that ConServe\ndelivers an average of 2.2$\\times$ higher throughput and reduces online serving\ntail latency by 2.9$\\times$ on average compared to state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Yifan Qiao"
                    },
                    {
                        "name": "Shu Anzai"
                    },
                    {
                        "name": "Shan Yu"
                    },
                    {
                        "name": "Haoran Ma"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Miryung Kim"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jiarong Xing"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Harry Xu"
                    }
                ],
                "author_detail": {
                    "name": "Harry Xu"
                },
                "author": "Harry Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01228v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03394v1",
                "updated": "2025-09-03T15:15:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    44,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T15:15:44Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    44,
                    2,
                    246,
                    0
                ],
                "title": "CloudFormer: An Attention-based Performance Prediction for Public Clouds\n  with Unknown Workload",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CloudFormer: An Attention-based Performance Prediction for Public Clouds\n  with Unknown Workload"
                },
                "summary": "Cloud platforms are increasingly relied upon to host diverse,\nresource-intensive workloads due to their scalability, flexibility, and\ncost-efficiency. In multi-tenant cloud environments, virtual machines are\nconsolidated on shared physical servers to improve resource utilization. While\nvirtualization guarantees resource partitioning for CPU, memory, and storage,\nit cannot ensure performance isolation. Competition for shared resources such\nas last-level cache, memory bandwidth, and network interfaces often leads to\nsevere performance degradation. Existing management techniques, including VM\nscheduling and resource provisioning, require accurate performance prediction\nto mitigate interference. However, this remains challenging in public clouds\ndue to the black-box nature of VMs and the highly dynamic nature of workloads.\nTo address these limitations, we propose CloudFormer, a dual-branch\nTransformer-based model designed to predict VM performance degradation in\nblack-box environments. CloudFormer jointly models temporal dynamics and\nsystem-level interactions, leveraging 206 system metrics at one-second\nresolution across both static and dynamic scenarios. This design enables the\nmodel to capture transient interference effects and adapt to varying workload\nconditions without scenario-specific tuning. Complementing the methodology, we\nprovide a fine-grained dataset that significantly expands the temporal\nresolution and metric diversity compared to existing benchmarks. Experimental\nresults demonstrate that CloudFormer consistently outperforms state-of-the-art\nbaselines across multiple evaluation metrics, achieving robust generalization\nacross diverse and previously unseen workloads. Notably, CloudFormer attains a\nmean absolute error (MAE) of just 7.8%, representing a substantial improvement\nin predictive accuracy and outperforming existing methods at least by 28%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud platforms are increasingly relied upon to host diverse,\nresource-intensive workloads due to their scalability, flexibility, and\ncost-efficiency. In multi-tenant cloud environments, virtual machines are\nconsolidated on shared physical servers to improve resource utilization. While\nvirtualization guarantees resource partitioning for CPU, memory, and storage,\nit cannot ensure performance isolation. Competition for shared resources such\nas last-level cache, memory bandwidth, and network interfaces often leads to\nsevere performance degradation. Existing management techniques, including VM\nscheduling and resource provisioning, require accurate performance prediction\nto mitigate interference. However, this remains challenging in public clouds\ndue to the black-box nature of VMs and the highly dynamic nature of workloads.\nTo address these limitations, we propose CloudFormer, a dual-branch\nTransformer-based model designed to predict VM performance degradation in\nblack-box environments. CloudFormer jointly models temporal dynamics and\nsystem-level interactions, leveraging 206 system metrics at one-second\nresolution across both static and dynamic scenarios. This design enables the\nmodel to capture transient interference effects and adapt to varying workload\nconditions without scenario-specific tuning. Complementing the methodology, we\nprovide a fine-grained dataset that significantly expands the temporal\nresolution and metric diversity compared to existing benchmarks. Experimental\nresults demonstrate that CloudFormer consistently outperforms state-of-the-art\nbaselines across multiple evaluation metrics, achieving robust generalization\nacross diverse and previously unseen workloads. Notably, CloudFormer attains a\nmean absolute error (MAE) of just 7.8%, representing a substantial improvement\nin predictive accuracy and outperforming existing methods at least by 28%."
                },
                "authors": [
                    {
                        "name": "Amirhossein Shahbazinia"
                    },
                    {
                        "name": "Darong Huang"
                    },
                    {
                        "name": "Luis Costero"
                    },
                    {
                        "name": "David Atienza"
                    }
                ],
                "author_detail": {
                    "name": "David Atienza"
                },
                "author": "David Atienza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00079v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00079v4",
                "updated": "2025-09-03T14:56:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    56,
                    29,
                    2,
                    246,
                    0
                ],
                "published": "2024-06-24T02:05:32Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    2,
                    5,
                    32,
                    0,
                    176,
                    0
                ],
                "title": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving"
                },
                "summary": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests."
                },
                "authors": [
                    {
                        "name": "Ruoyu Qin"
                    },
                    {
                        "name": "Zheming Li"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yongwei Wu"
                    },
                    {
                        "name": "Weimin Zheng"
                    },
                    {
                        "name": "Xinran Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xinran Xu"
                },
                "author": "Xinran Xu",
                "arxiv_comment": "23 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00079v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00079v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04416v2",
                "updated": "2025-09-03T14:28:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    28,
                    23,
                    2,
                    246,
                    0
                ],
                "published": "2025-07-06T15:08:49Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "title": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based\n  Sequence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based\n  Sequence Modeling"
                },
                "summary": "Transformers have become the cornerstone of modern large-scale language\nmodels, but their reliance on softmax attention poses a computational\nbottleneck at both training and inference. Recurrent models offer high\nefficiency, but compressing the full sequence into a fixed-size and holistic\nrepresentation suffers from memory degradation in long contexts and limits\nfine-grained retrieval. To address this, we propose RAT, an intermediate design\nthat bridges the efficiency of RNNs and capacity of attention. RAT partitions\nthe input into chunks, applies recurrence within each chunk for local\ndependencies, and softmax-based attention across chunks for long-range\ninteractions. This design mitigates memory degradation and enables direct\naccess to distant tokens, while retaining computational efficiency.\nEmpirically, with a chunk size of 16, the RAT block achieves a 7x improvement\nin training speed with 100K token sequences and 9x in generation at the 4K\nposition, while maintaining similar performance compared to standard attention.\nWe demonstrate this by training 1.3B parameter models from scratch and\nperforming large-scale evaluations, including short- and long-context\nbenchmarks, as well as supervised fine-tuning~(SFT). We further propose a\nhybrid architecture that interleaves RAT with local attention. By combining\nefficient long-range modeling with strong local interactions, this hybrid\ndesign not only improves inference speed and reduces cache memory usage, but\nalso consistently enhances performance and shows the overall best results. Code\nis available at https://github.com/CLAIRE-Labo/RAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the cornerstone of modern large-scale language\nmodels, but their reliance on softmax attention poses a computational\nbottleneck at both training and inference. Recurrent models offer high\nefficiency, but compressing the full sequence into a fixed-size and holistic\nrepresentation suffers from memory degradation in long contexts and limits\nfine-grained retrieval. To address this, we propose RAT, an intermediate design\nthat bridges the efficiency of RNNs and capacity of attention. RAT partitions\nthe input into chunks, applies recurrence within each chunk for local\ndependencies, and softmax-based attention across chunks for long-range\ninteractions. This design mitigates memory degradation and enables direct\naccess to distant tokens, while retaining computational efficiency.\nEmpirically, with a chunk size of 16, the RAT block achieves a 7x improvement\nin training speed with 100K token sequences and 9x in generation at the 4K\nposition, while maintaining similar performance compared to standard attention.\nWe demonstrate this by training 1.3B parameter models from scratch and\nperforming large-scale evaluations, including short- and long-context\nbenchmarks, as well as supervised fine-tuning~(SFT). We further propose a\nhybrid architecture that interleaves RAT with local attention. By combining\nefficient long-range modeling with strong local interactions, this hybrid\ndesign not only improves inference speed and reduces cache memory usage, but\nalso consistently enhances performance and shows the overall best results. Code\nis available at https://github.com/CLAIRE-Labo/RAT."
                },
                "authors": [
                    {
                        "name": "Xiuying Wei"
                    },
                    {
                        "name": "Anunay Yadav"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03560v1",
                "updated": "2025-09-03T11:23:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    23,
                    35,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T11:23:35Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    23,
                    35,
                    2,
                    246,
                    0
                ],
                "title": "A Cegar-centric Bounded Reachability Analysis for Compositional Affine\n  Hybrid Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Cegar-centric Bounded Reachability Analysis for Compositional Affine\n  Hybrid Systems"
                },
                "summary": "Reachability analysis of compositional hybrid systems, where individual\ncomponents are modeled as hybrid automata, poses unique challenges. In addition\nto preserving the compositional semantics while computing system behaviors,\nalgorithms have to cater to the explosion in the number of locations in the\nparallel product automaton. In this paper, we propose a bounded reachability\nanalysis algorithm for compositional hybrid systems with piecewise affine\ndynamics, based on the principle of counterexample guided abstraction\nrefinement (CEGAR). In particular, the algorithm searches for a counterexample\nin the discrete abstraction of the composition model, without explicitly\ncomputing a product automaton. When a counterexample is discovered in the\nabstraction, its validity is verified by a refinement of the state-space guided\nby the abstract counterexample. The state-space refinement is through a\nsymbolic reachability analysis, particularly using a state-of-the-art algorithm\nwith support functions as the continuous state representation. In addition, the\nalgorithm mixes different semantics of composition with the objective of\nimproved efficiency. Step compositional semantics is followed while exploring\nthe abstract (discrete) state-space, while shallow compositional semantics is\nfollowed during state-space refinement with symbolic reachability analysis.\nOptimizations such as caching the results of the symbolic reachability\nanalysis, which can be later reused, have been proposed. We implement this\nalgorithm in the tool SAT-Reach and demonstrate the scalability benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reachability analysis of compositional hybrid systems, where individual\ncomponents are modeled as hybrid automata, poses unique challenges. In addition\nto preserving the compositional semantics while computing system behaviors,\nalgorithms have to cater to the explosion in the number of locations in the\nparallel product automaton. In this paper, we propose a bounded reachability\nanalysis algorithm for compositional hybrid systems with piecewise affine\ndynamics, based on the principle of counterexample guided abstraction\nrefinement (CEGAR). In particular, the algorithm searches for a counterexample\nin the discrete abstraction of the composition model, without explicitly\ncomputing a product automaton. When a counterexample is discovered in the\nabstraction, its validity is verified by a refinement of the state-space guided\nby the abstract counterexample. The state-space refinement is through a\nsymbolic reachability analysis, particularly using a state-of-the-art algorithm\nwith support functions as the continuous state representation. In addition, the\nalgorithm mixes different semantics of composition with the objective of\nimproved efficiency. Step compositional semantics is followed while exploring\nthe abstract (discrete) state-space, while shallow compositional semantics is\nfollowed during state-space refinement with symbolic reachability analysis.\nOptimizations such as caching the results of the symbolic reachability\nanalysis, which can be later reused, have been proposed. We implement this\nalgorithm in the tool SAT-Reach and demonstrate the scalability benefits."
                },
                "authors": [
                    {
                        "name": "Atanu Kundu"
                    },
                    {
                        "name": "Pratyay Sarkar"
                    },
                    {
                        "name": "Rajarshi Ray"
                    }
                ],
                "author_detail": {
                    "name": "Rajarshi Ray"
                },
                "author": "Rajarshi Ray",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03136v1",
                "updated": "2025-09-03T08:38:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T08:38:40Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "title": "Adaptive KV-Cache Compression without Manually Setting Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive KV-Cache Compression without Manually Setting Budget"
                },
                "summary": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable."
                },
                "authors": [
                    {
                        "name": "Chenxia Tang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20353v2",
                "updated": "2025-09-03T06:56:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    6,
                    56,
                    21,
                    2,
                    246,
                    0
                ],
                "published": "2025-05-26T05:58:49Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    58,
                    49,
                    0,
                    146,
                    0
                ],
                "title": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation"
                },
                "summary": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Ben Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Nian Wu"
                },
                "author": "Ying Nian Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18002v2",
                "updated": "2025-09-02T18:10:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    18,
                    10,
                    0,
                    1,
                    245,
                    0
                ],
                "published": "2024-10-23T16:25:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges"
                },
                "summary": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks."
                },
                "authors": [
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Hanzhi Yu"
                    },
                    {
                        "name": "Mingzhe Chen"
                    },
                    {
                        "name": "Yuchen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Liu"
                },
                "author": "Yuchen Liu",
                "arxiv_comment": "Under Major Revision in IEEE Network",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02532v1",
                "updated": "2025-09-02T17:35:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    17,
                    35,
                    42,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T17:35:42Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    17,
                    35,
                    42,
                    1,
                    245,
                    0
                ],
                "title": "A Novel Coded Caching Scheme for Partially Cooperative Device-to-Device\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Coded Caching Scheme for Partially Cooperative Device-to-Device\n  Networks"
                },
                "summary": "Device-to-device (D2D) communication is one of the most promising techniques\nfor future wireless cellular communication systems. This paper considers coded\ncaching in a partially cooperative wireless D2D network, where only a subset of\nusers transmit during delivery, while all users request files. The\nnon-transmitting users are referred to as selfish users. All existing schemes\nthat do not require knowledge of the identity of selfish users before content\nplacement are limited to the high-memory regime, particularly when the number\nof selfish users is large. We propose a novel coded caching scheme for a\npartially cooperative D2D network that operates in all feasible memory regimes,\nregardless of the number of selfish users. We also derive a lower bound on the\ntransmission load of a partially cooperative D2D coded caching scheme. Using\nthis bound, the proposed scheme is shown to be optimal in the high-memory\nregime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Device-to-device (D2D) communication is one of the most promising techniques\nfor future wireless cellular communication systems. This paper considers coded\ncaching in a partially cooperative wireless D2D network, where only a subset of\nusers transmit during delivery, while all users request files. The\nnon-transmitting users are referred to as selfish users. All existing schemes\nthat do not require knowledge of the identity of selfish users before content\nplacement are limited to the high-memory regime, particularly when the number\nof selfish users is large. We propose a novel coded caching scheme for a\npartially cooperative D2D network that operates in all feasible memory regimes,\nregardless of the number of selfish users. We also derive a lower bound on the\ntransmission load of a partially cooperative D2D coded caching scheme. Using\nthis bound, the proposed scheme is shown to be optimal in the high-memory\nregime."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "7 pages and 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v5",
                "updated": "2025-09-02T16:39:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    39,
                    56,
                    1,
                    245,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation"
                },
                "summary": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. However, existing Ethernet-based solutions, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilization due to both increasing traffic demands and\nthe expanding scale of datacenter topologies, which also exacerbate network\nfailures. To address these limitations, we propose REPS, a lightweight\ndecentralized per-packet adaptive load balancing algorithm designed to optimize\nnetwork utilization while ensuring rapid recovery from link failures. REPS\nadapts to network conditions by caching good-performing paths. In case of a\nnetwork failure, REPS re-routes traffic away from it in less than 100\nmicroseconds. REPS is designed to be deployed with next-generation out-of-order\ntransports, such as Ultra Ethernet, and uses less than 25 bytes of\nper-connection state regardless of the topology size. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. However, existing Ethernet-based solutions, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilization due to both increasing traffic demands and\nthe expanding scale of datacenter topologies, which also exacerbate network\nfailures. To address these limitations, we propose REPS, a lightweight\ndecentralized per-packet adaptive load balancing algorithm designed to optimize\nnetwork utilization while ensuring rapid recovery from link failures. REPS\nadapts to network conditions by caching good-performing paths. In case of a\nnetwork failure, REPS re-routes traffic away from it in less than 100\nmicroseconds. REPS is designed to be deployed with next-generation out-of-order\ntransports, such as Ultra Ethernet, and uses less than 25 bytes of\nper-connection state regardless of the topology size. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Mikhail Khalilov"
                    },
                    {
                        "name": "Elias Achermann"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02480v1",
                "updated": "2025-09-02T16:30:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    30,
                    49,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T16:30:49Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    30,
                    49,
                    1,
                    245,
                    0
                ],
                "title": "MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to\n  Break the GPU Memory Wall",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to\n  Break the GPU Memory Wall"
                },
                "summary": "Training LLMs larger than the aggregated memory of multiple GPUs is\nincreasingly necessary due to the faster growth of LLM sizes compared to GPU\nmemory. To this end, multi-tier host memory or disk offloading techniques are\nproposed by state of art. Despite advanced asynchronous multi-tier read/write\nstrategies, such offloading strategies result in significant I/O overheads in\nthe critical path of training, resulting in slower iterations. To this end, we\npropose MLP-Offload, a novel multi-level, multi-path offloading engine\nspecifically designed for optimizing LLM training on resource-constrained\nsetups by mitigating I/O bottlenecks. We make several key observations that\ndrive the design of MLP-Offload, such as I/O overheads during the update\ndominate the iteration time; I/O bandwidth of the third-level remote storage\ntier remains unutilized; and, contention due to concurrent offloading amplifies\nI/O bottlenecks. Driven by these insights, we design and implement MLP-Offload\nto offload the optimizer states across multiple tiers in a cache-efficient and\nconcurrency-controlled fashion to mitigate I/O bottlenecks during the backward\nand update phases. Evaluations on models up to 280B parameters shows that\nMLP-Offload achieves 2.5$\\times$ faster iterations compared to the\nstate-of-the-art LLM training runtimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training LLMs larger than the aggregated memory of multiple GPUs is\nincreasingly necessary due to the faster growth of LLM sizes compared to GPU\nmemory. To this end, multi-tier host memory or disk offloading techniques are\nproposed by state of art. Despite advanced asynchronous multi-tier read/write\nstrategies, such offloading strategies result in significant I/O overheads in\nthe critical path of training, resulting in slower iterations. To this end, we\npropose MLP-Offload, a novel multi-level, multi-path offloading engine\nspecifically designed for optimizing LLM training on resource-constrained\nsetups by mitigating I/O bottlenecks. We make several key observations that\ndrive the design of MLP-Offload, such as I/O overheads during the update\ndominate the iteration time; I/O bandwidth of the third-level remote storage\ntier remains unutilized; and, contention due to concurrent offloading amplifies\nI/O bottlenecks. Driven by these insights, we design and implement MLP-Offload\nto offload the optimizer states across multiple tiers in a cache-efficient and\nconcurrency-controlled fashion to mitigate I/O bottlenecks during the backward\nand update phases. Evaluations on models up to 280B parameters shows that\nMLP-Offload achieves 2.5$\\times$ faster iterations compared to the\nstate-of-the-art LLM training runtimes."
                },
                "authors": [
                    {
                        "name": "Avinash Maurya"
                    },
                    {
                        "name": "M. Mustafa Rafique"
                    },
                    {
                        "name": "Franck Cappello"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Nicolae"
                },
                "author": "Bogdan Nicolae",
                "arxiv_doi": "10.1145/3712285.3759864",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3712285.3759864",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SC'25: The International Conference for High Performance Computing,\n  Networking, Storage and Analysis",
                "arxiv_journal_ref": "SC'25: The International Conference for High Performance\n  Computing, Networking, Storage and Analysis, 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.0; E.2; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02408v1",
                "updated": "2025-09-02T15:19:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    15,
                    19,
                    6,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T15:19:06Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    15,
                    19,
                    6,
                    1,
                    245,
                    0
                ],
                "title": "Cache Management for Mixture-of-Experts LLMs -- extended version",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Management for Mixture-of-Experts LLMs -- extended version"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks. One of the main challenges towards the successful\ndeployment of LLMs is memory management, since they typically involve billions\nof parameters. To this end, architectures based on Mixture-of-Experts have been\nproposed, which aim to reduce the size of the parameters that are activated\nwhen producing a token. This raises the equally critical issue of efficiently\nmanaging the limited cache of the system, in that frequently used experts\nshould be stored in the fast cache rather than in the slower secondary memory.\n  In this work, we introduce and study a new paging problem that models expert\nmanagement optimization. Our formulation captures both the layered architecture\nof LLMs and the requirement that experts are cached efficiently. We first\npresent lower bounds on the competitive ratio of both deterministic and\nrandomized algorithms, which show that under mild assumptions, LRU-like\npolicies have good theoretical competitive performance. We then propose a\nlayer-based extension of LRU that is tailored to the problem at hand.\n  Extensive simulations on both synthetic datasets and actual traces of MoE\nusage show that our algorithm outperforms policies for the classic paging\nproblem, such as the standard LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks. One of the main challenges towards the successful\ndeployment of LLMs is memory management, since they typically involve billions\nof parameters. To this end, architectures based on Mixture-of-Experts have been\nproposed, which aim to reduce the size of the parameters that are activated\nwhen producing a token. This raises the equally critical issue of efficiently\nmanaging the limited cache of the system, in that frequently used experts\nshould be stored in the fast cache rather than in the slower secondary memory.\n  In this work, we introduce and study a new paging problem that models expert\nmanagement optimization. Our formulation captures both the layered architecture\nof LLMs and the requirement that experts are cached efficiently. We first\npresent lower bounds on the competitive ratio of both deterministic and\nrandomized algorithms, which show that under mild assumptions, LRU-like\npolicies have good theoretical competitive performance. We then propose a\nlayer-based extension of LRU that is tailored to the problem at hand.\n  Extensive simulations on both synthetic datasets and actual traces of MoE\nusage show that our algorithm outperforms policies for the classic paging\nproblem, such as the standard LRU."
                },
                "authors": [
                    {
                        "name": "Spyros Angelopoulos"
                    },
                    {
                        "name": "Loris Marchal"
                    },
                    {
                        "name": "Adrien Obrecht"
                    },
                    {
                        "name": "Bertrand Simon"
                    }
                ],
                "author_detail": {
                    "name": "Bertrand Simon"
                },
                "author": "Bertrand Simon",
                "arxiv_doi": "10.1007/978-3-031-99872-0_2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-99872-0_2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v2",
                "updated": "2025-09-02T13:09:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    13,
                    9,
                    37,
                    1,
                    245,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing reliance on expensive vector\ndatabase lookups. To scale efficiently, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically skewed MedRAG workload reduces database calls by 78.9% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our work highlights that approximate caching\nis a viable and effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing reliance on expensive vector\ndatabase lookups. To scale efficiently, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically skewed MedRAG workload reduces database calls by 78.9% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our work highlights that approximate caching\nis a viable and effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Zhang Ji"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    }
                ],
                "author_detail": {
                    "name": "Martijn de Vos"
                },
                "author": "Martijn de Vos",
                "arxiv_doi": "10.1145/3721146.3721941",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721146.3721941",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02232v1",
                "updated": "2025-09-02T11:58:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    58,
                    6,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T11:58:06Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    58,
                    6,
                    1,
                    245,
                    0
                ],
                "title": "Efficient Geometry Compression and Communication for 3D Gaussian\n  Splatting Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Geometry Compression and Communication for 3D Gaussian\n  Splatting Point Clouds"
                },
                "summary": "Storage and transmission challenges in dynamic 3D scene representation based\non the i3DV platform, With increasing scene complexity, the explosive growth of\n3D Gaussian data volume causes excessive storage space occupancy. To address\nthis issue, we propose adopting the AVS PCRM reference software for efficient\ncompression of Gaussian point cloud geometry data. The strategy deeply\nintegrates the advanced encoding capabilities of AVS PCRM into the i3DV\nplatform, forming technical complementarity with the original rate-distortion\noptimization mechanism based on binary hash tables. On one hand, the hash table\nefficiently caches inter-frame Gaussian point transformation relationships,\nwhich allows for high-fidelity transmission within a 40 Mbps bandwidth\nconstraint. On the other hand, AVS PCRM performs precise compression on\ngeometry data. Experimental results demonstrate that the joint framework\nmaintains the advantages of fast rendering and high-quality synthesis in 3D\nGaussian technology while achieving significant 10\\%-25\\% bitrate savings on\nuniversal test sets. It provides a superior rate-distortion tradeoff solution\nfor the storage, transmission, and interaction of 3D volumetric video.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Storage and transmission challenges in dynamic 3D scene representation based\non the i3DV platform, With increasing scene complexity, the explosive growth of\n3D Gaussian data volume causes excessive storage space occupancy. To address\nthis issue, we propose adopting the AVS PCRM reference software for efficient\ncompression of Gaussian point cloud geometry data. The strategy deeply\nintegrates the advanced encoding capabilities of AVS PCRM into the i3DV\nplatform, forming technical complementarity with the original rate-distortion\noptimization mechanism based on binary hash tables. On one hand, the hash table\nefficiently caches inter-frame Gaussian point transformation relationships,\nwhich allows for high-fidelity transmission within a 40 Mbps bandwidth\nconstraint. On the other hand, AVS PCRM performs precise compression on\ngeometry data. Experimental results demonstrate that the joint framework\nmaintains the advantages of fast rendering and high-quality synthesis in 3D\nGaussian technology while achieving significant 10\\%-25\\% bitrate savings on\nuniversal test sets. It provides a superior rate-distortion tradeoff solution\nfor the storage, transmission, and interaction of 3D volumetric video."
                },
                "authors": [
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Yanting Li"
                    },
                    {
                        "name": "Luyang Tang"
                    },
                    {
                        "name": "Wei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Gao"
                },
                "author": "Wei Gao",
                "arxiv_doi": "10.1145/3680207.3765659",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3680207.3765659",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages,5 figures",
                "arxiv_journal_ref": "ACM MOBICOM 2025",
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15212v2",
                "updated": "2025-09-02T11:29:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    29,
                    34,
                    1,
                    245,
                    0
                ],
                "published": "2025-08-21T03:48:28Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    48,
                    28,
                    3,
                    233,
                    0
                ],
                "title": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning"
                },
                "summary": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Guanchen Li"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Emad Barsoum"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02121v1",
                "updated": "2025-09-02T09:17:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    9,
                    17,
                    40,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T09:17:40Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    9,
                    17,
                    40,
                    1,
                    245,
                    0
                ],
                "title": "Batch Query Processing and Optimization for Agentic Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch Query Processing and Optimization for Agentic Workflows"
                },
                "summary": "Large Language Models (LLMs) in agentic workflows combine multi-step\nreasoning, tool use, and collaboration across multiple specialized agents.\nExisting LLM serving engines optimize individual calls in isolation, while\nmulti-agent frameworks focus on orchestration without system-level performance\nplanning. As a result, repeated prompts, overlapping contexts, and concurrent\nexecutions create substantial redundancy and poor GPU utilization, especially\nin batch analytics scenarios. We introduce Halo, a system that brings batch\nquery processing and optimization into agentic LLM workflows. Halo represents\neach workflow as a structured query plan DAG and constructs a consolidated\ngraph for batched queries that exposes shared computation. Guided by a cost\nmodel that jointly considers prefill and decode costs, cache reuse, and GPU\nplacement, Halo performs plan-level optimization to minimize redundant\nexecution. Its runtime integrates adaptive batching, KV-cache sharing and\nmigration, along with compute-communication overlap to maximize hardware\nefficiency. Evaluation across six benchmarks shows that Halo achieves up to\n18.6x speedup for batch inference and 4.7x throughput improvement under online\nserving, scaling to workloads of tens of thousands of queries and complex\ngraphs. These gains are achieved without compromising output quality. By\nunifying query optimization with LLM serving, Halo enables efficient agentic\nworkflows in data analytics and decision-making applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) in agentic workflows combine multi-step\nreasoning, tool use, and collaboration across multiple specialized agents.\nExisting LLM serving engines optimize individual calls in isolation, while\nmulti-agent frameworks focus on orchestration without system-level performance\nplanning. As a result, repeated prompts, overlapping contexts, and concurrent\nexecutions create substantial redundancy and poor GPU utilization, especially\nin batch analytics scenarios. We introduce Halo, a system that brings batch\nquery processing and optimization into agentic LLM workflows. Halo represents\neach workflow as a structured query plan DAG and constructs a consolidated\ngraph for batched queries that exposes shared computation. Guided by a cost\nmodel that jointly considers prefill and decode costs, cache reuse, and GPU\nplacement, Halo performs plan-level optimization to minimize redundant\nexecution. Its runtime integrates adaptive batching, KV-cache sharing and\nmigration, along with compute-communication overlap to maximize hardware\nefficiency. Evaluation across six benchmarks shows that Halo achieves up to\n18.6x speedup for batch inference and 4.7x throughput improvement under online\nserving, scaling to workloads of tens of thousands of queries and complex\ngraphs. These gains are achieved without compromising output quality. By\nunifying query optimization with LLM serving, Halo enables efficient agentic\nworkflows in data analytics and decision-making applications."
                },
                "authors": [
                    {
                        "name": "Junyi Shen"
                    },
                    {
                        "name": "Noppanat Wadlom"
                    },
                    {
                        "name": "Yao Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Lu"
                },
                "author": "Yao Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02004v1",
                "updated": "2025-09-02T06:40:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    6,
                    40,
                    45,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T06:40:45Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    6,
                    40,
                    45,
                    1,
                    245,
                    0
                ],
                "title": "Augmented Shuffle Differential Privacy Protocols for Large-Domain\n  Categorical and Key-Value Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Shuffle Differential Privacy Protocols for Large-Domain\n  Categorical and Key-Value Data"
                },
                "summary": "Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy\nby introducing a shuffler who randomly shuffles data in a distributed system.\nHowever, most shuffle DP protocols are vulnerable to two attacks: collusion\nattacks by the data collector and users and data poisoning attacks. A recent\nstudy addresses this issue by introducing an augmented shuffle DP protocol,\nwhere users do not add noise and the shuffler performs random sampling and\ndummy data addition. However, it focuses on frequency estimation over\ncategorical data with a small domain and cannot be applied to a large domain\ndue to prohibitively high communication and computational costs.\n  In this paper, we fill this gap by introducing a novel augmented shuffle DP\nprotocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME\nprotocol uses a hash function to filter out unpopular items and then accurately\ncalculates frequencies for popular items. To perform this within one round of\ninteraction between users and the shuffler, our protocol carefully communicates\nwithin a system using multiple encryption. We also apply our FME protocol to\nmore advanced KV (Key-Value) statistics estimation with an additional technique\nto reduce bias. For both categorical and KV data, we prove that our protocol\nprovides computational DP, high robustness to the above two attacks, accuracy,\nand efficiency. We show the effectiveness of our proposals through comparisons\nwith twelve existing protocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy\nby introducing a shuffler who randomly shuffles data in a distributed system.\nHowever, most shuffle DP protocols are vulnerable to two attacks: collusion\nattacks by the data collector and users and data poisoning attacks. A recent\nstudy addresses this issue by introducing an augmented shuffle DP protocol,\nwhere users do not add noise and the shuffler performs random sampling and\ndummy data addition. However, it focuses on frequency estimation over\ncategorical data with a small domain and cannot be applied to a large domain\ndue to prohibitively high communication and computational costs.\n  In this paper, we fill this gap by introducing a novel augmented shuffle DP\nprotocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME\nprotocol uses a hash function to filter out unpopular items and then accurately\ncalculates frequencies for popular items. To perform this within one round of\ninteraction between users and the shuffler, our protocol carefully communicates\nwithin a system using multiple encryption. We also apply our FME protocol to\nmore advanced KV (Key-Value) statistics estimation with an additional technique\nto reduce bias. For both categorical and KV data, we prove that our protocol\nprovides computational DP, high robustness to the above two attacks, accuracy,\nand efficiency. We show the effectiveness of our proposals through comparisons\nwith twelve existing protocols."
                },
                "authors": [
                    {
                        "name": "Takao Murakami"
                    },
                    {
                        "name": "Yuichi Sei"
                    },
                    {
                        "name": "Reo Eriguchi"
                    }
                ],
                "author_detail": {
                    "name": "Reo Eriguchi"
                },
                "author": "Reo Eriguchi",
                "arxiv_comment": "Full version of the paper accepted at NDSS 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01395v1",
                "updated": "2025-09-01T11:41:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    41,
                    10,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T11:41:10Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    41,
                    10,
                    0,
                    244,
                    0
                ],
                "title": "LLMs cannot spot math errors, even when allowed to peek into the\n  solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs cannot spot math errors, even when allowed to peek into the\n  solution"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable performance on math word\nproblems, yet they have been shown to struggle with meta-reasoning tasks such\nas identifying errors in student solutions. In this work, we investigate the\nchallenge of locating the first error step in stepwise solutions using two\nerror reasoning datasets: VtG and PRM800K. Our experiments show that\nstate-of-the-art LLMs struggle to locate the first error step in student\nsolutions even when given access to the reference solution. To that end, we\npropose an approach that generates an intermediate corrected student solution,\naligning more closely with the original student's solution, which helps improve\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable performance on math word\nproblems, yet they have been shown to struggle with meta-reasoning tasks such\nas identifying errors in student solutions. In this work, we investigate the\nchallenge of locating the first error step in stepwise solutions using two\nerror reasoning datasets: VtG and PRM800K. Our experiments show that\nstate-of-the-art LLMs struggle to locate the first error step in student\nsolutions even when given access to the reference solution. To that end, we\npropose an approach that generates an intermediate corrected student solution,\naligning more closely with the original student's solution, which helps improve\nperformance."
                },
                "authors": [
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "Accepted to EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15779v2",
                "updated": "2025-09-01T07:26:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    7,
                    26,
                    57,
                    0,
                    244,
                    0
                ],
                "published": "2025-02-17T08:12:34Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    12,
                    34,
                    0,
                    48,
                    0
                ],
                "title": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer"
                },
                "summary": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP."
                },
                "authors": [
                    {
                        "name": "Euntae Choi"
                    },
                    {
                        "name": "Sumin Song"
                    },
                    {
                        "name": "Woosang Lim"
                    },
                    {
                        "name": "Sungjoo Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Sungjoo Yoo"
                },
                "author": "Sungjoo Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v3",
                "updated": "2025-09-01T03:51:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    51,
                    9,
                    0,
                    244,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01092v1",
                "updated": "2025-09-01T03:31:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T03:31:44Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "title": "REFRAG: Rethinking RAG based Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REFRAG: Rethinking RAG based Decoding"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes."
                },
                "authors": [
                    {
                        "name": "Xiaoqiang Lin"
                    },
                    {
                        "name": "Aritra Ghosh"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    },
                    {
                        "name": "Anshumali Shrivastava"
                    },
                    {
                        "name": "Vijai Mohan"
                    }
                ],
                "author_detail": {
                    "name": "Vijai Mohan"
                },
                "author": "Vijai Mohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06133v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06133v2",
                "updated": "2025-08-31T15:09:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    15,
                    9,
                    36,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-08T08:54:21Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "title": "LLM Serving Optimization with Variable Prefill and Decode Lengths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Serving Optimization with Variable Prefill and Decode Lengths"
                },
                "summary": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency."
                },
                "authors": [
                    {
                        "name": "Meixuan Wang"
                    },
                    {
                        "name": "Yinyu Ye"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06133v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06133v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00883v1",
                "updated": "2025-08-31T14:51:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    51,
                    19,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-31T14:51:19Z",
                "published_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    51,
                    19,
                    6,
                    243,
                    0
                ],
                "title": "Accelerating Latency-Critical Applications with AI-Powered\n  Semi-Automatic Fine-Grained Parallelization on SMT Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Latency-Critical Applications with AI-Powered\n  Semi-Automatic Fine-Grained Parallelization on SMT Processors"
                },
                "summary": "Latency-critical applications tend to show low utilization of functional\nunits due to frequent cache misses and mispredictions during speculative\nexecution in high-performance superscalar processors. However, due to\nsignificant impact on single-thread performance, Simultaneous Multithreading\n(SMT) technology is rarely used with heavy threads of latency-critical\napplications. In this paper, we explore utilization of SMT technology to\nsupport fine-grained parallelization of latency-critical applications.\nFollowing the advancements in the development of Large Language Models (LLMs),\nwe introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we\nextend AI Coding Agent in Cursor IDE with additional tools connected through\nModel Context Protocol, enabling end-to-end AI Agent for parallelization.\nAdditional connected tools enable LLM-guided hotspot detection, collection of\ndynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance\nsimulation to estimate performance gains. We apply Aira with Relic parallel\nframework for fine-grained task parallelism on SMT cores to parallelize\nlatency-critical benchmarks representing real-world applications used in\nindustry. We show 17% geomean performance gain from parallelization of\nlatency-critical benchmarks using Aira with Relic framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency-critical applications tend to show low utilization of functional\nunits due to frequent cache misses and mispredictions during speculative\nexecution in high-performance superscalar processors. However, due to\nsignificant impact on single-thread performance, Simultaneous Multithreading\n(SMT) technology is rarely used with heavy threads of latency-critical\napplications. In this paper, we explore utilization of SMT technology to\nsupport fine-grained parallelization of latency-critical applications.\nFollowing the advancements in the development of Large Language Models (LLMs),\nwe introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we\nextend AI Coding Agent in Cursor IDE with additional tools connected through\nModel Context Protocol, enabling end-to-end AI Agent for parallelization.\nAdditional connected tools enable LLM-guided hotspot detection, collection of\ndynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance\nsimulation to estimate performance gains. We apply Aira with Relic parallel\nframework for fine-grained task parallelism on SMT cores to parallelize\nlatency-critical benchmarks representing real-world applications used in\nindustry. We show 17% geomean performance gain from parallelization of\nlatency-critical benchmarks using Aira with Relic framework."
                },
                "authors": [
                    {
                        "name": "Denis Los"
                    },
                    {
                        "name": "Igor Petushkov"
                    }
                ],
                "author_detail": {
                    "name": "Igor Petushkov"
                },
                "author": "Igor Petushkov",
                "arxiv_journal_ref": "International Journal of Open Information Technologies, vol. 13,\n  no. 9, pp. 129-134, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10431v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10431v3",
                "updated": "2025-08-31T05:43:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    5,
                    43,
                    55,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-14T08:04:15Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    4,
                    15,
                    3,
                    226,
                    0
                ],
                "title": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches"
                },
                "summary": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE."
                },
                "authors": [
                    {
                        "name": "Chris Cao"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "arxiv_comment": "This version includes updated analysis of RCO Bugs (one additional\n  bug identified). Appendix added with code snippets for bug fixes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10431v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10431v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00625v1",
                "updated": "2025-08-30T22:47:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    22,
                    47,
                    15,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T22:47:15Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    22,
                    47,
                    15,
                    5,
                    242,
                    0
                ],
                "title": "NetGent: Agent-Based Automation of Network Application Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NetGent: Agent-Based Automation of Network Application Workflows"
                },
                "summary": "We present NetGent, an AI-agent framework for automating complex application\nworkflows to generate realistic network traffic datasets. Developing\ngeneralizable ML models for networking requires data collection from network\nenvironments with traffic that results from a diverse set of real-world web\napplications. However, using existing browser automation tools that are\ndiverse, repeatable, realistic, and efficient remains fragile and costly.\nNetGent addresses this challenge by allowing users to specify workflows as\nnatural-language rules that define state-dependent actions. These abstract\nspecifications are compiled into nondeterministic finite automata (NFAs), which\na state synthesis component translates into reusable, executable code. This\ndesign enables deterministic replay, reduces redundant LLM calls through state\ncaching, and adapts quickly when application interfaces change. In experiments,\nNetGent automated more than 50+ workflows spanning video-on-demand streaming,\nlive video streaming, video conferencing, social media, and web scraping,\nproducing realistic traffic traces while remaining robust to UI variability. By\ncombining the flexibility of language-based agents with the reliability of\ncompiled execution, NetGent provides a scalable foundation for generating the\ndiverse, repeatable datasets needed to advance ML in networking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present NetGent, an AI-agent framework for automating complex application\nworkflows to generate realistic network traffic datasets. Developing\ngeneralizable ML models for networking requires data collection from network\nenvironments with traffic that results from a diverse set of real-world web\napplications. However, using existing browser automation tools that are\ndiverse, repeatable, realistic, and efficient remains fragile and costly.\nNetGent addresses this challenge by allowing users to specify workflows as\nnatural-language rules that define state-dependent actions. These abstract\nspecifications are compiled into nondeterministic finite automata (NFAs), which\na state synthesis component translates into reusable, executable code. This\ndesign enables deterministic replay, reduces redundant LLM calls through state\ncaching, and adapts quickly when application interfaces change. In experiments,\nNetGent automated more than 50+ workflows spanning video-on-demand streaming,\nlive video streaming, video conferencing, social media, and web scraping,\nproducing realistic traffic traces while remaining robust to UI variability. By\ncombining the flexibility of language-based agents with the reliability of\ncompiled execution, NetGent provides a scalable foundation for generating the\ndiverse, repeatable datasets needed to advance ML in networking."
                },
                "authors": [
                    {
                        "name": "Jaber Daneshamooz"
                    },
                    {
                        "name": "Eugene Vuong"
                    },
                    {
                        "name": "Laasya Koduru"
                    },
                    {
                        "name": "Sanjay Chandrasekaran"
                    },
                    {
                        "name": "Arpit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Arpit Gupta"
                },
                "author": "Arpit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00579v1",
                "updated": "2025-08-30T18:25:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    18,
                    25,
                    19,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T18:25:19Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    18,
                    25,
                    19,
                    5,
                    242,
                    0
                ],
                "title": "KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for\n  KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for\n  KV Cache"
                },
                "summary": "Transformer-based large language models (LLMs) demonstrate impressive\npotential in various practical applications. However, long context inference\nposes a significant challenge due to the enormous memory requirements of the\nkey-value (KV) cache, which can scale to multiple gigabytes as sequence length\nand batch size increase. In this paper, we present KVComp, a generic and\nefficient KV cache management framework optimized for long-text generation that\nsynergistically works with both latency-critical and throughput-critical\ninference systems. KVComp employs novel lossy compression techniques\nspecifically designed for KV cache data characteristics, featuring careful\nco-design of compression algorithms and system architecture. Our approach\nmaintains compatibility with the growing nature of KV cache while preserving\nhigh computational efficiency. Experimental results show that KVComp achieves\non average 47\\% and up to 83\\% higher memory reduction rate compared to\nexisting methods with little/no model accuracy degradation. Furthermore, KVComp\nachieves extremely high execution throughput, effectively reducing\ndecompression overhead and, in some cases, even accelerating the matrix-vector\nmultiplication operation and outperform cuBLAS-based attention kernels with\nless data movement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) demonstrate impressive\npotential in various practical applications. However, long context inference\nposes a significant challenge due to the enormous memory requirements of the\nkey-value (KV) cache, which can scale to multiple gigabytes as sequence length\nand batch size increase. In this paper, we present KVComp, a generic and\nefficient KV cache management framework optimized for long-text generation that\nsynergistically works with both latency-critical and throughput-critical\ninference systems. KVComp employs novel lossy compression techniques\nspecifically designed for KV cache data characteristics, featuring careful\nco-design of compression algorithms and system architecture. Our approach\nmaintains compatibility with the growing nature of KV cache while preserving\nhigh computational efficiency. Experimental results show that KVComp achieves\non average 47\\% and up to 83\\% higher memory reduction rate compared to\nexisting methods with little/no model accuracy degradation. Furthermore, KVComp\nachieves extremely high execution throughput, effectively reducing\ndecompression overhead and, in some cases, even accelerating the matrix-vector\nmultiplication operation and outperform cuBLAS-based attention kernels with\nless data movement."
                },
                "authors": [
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Taolue Yang"
                    },
                    {
                        "name": "Youyuan Liu"
                    },
                    {
                        "name": "Chengming Zhang"
                    },
                    {
                        "name": "Xubin He"
                    },
                    {
                        "name": "Sian Jin"
                    }
                ],
                "author_detail": {
                    "name": "Sian Jin"
                },
                "author": "Sian Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.13777v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.13777v2",
                "updated": "2025-08-30T14:49:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    14,
                    49,
                    34,
                    5,
                    242,
                    0
                ],
                "published": "2023-10-20T19:22:58Z",
                "published_parsed": [
                    2023,
                    10,
                    20,
                    19,
                    22,
                    58,
                    4,
                    293,
                    0
                ],
                "title": "Discrete and Continuous Caching Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete and Continuous Caching Games"
                },
                "summary": "We investigate a discrete search game called the Multiple Caching Game where\nthe searcher's aim is to find all of a set of $d$ treasures hidden in $n$\nlocations. Allowed queries are sets of locations of size $k$, and the searcher\nwins if in all $d$ queries, at least one treasure is hidden in one of the $k$\npicked locations. P\\'alv\\\"olgyi showed that the value of the game is at most\n$\\frac{k^d}{\\binom{n+d-1}{d}}$, with equality for large enough $n$. We\nconjecture the exact cases of equality. We also investigate variants of the\ngame and show an example where their values are different, answering a question\nof P\\'alv\\\"olgyi.\n  This game is closely related to a continuous variant, Alpern's Caching Game,\nbased on which we define other continous variants of the multiple caching game\nand examine their values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate a discrete search game called the Multiple Caching Game where\nthe searcher's aim is to find all of a set of $d$ treasures hidden in $n$\nlocations. Allowed queries are sets of locations of size $k$, and the searcher\nwins if in all $d$ queries, at least one treasure is hidden in one of the $k$\npicked locations. P\\'alv\\\"olgyi showed that the value of the game is at most\n$\\frac{k^d}{\\binom{n+d-1}{d}}$, with equality for large enough $n$. We\nconjecture the exact cases of equality. We also investigate variants of the\ngame and show an example where their values are different, answering a question\nof P\\'alv\\\"olgyi.\n  This game is closely related to a continuous variant, Alpern's Caching Game,\nbased on which we define other continous variants of the multiple caching game\nand examine their values."
                },
                "authors": [
                    {
                        "name": "Áron Jánosik"
                    },
                    {
                        "name": "Csenge Miklós"
                    },
                    {
                        "name": "Dániel G. Simon"
                    },
                    {
                        "name": "Kristóf Zólomy"
                    }
                ],
                "author_detail": {
                    "name": "Kristóf Zólomy"
                },
                "author": "Kristóf Zólomy",
                "arxiv_doi": "10.1142/S0219198925500057",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1142/S0219198925500057",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.13777v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.13777v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "International Game Theory Review 27 (3), 2025",
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91A05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v3",
                "updated": "2025-08-30T09:35:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    9,
                    35,
                    22,
                    5,
                    242,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "DiffKV: Differentiated Memory Management for Large Language Models with\n  Parallel KV Compaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffKV: Differentiated Memory Management for Large Language Models with\n  Parallel KV Compaction"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable capabilities but face\nsubstantial serving costs due to their high memory demands, with the key-value\n(KV) cache being a primary bottleneck. State-of-the-art KV cache compression\ntechniques, such as quantization and pruning, apply uniform treatment to both\nkeys and values, and discard unimportant tokens entirely, overlooking the\nfine-grained distinctions in the significance of individual KV cache\ncomponents. To address such limitations, we introduce \\textit{DiffKV}, a novel\nframework for efficient KV cache compression that exploits three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. These levels of\ndifferentiation introduce irregular memory usage patterns across different\nrequests and attention heads, posing significant scalability challenges for\nmemory management. To address these challenges, DiffKV proposes an on-GPU\nmemory manager that compacts fragmented free memory list into contiguous\nregions in parallel, effectively translating sparsity in the KV cache into\nperformance gains. We evaluate DiffKV on several mainstream LLMs, including the\nemerging thinking models that generate extended chains of thought. DiffKV is\nable to compress the KV cache by $2.7\\times$ to $5.7\\times$ with near-lossless\naccuracy on complex workloads requiring sophisticated reasoning and\nlong-generation capabilities, and enhances throughput by $1.9\\times$ to\n$5.4\\times$. Source codes of DiffKV are available at\nhttps://github.com/zyqCSL/DiffKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable capabilities but face\nsubstantial serving costs due to their high memory demands, with the key-value\n(KV) cache being a primary bottleneck. State-of-the-art KV cache compression\ntechniques, such as quantization and pruning, apply uniform treatment to both\nkeys and values, and discard unimportant tokens entirely, overlooking the\nfine-grained distinctions in the significance of individual KV cache\ncomponents. To address such limitations, we introduce \\textit{DiffKV}, a novel\nframework for efficient KV cache compression that exploits three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. These levels of\ndifferentiation introduce irregular memory usage patterns across different\nrequests and attention heads, posing significant scalability challenges for\nmemory management. To address these challenges, DiffKV proposes an on-GPU\nmemory manager that compacts fragmented free memory list into contiguous\nregions in parallel, effectively translating sparsity in the KV cache into\nperformance gains. We evaluate DiffKV on several mainstream LLMs, including the\nemerging thinking models that generate extended chains of thought. DiffKV is\nable to compress the KV cache by $2.7\\times$ to $5.7\\times$ with near-lossless\naccuracy on complex workloads requiring sophisticated reasoning and\nlong-generation capabilities, and enhances throughput by $1.9\\times$ to\n$5.4\\times$. Source codes of DiffKV are available at\nhttps://github.com/zyqCSL/DiffKV."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "SOSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00419v1",
                "updated": "2025-08-30T08:57:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    8,
                    57,
                    53,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T08:57:53Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    8,
                    57,
                    53,
                    5,
                    242,
                    0
                ],
                "title": "LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging\n  and KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging\n  and KV Cache Compression"
                },
                "summary": "In this paper, we introduce LightVLM, a simple but effective method that can\nbe seamlessly deployed upon existing Vision-Language Models (VLMs) to greatly\naccelerate the inference process in a training-free manner. We divide the\ninference procedure of VLMs into two stages, i.e., encoding and decoding, and\npropose to simultaneously accelerate VLMs in both stages to largely improve\nmodel efficiency. During encoding, we propose pyramid token merging to reduce\ntokens of different LLM layers in a hierarchical manner by finally only keeping\na few dominant tokens to achieve high efficiency. During decoding, aimed at\nreducing the high latency of outputting long sequences, we propose KV Cache\ncompression to remove unnecessary caches to increase the network throughput.\nExperimental results show that LightVLM successfully retains 100% performance\nwhen only preserving 35% image tokens, and maintains around 98% performance\nwhen keeping only 3% image tokens. LightVLM could 2.02$\\times$ the network\nthroughput and reduce the prefilling time by 3.65$\\times$. LightVLM also makes\nlarge VLMs faster again by enabling a heavy model (e.g., InternVL2.5 26B) to\ninfer faster than significantly smaller models (e.g., InternVL2.5 8B),\nhopefully facilitating the real-world deployment. When generating long text\nsequences (e.g., 4096 tokens), LightVLM could reduce the inference time by\n3.21$\\times$, largely outperforming existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce LightVLM, a simple but effective method that can\nbe seamlessly deployed upon existing Vision-Language Models (VLMs) to greatly\naccelerate the inference process in a training-free manner. We divide the\ninference procedure of VLMs into two stages, i.e., encoding and decoding, and\npropose to simultaneously accelerate VLMs in both stages to largely improve\nmodel efficiency. During encoding, we propose pyramid token merging to reduce\ntokens of different LLM layers in a hierarchical manner by finally only keeping\na few dominant tokens to achieve high efficiency. During decoding, aimed at\nreducing the high latency of outputting long sequences, we propose KV Cache\ncompression to remove unnecessary caches to increase the network throughput.\nExperimental results show that LightVLM successfully retains 100% performance\nwhen only preserving 35% image tokens, and maintains around 98% performance\nwhen keeping only 3% image tokens. LightVLM could 2.02$\\times$ the network\nthroughput and reduce the prefilling time by 3.65$\\times$. LightVLM also makes\nlarge VLMs faster again by enabling a heavy model (e.g., InternVL2.5 26B) to\ninfer faster than significantly smaller models (e.g., InternVL2.5 8B),\nhopefully facilitating the real-world deployment. When generating long text\nsequences (e.g., 4096 tokens), LightVLM could reduce the inference time by\n3.21$\\times$, largely outperforming existing methods."
                },
                "authors": [
                    {
                        "name": "Lianyu Hu"
                    },
                    {
                        "name": "Fanhua Shang"
                    },
                    {
                        "name": "Wei Feng"
                    },
                    {
                        "name": "Liang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wan"
                },
                "author": "Liang Wan",
                "arxiv_comment": "EMNLP2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00388v1",
                "updated": "2025-08-30T06:56:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    6,
                    56,
                    28,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T06:56:28Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    6,
                    56,
                    28,
                    5,
                    242,
                    0
                ],
                "title": "GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV\n  Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV\n  Cache Eviction"
                },
                "summary": "Efficient Key-Value (KV) cache management is essential for processing long\ntext sequences in large language models (LLMs), where memory constraints often\nlimit performance. Conventional KV eviction strategies, such as top-k selection\nbased on attention scores, depend on static heuristics that fail to capture the\nevolving implicit dependencies among tokens during inference. To overcome this,\nwe propose GraphKV, a graph-based framework that redefines token selection for\nKV cache compression. In GraphKV, tokens are modeled as nodes with importance\nscores, and edges represent their similarity relationships. Through a\ndecay-signal-propagation mechanism, token importance is dynamically updated by\npropagating information across the graph, enabling adaptive retention of the\nmost contextually significant tokens. GraphKV can be seamlessly utilized in\nexisting KV cache eviction methods such as SnapKV and PyramidKV in a\nplug-and-play manner. Codes will be released on Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Key-Value (KV) cache management is essential for processing long\ntext sequences in large language models (LLMs), where memory constraints often\nlimit performance. Conventional KV eviction strategies, such as top-k selection\nbased on attention scores, depend on static heuristics that fail to capture the\nevolving implicit dependencies among tokens during inference. To overcome this,\nwe propose GraphKV, a graph-based framework that redefines token selection for\nKV cache compression. In GraphKV, tokens are modeled as nodes with importance\nscores, and edges represent their similarity relationships. Through a\ndecay-signal-propagation mechanism, token importance is dynamically updated by\npropagating information across the graph, enabling adaptive retention of the\nmost contextually significant tokens. GraphKV can be seamlessly utilized in\nexisting KV cache eviction methods such as SnapKV and PyramidKV in a\nplug-and-play manner. Codes will be released on Github."
                },
                "authors": [
                    {
                        "name": "Xuelin Li"
                    },
                    {
                        "name": "Xiangqi Jin"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11435v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11435v2",
                "updated": "2025-08-29T20:39:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    20,
                    39,
                    21,
                    4,
                    241,
                    0
                ],
                "published": "2025-04-15T17:51:39Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    51,
                    39,
                    1,
                    105,
                    0
                ],
                "title": "Robust Containment Queries over Collections of Trimmed NURBS Surfaces\n  via Generalized Winding Numbers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Containment Queries over Collections of Trimmed NURBS Surfaces\n  via Generalized Winding Numbers"
                },
                "summary": "We propose a containment query that is robust to the watertightness of\nregions bound by trimmed NURBS surfaces, as this property is difficult to\nguarantee for in-the-wild CAD models. Containment is determined through the\ngeneralized winding number (GWN), a mathematical construction that is\nindifferent to the arrangement of surfaces in the shape. Applying contemporary\ntechniques for the 3D GWN to trimmed NURBS surfaces requires some form of\ngeometric discretization, introducing computational inefficiency to the\nalgorithm and even risking containment misclassifications near the surface. In\ncontrast, our proposed method uses a novel reformulation of the relevant\nsurface integral based on Stokes' theorem, which operates on the boundary and\ntrimming curves as provided through rapidly converging adaptive quadrature.\nBatches of queries are further accelerated by memoizing (i.e.\\ caching and\nreusing) quadrature node positions and tangents as they are evaluated. We\ndemonstrate that our GWN method is robust to complex trimming geometry in a CAD\nmodel, and is accurate up to arbitrary precision at arbitrary distances from\nthe surface. The derived containment query is therefore robust to model\nnon-watertightness while respecting all curved features of the input shape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a containment query that is robust to the watertightness of\nregions bound by trimmed NURBS surfaces, as this property is difficult to\nguarantee for in-the-wild CAD models. Containment is determined through the\ngeneralized winding number (GWN), a mathematical construction that is\nindifferent to the arrangement of surfaces in the shape. Applying contemporary\ntechniques for the 3D GWN to trimmed NURBS surfaces requires some form of\ngeometric discretization, introducing computational inefficiency to the\nalgorithm and even risking containment misclassifications near the surface. In\ncontrast, our proposed method uses a novel reformulation of the relevant\nsurface integral based on Stokes' theorem, which operates on the boundary and\ntrimming curves as provided through rapidly converging adaptive quadrature.\nBatches of queries are further accelerated by memoizing (i.e.\\ caching and\nreusing) quadrature node positions and tangents as they are evaluated. We\ndemonstrate that our GWN method is robust to complex trimming geometry in a CAD\nmodel, and is accurate up to arbitrary precision at arbitrary distances from\nthe surface. The derived containment query is therefore robust to model\nnon-watertightness while respecting all curved features of the input shape."
                },
                "authors": [
                    {
                        "name": "Jacob Spainhour"
                    },
                    {
                        "name": "Kenneth Weiss"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth Weiss"
                },
                "author": "Kenneth Weiss",
                "arxiv_comment": "18 Pages, 16 Figures, 1 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11435v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11435v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68U05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.3.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00202v1",
                "updated": "2025-08-29T19:23:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    23,
                    35,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T19:23:35Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    23,
                    35,
                    4,
                    241,
                    0
                ],
                "title": "From TLinFormer to TConstFormer: The Leap to Constant-Time Transformer\n  Attention: Achieving O(1) Computation and O(1) KV Cache during Autoregressive\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From TLinFormer to TConstFormer: The Leap to Constant-Time Transformer\n  Attention: Achieving O(1) Computation and O(1) KV Cache during Autoregressive\n  Inference"
                },
                "summary": "Although the Transformer has become the cornerstone of modern AI, its\nautoregressive inference suffers from a linearly growing KV Cache and a\ncomputational complexity of O(N^2 d), severely hindering its ability to process\nultra-long sequences. To overcome this limitation, this paper introduces the\nTConstFormer architecture, building upon our previous work, TLinFormer.\nTConstFormer employs an innovative periodic state update mechanism to achieve a\ntruly constant-size O(1) KV Cache. The computational complexity of this\nmechanism is also O(1) in an amortized sense: it performs purely constant-time\ncomputations for $k-1$ consecutive steps (e.g., $k=256$) and executes a single\nlinear-time global information synchronization only on the $k$-th step.\nTheoretical calculations and experimental results demonstrate that TConstFormer\nexhibits an overwhelming advantage over baseline models in terms of speed,\nmemory efficiency, and overall performance on long-text inference tasks. This\nbreakthrough paves the way for efficient and robust streaming language model\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although the Transformer has become the cornerstone of modern AI, its\nautoregressive inference suffers from a linearly growing KV Cache and a\ncomputational complexity of O(N^2 d), severely hindering its ability to process\nultra-long sequences. To overcome this limitation, this paper introduces the\nTConstFormer architecture, building upon our previous work, TLinFormer.\nTConstFormer employs an innovative periodic state update mechanism to achieve a\ntruly constant-size O(1) KV Cache. The computational complexity of this\nmechanism is also O(1) in an amortized sense: it performs purely constant-time\ncomputations for $k-1$ consecutive steps (e.g., $k=256$) and executes a single\nlinear-time global information synchronization only on the $k$-th step.\nTheoretical calculations and experimental results demonstrate that TConstFormer\nexhibits an overwhelming advantage over baseline models in terms of speed,\nmemory efficiency, and overall performance on long-text inference tasks. This\nbreakthrough paves the way for efficient and robust streaming language model\napplications."
                },
                "authors": [
                    {
                        "name": "Zhongpan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongpan Tang"
                },
                "author": "Zhongpan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00195v1",
                "updated": "2025-08-29T19:12:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    12,
                    4,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T19:12:04Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    12,
                    4,
                    4,
                    241,
                    0
                ],
                "title": "Democratizing Agentic AI with Fast Test-Time Scaling on the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Democratizing Agentic AI with Fast Test-Time Scaling on the Edge"
                },
                "summary": "Deploying agentic AI on edge devices is crucial for privacy and\nresponsiveness, but memory constraints typically relegate these systems to\nsmaller Large Language Models (LLMs) with inferior reasoning capabilities.\nTest-Time Scaling (TTS) can bridge this reasoning gap by dedicating more\ncompute during inference, but existing methods incur prohibitive overhead on\nedge hardware. To overcome this, we introduce FlashTTS, a serving system that\nmakes TTS practical for memory-constrained LLM reasoning. FlashTTS introduces\nthree synergistic optimizations: (i) Speculative Beam Extension to mitigate\nsystem stragglers from irregular reasoning paths; (ii) Asymmetric Multi-Model\nMemory Allocation to dynamically balance memory between generation and\nverification; and (iii) Dynamic Prefix-Aware Scheduling to maximize KV-cache\nreuse. Built as a plug-and-play library for vLLM, FlashTTS enables edge LLMs on\na single consumer GPU (24 GB) to match the accuracy and latency of large cloud\nmodels. Our evaluation demonstrates that FlashTTS achieves an average 2.2x\nhigher goodput and reduces latency by 38%-68% compared to a vLLM baseline,\npaving the way for democratized, high-performance agentic AI on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying agentic AI on edge devices is crucial for privacy and\nresponsiveness, but memory constraints typically relegate these systems to\nsmaller Large Language Models (LLMs) with inferior reasoning capabilities.\nTest-Time Scaling (TTS) can bridge this reasoning gap by dedicating more\ncompute during inference, but existing methods incur prohibitive overhead on\nedge hardware. To overcome this, we introduce FlashTTS, a serving system that\nmakes TTS practical for memory-constrained LLM reasoning. FlashTTS introduces\nthree synergistic optimizations: (i) Speculative Beam Extension to mitigate\nsystem stragglers from irregular reasoning paths; (ii) Asymmetric Multi-Model\nMemory Allocation to dynamically balance memory between generation and\nverification; and (iii) Dynamic Prefix-Aware Scheduling to maximize KV-cache\nreuse. Built as a plug-and-play library for vLLM, FlashTTS enables edge LLMs on\na single consumer GPU (24 GB) to match the accuracy and latency of large cloud\nmodels. Our evaluation demonstrates that FlashTTS achieves an average 2.2x\nhigher goodput and reduces latency by 38%-68% compared to a vLLM baseline,\npaving the way for democratized, high-performance agentic AI on edge devices."
                },
                "authors": [
                    {
                        "name": "Hao Mark Chen"
                    },
                    {
                        "name": "Zhiwen Mo"
                    },
                    {
                        "name": "Guanxi Lu"
                    },
                    {
                        "name": "Shuang Liang"
                    },
                    {
                        "name": "Lingxiao Ma"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "author": "Hongxiang Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16217v2",
                "updated": "2025-08-29T18:45:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    18,
                    45,
                    22,
                    4,
                    241,
                    0
                ],
                "published": "2025-07-22T04:21:03Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    21,
                    3,
                    1,
                    203,
                    0
                ],
                "title": "Towards Compute-Optimal Many-Shot In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Compute-Optimal Many-Shot In-Context Learning"
                },
                "summary": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL."
                },
                "authors": [
                    {
                        "name": "Shahriar Golchin"
                    },
                    {
                        "name": "Yanfei Chen"
                    },
                    {
                        "name": "Rujun Han"
                    },
                    {
                        "name": "Manan Gandhi"
                    },
                    {
                        "name": "Tianli Yu"
                    },
                    {
                        "name": "Swaroop Mishra"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Chen-Yu Lee"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "arxiv_comment": "Final version; accepted at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05930v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05930v2",
                "updated": "2025-08-29T09:58:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    58,
                    17,
                    4,
                    241,
                    0
                ],
                "published": "2025-06-06T09:55:59Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    9,
                    55,
                    59,
                    4,
                    157,
                    0
                ],
                "title": "Neural Visibility Cache for Real-Time Light Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Visibility Cache for Real-Time Light Sampling"
                },
                "summary": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR)."
                },
                "authors": [
                    {
                        "name": "Jakub Bokšanský"
                    },
                    {
                        "name": "Daniel Meister"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Meister"
                },
                "author": "Daniel Meister",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05930v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05930v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15683v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15683v2",
                "updated": "2025-08-29T07:40:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    7,
                    40,
                    34,
                    4,
                    241,
                    0
                ],
                "published": "2025-05-21T15:58:08Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "title": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting\n  Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting\n  Framework for Large Language Models"
                },
                "summary": "Private data holds promise for improving LLMs due to its high quality, but\nits scattered distribution across data silos and the high computational demands\nof LLMs limit their deployment in federated environments. To address this, the\ntransformer-based federated split models are proposed, which offload most model\nparameters to the server (or distributed clients) while retaining only a small\nportion on the client to ensure data privacy. Despite this design, they still\nface three challenges: 1) Peer-to-peer key encryption struggles to secure\ntransmitted vectors effectively; 2) The auto-regressive nature of LLMs means\nthat federated split learning can only train and infer sequentially, causing\nhigh communication overhead; 3) Fixed partition points lack adaptability to\ndownstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure,\nEfficient, and Adaptive Federated splitting framework based on LLaMA2. First,\nwe inject Gaussian noise into forward-pass hidden states to enable secure\nend-to-end vector transmission. Second, we employ attention-mask compression\nand KV cache collaboration to reduce communication costs, accelerating training\nand inference. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements. Experiments on\nnatural language understanding, summarization, and conversational QA tasks show\nthat FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and\nachieves up to 8x speedups in training and inference. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FedSEA-LLaMA in security and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private data holds promise for improving LLMs due to its high quality, but\nits scattered distribution across data silos and the high computational demands\nof LLMs limit their deployment in federated environments. To address this, the\ntransformer-based federated split models are proposed, which offload most model\nparameters to the server (or distributed clients) while retaining only a small\nportion on the client to ensure data privacy. Despite this design, they still\nface three challenges: 1) Peer-to-peer key encryption struggles to secure\ntransmitted vectors effectively; 2) The auto-regressive nature of LLMs means\nthat federated split learning can only train and infer sequentially, causing\nhigh communication overhead; 3) Fixed partition points lack adaptability to\ndownstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure,\nEfficient, and Adaptive Federated splitting framework based on LLaMA2. First,\nwe inject Gaussian noise into forward-pass hidden states to enable secure\nend-to-end vector transmission. Second, we employ attention-mask compression\nand KV cache collaboration to reduce communication costs, accelerating training\nand inference. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements. Experiments on\nnatural language understanding, summarization, and conversational QA tasks show\nthat FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and\nachieves up to 8x speedups in training and inference. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FedSEA-LLaMA in security and adaptability."
                },
                "authors": [
                    {
                        "name": "Zishuai Zhang"
                    },
                    {
                        "name": "Hainan zhang"
                    },
                    {
                        "name": "Weihua Li"
                    },
                    {
                        "name": "Qinnan zhang"
                    },
                    {
                        "name": "jin Dong"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15683v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15683v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.15224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15224v1",
                "updated": "2025-09-18T17:59:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    59,
                    51,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:59:51Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    59,
                    51,
                    3,
                    261,
                    0
                ],
                "title": "Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based\n  Monocular Depth Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based\n  Monocular Depth Estimation"
                },
                "summary": "Event cameras capture sparse, high-temporal-resolution visual information,\nmaking them particularly suitable for challenging environments with high-speed\nmotion and strongly varying lighting conditions. However, the lack of large\ndatasets with dense ground-truth depth annotations hinders learning-based\nmonocular depth estimation from event data. To address this limitation, we\npropose a cross-modal distillation paradigm to generate dense proxy labels\nleveraging a Vision Foundation Model (VFM). Our strategy requires an event\nstream spatially aligned with RGB frames, a simple setup even available\noff-the-shelf, and exploits the robustness of large-scale VFMs. Additionally,\nwe propose to adapt VFMs, either a vanilla one like Depth Anything v2 (DAv2),\nor deriving from it a novel recurrent architecture to infer depth from\nmonocular event cameras. We evaluate our approach with synthetic and real-world\ndatasets, demonstrating that i) our cross-modal paradigm achieves competitive\nperformance compared to fully supervised methods without requiring expensive\ndepth annotations, and ii) our VFM-based models achieve state-of-the-art\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event cameras capture sparse, high-temporal-resolution visual information,\nmaking them particularly suitable for challenging environments with high-speed\nmotion and strongly varying lighting conditions. However, the lack of large\ndatasets with dense ground-truth depth annotations hinders learning-based\nmonocular depth estimation from event data. To address this limitation, we\npropose a cross-modal distillation paradigm to generate dense proxy labels\nleveraging a Vision Foundation Model (VFM). Our strategy requires an event\nstream spatially aligned with RGB frames, a simple setup even available\noff-the-shelf, and exploits the robustness of large-scale VFMs. Additionally,\nwe propose to adapt VFMs, either a vanilla one like Depth Anything v2 (DAv2),\nor deriving from it a novel recurrent architecture to infer depth from\nmonocular event cameras. We evaluate our approach with synthetic and real-world\ndatasets, demonstrating that i) our cross-modal paradigm achieves competitive\nperformance compared to fully supervised methods without requiring expensive\ndepth annotations, and ii) our VFM-based models achieve state-of-the-art\nperformance."
                },
                "authors": [
                    {
                        "name": "Luca Bartolomei"
                    },
                    {
                        "name": "Enrico Mannocci"
                    },
                    {
                        "name": "Fabio Tosi"
                    },
                    {
                        "name": "Matteo Poggi"
                    },
                    {
                        "name": "Stefano Mattoccia"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Mattoccia"
                },
                "author": "Stefano Mattoccia",
                "arxiv_comment": "ICCV 2025. Code: https://github.com/bartn8/depthanyevent/ Project\n  Page: https://bartn8.github.io/depthanyevent/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15223v1",
                "updated": "2025-09-18T17:59:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    59,
                    30,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:59:30Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    59,
                    30,
                    3,
                    261,
                    0
                ],
                "title": "Parameter sensitivity of cosmic pairwise velocities in the non-linear\n  regime of structure formation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter sensitivity of cosmic pairwise velocities in the non-linear\n  regime of structure formation"
                },
                "summary": "The peculiar velocities of dark matter tracers drive the growth of cosmic\nstructures, providing a sensitive test of cosmological models and strengthening\nconstraints on the nature of dark energy. In this work, we investigate the mean\npairwise velocities, $v_{12}$, of dark matter tracers as a cosmological probe\nin the non-linear regime of cosmic structure formation. Using N-body dark\nmatter-only simulations, we measure $v_{12}$ for pair separations up to 50\n$h^{-1}$Mpc and model it by solving the pair conservation equation for a\nself-gravitating particle system, along with various prescriptions of the\nnonlinear matter power spectrum. We quantified the sensitivity of $v_{12}$ to\nvariations in key cosmological parameters such as $\\Omega_{\\mathrm{m}}$,\n$\\sigma_8$, $h$, $M_\\nu$, and $w$. Our parameter inference analysis using MCMC\nshows sub-11% agreement with simulation data, with notable degeneracies,\nparticularly between $\\Omega_\\mathrm{m}$ and $\\sigma_8$. We further compute the\nstable clustering crossing scale across redshifts $z=0$, $0.5$, and $1$,\nassessing its dependence on cosmology. Among the tested power spectrum modeling\napproaches, we find that the CSSTEmu emulator provides the most accurate\npredictions, with deviations below 5% for $r > 10$ $h^{-1}$Mpc at $z=0.5$. Our\nresults are validated using independent simulation suites, demonstrating that\nour framework offers a robust method for extracting cosmological constraints\nfrom upcoming peculiar velocity data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The peculiar velocities of dark matter tracers drive the growth of cosmic\nstructures, providing a sensitive test of cosmological models and strengthening\nconstraints on the nature of dark energy. In this work, we investigate the mean\npairwise velocities, $v_{12}$, of dark matter tracers as a cosmological probe\nin the non-linear regime of cosmic structure formation. Using N-body dark\nmatter-only simulations, we measure $v_{12}$ for pair separations up to 50\n$h^{-1}$Mpc and model it by solving the pair conservation equation for a\nself-gravitating particle system, along with various prescriptions of the\nnonlinear matter power spectrum. We quantified the sensitivity of $v_{12}$ to\nvariations in key cosmological parameters such as $\\Omega_{\\mathrm{m}}$,\n$\\sigma_8$, $h$, $M_\\nu$, and $w$. Our parameter inference analysis using MCMC\nshows sub-11% agreement with simulation data, with notable degeneracies,\nparticularly between $\\Omega_\\mathrm{m}$ and $\\sigma_8$. We further compute the\nstable clustering crossing scale across redshifts $z=0$, $0.5$, and $1$,\nassessing its dependence on cosmology. Among the tested power spectrum modeling\napproaches, we find that the CSSTEmu emulator provides the most accurate\npredictions, with deviations below 5% for $r > 10$ $h^{-1}$Mpc at $z=0.5$. Our\nresults are validated using independent simulation suites, demonstrating that\nour framework offers a robust method for extracting cosmological constraints\nfrom upcoming peculiar velocity data."
                },
                "authors": [
                    {
                        "name": "Jorge Enrique García-Farieta"
                    },
                    {
                        "name": "Héctor J. Hortúa"
                    }
                ],
                "author_detail": {
                    "name": "Héctor J. Hortúa"
                },
                "author": "Héctor J. Hortúa",
                "arxiv_comment": "16 pages, 8 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15218v1",
                "updated": "2025-09-18T17:59:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    59,
                    16,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:59:16Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    59,
                    16,
                    3,
                    261,
                    0
                ],
                "title": "LNE-Blocking: An Efficient Framework for Contamination Mitigation\n  Evaluation on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LNE-Blocking: An Efficient Framework for Contamination Mitigation\n  Evaluation on Large Language Models"
                },
                "summary": "The problem of data contamination is now almost inevitable during the\ndevelopment of large language models (LLMs), with the training data commonly\nintegrating those evaluation benchmarks even unintentionally. This problem\nsubsequently makes it hard to benchmark LLMs fairly. Instead of constructing\ncontamination-free datasets (quite hard), we propose a novel framework,\n\\textbf{LNE-Blocking}, to restore model performance prior to contamination on\npotentially leaked datasets. Our framework consists of two components:\ncontamination detection and disruption operation. For the prompt, the framework\nfirst uses the contamination detection method, \\textbf{LNE}, to assess the\nextent of contamination in the model. Based on this, it adjusts the intensity\nof the disruption operation, \\textbf{Blocking}, to elicit non-memorized\nresponses from the model. Our framework is the first to efficiently restore the\nmodel's greedy decoding performance. This comes with a strong performance on\nmultiple datasets with potential leakage risks, and it consistently achieves\nstable recovery results across different models and varying levels of data\ncontamination. We release the code at https://github.com/RuijieH/LNE-Blocking\nto facilitate research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The problem of data contamination is now almost inevitable during the\ndevelopment of large language models (LLMs), with the training data commonly\nintegrating those evaluation benchmarks even unintentionally. This problem\nsubsequently makes it hard to benchmark LLMs fairly. Instead of constructing\ncontamination-free datasets (quite hard), we propose a novel framework,\n\\textbf{LNE-Blocking}, to restore model performance prior to contamination on\npotentially leaked datasets. Our framework consists of two components:\ncontamination detection and disruption operation. For the prompt, the framework\nfirst uses the contamination detection method, \\textbf{LNE}, to assess the\nextent of contamination in the model. Based on this, it adjusts the intensity\nof the disruption operation, \\textbf{Blocking}, to elicit non-memorized\nresponses from the model. Our framework is the first to efficiently restore the\nmodel's greedy decoding performance. This comes with a strong performance on\nmultiple datasets with potential leakage risks, and it consistently achieves\nstable recovery results across different models and varying levels of data\ncontamination. We release the code at https://github.com/RuijieH/LNE-Blocking\nto facilitate research."
                },
                "authors": [
                    {
                        "name": "Ruijie Hou"
                    },
                    {
                        "name": "Yueyang Jiao"
                    },
                    {
                        "name": "Hanxu Hu"
                    },
                    {
                        "name": "Yingming Li"
                    },
                    {
                        "name": "Wai Lam"
                    },
                    {
                        "name": "Huajian Zhang"
                    },
                    {
                        "name": "Hongyuan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Hongyuan Lu"
                },
                "author": "Hongyuan Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15216v1",
                "updated": "2025-09-18T17:59:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    59,
                    5,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:59:05Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    59,
                    5,
                    3,
                    261,
                    0
                ],
                "title": "Assessing Historical Structural Oppression Worldwide via Rule-Guided\n  Prompting of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Historical Structural Oppression Worldwide via Rule-Guided\n  Prompting of Large Language Models"
                },
                "summary": "Traditional efforts to measure historical structural oppression struggle with\ncross-national validity due to the unique, locally specified histories of\nexclusion, colonization, and social status in each country, and often have\nrelied on structured indices that privilege material resources while\noverlooking lived, identity-based exclusion. We introduce a novel framework for\noppression measurement that leverages Large Language Models (LLMs) to generate\ncontext-sensitive scores of lived historical disadvantage across diverse\ngeopolitical settings. Using unstructured self-identified ethnicity utterances\nfrom a multilingual COVID-19 global study, we design rule-guided prompting\nstrategies that encourage models to produce interpretable, theoretically\ngrounded estimations of oppression. We systematically evaluate these strategies\nacross multiple state-of-the-art LLMs. Our results demonstrate that LLMs, when\nguided by explicit rules, can capture nuanced forms of identity-based\nhistorical oppression within nations. This approach provides a complementary\nmeasurement tool that highlights dimensions of systemic exclusion, offering a\nscalable, cross-cultural lens for understanding how oppression manifests in\ndata-driven research and public health contexts. To support reproducible\nevaluation, we release an open-sourced benchmark dataset for assessing LLMs on\noppression measurement\n(https://github.com/chattergpt/llm-oppression-benchmark).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional efforts to measure historical structural oppression struggle with\ncross-national validity due to the unique, locally specified histories of\nexclusion, colonization, and social status in each country, and often have\nrelied on structured indices that privilege material resources while\noverlooking lived, identity-based exclusion. We introduce a novel framework for\noppression measurement that leverages Large Language Models (LLMs) to generate\ncontext-sensitive scores of lived historical disadvantage across diverse\ngeopolitical settings. Using unstructured self-identified ethnicity utterances\nfrom a multilingual COVID-19 global study, we design rule-guided prompting\nstrategies that encourage models to produce interpretable, theoretically\ngrounded estimations of oppression. We systematically evaluate these strategies\nacross multiple state-of-the-art LLMs. Our results demonstrate that LLMs, when\nguided by explicit rules, can capture nuanced forms of identity-based\nhistorical oppression within nations. This approach provides a complementary\nmeasurement tool that highlights dimensions of systemic exclusion, offering a\nscalable, cross-cultural lens for understanding how oppression manifests in\ndata-driven research and public health contexts. To support reproducible\nevaluation, we release an open-sourced benchmark dataset for assessing LLMs on\noppression measurement\n(https://github.com/chattergpt/llm-oppression-benchmark)."
                },
                "authors": [
                    {
                        "name": "Sreejato Chatterjee"
                    },
                    {
                        "name": "Linh Tran"
                    },
                    {
                        "name": "Quoc Duy Nguyen"
                    },
                    {
                        "name": "Roni Kirson"
                    },
                    {
                        "name": "Drue Hamlin"
                    },
                    {
                        "name": "Harvest Aquino"
                    },
                    {
                        "name": "Hanjia Lyu"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Timothy Dye"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Dye"
                },
                "author": "Timothy Dye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15213v1",
                "updated": "2025-09-18T17:58:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    58,
                    15,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:58:15Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    58,
                    15,
                    3,
                    261,
                    0
                ],
                "title": "Evil Vizier: Vulnerabilities of LLM-Integrated XR Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evil Vizier: Vulnerabilities of LLM-Integrated XR Systems"
                },
                "summary": "Extended reality (XR) applications increasingly integrate Large Language\nModels (LLMs) to enhance user experience, scene understanding, and even\ngenerate executable XR content, and are often called \"AI glasses\". Despite\nthese potential benefits, the integrated XR-LLM pipeline makes XR applications\nvulnerable to new forms of attacks. In this paper, we analyze LLM-Integated XR\nsystems in the literature and in practice and categorize them along different\ndimensions from a systems perspective. Building on this categorization, we\nidentify a common threat model and demonstrate a series of proof-of-concept\nattacks on multiple XR platforms that employ various LLM models (Meta Quest 3,\nMeta Ray-Ban, Android, and Microsoft HoloLens 2 running Llama and GPT models).\nAlthough these platforms each implement LLM integration differently, they share\nvulnerabilities where an attacker can modify the public context surrounding a\nlegitimate LLM query, resulting in erroneous visual or auditory feedback to\nusers, thus compromising their safety or privacy, sowing confusion, or other\nharmful effects. To defend against these threats, we discuss mitigation\nstrategies and best practices for developers, including an initial defense\nprototype, and call on the community to develop new protection mechanisms to\nmitigate these risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extended reality (XR) applications increasingly integrate Large Language\nModels (LLMs) to enhance user experience, scene understanding, and even\ngenerate executable XR content, and are often called \"AI glasses\". Despite\nthese potential benefits, the integrated XR-LLM pipeline makes XR applications\nvulnerable to new forms of attacks. In this paper, we analyze LLM-Integated XR\nsystems in the literature and in practice and categorize them along different\ndimensions from a systems perspective. Building on this categorization, we\nidentify a common threat model and demonstrate a series of proof-of-concept\nattacks on multiple XR platforms that employ various LLM models (Meta Quest 3,\nMeta Ray-Ban, Android, and Microsoft HoloLens 2 running Llama and GPT models).\nAlthough these platforms each implement LLM integration differently, they share\nvulnerabilities where an attacker can modify the public context surrounding a\nlegitimate LLM query, resulting in erroneous visual or auditory feedback to\nusers, thus compromising their safety or privacy, sowing confusion, or other\nharmful effects. To defend against these threats, we discuss mitigation\nstrategies and best practices for developers, including an initial defense\nprototype, and call on the community to develop new protection mechanisms to\nmitigate these risks."
                },
                "authors": [
                    {
                        "name": "Yicheng Zhang"
                    },
                    {
                        "name": "Zijian Huang"
                    },
                    {
                        "name": "Sophie Chen"
                    },
                    {
                        "name": "Erfan Shayegani"
                    },
                    {
                        "name": "Jiasi Chen"
                    },
                    {
                        "name": "Nael Abu-Ghazaleh"
                    }
                ],
                "author_detail": {
                    "name": "Nael Abu-Ghazaleh"
                },
                "author": "Nael Abu-Ghazaleh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15207v1",
                "updated": "2025-09-18T17:56:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    56,
                    36,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:56:36Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    56,
                    36,
                    3,
                    261,
                    0
                ],
                "title": "FlowRL: Matching Reward Distributions for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowRL: Matching Reward Distributions for LLM Reasoning"
                },
                "summary": "We propose FlowRL: matching the full reward distribution via flow balancing\ninstead of maximizing rewards in large language model (LLM) reinforcement\nlearning (RL). Recent advanced reasoning models adopt reward-maximizing methods\n(\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while\nneglecting less frequent but valid reasoning paths, thus reducing diversity. In\ncontrast, we transform scalar rewards into a normalized target distribution\nusing a learnable partition function, and then minimize the reverse KL\ndivergence between the policy and the target distribution. We implement this\nidea as a flow-balanced optimization method that promotes diverse exploration\nand generalizable reasoning trajectories. We conduct experiments on math and\ncode reasoning tasks: FlowRL achieves a significant average improvement of\n$10.0\\%$ over GRPO and $5.1\\%$ over PPO on math benchmarks, and performs\nconsistently better on code reasoning tasks. These results highlight reward\ndistribution-matching as a key step toward efficient exploration and diverse\nreasoning in LLM reinforcement learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose FlowRL: matching the full reward distribution via flow balancing\ninstead of maximizing rewards in large language model (LLM) reinforcement\nlearning (RL). Recent advanced reasoning models adopt reward-maximizing methods\n(\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while\nneglecting less frequent but valid reasoning paths, thus reducing diversity. In\ncontrast, we transform scalar rewards into a normalized target distribution\nusing a learnable partition function, and then minimize the reverse KL\ndivergence between the policy and the target distribution. We implement this\nidea as a flow-balanced optimization method that promotes diverse exploration\nand generalizable reasoning trajectories. We conduct experiments on math and\ncode reasoning tasks: FlowRL achieves a significant average improvement of\n$10.0\\%$ over GRPO and $5.1\\%$ over PPO on math benchmarks, and performs\nconsistently better on code reasoning tasks. These results highlight reward\ndistribution-matching as a key step toward efficient exploration and diverse\nreasoning in LLM reinforcement learning."
                },
                "authors": [
                    {
                        "name": "Xuekai Zhu"
                    },
                    {
                        "name": "Daixuan Cheng"
                    },
                    {
                        "name": "Dinghuai Zhang"
                    },
                    {
                        "name": "Hengli Li"
                    },
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Che Jiang"
                    },
                    {
                        "name": "Youbang Sun"
                    },
                    {
                        "name": "Ermo Hua"
                    },
                    {
                        "name": "Yuxin Zuo"
                    },
                    {
                        "name": "Xingtai Lv"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Lin Chen"
                    },
                    {
                        "name": "Fanghao Shao"
                    },
                    {
                        "name": "Bo Xue"
                    },
                    {
                        "name": "Yunchong Song"
                    },
                    {
                        "name": "Zhenjie Yang"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Bowen Zhou"
                    },
                    {
                        "name": "Hongyuan Mei"
                    },
                    {
                        "name": "Zhouhan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouhan Lin"
                },
                "author": "Zhouhan Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15202v1",
                "updated": "2025-09-18T17:54:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    54,
                    31,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:54:31Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    54,
                    31,
                    3,
                    261,
                    0
                ],
                "title": "Beyond Surface Alignment: Rebuilding LLMs Safety Mechanism via\n  Probabilistically Ablating Refusal Direction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Surface Alignment: Rebuilding LLMs Safety Mechanism via\n  Probabilistically Ablating Refusal Direction"
                },
                "summary": "Jailbreak attacks pose persistent threats to large language models (LLMs).\nCurrent safety alignment methods have attempted to address these issues, but\nthey experience two significant limitations: insufficient safety alignment\ndepth and unrobust internal defense mechanisms. These limitations make them\nvulnerable to adversarial attacks such as prefilling and refusal direction\nmanipulation. We introduce DeepRefusal, a robust safety alignment framework\nthat overcomes these issues. DeepRefusal forces the model to dynamically\nrebuild its refusal mechanisms from jailbreak states. This is achieved by\nprobabilistically ablating the refusal direction across layers and token depths\nduring fine-tuning. Our method not only defends against prefilling and refusal\ndirection attacks but also demonstrates strong resilience against other unseen\njailbreak strategies. Extensive evaluations on four open-source LLM families\nand six representative attacks show that DeepRefusal reduces attack success\nrates by approximately 95%, while maintaining model capabilities with minimal\nperformance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak attacks pose persistent threats to large language models (LLMs).\nCurrent safety alignment methods have attempted to address these issues, but\nthey experience two significant limitations: insufficient safety alignment\ndepth and unrobust internal defense mechanisms. These limitations make them\nvulnerable to adversarial attacks such as prefilling and refusal direction\nmanipulation. We introduce DeepRefusal, a robust safety alignment framework\nthat overcomes these issues. DeepRefusal forces the model to dynamically\nrebuild its refusal mechanisms from jailbreak states. This is achieved by\nprobabilistically ablating the refusal direction across layers and token depths\nduring fine-tuning. Our method not only defends against prefilling and refusal\ndirection attacks but also demonstrates strong resilience against other unseen\njailbreak strategies. Extensive evaluations on four open-source LLM families\nand six representative attacks show that DeepRefusal reduces attack success\nrates by approximately 95%, while maintaining model capabilities with minimal\nperformance degradation."
                },
                "authors": [
                    {
                        "name": "Yuanbo Xie"
                    },
                    {
                        "name": "Yingjie Zhang"
                    },
                    {
                        "name": "Tianyun Liu"
                    },
                    {
                        "name": "Duohe Ma"
                    },
                    {
                        "name": "Tingwen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tingwen Liu"
                },
                "author": "Tingwen Liu",
                "arxiv_comment": "Accepted by EMNLP2025 Finding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15196v1",
                "updated": "2025-09-18T17:52:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    52,
                    23,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:52:23Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    52,
                    23,
                    3,
                    261,
                    0
                ],
                "title": "Measuring the Two-Dimensional Thermal Structures of Protoplanetary Disks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring the Two-Dimensional Thermal Structures of Protoplanetary Disks"
                },
                "summary": "We present a flexible, annulus-by-annulus method to constrain the 2-D thermal\nstructure of a protoplanetary disk from optically thick spectral line emission.\nUsing synthetic disk models with a known temperature and density structure, we\nextracted the vertical emission surfaces and brightness temperatures in radial\nannuli for multiple CO isotopologue transitions and used them to infer the\nvertical temperature profiles. This approach reliably recovers the injected\ntemperature structure despite noise and finite resolution. We demonstrated that\neven a modest set of emission lines can constrain the temperature across a wide\nrange of radii and elevations. Nevertheless, biases in the extracted emission\nsurfaces constitute a major source of systematic error. Finally, we applied\nthis method to archival ALMA observations of the HD 163296 disk, revealing that\nsimple parametric radial temperature models may obscure the complexity of real\ndisks and that additional observations are necessary to distinguish between\ndifferent models of the vertical structure. This flexible framework can be\nreadily applied to other systems, helping to characterize the thermal\nenvironments that shape planet formation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a flexible, annulus-by-annulus method to constrain the 2-D thermal\nstructure of a protoplanetary disk from optically thick spectral line emission.\nUsing synthetic disk models with a known temperature and density structure, we\nextracted the vertical emission surfaces and brightness temperatures in radial\nannuli for multiple CO isotopologue transitions and used them to infer the\nvertical temperature profiles. This approach reliably recovers the injected\ntemperature structure despite noise and finite resolution. We demonstrated that\neven a modest set of emission lines can constrain the temperature across a wide\nrange of radii and elevations. Nevertheless, biases in the extracted emission\nsurfaces constitute a major source of systematic error. Finally, we applied\nthis method to archival ALMA observations of the HD 163296 disk, revealing that\nsimple parametric radial temperature models may obscure the complexity of real\ndisks and that additional observations are necessary to distinguish between\ndifferent models of the vertical structure. This flexible framework can be\nreadily applied to other systems, helping to characterize the thermal\nenvironments that shape planet formation."
                },
                "authors": [
                    {
                        "name": "Anna J. Fehr"
                    },
                    {
                        "name": "Sean M. Andrews"
                    }
                ],
                "author_detail": {
                    "name": "Sean M. Andrews"
                },
                "author": "Sean M. Andrews",
                "arxiv_comment": "19 pages, 15 figures, accepted by ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15195v1",
                "updated": "2025-09-18T17:52:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    52,
                    6,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:52:06Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    52,
                    6,
                    3,
                    261,
                    0
                ],
                "title": "Orion: Fuzzing Workflow Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orion: Fuzzing Workflow Automation"
                },
                "summary": "Fuzz testing is one of the most effective techniques for finding software\nvulnerabilities. While modern fuzzers can generate inputs and monitor\nexecutions automatically, the overall workflow, from analyzing a codebase, to\nconfiguring harnesses, to triaging results, still requires substantial manual\neffort. Prior attempts focused on single stages such as harness synthesis or\ninput minimization, leaving researchers to manually connect the pieces into a\ncomplete fuzzing campaign.\n  We introduce Orion, a framework that automates the the manual bottlenecks of\nfuzzing by integrating LLM reasoning with traditional tools, allowing campaigns\nto scale to settings where human effort alone was impractical. Orion uses LLMs\nfor code reasoning and semantic guidance, while relying on deterministic tools\nfor verification, iterative refinement, and tasks that require precision.\nAcross our benchmark suite, Orion reduces human effort by 46-204x depending on\nthe workflow stage, and we demonstrate its effectiveness through the discovery\nof two previously unknown vulnerabilities in the widely used open-source clib\nlibrary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fuzz testing is one of the most effective techniques for finding software\nvulnerabilities. While modern fuzzers can generate inputs and monitor\nexecutions automatically, the overall workflow, from analyzing a codebase, to\nconfiguring harnesses, to triaging results, still requires substantial manual\neffort. Prior attempts focused on single stages such as harness synthesis or\ninput minimization, leaving researchers to manually connect the pieces into a\ncomplete fuzzing campaign.\n  We introduce Orion, a framework that automates the the manual bottlenecks of\nfuzzing by integrating LLM reasoning with traditional tools, allowing campaigns\nto scale to settings where human effort alone was impractical. Orion uses LLMs\nfor code reasoning and semantic guidance, while relying on deterministic tools\nfor verification, iterative refinement, and tasks that require precision.\nAcross our benchmark suite, Orion reduces human effort by 46-204x depending on\nthe workflow stage, and we demonstrate its effectiveness through the discovery\nof two previously unknown vulnerabilities in the widely used open-source clib\nlibrary."
                },
                "authors": [
                    {
                        "name": "Max Bazalii"
                    },
                    {
                        "name": "Marius Fleischer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Fleischer"
                },
                "author": "Marius Fleischer",
                "arxiv_comment": "11 pages, 3 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.6; I.2.2; D.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15194v1",
                "updated": "2025-09-18T17:50:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    50,
                    4,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:50:04Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    50,
                    4,
                    3,
                    261,
                    0
                ],
                "title": "Evolving Language Models without Labels: Majority Drives Selection,\n  Novelty Promotes Variation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolving Language Models without Labels: Majority Drives Selection,\n  Novelty Promotes Variation"
                },
                "summary": "Large language models (LLMs) are increasingly trained with reinforcement\nlearning from verifiable rewards (RLVR), yet real-world deployment demands\nmodels that can self-improve without labels or external judges. Existing\nlabel-free methods, confidence minimization, self-consistency, or majority-vote\nobjectives, stabilize learning but steadily shrink exploration, causing an\nentropy collapse: generations become shorter, less diverse, and brittle. Unlike\nprior approaches such as Test-Time Reinforcement Learning (TTRL), which\nprimarily adapt models to the immediate unlabeled dataset at hand, our goal is\nbroader: to enable general improvements without sacrificing the model's\ninherent exploration capacity and generalization ability, i.e., evolving. We\nformalize this issue and propose EVolution-Oriented and Label-free\nReinforcement Learning (EVOL-RL), a simple rule that couples stability with\nvariation under a label-free setting. EVOL-RL keeps the majority-voted answer\nas a stable anchor (selection) while adding a novelty-aware reward that favors\nresponses whose reasoning differs from what has already been produced\n(variation), measured in semantic space. Implemented with GRPO, EVOL-RL also\nuses asymmetric clipping to preserve strong signals and an entropy regularizer\nto sustain search. This majority-for-selection + novelty-for-variation design\nprevents collapse, maintains longer and more informative chains of thought, and\nimproves both pass@1 and pass@n. EVOL-RL consistently outperforms the\nmajority-only TTRL baseline; e.g., training on label-free AIME24 lifts\nQwen3-4B-Base AIME25 pass@1 from TTRL's 4.6% to 16.4%, and pass@16 from 18.5%\nto 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks\nstronger generalization across domains (e.g., GPQA). Furthermore, we\ndemonstrate that EVOL-RL also boosts performance in the RLVR setting,\nhighlighting its broad applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly trained with reinforcement\nlearning from verifiable rewards (RLVR), yet real-world deployment demands\nmodels that can self-improve without labels or external judges. Existing\nlabel-free methods, confidence minimization, self-consistency, or majority-vote\nobjectives, stabilize learning but steadily shrink exploration, causing an\nentropy collapse: generations become shorter, less diverse, and brittle. Unlike\nprior approaches such as Test-Time Reinforcement Learning (TTRL), which\nprimarily adapt models to the immediate unlabeled dataset at hand, our goal is\nbroader: to enable general improvements without sacrificing the model's\ninherent exploration capacity and generalization ability, i.e., evolving. We\nformalize this issue and propose EVolution-Oriented and Label-free\nReinforcement Learning (EVOL-RL), a simple rule that couples stability with\nvariation under a label-free setting. EVOL-RL keeps the majority-voted answer\nas a stable anchor (selection) while adding a novelty-aware reward that favors\nresponses whose reasoning differs from what has already been produced\n(variation), measured in semantic space. Implemented with GRPO, EVOL-RL also\nuses asymmetric clipping to preserve strong signals and an entropy regularizer\nto sustain search. This majority-for-selection + novelty-for-variation design\nprevents collapse, maintains longer and more informative chains of thought, and\nimproves both pass@1 and pass@n. EVOL-RL consistently outperforms the\nmajority-only TTRL baseline; e.g., training on label-free AIME24 lifts\nQwen3-4B-Base AIME25 pass@1 from TTRL's 4.6% to 16.4%, and pass@16 from 18.5%\nto 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks\nstronger generalization across domains (e.g., GPQA). Furthermore, we\ndemonstrate that EVOL-RL also boosts performance in the RLVR setting,\nhighlighting its broad applicability."
                },
                "authors": [
                    {
                        "name": "Yujun Zhou"
                    },
                    {
                        "name": "Zhenwen Liang"
                    },
                    {
                        "name": "Haolin Liu"
                    },
                    {
                        "name": "Wenhao Yu"
                    },
                    {
                        "name": "Kishan Panaganti"
                    },
                    {
                        "name": "Linfeng Song"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09409v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09409v3",
                "updated": "2025-09-18T17:48:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    48,
                    23,
                    3,
                    261,
                    0
                ],
                "published": "2025-01-16T09:35:15Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    9,
                    35,
                    15,
                    3,
                    16,
                    0
                ],
                "title": "Mind the Inclusivity Gap: Multilingual Gender-Neutral Translation\n  Evaluation with mGeNTE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Inclusivity Gap: Multilingual Gender-Neutral Translation\n  Evaluation with mGeNTE"
                },
                "summary": "Avoiding the propagation of undue (binary) gender inferences and default\nmasculine language remains a key challenge towards inclusive multilingual\ntechnologies, particularly when translating into languages with extensive\ngendered morphology. Gender-neutral translation (GNT) represents a linguistic\nstrategy towards fairer communication across languages. However, research on\nGNT is limited to a few resources and language pairs. To address this gap, we\nintroduce mGeNTE, an expert-curated resource, and use it to conduct the first\nsystematic multilingual evaluation of inclusive translation with\nstate-of-the-art instruction-following language models (LMs). Experiments on\nen-es/de/it/el reveal that while models can recognize when neutrality is\nappropriate, they cannot consistently produce neutral translations, limiting\ntheir usability. To probe this behavior, we enrich our evaluation with\ninterpretability analyses that identify task-relevant features and offer\ninitial insights into the internal dynamics of LM-based GNT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Avoiding the propagation of undue (binary) gender inferences and default\nmasculine language remains a key challenge towards inclusive multilingual\ntechnologies, particularly when translating into languages with extensive\ngendered morphology. Gender-neutral translation (GNT) represents a linguistic\nstrategy towards fairer communication across languages. However, research on\nGNT is limited to a few resources and language pairs. To address this gap, we\nintroduce mGeNTE, an expert-curated resource, and use it to conduct the first\nsystematic multilingual evaluation of inclusive translation with\nstate-of-the-art instruction-following language models (LMs). Experiments on\nen-es/de/it/el reveal that while models can recognize when neutrality is\nappropriate, they cannot consistently produce neutral translations, limiting\ntheir usability. To probe this behavior, we enrich our evaluation with\ninterpretability analyses that identify task-relevant features and offer\ninitial insights into the internal dynamics of LM-based GNT."
                },
                "authors": [
                    {
                        "name": "Beatrice Savoldi"
                    },
                    {
                        "name": "Giuseppe Attanasio"
                    },
                    {
                        "name": "Eleonora Cupin"
                    },
                    {
                        "name": "Eleni Gkovedarou"
                    },
                    {
                        "name": "Janiça Hackenbuchner"
                    },
                    {
                        "name": "Anne Lauscher"
                    },
                    {
                        "name": "Matteo Negri"
                    },
                    {
                        "name": "Andrea Piergentili"
                    },
                    {
                        "name": "Manjinder Thind"
                    },
                    {
                        "name": "Luisa Bentivogli"
                    }
                ],
                "author_detail": {
                    "name": "Luisa Bentivogli"
                },
                "author": "Luisa Bentivogli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09409v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09409v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15188v1",
                "updated": "2025-09-18T17:48:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    48,
                    21,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:48:21Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    48,
                    21,
                    3,
                    261,
                    0
                ],
                "title": "Fast and Fluent Diffusion Language Models via Convolutional Decoding and\n  Rejective Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Fluent Diffusion Language Models via Convolutional Decoding and\n  Rejective Fine-tuning"
                },
                "summary": "Autoregressive (AR) language models generate text one token at a time, which\nlimits their inference speed. Diffusion-based language models offer a promising\nalternative, as they can decode multiple tokens in parallel. However, we\nidentify a key bottleneck in current diffusion LMs: the long decoding-window\nproblem, where tokens generated far from the input context often become\nirrelevant or repetitive. Previous solutions like semi-autoregressive address\nthis issue by splitting windows into blocks, but this sacrifices speed and\nbidirectionality, eliminating the main advantage of diffusion models. To\novercome this, we propose Convolutional decoding (Conv), a normalization-based\nmethod that narrows the decoding window without hard segmentation, leading to\nbetter fluency and flexibility. Additionally, we introduce Rejecting Rule-based\nFine-Tuning (R2FT), a post-hoc training scheme that better aligns tokens at\npositions far from context. Our methods achieve state-of-the-art results on\nopen-ended generation benchmarks (e.g., AlpacaEval) among diffusion LM\nbaselines, with significantly lower step size than previous works,\ndemonstrating both speed and quality improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) language models generate text one token at a time, which\nlimits their inference speed. Diffusion-based language models offer a promising\nalternative, as they can decode multiple tokens in parallel. However, we\nidentify a key bottleneck in current diffusion LMs: the long decoding-window\nproblem, where tokens generated far from the input context often become\nirrelevant or repetitive. Previous solutions like semi-autoregressive address\nthis issue by splitting windows into blocks, but this sacrifices speed and\nbidirectionality, eliminating the main advantage of diffusion models. To\novercome this, we propose Convolutional decoding (Conv), a normalization-based\nmethod that narrows the decoding window without hard segmentation, leading to\nbetter fluency and flexibility. Additionally, we introduce Rejecting Rule-based\nFine-Tuning (R2FT), a post-hoc training scheme that better aligns tokens at\npositions far from context. Our methods achieve state-of-the-art results on\nopen-ended generation benchmarks (e.g., AlpacaEval) among diffusion LM\nbaselines, with significantly lower step size than previous works,\ndemonstrating both speed and quality improvements."
                },
                "authors": [
                    {
                        "name": "Yeongbin Seo"
                    },
                    {
                        "name": "Dongha Lee"
                    },
                    {
                        "name": "Jaehyung Kim"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    }
                ],
                "author_detail": {
                    "name": "Jinyoung Yeo"
                },
                "author": "Jinyoung Yeo",
                "arxiv_comment": "NeurIPS 2025 spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15187v1",
                "updated": "2025-09-18T17:48:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    48,
                    20,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:48:20Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    48,
                    20,
                    3,
                    261,
                    0
                ],
                "title": "MaRVIn: A Cross-Layer Mixed-Precision RISC-V Framework for DNN\n  Inference, from ISA Extension to Hardware Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MaRVIn: A Cross-Layer Mixed-Precision RISC-V Framework for DNN\n  Inference, from ISA Extension to Hardware Acceleration"
                },
                "summary": "The evolution of quantization and mixed-precision techniques has unlocked new\npossibilities for enhancing the speed and energy efficiency of NNs. Several\nrecent studies indicate that adapting precision levels across different\nparameters can maintain accuracy comparable to full-precision models while\nsignificantly reducing computational demands. However, existing embedded\nmicroprocessors lack sufficient architectural support for efficiently executing\nmixed-precision NNs, both in terms of ISA extensions and hardware design,\nresulting in inefficiencies such as excessive data packing/unpacking and\nunderutilized arithmetic units. In this work, we propose novel ISA extensions\nand a micro-architecture implementation specifically designed to optimize\nmixed-precision execution, enabling energy-efficient deep learning inference on\nRISC-V architectures. We introduce MaRVIn, a cross-layer hardware-software\nco-design framework that enhances power efficiency and performance through a\ncombination of hardware improvements, mixed-precision quantization, ISA-level\noptimizations, and cycle-accurate emulation. At the hardware level, we enhance\nthe ALU with configurable mixed-precision arithmetic (2, 4, 8 bits) for\nweights/activations and employ multi-pumping to reduce execution latency while\nimplementing soft SIMD for efficient 2-bit ops. At the software level, we\nintegrate a pruning-aware fine-tuning method to optimize model compression and\na greedy-based DSE approach to efficiently search for Pareto-optimal\nmixed-quantized models. Additionally, we incorporate voltage scaling to boost\nthe power efficiency of our system. Our experimental evaluation over widely\nused DNNs and datasets, such as CIFAR10 and ImageNet, demonstrates that our\nframework can achieve, on average, 17.6x speedup for less than 1% accuracy loss\nand outperforms the ISA-agnostic state-of-the-art RISC-V cores, delivering up\nto 1.8 TOPs/W.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of quantization and mixed-precision techniques has unlocked new\npossibilities for enhancing the speed and energy efficiency of NNs. Several\nrecent studies indicate that adapting precision levels across different\nparameters can maintain accuracy comparable to full-precision models while\nsignificantly reducing computational demands. However, existing embedded\nmicroprocessors lack sufficient architectural support for efficiently executing\nmixed-precision NNs, both in terms of ISA extensions and hardware design,\nresulting in inefficiencies such as excessive data packing/unpacking and\nunderutilized arithmetic units. In this work, we propose novel ISA extensions\nand a micro-architecture implementation specifically designed to optimize\nmixed-precision execution, enabling energy-efficient deep learning inference on\nRISC-V architectures. We introduce MaRVIn, a cross-layer hardware-software\nco-design framework that enhances power efficiency and performance through a\ncombination of hardware improvements, mixed-precision quantization, ISA-level\noptimizations, and cycle-accurate emulation. At the hardware level, we enhance\nthe ALU with configurable mixed-precision arithmetic (2, 4, 8 bits) for\nweights/activations and employ multi-pumping to reduce execution latency while\nimplementing soft SIMD for efficient 2-bit ops. At the software level, we\nintegrate a pruning-aware fine-tuning method to optimize model compression and\na greedy-based DSE approach to efficiently search for Pareto-optimal\nmixed-quantized models. Additionally, we incorporate voltage scaling to boost\nthe power efficiency of our system. Our experimental evaluation over widely\nused DNNs and datasets, such as CIFAR10 and ImageNet, demonstrates that our\nframework can achieve, on average, 17.6x speedup for less than 1% accuracy loss\nand outperforms the ISA-agnostic state-of-the-art RISC-V cores, delivering up\nto 1.8 TOPs/W."
                },
                "authors": [
                    {
                        "name": "Giorgos Armeniakos"
                    },
                    {
                        "name": "Alexis Maras"
                    },
                    {
                        "name": "Sotirios Xydis"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Soudris"
                },
                "author": "Dimitrios Soudris",
                "arxiv_comment": "Accepted for publication by IEEE Transactions on Computer-Aided\n  Design of Integrated Circuits and Systems, March 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15182v1",
                "updated": "2025-09-18T17:43:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    43,
                    20,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:43:20Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    43,
                    20,
                    3,
                    261,
                    0
                ],
                "title": "Conditional Prior-based Non-stationary Channel Estimation Using\n  Accelerated Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditional Prior-based Non-stationary Channel Estimation Using\n  Accelerated Diffusion Models"
                },
                "summary": "Wireless channels in motion-rich urban microcell (UMi) settings are\nnon-stationary; mobility and scatterer dynamics shift the distribution over\ntime, degrading classical and deep estimators. This work proposes conditional\nprior diffusion for channel estimation, which learns a history-conditioned\nscore to denoise noisy channel snapshots. A temporal encoder with cross-time\nattention compresses a short observation window into a context vector, which\ncaptures the channel's instantaneous coherence and steers the denoiser via\nfeature-wise modulation. In inference, an SNR-matched initialization selects\nthe diffusion step whose marginal aligns with the measured input SNR, and the\nprocess follows a shortened, geometrically spaced schedule, preserving the\nsignal-to-noise trajectory with far fewer iterations. Temporal\nself-conditioning with the previous channel estimate and a training-only\nsmoothness penalty further stabilizes evolution without biasing the test-time\nestimator. Evaluations on a 3GPP benchmark show lower NMSE across all SNRs than\nLMMSE, GMM, LSTM, and LDAMP baselines, demonstrating stable performance and\nstrong high SNR fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless channels in motion-rich urban microcell (UMi) settings are\nnon-stationary; mobility and scatterer dynamics shift the distribution over\ntime, degrading classical and deep estimators. This work proposes conditional\nprior diffusion for channel estimation, which learns a history-conditioned\nscore to denoise noisy channel snapshots. A temporal encoder with cross-time\nattention compresses a short observation window into a context vector, which\ncaptures the channel's instantaneous coherence and steers the denoiser via\nfeature-wise modulation. In inference, an SNR-matched initialization selects\nthe diffusion step whose marginal aligns with the measured input SNR, and the\nprocess follows a shortened, geometrically spaced schedule, preserving the\nsignal-to-noise trajectory with far fewer iterations. Temporal\nself-conditioning with the previous channel estimate and a training-only\nsmoothness penalty further stabilizes evolution without biasing the test-time\nestimator. Evaluations on a 3GPP benchmark show lower NMSE across all SNRs than\nLMMSE, GMM, LSTM, and LDAMP baselines, demonstrating stable performance and\nstrong high SNR fidelity."
                },
                "authors": [
                    {
                        "name": "Muhammad Ahmed Mohsin"
                    },
                    {
                        "name": "Ahsan Bilal"
                    },
                    {
                        "name": "Muhammad Umer"
                    },
                    {
                        "name": "Asad Aali"
                    },
                    {
                        "name": "Muhammad Ali Jamshed"
                    },
                    {
                        "name": "Dean F. Hougen"
                    },
                    {
                        "name": "John M. Cioffi"
                    }
                ],
                "author_detail": {
                    "name": "John M. Cioffi"
                },
                "author": "John M. Cioffi",
                "arxiv_comment": "ICASSP 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15181v1",
                "updated": "2025-09-18T17:41:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    41,
                    59,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:41:59Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    41,
                    59,
                    3,
                    261,
                    0
                ],
                "title": "Maize Seedling Detection Dataset (MSDD): A Curated High-Resolution RGB\n  Dataset for Seedling Maize Detection and Benchmarking with YOLOv9, YOLO11,\n  YOLOv12 and Faster-RCNN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maize Seedling Detection Dataset (MSDD): A Curated High-Resolution RGB\n  Dataset for Seedling Maize Detection and Benchmarking with YOLOv9, YOLO11,\n  YOLOv12 and Faster-RCNN"
                },
                "summary": "Accurate maize seedling detection is crucial for precision agriculture, yet\ncurated datasets remain scarce. We introduce MSDD, a high-quality aerial image\ndataset for maize seedling stand counting, with applications in early-season\ncrop monitoring, yield prediction, and in-field management. Stand counting\ndetermines how many plants germinated, guiding timely decisions such as\nreplanting or adjusting inputs. Traditional methods are labor-intensive and\nerror-prone, while computer vision enables efficient, accurate detection. MSDD\ncontains three classes-single, double, and triple plants-capturing diverse\ngrowth stages, planting setups, soil types, lighting conditions, camera angles,\nand densities, ensuring robustness for real-world use. Benchmarking shows\ndetection is most reliable during V4-V6 stages and under nadir views. Among\ntested models, YOLO11 is fastest, while YOLOv9 yields the highest accuracy for\nsingle plants. Single plant detection achieves precision up to 0.984 and recall\nup to 0.873, but detecting doubles and triples remains difficult due to rarity\nand irregular appearance, often from planting errors. Class imbalance further\nreduces accuracy in multi-plant detection. Despite these challenges, YOLO11\nmaintains efficient inference at 35 ms per image, with an additional 120 ms for\nsaving outputs. MSDD establishes a strong foundation for developing models that\nenhance stand counting, optimize resource allocation, and support real-time\ndecision-making. This dataset marks a step toward automating agricultural\nmonitoring and advancing precision agriculture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate maize seedling detection is crucial for precision agriculture, yet\ncurated datasets remain scarce. We introduce MSDD, a high-quality aerial image\ndataset for maize seedling stand counting, with applications in early-season\ncrop monitoring, yield prediction, and in-field management. Stand counting\ndetermines how many plants germinated, guiding timely decisions such as\nreplanting or adjusting inputs. Traditional methods are labor-intensive and\nerror-prone, while computer vision enables efficient, accurate detection. MSDD\ncontains three classes-single, double, and triple plants-capturing diverse\ngrowth stages, planting setups, soil types, lighting conditions, camera angles,\nand densities, ensuring robustness for real-world use. Benchmarking shows\ndetection is most reliable during V4-V6 stages and under nadir views. Among\ntested models, YOLO11 is fastest, while YOLOv9 yields the highest accuracy for\nsingle plants. Single plant detection achieves precision up to 0.984 and recall\nup to 0.873, but detecting doubles and triples remains difficult due to rarity\nand irregular appearance, often from planting errors. Class imbalance further\nreduces accuracy in multi-plant detection. Despite these challenges, YOLO11\nmaintains efficient inference at 35 ms per image, with an additional 120 ms for\nsaving outputs. MSDD establishes a strong foundation for developing models that\nenhance stand counting, optimize resource allocation, and support real-time\ndecision-making. This dataset marks a step toward automating agricultural\nmonitoring and advancing precision agriculture."
                },
                "authors": [
                    {
                        "name": "Dewi Endah Kharismawati"
                    },
                    {
                        "name": "Toni Kazic"
                    }
                ],
                "author_detail": {
                    "name": "Toni Kazic"
                },
                "author": "Toni Kazic",
                "arxiv_comment": "18 pages, 10 figures, 8 tables. Submitted to IEEE Journal of Selected\n  Topics in Signal Processing (JSTSP) Special Series on Artificial Intelligence\n  for Smart Agriculture",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05755v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05755v3",
                "updated": "2025-09-18T17:38:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    38,
                    28,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-06T15:48:49Z",
                "published_parsed": [
                    2025,
                    9,
                    6,
                    15,
                    48,
                    49,
                    5,
                    249,
                    0
                ],
                "title": "Exploit Tool Invocation Prompt for Tool Behavior Hijacking in LLM-Based\n  Agentic System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploit Tool Invocation Prompt for Tool Behavior Hijacking in LLM-Based\n  Agentic System"
                },
                "summary": "LLM-based agentic systems leverage large language models to handle user\nqueries, make decisions, and execute external tools for complex tasks across\ndomains like chatbots, customer service, and software engineering. A critical\ncomponent of these systems is the Tool Invocation Prompt (TIP), which defines\ntool interaction protocols and guides LLMs to ensure the security and\ncorrectness of tool usage. Despite its importance, TIP security has been\nlargely overlooked. This work investigates TIP-related security risks,\nrevealing that major LLM-based systems like Cursor, Claude Code, and others are\nvulnerable to attacks such as remote code execution (RCE) and denial of service\n(DoS). Through a systematic TIP exploitation workflow (TEW), we demonstrate\nexternal tool behavior hijacking via manipulated tool invocations. We also\npropose defense mechanisms to enhance TIP security in LLM-based agentic\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agentic systems leverage large language models to handle user\nqueries, make decisions, and execute external tools for complex tasks across\ndomains like chatbots, customer service, and software engineering. A critical\ncomponent of these systems is the Tool Invocation Prompt (TIP), which defines\ntool interaction protocols and guides LLMs to ensure the security and\ncorrectness of tool usage. Despite its importance, TIP security has been\nlargely overlooked. This work investigates TIP-related security risks,\nrevealing that major LLM-based systems like Cursor, Claude Code, and others are\nvulnerable to attacks such as remote code execution (RCE) and denial of service\n(DoS). Through a systematic TIP exploitation workflow (TEW), we demonstrate\nexternal tool behavior hijacking via manipulated tool invocations. We also\npropose defense mechanisms to enhance TIP security in LLM-based agentic\nsystems."
                },
                "authors": [
                    {
                        "name": "Yuchong Xie"
                    },
                    {
                        "name": "Mingyu Luo"
                    },
                    {
                        "name": "Zesen Liu"
                    },
                    {
                        "name": "Zhixiang Zhang"
                    },
                    {
                        "name": "Kaikai Zhang"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Zongjie Li"
                    },
                    {
                        "name": "Ping Chen"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Dongdong She"
                    }
                ],
                "author_detail": {
                    "name": "Dongdong She"
                },
                "author": "Dongdong She",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05755v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05755v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08123v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08123v3",
                "updated": "2025-09-18T17:36:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    36,
                    13,
                    3,
                    261,
                    0
                ],
                "published": "2025-06-09T18:24:57Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    18,
                    24,
                    57,
                    0,
                    160,
                    0
                ],
                "title": "QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA"
                },
                "summary": "Alignment of large language models (LLMs) with principles like helpfulness,\nhonesty, and harmlessness typically relies on scalar rewards that obscure which\nobjectives drive the training signal. We introduce QA-LIGN, which decomposes\nmonolithic rewards into interpretable principle-specific evaluations through\nstructured natural language programs. Models learn through a draft, critique,\nand revise pipeline, where symbolic evaluation against the rubrics provides\ntransparent feedback for both initial and revised responses during GRPO\ntraining. Applied to uncensored Llama-3.1-8B-Instruct, QA-LIGN reduces attack\nsuccess rates by up to 68.7% while maintaining a 0.67% false refusal rate,\nachieving Pareto optimal safety-helpfulness performance and outperforming both\nDPO and GRPO with state-of-the-art reward models given equivalent training.\nThese results demonstrate that making reward signals interpretable and modular\nimproves alignment effectiveness, suggesting transparency enhances LLM safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment of large language models (LLMs) with principles like helpfulness,\nhonesty, and harmlessness typically relies on scalar rewards that obscure which\nobjectives drive the training signal. We introduce QA-LIGN, which decomposes\nmonolithic rewards into interpretable principle-specific evaluations through\nstructured natural language programs. Models learn through a draft, critique,\nand revise pipeline, where symbolic evaluation against the rubrics provides\ntransparent feedback for both initial and revised responses during GRPO\ntraining. Applied to uncensored Llama-3.1-8B-Instruct, QA-LIGN reduces attack\nsuccess rates by up to 68.7% while maintaining a 0.67% false refusal rate,\nachieving Pareto optimal safety-helpfulness performance and outperforming both\nDPO and GRPO with state-of-the-art reward models given equivalent training.\nThese results demonstrate that making reward signals interpretable and modular\nimproves alignment effectiveness, suggesting transparency enhances LLM safety."
                },
                "authors": [
                    {
                        "name": "Jacob Dineen"
                    },
                    {
                        "name": "Aswin RRV"
                    },
                    {
                        "name": "Qin Liu"
                    },
                    {
                        "name": "Zhikun Xu"
                    },
                    {
                        "name": "Xiao Ye"
                    },
                    {
                        "name": "Ming Shen"
                    },
                    {
                        "name": "Zhaonan Li"
                    },
                    {
                        "name": "Shijie Lu"
                    },
                    {
                        "name": "Chitta Baral"
                    },
                    {
                        "name": "Muhao Chen"
                    },
                    {
                        "name": "Ben Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Ben Zhou"
                },
                "arxiv_affiliation": "Arizona State University",
                "author": "Ben Zhou",
                "arxiv_comment": "Accepted to Findings of EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08123v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08123v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15178v1",
                "updated": "2025-09-18T17:35:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    35,
                    50,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:35:50Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    35,
                    50,
                    3,
                    261,
                    0
                ],
                "title": "Unleashing the Potential of Multimodal LLMs for Zero-Shot\n  Spatio-Temporal Video Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of Multimodal LLMs for Zero-Shot\n  Spatio-Temporal Video Grounding"
                },
                "summary": "Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal\ntube of a video, as specified by the input text query. In this paper, we\nutilize multimodal large language models (MLLMs) to explore a zero-shot\nsolution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to\ndynamically assign special tokens, referred to as \\textit{grounding tokens},\nfor grounding the text query; and (2) MLLMs often suffer from suboptimal\ngrounding due to the inability to fully integrate the cues in the text query\n(\\textit{e.g.}, attributes, actions) for inference. Based on these insights, we\npropose a MLLM-based zero-shot framework for STVG, which includes novel\ndecomposed spatio-temporal highlighting (DSTH) and temporal-augmented\nassembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH\nstrategy first decouples the original query into attribute and action\nsub-queries for inquiring the existence of the target both spatially and\ntemporally. It then uses a novel logit-guided re-attention (LRA) module to\nlearn latent variables as spatial and temporal prompts, by regularizing token\npredictions for each sub-query. These prompts highlight attribute and action\ncues, respectively, directing the model's attention to reliable spatial and\ntemporal related visual regions. In addition, as the spatial grounding by the\nattribute sub-query should be temporally consistent, we introduce the TAS\nstrategy to assemble the predictions using the original video frames and the\ntemporal-augmented frames as inputs to help improve temporal consistency. We\nevaluate our method on various MLLMs, and show that it outperforms SOTA methods\non three common STVG benchmarks.\n  The code will be available at https://github.com/zaiquanyang/LLaVA_Next_STVG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal\ntube of a video, as specified by the input text query. In this paper, we\nutilize multimodal large language models (MLLMs) to explore a zero-shot\nsolution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to\ndynamically assign special tokens, referred to as \\textit{grounding tokens},\nfor grounding the text query; and (2) MLLMs often suffer from suboptimal\ngrounding due to the inability to fully integrate the cues in the text query\n(\\textit{e.g.}, attributes, actions) for inference. Based on these insights, we\npropose a MLLM-based zero-shot framework for STVG, which includes novel\ndecomposed spatio-temporal highlighting (DSTH) and temporal-augmented\nassembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH\nstrategy first decouples the original query into attribute and action\nsub-queries for inquiring the existence of the target both spatially and\ntemporally. It then uses a novel logit-guided re-attention (LRA) module to\nlearn latent variables as spatial and temporal prompts, by regularizing token\npredictions for each sub-query. These prompts highlight attribute and action\ncues, respectively, directing the model's attention to reliable spatial and\ntemporal related visual regions. In addition, as the spatial grounding by the\nattribute sub-query should be temporally consistent, we introduce the TAS\nstrategy to assemble the predictions using the original video frames and the\ntemporal-augmented frames as inputs to help improve temporal consistency. We\nevaluate our method on various MLLMs, and show that it outperforms SOTA methods\non three common STVG benchmarks.\n  The code will be available at https://github.com/zaiquanyang/LLaVA_Next_STVG."
                },
                "authors": [
                    {
                        "name": "Zaiquan Yang"
                    },
                    {
                        "name": "Yuhao Liu"
                    },
                    {
                        "name": "Gerhard Hancke"
                    },
                    {
                        "name": "Rynson W. H. Lau"
                    }
                ],
                "author_detail": {
                    "name": "Rynson W. H. Lau"
                },
                "author": "Rynson W. H. Lau",
                "arxiv_journal_ref": "NeurIPS2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15176v1",
                "updated": "2025-09-18T17:34:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    34,
                    4,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:34:04Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    34,
                    4,
                    3,
                    261,
                    0
                ],
                "title": "To CLEAN or not to CLEAN: Data Processing in the ngVLA era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To CLEAN or not to CLEAN: Data Processing in the ngVLA era"
                },
                "summary": "Radio interferometric imaging has long relied on the CLEAN algorithm, valued\nfor its speed, robustness, and integration with calibration pipelines. However,\nnext-generation facilities such as the ngVLA, SKA, and ALMAs Wideband\nSensitivity Upgrade will produce data volumes and dynamic ranges that exceed\nthe scalability of traditional methods. CLEAN remains dominant due to its\nsimplicity and accumulated expertise, yet its assumption of modeling the sky as\npoint sources limits its ability to recover extended emission and hampers\nautomation. We review CLEANs limitations and survey alternatives, including\nmultiscale extensions, compressive sensing, Regularized Maximum Likelihood,\nBayesian inference, and AI-driven approaches. Forward-modeling methods enable\nhigher fidelity, flexible priors, and uncertainty quantification, albeit at\ngreater computational cost. Hybrid approaches such as Autocorr-CLEAN, CG-CLEAN,\nand PolyCLEAN retain CLEANs workflow while incorporating modern optimization.\nWe argue hybrids are best suited for the near term, while Bayesian and AI-based\nframeworks represent the long-term future of interferometric imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radio interferometric imaging has long relied on the CLEAN algorithm, valued\nfor its speed, robustness, and integration with calibration pipelines. However,\nnext-generation facilities such as the ngVLA, SKA, and ALMAs Wideband\nSensitivity Upgrade will produce data volumes and dynamic ranges that exceed\nthe scalability of traditional methods. CLEAN remains dominant due to its\nsimplicity and accumulated expertise, yet its assumption of modeling the sky as\npoint sources limits its ability to recover extended emission and hampers\nautomation. We review CLEANs limitations and survey alternatives, including\nmultiscale extensions, compressive sensing, Regularized Maximum Likelihood,\nBayesian inference, and AI-driven approaches. Forward-modeling methods enable\nhigher fidelity, flexible priors, and uncertainty quantification, albeit at\ngreater computational cost. Hybrid approaches such as Autocorr-CLEAN, CG-CLEAN,\nand PolyCLEAN retain CLEANs workflow while incorporating modern optimization.\nWe argue hybrids are best suited for the near term, while Bayesian and AI-based\nframeworks represent the long-term future of interferometric imaging."
                },
                "authors": [
                    {
                        "name": "Hendrik Müller"
                    }
                ],
                "author_detail": {
                    "name": "Hendrik Müller"
                },
                "author": "Hendrik Müller",
                "arxiv_comment": "IAU conference proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15174v1",
                "updated": "2025-09-18T17:30:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    30,
                    36,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:30:36Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    30,
                    36,
                    3,
                    261,
                    0
                ],
                "title": "SMARTER: A Data-efficient Framework to Improve Toxicity Detection with\n  Explanation via Self-augmenting Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMARTER: A Data-efficient Framework to Improve Toxicity Detection with\n  Explanation via Self-augmenting Large Language Models"
                },
                "summary": "WARNING: This paper contains examples of offensive materials. Toxic content\nhas become pervasive on social media platforms. We introduce SMARTER, a\ndata-efficient two-stage framework for explainable content moderation using\nLarge Language Models (LLMs). In Stage 1, we leverage LLMs' own outputs to\ngenerate synthetic explanations for both correct and incorrect labels, enabling\nalignment via preference optimization with minimal human supervision. In Stage\n2, we refine explanation quality through cross-model training, allowing weaker\nmodels to align stylistically and semantically with stronger ones. Experiments\non three benchmark tasks -- HateXplain, Latent Hate, and Implicit Hate --\ndemonstrate that SMARTER enables LLMs to achieve up to a 13.5% macro-F1\nimprovement over standard few-shot baselines while using only a fraction of the\nfull training data. Our framework offers a scalable strategy for low-resource\nsettings by harnessing LLMs' self-improving capabilities for both\nclassification and explanation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WARNING: This paper contains examples of offensive materials. Toxic content\nhas become pervasive on social media platforms. We introduce SMARTER, a\ndata-efficient two-stage framework for explainable content moderation using\nLarge Language Models (LLMs). In Stage 1, we leverage LLMs' own outputs to\ngenerate synthetic explanations for both correct and incorrect labels, enabling\nalignment via preference optimization with minimal human supervision. In Stage\n2, we refine explanation quality through cross-model training, allowing weaker\nmodels to align stylistically and semantically with stronger ones. Experiments\non three benchmark tasks -- HateXplain, Latent Hate, and Implicit Hate --\ndemonstrate that SMARTER enables LLMs to achieve up to a 13.5% macro-F1\nimprovement over standard few-shot baselines while using only a fraction of the\nfull training data. Our framework offers a scalable strategy for low-resource\nsettings by harnessing LLMs' self-improving capabilities for both\nclassification and explanation."
                },
                "authors": [
                    {
                        "name": "Huy Nghiem"
                    },
                    {
                        "name": "Advik Sachdeva"
                    },
                    {
                        "name": "Hal Daumé III"
                    }
                ],
                "author_detail": {
                    "name": "Hal Daumé III"
                },
                "author": "Hal Daumé III",
                "arxiv_comment": "NLP, Hate speech detection, explanation, LLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15172v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15172v1",
                "updated": "2025-09-18T17:27:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    27,
                    28,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:27:28Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    27,
                    28,
                    3,
                    261,
                    0
                ],
                "title": "Internalizing Self-Consistency in Language Models: Multi-Agent Consensus\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internalizing Self-Consistency in Language Models: Multi-Agent Consensus\n  Alignment"
                },
                "summary": "Language Models (LMs) are inconsistent reasoners, often generating\ncontradictory responses to identical prompts. While inference-time methods can\nmitigate these inconsistencies, they fail to address the core problem: LMs\nstruggle to reliably select reasoning pathways leading to consistent outcomes\nunder exploratory sampling. To address this, we formalize self-consistency as\nan intrinsic property of well-aligned reasoning models and introduce\nMulti-Agent Consensus Alignment (MACA), a reinforcement learning framework that\npost-trains models to favor reasoning trajectories aligned with their internal\nconsensus using majority/minority outcomes from multi-agent debate. These\ntrajectories emerge from deliberative exchanges where agents ground reasoning\nin peer arguments, not just aggregation of independent attempts, creating\nricher consensus signals than single-round majority voting. MACA enables agents\nto teach themselves to be more decisive and concise, and better leverage peer\ninsights in multi-agent settings without external supervision, driving\nsubstantial improvements across self-consistency (+27.6% on GSM8K),\nsingle-agent reasoning (+23.7% on MATH), sampling-based inference (+22.4%\nPass@20 on MATH), and multi-agent ensemble decision-making (+42.7% on MathQA).\nThese findings, coupled with strong generalization to unseen benchmarks (+16.3%\non GPQA, +11.6% on CommonsenseQA), demonstrate robust self-alignment that more\nreliably unlocks latent reasoning potential of language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) are inconsistent reasoners, often generating\ncontradictory responses to identical prompts. While inference-time methods can\nmitigate these inconsistencies, they fail to address the core problem: LMs\nstruggle to reliably select reasoning pathways leading to consistent outcomes\nunder exploratory sampling. To address this, we formalize self-consistency as\nan intrinsic property of well-aligned reasoning models and introduce\nMulti-Agent Consensus Alignment (MACA), a reinforcement learning framework that\npost-trains models to favor reasoning trajectories aligned with their internal\nconsensus using majority/minority outcomes from multi-agent debate. These\ntrajectories emerge from deliberative exchanges where agents ground reasoning\nin peer arguments, not just aggregation of independent attempts, creating\nricher consensus signals than single-round majority voting. MACA enables agents\nto teach themselves to be more decisive and concise, and better leverage peer\ninsights in multi-agent settings without external supervision, driving\nsubstantial improvements across self-consistency (+27.6% on GSM8K),\nsingle-agent reasoning (+23.7% on MATH), sampling-based inference (+22.4%\nPass@20 on MATH), and multi-agent ensemble decision-making (+42.7% on MathQA).\nThese findings, coupled with strong generalization to unseen benchmarks (+16.3%\non GPQA, +11.6% on CommonsenseQA), demonstrate robust self-alignment that more\nreliably unlocks latent reasoning potential of language models."
                },
                "authors": [
                    {
                        "name": "Ankur Samanta"
                    },
                    {
                        "name": "Akshayaa Magesh"
                    },
                    {
                        "name": "Youliang Yu"
                    },
                    {
                        "name": "Runzhe Wu"
                    },
                    {
                        "name": "Ayush Jain"
                    },
                    {
                        "name": "Daniel Jiang"
                    },
                    {
                        "name": "Boris Vidolov"
                    },
                    {
                        "name": "Paul Sajda"
                    },
                    {
                        "name": "Yonathan Efroni"
                    },
                    {
                        "name": "Kaveh Hassani"
                    }
                ],
                "author_detail": {
                    "name": "Kaveh Hassani"
                },
                "author": "Kaveh Hassani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15172v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15165v1",
                "updated": "2025-09-18T17:16:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    16,
                    58,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:16:58Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    16,
                    58,
                    3,
                    261,
                    0
                ],
                "title": "Invariant Modeling for Joint Distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invariant Modeling for Joint Distributions"
                },
                "summary": "A common theme underlying many problems in statistics and economics involves\nthe determination of a systematic method of selecting a joint distribution\nconsistent with a specified list of categorical marginals, some of which have\nan ordinal structure. We propose guidance in narrowing down the set of possible\nmethods by introducing Invariant Aggregation (IA), a natural property that\nrequires merging adjacent categories in one marginal not to alter the joint\ndistribution over unaffected values. We prove that a model satisfies IA if and\nonly if it is a copula model. This characterization ensures i) robustness\nagainst data manipulation and survey design, and ii) allows seamless\nincorporation of new variables. Our results provide both theoretical clarity\nand practical safeguards for inference under marginal constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common theme underlying many problems in statistics and economics involves\nthe determination of a systematic method of selecting a joint distribution\nconsistent with a specified list of categorical marginals, some of which have\nan ordinal structure. We propose guidance in narrowing down the set of possible\nmethods by introducing Invariant Aggregation (IA), a natural property that\nrequires merging adjacent categories in one marginal not to alter the joint\ndistribution over unaffected values. We prove that a model satisfies IA if and\nonly if it is a copula model. This characterization ensures i) robustness\nagainst data manipulation and survey design, and ii) allows seamless\nincorporation of new variables. Our results provide both theoretical clarity\nand practical safeguards for inference under marginal constraints."
                },
                "authors": [
                    {
                        "name": "Christopher P. Chambers"
                    },
                    {
                        "name": "Yusufcan Masatlioglu"
                    },
                    {
                        "name": "Ruodu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ruodu Wang"
                },
                "author": "Ruodu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.TH",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15164v1",
                "updated": "2025-09-18T17:13:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    13,
                    48,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:13:48Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    13,
                    48,
                    3,
                    261,
                    0
                ],
                "title": "Bayesian inference for spatio-temporal hidden Markov models using the\n  exchange algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference for spatio-temporal hidden Markov models using the\n  exchange algorithm"
                },
                "summary": "Spatio-temporal hidden Markov models are extremely difficult to estimate\nbecause their latent joint distributions are available only in trivial cases.\nIn the estimation phase, these latent distributions are usually substituted\nwith pseudo-distributions, which could affect the estimation results, in\nparticular in the presence of strong dependence between the latent variables.\nIn this work, we propose a spatio-temporal hidden Markov model where the latent\nprocess is an extension of the autologistic model. We show how inference can be\ncarried out in a Bayesian framework using an approximate exchange algorithm,\nwhich circumvents the impractical calculations of the normalizing constants\nthat arise in the model. Our proposed method leads to a Markov chain Monte\nCarlo sampler that targets the correct posterior distribution of the model and\nnot a pseudo-posterior. In addition, we develop a new initialization approach\nfor the approximate exchange method, reducing the computational time of the\nalgorithm. An extensive simulation study shows that the approximate exchange\nalgorithm generally outperforms the pseudo-distribution approach, yielding more\naccurate parameter estimates. Finally, the proposed methodology is applied to a\nreal-world case study analyzing rainfall levels across Italian regions over\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatio-temporal hidden Markov models are extremely difficult to estimate\nbecause their latent joint distributions are available only in trivial cases.\nIn the estimation phase, these latent distributions are usually substituted\nwith pseudo-distributions, which could affect the estimation results, in\nparticular in the presence of strong dependence between the latent variables.\nIn this work, we propose a spatio-temporal hidden Markov model where the latent\nprocess is an extension of the autologistic model. We show how inference can be\ncarried out in a Bayesian framework using an approximate exchange algorithm,\nwhich circumvents the impractical calculations of the normalizing constants\nthat arise in the model. Our proposed method leads to a Markov chain Monte\nCarlo sampler that targets the correct posterior distribution of the model and\nnot a pseudo-posterior. In addition, we develop a new initialization approach\nfor the approximate exchange method, reducing the computational time of the\nalgorithm. An extensive simulation study shows that the approximate exchange\nalgorithm generally outperforms the pseudo-distribution approach, yielding more\naccurate parameter estimates. Finally, the proposed methodology is applied to a\nreal-world case study analyzing rainfall levels across Italian regions over\ntime."
                },
                "authors": [
                    {
                        "name": "Daniele Tancini"
                    },
                    {
                        "name": "Riccardo Rastelli"
                    },
                    {
                        "name": "Francesco Bartolucci"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Bartolucci"
                },
                "author": "Francesco Bartolucci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15159v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15159v1",
                "updated": "2025-09-18T17:06:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    6,
                    53,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:06:53Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    6,
                    53,
                    3,
                    261,
                    0
                ],
                "title": "AIP: Subverting Retrieval-Augmented Generation via Adversarial\n  Instructional Prompt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIP: Subverting Retrieval-Augmented Generation via Adversarial\n  Instructional Prompt"
                },
                "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nretrieving relevant documents from external sources to improve factual accuracy\nand verifiability. However, this reliance introduces new attack surfaces within\nthe retrieval pipeline, beyond the LLM itself. While prior RAG attacks have\nexposed such vulnerabilities, they largely rely on manipulating user queries,\nwhich is often infeasible in practice due to fixed or protected user inputs.\nThis narrow focus overlooks a more realistic and stealthy vector: instructional\nprompts, which are widely reused, publicly shared, and rarely audited. Their\nimplicit trust makes them a compelling target for adversaries to manipulate RAG\nbehavior covertly.\n  We introduce a novel attack for Adversarial Instructional Prompt (AIP) that\nexploits adversarial instructional prompts to manipulate RAG outputs by subtly\naltering retrieval behavior. By shifting the attack surface to the\ninstructional prompts, AIP reveals how trusted yet seemingly benign interface\ncomponents can be weaponized to degrade system integrity. The attack is crafted\nto achieve three goals: (1) naturalness, to evade user detection; (2) utility,\nto encourage use of prompts; and (3) robustness, to remain effective across\ndiverse query variations. We propose a diverse query generation strategy that\nsimulates realistic linguistic variation in user queries, enabling the\ndiscovery of prompts that generalize across paraphrases and rephrasings.\nBuilding on this, a genetic algorithm-based joint optimization is developed to\nevolve adversarial prompts by balancing attack success, clean-task utility, and\nstealthiness. Experimental results show that AIP achieves up to 95.23% ASR\nwhile preserving benign functionality. These findings uncover a critical and\npreviously overlooked vulnerability in RAG systems, emphasizing the need to\nreassess the shared instructional prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nretrieving relevant documents from external sources to improve factual accuracy\nand verifiability. However, this reliance introduces new attack surfaces within\nthe retrieval pipeline, beyond the LLM itself. While prior RAG attacks have\nexposed such vulnerabilities, they largely rely on manipulating user queries,\nwhich is often infeasible in practice due to fixed or protected user inputs.\nThis narrow focus overlooks a more realistic and stealthy vector: instructional\nprompts, which are widely reused, publicly shared, and rarely audited. Their\nimplicit trust makes them a compelling target for adversaries to manipulate RAG\nbehavior covertly.\n  We introduce a novel attack for Adversarial Instructional Prompt (AIP) that\nexploits adversarial instructional prompts to manipulate RAG outputs by subtly\naltering retrieval behavior. By shifting the attack surface to the\ninstructional prompts, AIP reveals how trusted yet seemingly benign interface\ncomponents can be weaponized to degrade system integrity. The attack is crafted\nto achieve three goals: (1) naturalness, to evade user detection; (2) utility,\nto encourage use of prompts; and (3) robustness, to remain effective across\ndiverse query variations. We propose a diverse query generation strategy that\nsimulates realistic linguistic variation in user queries, enabling the\ndiscovery of prompts that generalize across paraphrases and rephrasings.\nBuilding on this, a genetic algorithm-based joint optimization is developed to\nevolve adversarial prompts by balancing attack success, clean-task utility, and\nstealthiness. Experimental results show that AIP achieves up to 95.23% ASR\nwhile preserving benign functionality. These findings uncover a critical and\npreviously overlooked vulnerability in RAG systems, emphasizing the need to\nreassess the shared instructional prompts."
                },
                "authors": [
                    {
                        "name": "Saket S. Chaturvedi"
                    },
                    {
                        "name": "Gaurav Bagwe"
                    },
                    {
                        "name": "Lan Zhang"
                    },
                    {
                        "name": "Xiaoyong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyong Yuan"
                },
                "author": "Xiaoyong Yuan",
                "arxiv_comment": "Accepted at EMNLP 2025 Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15159v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15148v1",
                "updated": "2025-09-18T16:55:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    55,
                    9,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T16:55:09Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    55,
                    9,
                    3,
                    261,
                    0
                ],
                "title": "A1: Asynchronous Test-Time Scaling via Conformal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A1: Asynchronous Test-Time Scaling via Conformal Prediction"
                },
                "summary": "Large language models (LLMs) benefit from test-time scaling, but existing\nmethods face significant challenges, including severe synchronization overhead,\nmemory bottlenecks, and latency, especially during speculative decoding with\nlong reasoning chains. We introduce A1 (Asynchronous Test-Time Scaling), a\nstatistically guaranteed adaptive inference framework that addresses these\nchallenges. A1 refines arithmetic intensity to identify synchronization as the\ndominant bottleneck, proposes an online calibration strategy to enable\nasynchronous inference, and designs a three-stage rejection sampling pipeline\nthat supports both sequential and parallel scaling. Through experiments on the\nMATH, AMC23, AIME24, and AIME25 datasets, across various draft-target model\nfamilies, we demonstrate that A1 achieves a remarkable 56.7x speedup in\ntest-time scaling and a 4.14x improvement in throughput, all while maintaining\naccurate rejection-rate control, reducing latency and memory overhead, and no\naccuracy loss compared to using target model scaling alone. These results\nposition A1 as an efficient and principled solution for scalable LLM inference.\nWe have released the code at\nhttps://github.com/menik1126/asynchronous-test-time-scaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) benefit from test-time scaling, but existing\nmethods face significant challenges, including severe synchronization overhead,\nmemory bottlenecks, and latency, especially during speculative decoding with\nlong reasoning chains. We introduce A1 (Asynchronous Test-Time Scaling), a\nstatistically guaranteed adaptive inference framework that addresses these\nchallenges. A1 refines arithmetic intensity to identify synchronization as the\ndominant bottleneck, proposes an online calibration strategy to enable\nasynchronous inference, and designs a three-stage rejection sampling pipeline\nthat supports both sequential and parallel scaling. Through experiments on the\nMATH, AMC23, AIME24, and AIME25 datasets, across various draft-target model\nfamilies, we demonstrate that A1 achieves a remarkable 56.7x speedup in\ntest-time scaling and a 4.14x improvement in throughput, all while maintaining\naccurate rejection-rate control, reducing latency and memory overhead, and no\naccuracy loss compared to using target model scaling alone. These results\nposition A1 as an efficient and principled solution for scalable LLM inference.\nWe have released the code at\nhttps://github.com/menik1126/asynchronous-test-time-scaling."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Qiujiang Chen"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Chenyang Zhao"
                    },
                    {
                        "name": "Hui Shen"
                    },
                    {
                        "name": "Alexander Hanbo Li"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Haochen Tan"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "Tech Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11277v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11277v2",
                "updated": "2025-09-18T16:45:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    45,
                    16,
                    3,
                    261,
                    0
                ],
                "published": "2025-08-15T07:29:42Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    29,
                    42,
                    4,
                    227,
                    0
                ],
                "title": "Probing the Representational Power of Sparse Autoencoders in Vision\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing the Representational Power of Sparse Autoencoders in Vision\n  Models"
                },
                "summary": "Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting\nthe hidden states of large language models (LLMs). By learning to reconstruct\nactivations from a sparse bottleneck layer, SAEs discover interpretable\nfeatures from the high-dimensional internal representations of LLMs. Despite\ntheir popularity with language models, SAEs remain understudied in the visual\ndomain. In this work, we provide an extensive evaluation the representational\npower of SAEs for vision models using a broad range of image-based tasks. Our\nexperimental results demonstrate that SAE features are semantically meaningful,\nimprove out-of-distribution generalization, and enable controllable generation\nacross three vision model architectures: vision embedding models, multi-modal\nLMMs and diffusion models. In vision embedding models, we find that learned SAE\nfeatures can be used for OOD detection and provide evidence that they recover\nthe ontological structure of the underlying model. For diffusion models, we\ndemonstrate that SAEs enable semantic steering through text encoder\nmanipulation and develop an automated pipeline for discovering\nhuman-interpretable attributes. Finally, we conduct exploratory experiments on\nmulti-modal LLMs, finding evidence that SAE features reveal shared\nrepresentations across vision and language modalities. Our study provides a\nfoundation for SAE evaluation in vision models, highlighting their strong\npotential improving interpretability, generalization, and steerability in the\nvisual domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting\nthe hidden states of large language models (LLMs). By learning to reconstruct\nactivations from a sparse bottleneck layer, SAEs discover interpretable\nfeatures from the high-dimensional internal representations of LLMs. Despite\ntheir popularity with language models, SAEs remain understudied in the visual\ndomain. In this work, we provide an extensive evaluation the representational\npower of SAEs for vision models using a broad range of image-based tasks. Our\nexperimental results demonstrate that SAE features are semantically meaningful,\nimprove out-of-distribution generalization, and enable controllable generation\nacross three vision model architectures: vision embedding models, multi-modal\nLMMs and diffusion models. In vision embedding models, we find that learned SAE\nfeatures can be used for OOD detection and provide evidence that they recover\nthe ontological structure of the underlying model. For diffusion models, we\ndemonstrate that SAEs enable semantic steering through text encoder\nmanipulation and develop an automated pipeline for discovering\nhuman-interpretable attributes. Finally, we conduct exploratory experiments on\nmulti-modal LLMs, finding evidence that SAE features reveal shared\nrepresentations across vision and language modalities. Our study provides a\nfoundation for SAE evaluation in vision models, highlighting their strong\npotential improving interpretability, generalization, and steerability in the\nvisual domain."
                },
                "authors": [
                    {
                        "name": "Matthew Lyle Olson"
                    },
                    {
                        "name": "Musashi Hinck"
                    },
                    {
                        "name": "Neale Ratzlaff"
                    },
                    {
                        "name": "Changbai Li"
                    },
                    {
                        "name": "Phillip Howard"
                    },
                    {
                        "name": "Vasudev Lal"
                    },
                    {
                        "name": "Shao-Yen Tseng"
                    }
                ],
                "author_detail": {
                    "name": "Shao-Yen Tseng"
                },
                "author": "Shao-Yen Tseng",
                "arxiv_comment": "ICCV 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11277v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11277v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15132v1",
                "updated": "2025-09-18T16:42:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    42,
                    1,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T16:42:01Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    42,
                    1,
                    3,
                    261,
                    0
                ],
                "title": "From Pixels to Urban Policy-Intelligence: Recovering Legacy Effects of\n  Redlining with a Multimodal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Pixels to Urban Policy-Intelligence: Recovering Legacy Effects of\n  Redlining with a Multimodal LLM"
                },
                "summary": "This paper shows how a multimodal large language model (MLLM) can expand\nurban measurement capacity and support tracking of place-based policy\ninterventions. Using a structured, reason-then-estimate pipeline on street-view\nimagery, GPT-4o infers neighborhood poverty and tree canopy, which we embed in\na quasi-experimental design evaluating the legacy of 1930s redlining. GPT-4o\nrecovers the expected adverse socio-environmental legacy effects of redlining,\nwith estimates statistically indistinguishable from authoritative sources, and\nit outperforms a conventional pixel-based segmentation baseline-consistent with\nthe idea that holistic scene reasoning extracts higher-order information beyond\nobject counts alone. These results position MLLMs as policy-grade instruments\nfor neighborhood measurement and motivate broader validation across\npolicy-evaluation settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper shows how a multimodal large language model (MLLM) can expand\nurban measurement capacity and support tracking of place-based policy\ninterventions. Using a structured, reason-then-estimate pipeline on street-view\nimagery, GPT-4o infers neighborhood poverty and tree canopy, which we embed in\na quasi-experimental design evaluating the legacy of 1930s redlining. GPT-4o\nrecovers the expected adverse socio-environmental legacy effects of redlining,\nwith estimates statistically indistinguishable from authoritative sources, and\nit outperforms a conventional pixel-based segmentation baseline-consistent with\nthe idea that holistic scene reasoning extracts higher-order information beyond\nobject counts alone. These results position MLLMs as policy-grade instruments\nfor neighborhood measurement and motivate broader validation across\npolicy-evaluation settings."
                },
                "authors": [
                    {
                        "name": "Anthony Howell"
                    },
                    {
                        "name": "Nancy Wu"
                    },
                    {
                        "name": "Sharmistha Bagchi"
                    },
                    {
                        "name": "Yushim Kim"
                    },
                    {
                        "name": "Chayn Sun"
                    }
                ],
                "author_detail": {
                    "name": "Chayn Sun"
                },
                "author": "Chayn Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15131v1",
                "updated": "2025-09-18T16:41:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    41,
                    51,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T16:41:51Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    41,
                    51,
                    3,
                    261,
                    0
                ],
                "title": "Polarimeter to Unify the Corona and Heliosphere (PUNCH)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Polarimeter to Unify the Corona and Heliosphere (PUNCH)"
                },
                "summary": "The Polarimeter to Unify the Corona and Heliosphere (PUNCH) mission is a NASA\nSmall Explorer to determine the cross-scale processes that unify the solar\ncorona and heliosphere. PUNCH has two science objectives: (1) understand how\ncoronal structures become the ambient solar wind, and (2) understand the\ndynamic evolution of transient structures, such as coronal mass ejections, in\nthe young solar wind. To address these objectives, PUNCH uses a constellation\nof four small spacecraft in Sun-synchronous low Earth orbit, to collect\nlinearly polarized images of the K corona and young solar wind. The four\nspacecraft each carry one visible-light imager in a 1+3 configuration: a single\nNarrow Field Imager solar coronagraph captures images of the outer corona at\nall position angles, and at solar elongations from 1.5{\\deg} (6 R$_\\odot$) to\n8{\\deg} (32 R$_\\odot$); and three separate Wide Field Imager heliospheric\nimagers together capture views of the entire inner solar system, at solar\nelongations from 3{\\deg} (12 R$_\\odot$) to 45{\\deg} (180 R$_\\odot$) from the\nSun. PUNCH images include linear-polarization data, to enable inferring the\nthree-dimensional structure of visible features without stereoscopy. The\ninstruments are matched in wavelength passband, support overlapping\ninstantaneous fields of view, and are operated synchronously, to act as a\nsingle ``virtual instrument'' with a 90{\\deg} wide field of view, centered on\nthe Sun. PUNCH launched in March of 2025 and began science operations in June\nof 2025. PUNCH has an open data policy with no proprietary period, and PUNCH\nScience Team Meetings are open to all.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Polarimeter to Unify the Corona and Heliosphere (PUNCH) mission is a NASA\nSmall Explorer to determine the cross-scale processes that unify the solar\ncorona and heliosphere. PUNCH has two science objectives: (1) understand how\ncoronal structures become the ambient solar wind, and (2) understand the\ndynamic evolution of transient structures, such as coronal mass ejections, in\nthe young solar wind. To address these objectives, PUNCH uses a constellation\nof four small spacecraft in Sun-synchronous low Earth orbit, to collect\nlinearly polarized images of the K corona and young solar wind. The four\nspacecraft each carry one visible-light imager in a 1+3 configuration: a single\nNarrow Field Imager solar coronagraph captures images of the outer corona at\nall position angles, and at solar elongations from 1.5{\\deg} (6 R$_\\odot$) to\n8{\\deg} (32 R$_\\odot$); and three separate Wide Field Imager heliospheric\nimagers together capture views of the entire inner solar system, at solar\nelongations from 3{\\deg} (12 R$_\\odot$) to 45{\\deg} (180 R$_\\odot$) from the\nSun. PUNCH images include linear-polarization data, to enable inferring the\nthree-dimensional structure of visible features without stereoscopy. The\ninstruments are matched in wavelength passband, support overlapping\ninstantaneous fields of view, and are operated synchronously, to act as a\nsingle ``virtual instrument'' with a 90{\\deg} wide field of view, centered on\nthe Sun. PUNCH launched in March of 2025 and began science operations in June\nof 2025. PUNCH has an open data policy with no proprietary period, and PUNCH\nScience Team Meetings are open to all."
                },
                "authors": [
                    {
                        "name": "Craig DeForest"
                    },
                    {
                        "name": "Sarah Gibson"
                    },
                    {
                        "name": "Ronnie Killough"
                    },
                    {
                        "name": "Nick Waltham"
                    },
                    {
                        "name": "Matt Beasley"
                    },
                    {
                        "name": "Robin Colaninno"
                    },
                    {
                        "name": "Glenn Laurent"
                    },
                    {
                        "name": "Daniel Seaton"
                    },
                    {
                        "name": "Marcus Hughes"
                    },
                    {
                        "name": "Madhulika Guhathakurta"
                    },
                    {
                        "name": "Nicholeen Viall"
                    },
                    {
                        "name": "Raphael Attie"
                    },
                    {
                        "name": "Dipankar Banerjee"
                    },
                    {
                        "name": "Luke Barnar"
                    },
                    {
                        "name": "Doug Biesecker"
                    },
                    {
                        "name": "Mario Bisi"
                    },
                    {
                        "name": "Volker Bothmer"
                    },
                    {
                        "name": "Antonina Brody"
                    },
                    {
                        "name": "Joan Burkepile"
                    },
                    {
                        "name": "Iver Cairns"
                    },
                    {
                        "name": "Jennifer Campbell"
                    },
                    {
                        "name": "david Cheney"
                    },
                    {
                        "name": "Traci Case"
                    },
                    {
                        "name": "Amir Caspi"
                    },
                    {
                        "name": "Rohit Chhiber"
                    },
                    {
                        "name": "Matthew Clapp"
                    },
                    {
                        "name": "Steven Cranmer"
                    },
                    {
                        "name": "Jackie Davies"
                    },
                    {
                        "name": "Curt de Koning"
                    },
                    {
                        "name": "Mihir Desai"
                    },
                    {
                        "name": "Heather Elliott"
                    },
                    {
                        "name": "Samaiyah Farid"
                    },
                    {
                        "name": "Bea Gallardo-Lacourt"
                    },
                    {
                        "name": "Chris Gilly"
                    },
                    {
                        "name": "Caden Gobat"
                    },
                    {
                        "name": "Mary Hanson"
                    },
                    {
                        "name": "Richard Harrison"
                    },
                    {
                        "name": "Donald Hassler"
                    },
                    {
                        "name": "Chase Henley"
                    },
                    {
                        "name": "Alan Henry"
                    },
                    {
                        "name": "Russell Howard"
                    },
                    {
                        "name": "Bernard Jackson"
                    },
                    {
                        "name": "Samuel Jones"
                    },
                    {
                        "name": "Don Kolinski"
                    },
                    {
                        "name": "Derek Lamb"
                    },
                    {
                        "name": "Florine Lehtinen"
                    },
                    {
                        "name": "Chris Lowder"
                    },
                    {
                        "name": "Anna Malanushenko"
                    },
                    {
                        "name": "William Matthaeus"
                    },
                    {
                        "name": "David McComas"
                    },
                    {
                        "name": "Jacob McGee"
                    },
                    {
                        "name": "Huw Morgan"
                    },
                    {
                        "name": "Divya Oberoi"
                    },
                    {
                        "name": "Dusan Odstrcil"
                    },
                    {
                        "name": "Chris Parmenter"
                    },
                    {
                        "name": "Ritesh Patel"
                    },
                    {
                        "name": "Francesco Pecora"
                    },
                    {
                        "name": "Steve Persyn"
                    },
                    {
                        "name": "Victor Pizzo"
                    },
                    {
                        "name": "Simon Plunkett"
                    },
                    {
                        "name": "Elena Provornikova"
                    },
                    {
                        "name": "Nour Eddine Raouafi"
                    },
                    {
                        "name": "Jillian Redfern"
                    },
                    {
                        "name": "Alexis Rouillard"
                    },
                    {
                        "name": "Kelly Smith"
                    },
                    {
                        "name": "Keith Smith"
                    },
                    {
                        "name": "Zachary Talpas"
                    },
                    {
                        "name": "James Tappin"
                    },
                    {
                        "name": "Arnaud Thernisien"
                    },
                    {
                        "name": "Barbara Thompson"
                    },
                    {
                        "name": "Samuel Van Kooten"
                    },
                    {
                        "name": "Kevin Walsh"
                    },
                    {
                        "name": "David Webb"
                    },
                    {
                        "name": "William Wells"
                    },
                    {
                        "name": "Matthew West"
                    },
                    {
                        "name": "Zachary Wiens"
                    },
                    {
                        "name": "Yan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yan Yang"
                },
                "author": "Yan Yang",
                "arxiv_comment": "Submitted to the journal Solar Physics; this preprint is not yet peer\n  reviewed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15130v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15130v1",
                "updated": "2025-09-18T16:40:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    40,
                    47,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T16:40:47Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    40,
                    47,
                    3,
                    261,
                    0
                ],
                "title": "WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model\n  via Training-Free Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model\n  via Training-Free Guidance"
                },
                "summary": "Recent video diffusion models demonstrate strong potential in spatial\nintelligence tasks due to their rich latent world priors. However, this\npotential is hindered by their limited controllability and geometric\ninconsistency, creating a gap between their strong priors and their practical\nuse in 3D/4D tasks. As a result, current approaches often rely on retraining or\nfine-tuning, which risks degrading pretrained knowledge and incurs high\ncomputational costs. To address this, we propose WorldForge, a training-free,\ninference-time framework composed of three tightly coupled modules. Intra-Step\nRecursive Refinement introduces a recursive refinement mechanism during\ninference, which repeatedly optimizes network predictions within each denoising\nstep to enable precise trajectory injection. Flow-Gated Latent Fusion leverages\noptical flow similarity to decouple motion from appearance in the latent space\nand selectively inject trajectory guidance into motion-related channels.\nDual-Path Self-Corrective Guidance compares guided and unguided denoising paths\nto adaptively correct trajectory drift caused by noisy or misaligned structural\nsignals. Together, these components inject fine-grained, trajectory-aligned\nguidance without training, achieving both accurate motion control and\nphotorealistic content generation. Extensive experiments across diverse\nbenchmarks validate our method's superiority in realism, trajectory\nconsistency, and visual fidelity. This work introduces a novel plug-and-play\nparadigm for controllable video synthesis, offering a new perspective on\nleveraging generative priors for spatial intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent video diffusion models demonstrate strong potential in spatial\nintelligence tasks due to their rich latent world priors. However, this\npotential is hindered by their limited controllability and geometric\ninconsistency, creating a gap between their strong priors and their practical\nuse in 3D/4D tasks. As a result, current approaches often rely on retraining or\nfine-tuning, which risks degrading pretrained knowledge and incurs high\ncomputational costs. To address this, we propose WorldForge, a training-free,\ninference-time framework composed of three tightly coupled modules. Intra-Step\nRecursive Refinement introduces a recursive refinement mechanism during\ninference, which repeatedly optimizes network predictions within each denoising\nstep to enable precise trajectory injection. Flow-Gated Latent Fusion leverages\noptical flow similarity to decouple motion from appearance in the latent space\nand selectively inject trajectory guidance into motion-related channels.\nDual-Path Self-Corrective Guidance compares guided and unguided denoising paths\nto adaptively correct trajectory drift caused by noisy or misaligned structural\nsignals. Together, these components inject fine-grained, trajectory-aligned\nguidance without training, achieving both accurate motion control and\nphotorealistic content generation. Extensive experiments across diverse\nbenchmarks validate our method's superiority in realism, trajectory\nconsistency, and visual fidelity. This work introduces a novel plug-and-play\nparadigm for controllable video synthesis, offering a new perspective on\nleveraging generative priors for spatial intelligence."
                },
                "authors": [
                    {
                        "name": "Chenxi Song"
                    },
                    {
                        "name": "Yanming Yang"
                    },
                    {
                        "name": "Tong Zhao"
                    },
                    {
                        "name": "Ruibo Li"
                    },
                    {
                        "name": "Chi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chi Zhang"
                },
                "author": "Chi Zhang",
                "arxiv_comment": "Project Webpage: https://worldforge-agi.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15130v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15130v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20020v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20020v2",
                "updated": "2025-09-18T16:34:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    34,
                    54,
                    3,
                    261,
                    0
                ],
                "published": "2025-04-28T17:42:02Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    42,
                    2,
                    0,
                    118,
                    0
                ],
                "title": "Modular Machine Learning: An Indispensable Path towards New-Generation\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modular Machine Learning: An Indispensable Path towards New-Generation\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) have substantially advanced machine learning\nresearch, including natural language processing, computer vision, data mining,\netc., yet they still exhibit critical limitations in explainability,\nreliability, adaptability, and extensibility. In this paper, we overview a\npromising learning paradigm, i.e., Modular Machine Learning (MML), as an\nessential approach toward new-generation LLMs capable of addressing these\nissues. We begin by systematically and comprehensively surveying the existing\nliterature on modular machine learning, with a particular focus on modular data\nrepresentation and modular models. Then, we propose a unified MML framework for\nLLMs, which decomposes the complex structure of LLMs into three interdependent\ncomponents: modular representation, modular model, and modular reasoning.\nSpecifically, the MML paradigm discussed in this article is able to: i) clarify\nthe internal working mechanism of LLMs through the disentanglement of semantic\ncomponents; ii) allow for flexible and task-adaptive model design; iii) enable\nan interpretable and logic-driven decision-making process. We further elaborate\na feasible implementation of MML-based LLMs via leveraging advanced techniques\nsuch as disentangled representation learning, neural architecture search and\nneuro-symbolic learning. Last but not least, we critically identify the\nremaining key challenges, such as the integration of continuous neural and\ndiscrete symbolic processes, joint optimization, and computational scalability,\npresent promising future research directions that deserve further exploration.\nUltimately, we believe the integration of the MML with LLMs has the potential\nto bridge the gap between statistical (deep) learning and formal (logical)\nreasoning, thereby paving the way for robust, adaptable, and trustworthy AI\nsystems across a wide range of real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have substantially advanced machine learning\nresearch, including natural language processing, computer vision, data mining,\netc., yet they still exhibit critical limitations in explainability,\nreliability, adaptability, and extensibility. In this paper, we overview a\npromising learning paradigm, i.e., Modular Machine Learning (MML), as an\nessential approach toward new-generation LLMs capable of addressing these\nissues. We begin by systematically and comprehensively surveying the existing\nliterature on modular machine learning, with a particular focus on modular data\nrepresentation and modular models. Then, we propose a unified MML framework for\nLLMs, which decomposes the complex structure of LLMs into three interdependent\ncomponents: modular representation, modular model, and modular reasoning.\nSpecifically, the MML paradigm discussed in this article is able to: i) clarify\nthe internal working mechanism of LLMs through the disentanglement of semantic\ncomponents; ii) allow for flexible and task-adaptive model design; iii) enable\nan interpretable and logic-driven decision-making process. We further elaborate\na feasible implementation of MML-based LLMs via leveraging advanced techniques\nsuch as disentangled representation learning, neural architecture search and\nneuro-symbolic learning. Last but not least, we critically identify the\nremaining key challenges, such as the integration of continuous neural and\ndiscrete symbolic processes, joint optimization, and computational scalability,\npresent promising future research directions that deserve further exploration.\nUltimately, we believe the integration of the MML with LLMs has the potential\nto bridge the gap between statistical (deep) learning and formal (logical)\nreasoning, thereby paving the way for robust, adaptable, and trustworthy AI\nsystems across a wide range of real-world applications."
                },
                "authors": [
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Haibo Chen"
                    },
                    {
                        "name": "Zeyang Zhang"
                    },
                    {
                        "name": "Wenwu Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Wenwu Zhu"
                },
                "author": "Wenwu Zhu",
                "arxiv_comment": "20 pages, 4 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20020v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20020v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15124v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15124v1",
                "updated": "2025-09-18T16:29:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    29,
                    45,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T16:29:45Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    29,
                    45,
                    3,
                    261,
                    0
                ],
                "title": "Learning Mechanistic Subtypes of Neurodegeneration with a\n  Physics-Informed Variational Autoencoder Mixture Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Mechanistic Subtypes of Neurodegeneration with a\n  Physics-Informed Variational Autoencoder Mixture Model"
                },
                "summary": "Modelling the underlying mechanisms of neurodegenerative diseases demands\nmethods that capture heterogeneous and spatially varying dynamics from sparse,\nhigh-dimensional neuroimaging data. Integrating partial differential equation\n(PDE) based physics knowledge with machine learning provides enhanced\ninterpretability and utility over classic numerical methods. However, current\nphysics-integrated machine learning methods are limited to considering a single\nPDE, severely limiting their application to diseases where multiple mechanisms\nare responsible for different groups (i.e., subtypes) and aggravating problems\nwith model misspecification and degeneracy. Here, we present a deep generative\nmodel for learning mixtures of latent dynamic models governed by physics-based\nPDEs, going beyond traditional approaches that assume a single PDE structure.\nOur method integrates reaction-diffusion PDEs within a variational autoencoder\n(VAE) mixture model framework, supporting inference of subtypes of\ninterpretable latent variables (e.g. diffusivity and reaction rates) from\nneuroimaging data. We evaluate our method on synthetic benchmarks and\ndemonstrate its potential for uncovering mechanistic subtypes of Alzheimer's\ndisease progression from positron emission tomography (PET) data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling the underlying mechanisms of neurodegenerative diseases demands\nmethods that capture heterogeneous and spatially varying dynamics from sparse,\nhigh-dimensional neuroimaging data. Integrating partial differential equation\n(PDE) based physics knowledge with machine learning provides enhanced\ninterpretability and utility over classic numerical methods. However, current\nphysics-integrated machine learning methods are limited to considering a single\nPDE, severely limiting their application to diseases where multiple mechanisms\nare responsible for different groups (i.e., subtypes) and aggravating problems\nwith model misspecification and degeneracy. Here, we present a deep generative\nmodel for learning mixtures of latent dynamic models governed by physics-based\nPDEs, going beyond traditional approaches that assume a single PDE structure.\nOur method integrates reaction-diffusion PDEs within a variational autoencoder\n(VAE) mixture model framework, supporting inference of subtypes of\ninterpretable latent variables (e.g. diffusivity and reaction rates) from\nneuroimaging data. We evaluate our method on synthetic benchmarks and\ndemonstrate its potential for uncovering mechanistic subtypes of Alzheimer's\ndisease progression from positron emission tomography (PET) data."
                },
                "authors": [
                    {
                        "name": "Sanduni Pinnawala"
                    },
                    {
                        "name": "Annabelle Hartanto"
                    },
                    {
                        "name": "Ivor J. A. Simpson"
                    },
                    {
                        "name": "Peter A. Wijeratne"
                    }
                ],
                "author_detail": {
                    "name": "Peter A. Wijeratne"
                },
                "author": "Peter A. Wijeratne",
                "arxiv_comment": "13 pages, 5 figures, accepted at SASHIMI workshop, MICCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15124v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15124v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05129v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05129v2",
                "updated": "2025-09-18T16:29:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    29,
                    0,
                    3,
                    261,
                    0
                ],
                "published": "2025-07-07T15:41:38Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    41,
                    38,
                    0,
                    188,
                    0
                ],
                "title": "SMART: Simulated Students Aligned with Item Response Theory for Question\n  Difficulty Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMART: Simulated Students Aligned with Item Response Theory for Question\n  Difficulty Prediction"
                },
                "summary": "Item (question) difficulties play a crucial role in educational assessments,\nenabling accurate and efficient assessment of student abilities and\npersonalization to maximize learning outcomes. Traditionally, estimating item\ndifficulties can be costly, requiring real students to respond to items,\nfollowed by fitting an item response theory (IRT) model to get difficulty\nestimates. This approach cannot be applied to the cold-start setting for\npreviously unseen items either. In this work, we present SMART (Simulated\nStudents Aligned with IRT), a novel method for aligning simulated students with\ninstructed ability, which can then be used in simulations to predict the\ndifficulty of open-ended items. We achieve this alignment using direct\npreference optimization (DPO), where we form preference pairs based on how\nlikely responses are under a ground-truth IRT model. We perform a simulation by\ngenerating thousands of responses, evaluating them with a large language model\n(LLM)-based scoring model, and fit the resulting data to an IRT model to obtain\nitem difficulty estimates. Through extensive experiments on two real-world\nstudent response datasets, we show that SMART outperforms other item difficulty\nprediction methods by leveraging its improved ability alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Item (question) difficulties play a crucial role in educational assessments,\nenabling accurate and efficient assessment of student abilities and\npersonalization to maximize learning outcomes. Traditionally, estimating item\ndifficulties can be costly, requiring real students to respond to items,\nfollowed by fitting an item response theory (IRT) model to get difficulty\nestimates. This approach cannot be applied to the cold-start setting for\npreviously unseen items either. In this work, we present SMART (Simulated\nStudents Aligned with IRT), a novel method for aligning simulated students with\ninstructed ability, which can then be used in simulations to predict the\ndifficulty of open-ended items. We achieve this alignment using direct\npreference optimization (DPO), where we form preference pairs based on how\nlikely responses are under a ground-truth IRT model. We perform a simulation by\ngenerating thousands of responses, evaluating them with a large language model\n(LLM)-based scoring model, and fit the resulting data to an IRT model to obtain\nitem difficulty estimates. Through extensive experiments on two real-world\nstudent response datasets, we show that SMART outperforms other item difficulty\nprediction methods by leveraging its improved ability alignment."
                },
                "authors": [
                    {
                        "name": "Alexander Scarlatos"
                    },
                    {
                        "name": "Nigel Fernandez"
                    },
                    {
                        "name": "Christopher Ormerod"
                    },
                    {
                        "name": "Susan Lottridge"
                    },
                    {
                        "name": "Andrew Lan"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Lan"
                },
                "author": "Andrew Lan",
                "arxiv_comment": "Published in EMNLP 2025: The 2025 Conference on Empirical Methods in\n  Natural Language Processing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05129v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05129v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15122v1",
                "updated": "2025-09-18T16:28:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    28,
                    19,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T16:28:19Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    28,
                    19,
                    3,
                    261,
                    0
                ],
                "title": "Prestige over merit: An adapted audit of LLM bias in peer review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prestige over merit: An adapted audit of LLM bias in peer review"
                },
                "summary": "Large language models (LLMs) are playing an increasingly integral, though\nlargely informal, role in scholarly peer review. Yet it remains unclear whether\nLLMs reproduce the biases observed in human decision-making. We adapt a\nresume-style audit to scientific publishing, developing a multi-role LLM\nsimulation (editor/reviewer) that evaluates a representative set of\nhigh-quality manuscripts across the physical, biological, and social sciences\nunder randomized author identities (institutional prestige, gender, race). The\naudit reveals a strong and consistent institutional-prestige bias: identical\npapers attributed to low-prestige affiliations face a significantly higher risk\nof rejection, despite only modest differences in LLM-assessed quality. To probe\nmechanisms, we generate synthetic CVs for the same author profiles; these\nencode large prestige-linked disparities and an inverted prestige-tenure\ngradient relative to national benchmarks. The results suggest that both domain\nnorms and prestige-linked priors embedded in training data shape paper-level\noutcomes once identity is visible, converting affiliation into a decisive\nstatus cue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are playing an increasingly integral, though\nlargely informal, role in scholarly peer review. Yet it remains unclear whether\nLLMs reproduce the biases observed in human decision-making. We adapt a\nresume-style audit to scientific publishing, developing a multi-role LLM\nsimulation (editor/reviewer) that evaluates a representative set of\nhigh-quality manuscripts across the physical, biological, and social sciences\nunder randomized author identities (institutional prestige, gender, race). The\naudit reveals a strong and consistent institutional-prestige bias: identical\npapers attributed to low-prestige affiliations face a significantly higher risk\nof rejection, despite only modest differences in LLM-assessed quality. To probe\nmechanisms, we generate synthetic CVs for the same author profiles; these\nencode large prestige-linked disparities and an inverted prestige-tenure\ngradient relative to national benchmarks. The results suggest that both domain\nnorms and prestige-linked priors embedded in training data shape paper-level\noutcomes once identity is visible, converting affiliation into a decisive\nstatus cue."
                },
                "authors": [
                    {
                        "name": "Anthony Howell"
                    },
                    {
                        "name": "Jieshu Wang"
                    },
                    {
                        "name": "Luyu Du"
                    },
                    {
                        "name": "Julia Melkers"
                    },
                    {
                        "name": "Varshil Shah"
                    }
                ],
                "author_detail": {
                    "name": "Varshil Shah"
                },
                "author": "Varshil Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16815v2",
                "updated": "2025-09-18T16:26:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    26,
                    53,
                    3,
                    261,
                    0
                ],
                "published": "2025-07-22T17:59:46Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    59,
                    46,
                    1,
                    203,
                    0
                ],
                "title": "ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent\n  Planning"
                },
                "summary": "Vision-language-action (VLA) reasoning tasks require agents to interpret\nmultimodal instructions, perform long-horizon planning, and act adaptively in\ndynamic environments. Existing approaches typically train VLA models in an\nend-to-end fashion, directly mapping inputs to actions without explicit\nreasoning, which hinders their ability to plan over multiple steps or adapt to\ncomplex task variations. In this paper, we propose ThinkAct, a dual-system\nframework that bridges high-level reasoning with low-level action execution via\nreinforced visual latent planning. ThinkAct trains a multimodal LLM to generate\nembodied reasoning plans guided by reinforcing action-aligned visual rewards\nbased on goal completion and trajectory consistency. These reasoning plans are\ncompressed into a visual plan latent that conditions a downstream action model\nfor robust action execution on target environments. Extensive experiments on\nembodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct\nenables few-shot adaptation, long-horizon planning, and self-correction\nbehaviors in complex embodied AI tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language-action (VLA) reasoning tasks require agents to interpret\nmultimodal instructions, perform long-horizon planning, and act adaptively in\ndynamic environments. Existing approaches typically train VLA models in an\nend-to-end fashion, directly mapping inputs to actions without explicit\nreasoning, which hinders their ability to plan over multiple steps or adapt to\ncomplex task variations. In this paper, we propose ThinkAct, a dual-system\nframework that bridges high-level reasoning with low-level action execution via\nreinforced visual latent planning. ThinkAct trains a multimodal LLM to generate\nembodied reasoning plans guided by reinforcing action-aligned visual rewards\nbased on goal completion and trajectory consistency. These reasoning plans are\ncompressed into a visual plan latent that conditions a downstream action model\nfor robust action execution on target environments. Extensive experiments on\nembodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct\nenables few-shot adaptation, long-horizon planning, and self-correction\nbehaviors in complex embodied AI tasks."
                },
                "authors": [
                    {
                        "name": "Chi-Pin Huang"
                    },
                    {
                        "name": "Yueh-Hua Wu"
                    },
                    {
                        "name": "Min-Hung Chen"
                    },
                    {
                        "name": "Yu-Chiang Frank Wang"
                    },
                    {
                        "name": "Fu-En Yang"
                    }
                ],
                "author_detail": {
                    "name": "Fu-En Yang"
                },
                "author": "Fu-En Yang",
                "arxiv_comment": "NeurIPS 2025. Project page:\n  https://jasper0314-huang.github.io/thinkact-vla/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02279v2",
                "updated": "2025-09-18T16:24:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    24,
                    21,
                    3,
                    261,
                    0
                ],
                "published": "2025-06-02T21:38:21Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    21,
                    38,
                    21,
                    0,
                    153,
                    0
                ],
                "title": "ImpRAG: Retrieval-Augmented Generation with Implicit Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ImpRAG: Retrieval-Augmented Generation with Implicit Queries"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems traditionally treat retrieval\nand generation as separate processes, requiring explicit textual queries to\nconnect them. This separation can limit the ability of models to generalize\nacross diverse tasks. In this work, we propose a query-free RAG system, named\nImpRAG, which integrates retrieval and generation into a unified model. ImpRAG\nallows models to implicitly express their information needs, eliminating the\nneed for human-specified queries. By dividing pretrained decoder-only language\nmodels into specialized layer groups, ImpRAG optimizes retrieval and generation\ntasks simultaneously. Our approach employs a two-stage inference process, using\nthe same model parameters and forward pass for both retrieval and generation,\nthereby minimizing the disparity between retrievers and language models.\nExperiments on 8 knowledge-intensive tasks demonstrate that ImpRAG achieves\n3.6-11.5 improvements in exact match scores on unseen tasks with diverse\nformats, highlighting its effectiveness in enabling models to articulate their\nown information needs and generalize across tasks. Our analysis underscores the\nimportance of balancing retrieval and generation parameters and leveraging\ngeneration perplexities as retrieval training objectives for enhanced\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems traditionally treat retrieval\nand generation as separate processes, requiring explicit textual queries to\nconnect them. This separation can limit the ability of models to generalize\nacross diverse tasks. In this work, we propose a query-free RAG system, named\nImpRAG, which integrates retrieval and generation into a unified model. ImpRAG\nallows models to implicitly express their information needs, eliminating the\nneed for human-specified queries. By dividing pretrained decoder-only language\nmodels into specialized layer groups, ImpRAG optimizes retrieval and generation\ntasks simultaneously. Our approach employs a two-stage inference process, using\nthe same model parameters and forward pass for both retrieval and generation,\nthereby minimizing the disparity between retrievers and language models.\nExperiments on 8 knowledge-intensive tasks demonstrate that ImpRAG achieves\n3.6-11.5 improvements in exact match scores on unseen tasks with diverse\nformats, highlighting its effectiveness in enabling models to articulate their\nown information needs and generalize across tasks. Our analysis underscores the\nimportance of balancing retrieval and generation parameters and leveraging\ngeneration perplexities as retrieval training objectives for enhanced\nperformance."
                },
                "authors": [
                    {
                        "name": "Wenzheng Zhang"
                    },
                    {
                        "name": "Xi Victoria Lin"
                    },
                    {
                        "name": "Karl Stratos"
                    },
                    {
                        "name": "Wen-tau Yih"
                    },
                    {
                        "name": "Mingda Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingda Chen"
                },
                "author": "Mingda Chen",
                "arxiv_comment": "Findings of EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15113v1",
                "updated": "2025-09-18T16:17:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    17,
                    44,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T16:17:44Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    17,
                    44,
                    3,
                    261,
                    0
                ],
                "title": "Low-rank surrogate modeling and stochastic zero-order optimization for\n  training of neural networks with black-box layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rank surrogate modeling and stochastic zero-order optimization for\n  training of neural networks with black-box layers"
                },
                "summary": "The growing demand for energy-efficient, high-performance AI systems has led\nto increased attention on alternative computing platforms (e.g., photonic,\nneuromorphic) due to their potential to accelerate learning and inference.\nHowever, integrating such physical components into deep learning pipelines\nremains challenging, as physical devices often offer limited expressiveness,\nand their non-differentiable nature renders on-device backpropagation difficult\nor infeasible. This motivates the development of hybrid architectures that\ncombine digital neural networks with reconfigurable physical layers, which\neffectively behave as black boxes. In this work, we present a framework for the\nend-to-end training of such hybrid networks. This framework integrates\nstochastic zeroth-order optimization for updating the physical layer's internal\nparameters with a dynamic low-rank surrogate model that enables gradient\npropagation through the physical layer. A key component of our approach is the\nimplicit projector-splitting integrator algorithm, which updates the\nlightweight surrogate model after each forward pass with minimal hardware\nqueries, thereby avoiding costly full matrix reconstruction. We demonstrate our\nmethod across diverse deep learning tasks, including: computer vision, audio\nclassification, and language modeling. Notably, across all modalities, the\nproposed approach achieves near-digital baseline accuracy and consistently\nenables effective end-to-end training of hybrid models incorporating various\nnon-differentiable physical components (spatial light modulators, microring\nresonators, and Mach-Zehnder interferometers). This work bridges hardware-aware\ndeep learning and gradient-free optimization, thereby offering a practical\npathway for integrating non-differentiable physical components into scalable,\nend-to-end trainable AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for energy-efficient, high-performance AI systems has led\nto increased attention on alternative computing platforms (e.g., photonic,\nneuromorphic) due to their potential to accelerate learning and inference.\nHowever, integrating such physical components into deep learning pipelines\nremains challenging, as physical devices often offer limited expressiveness,\nand their non-differentiable nature renders on-device backpropagation difficult\nor infeasible. This motivates the development of hybrid architectures that\ncombine digital neural networks with reconfigurable physical layers, which\neffectively behave as black boxes. In this work, we present a framework for the\nend-to-end training of such hybrid networks. This framework integrates\nstochastic zeroth-order optimization for updating the physical layer's internal\nparameters with a dynamic low-rank surrogate model that enables gradient\npropagation through the physical layer. A key component of our approach is the\nimplicit projector-splitting integrator algorithm, which updates the\nlightweight surrogate model after each forward pass with minimal hardware\nqueries, thereby avoiding costly full matrix reconstruction. We demonstrate our\nmethod across diverse deep learning tasks, including: computer vision, audio\nclassification, and language modeling. Notably, across all modalities, the\nproposed approach achieves near-digital baseline accuracy and consistently\nenables effective end-to-end training of hybrid models incorporating various\nnon-differentiable physical components (spatial light modulators, microring\nresonators, and Mach-Zehnder interferometers). This work bridges hardware-aware\ndeep learning and gradient-free optimization, thereby offering a practical\npathway for integrating non-differentiable physical components into scalable,\nend-to-end trainable AI systems."
                },
                "authors": [
                    {
                        "name": "Andrei Chertkov"
                    },
                    {
                        "name": "Artem Basharin"
                    },
                    {
                        "name": "Mikhail Saygin"
                    },
                    {
                        "name": "Evgeny Frolov"
                    },
                    {
                        "name": "Stanislav Straupe"
                    },
                    {
                        "name": "Ivan Oseledets"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Oseledets"
                },
                "author": "Ivan Oseledets",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10943v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10943v2",
                "updated": "2025-09-18T16:17:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    17,
                    30,
                    3,
                    261,
                    0
                ],
                "published": "2025-06-12T17:48:13Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    17,
                    48,
                    13,
                    3,
                    163,
                    0
                ],
                "title": "Self-Adapting Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Adapting Language Models"
                },
                "summary": "Large language models (LLMs) are powerful but static; they lack mechanisms to\nadapt their weights in response to new tasks, knowledge, or examples. We\nintroduce Self-Adapting LLMs (SEAL), a framework that enables LLMs to\nself-adapt by generating their own finetuning data and update directives. Given\na new input, the model produces a self-edit-a generation that may restructure\nthe information in different ways, specify optimization hyperparameters, or\ninvoke tools for data augmentation and gradient-based updates. Through\nsupervised finetuning (SFT), these self-edits result in persistent weight\nupdates, enabling lasting adaptation. To train the model to produce effective\nself-edits, we use a reinforcement learning loop with the downstream\nperformance of the updated model as the reward signal. Unlike prior approaches\nthat rely on separate adaptation modules or auxiliary networks, SEAL directly\nuses the model's own generation to control its adaptation process. Experiments\non knowledge incorporation and few-shot generalization show that SEAL is a\npromising step toward language models capable of self-directed adaptation. Our\nwebsite and code is available at https://jyopari.github.io/posts/seal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are powerful but static; they lack mechanisms to\nadapt their weights in response to new tasks, knowledge, or examples. We\nintroduce Self-Adapting LLMs (SEAL), a framework that enables LLMs to\nself-adapt by generating their own finetuning data and update directives. Given\na new input, the model produces a self-edit-a generation that may restructure\nthe information in different ways, specify optimization hyperparameters, or\ninvoke tools for data augmentation and gradient-based updates. Through\nsupervised finetuning (SFT), these self-edits result in persistent weight\nupdates, enabling lasting adaptation. To train the model to produce effective\nself-edits, we use a reinforcement learning loop with the downstream\nperformance of the updated model as the reward signal. Unlike prior approaches\nthat rely on separate adaptation modules or auxiliary networks, SEAL directly\nuses the model's own generation to control its adaptation process. Experiments\non knowledge incorporation and few-shot generalization show that SEAL is a\npromising step toward language models capable of self-directed adaptation. Our\nwebsite and code is available at https://jyopari.github.io/posts/seal."
                },
                "authors": [
                    {
                        "name": "Adam Zweiger"
                    },
                    {
                        "name": "Jyothish Pari"
                    },
                    {
                        "name": "Han Guo"
                    },
                    {
                        "name": "Ekin Akyürek"
                    },
                    {
                        "name": "Yoon Kim"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    }
                ],
                "author_detail": {
                    "name": "Pulkit Agrawal"
                },
                "author": "Pulkit Agrawal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10943v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15110v1",
                "updated": "2025-09-18T16:14:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    14,
                    34,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T16:14:34Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    14,
                    34,
                    3,
                    261,
                    0
                ],
                "title": "TDRM: Smooth Reward Models with Temporal Difference for LLM RL and\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TDRM: Smooth Reward Models with Temporal Difference for LLM RL and\n  Inference"
                },
                "summary": "Reward models are central to both reinforcement learning (RL) with language\nmodels and inference-time verification. However, existing reward models often\nlack temporal consistency, leading to ineffective policy updates and unstable\nRL training. We introduce TDRM, a method for learning smoother and more\nreliable reward models by minimizing temporal differences during training. This\ntemporal-difference (TD) regularization produces smooth rewards and improves\nalignment with long-term objectives. Incorporating TDRM into the actor-critic\nstyle online RL loop yields consistent empirical gains. It is worth noting that\nTDRM is a supplement to verifiable reward methods, and both can be used in\nseries. Experiments show that TD-trained process reward models (PRMs) improve\nperformance across Best-of-N (up to 6.6%) and tree-search (up to 23.7%)\nsettings. When combined with Reinforcement Learning with Verifiable Rewards\n(RLVR), TD-trained PRMs lead to more data-efficient RL -- achieving comparable\nperformance with just 2.5k data to what baseline methods require 50.1k data to\nattain -- and yield higher-quality language model policies on 8 model variants\n(5 series), e.g., Qwen2.5-(0.5B, 1,5B), GLM4-9B-0414, GLM-Z1-9B-0414,\nQwen2.5-Math-(1.5B, 7B), and DeepSeek-R1-Distill-Qwen-(1.5B, 7B). We release\nall code at https://github.com/THUDM/TDRM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models are central to both reinforcement learning (RL) with language\nmodels and inference-time verification. However, existing reward models often\nlack temporal consistency, leading to ineffective policy updates and unstable\nRL training. We introduce TDRM, a method for learning smoother and more\nreliable reward models by minimizing temporal differences during training. This\ntemporal-difference (TD) regularization produces smooth rewards and improves\nalignment with long-term objectives. Incorporating TDRM into the actor-critic\nstyle online RL loop yields consistent empirical gains. It is worth noting that\nTDRM is a supplement to verifiable reward methods, and both can be used in\nseries. Experiments show that TD-trained process reward models (PRMs) improve\nperformance across Best-of-N (up to 6.6%) and tree-search (up to 23.7%)\nsettings. When combined with Reinforcement Learning with Verifiable Rewards\n(RLVR), TD-trained PRMs lead to more data-efficient RL -- achieving comparable\nperformance with just 2.5k data to what baseline methods require 50.1k data to\nattain -- and yield higher-quality language model policies on 8 model variants\n(5 series), e.g., Qwen2.5-(0.5B, 1,5B), GLM4-9B-0414, GLM-Z1-9B-0414,\nQwen2.5-Math-(1.5B, 7B), and DeepSeek-R1-Distill-Qwen-(1.5B, 7B). We release\nall code at https://github.com/THUDM/TDRM."
                },
                "authors": [
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Min Cai"
                    },
                    {
                        "name": "Jonathan Li"
                    },
                    {
                        "name": "Ziniu Hu"
                    },
                    {
                        "name": "Yisong Yue"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "arxiv_comment": "9 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15099v1",
                "updated": "2025-09-18T15:56:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    56,
                    20,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:56:20Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    56,
                    20,
                    3,
                    261,
                    0
                ],
                "title": "Digital Twin-based Cooperative Autonomous Driving in Smart\n  Intersections: A Multi-Agent Reinforcement Learning Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Twin-based Cooperative Autonomous Driving in Smart\n  Intersections: A Multi-Agent Reinforcement Learning Approach"
                },
                "summary": "Unsignalized intersections pose safety and efficiency challenges due to\ncomplex traffic flows and blind spots. In this paper, a digital twin (DT)-based\ncooperative driving system with roadside unit (RSU)-centric architecture is\nproposed for enhancing safety and efficiency at unsignalized intersections. The\nsystem leverages comprehensive bird-eye-view (BEV) perception to eliminate\nblind spots and employs a hybrid reinforcement learning (RL) framework\ncombining offline pre-training with online fine-tuning. Specifically, driving\npolicies are initially trained using conservative Q-learning (CQL) with\nbehavior cloning (BC) on real datasets, then fine-tuned using multi-agent\nproximal policy optimization (MAPPO) with self-attention mechanisms to handle\ndynamic multi-agent coordination. The RSU implements real-time commands via\nvehicle-to-infrastructure (V2I) communications. Experimental results show that\nthe proposed method yields failure rates below 0.03\\% coordinating up to three\nconnected autonomous vehicles (CAVs), significantly outperforming traditional\nmethods. In addition, the system exhibits sub-linear computational scaling with\ninference times under 40 ms. Furthermore, it demonstrates robust generalization\nacross diverse unsignalized intersection scenarios, indicating its practicality\nand readiness for real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsignalized intersections pose safety and efficiency challenges due to\ncomplex traffic flows and blind spots. In this paper, a digital twin (DT)-based\ncooperative driving system with roadside unit (RSU)-centric architecture is\nproposed for enhancing safety and efficiency at unsignalized intersections. The\nsystem leverages comprehensive bird-eye-view (BEV) perception to eliminate\nblind spots and employs a hybrid reinforcement learning (RL) framework\ncombining offline pre-training with online fine-tuning. Specifically, driving\npolicies are initially trained using conservative Q-learning (CQL) with\nbehavior cloning (BC) on real datasets, then fine-tuned using multi-agent\nproximal policy optimization (MAPPO) with self-attention mechanisms to handle\ndynamic multi-agent coordination. The RSU implements real-time commands via\nvehicle-to-infrastructure (V2I) communications. Experimental results show that\nthe proposed method yields failure rates below 0.03\\% coordinating up to three\nconnected autonomous vehicles (CAVs), significantly outperforming traditional\nmethods. In addition, the system exhibits sub-linear computational scaling with\ninference times under 40 ms. Furthermore, it demonstrates robust generalization\nacross diverse unsignalized intersection scenarios, indicating its practicality\nand readiness for real-world deployment."
                },
                "authors": [
                    {
                        "name": "Taoyuan Yu"
                    },
                    {
                        "name": "Kui Wang"
                    },
                    {
                        "name": "Zongdian Li"
                    },
                    {
                        "name": "Tao Yu"
                    },
                    {
                        "name": "Kei Sakaguchi"
                    },
                    {
                        "name": "Walid Saad"
                    }
                ],
                "author_detail": {
                    "name": "Walid Saad"
                },
                "author": "Walid Saad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15098v1",
                "updated": "2025-09-18T15:55:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    55,
                    19,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:55:19Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    55,
                    19,
                    3,
                    261,
                    0
                ],
                "title": "TextMine: LLM-Powered Knowledge Extraction for Humanitarian Mine Action",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextMine: LLM-Powered Knowledge Extraction for Humanitarian Mine Action"
                },
                "summary": "Humanitarian Mine Action has generated extensive best-practice knowledge, but\nmuch remains locked in unstructured reports. We introduce TextMine, an\nontology-guided pipeline that uses Large Language Models to extract knowledge\ntriples from HMA texts. TextMine integrates document chunking, domain-aware\nprompting, triple extraction, and both reference-based and LLM-as-a-Judge\nevaluation. We also create the first HMA ontology and a curated dataset of\nreal-world demining reports. Experiments show ontology-aligned prompts boost\nextraction accuracy by 44.2%, cut hallucinations by 22.5%, and improve format\nconformance by 20.9% over baselines. While validated on Cambodian reports,\nTextMine can adapt to global demining efforts or other domains, transforming\nunstructured data into structured knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humanitarian Mine Action has generated extensive best-practice knowledge, but\nmuch remains locked in unstructured reports. We introduce TextMine, an\nontology-guided pipeline that uses Large Language Models to extract knowledge\ntriples from HMA texts. TextMine integrates document chunking, domain-aware\nprompting, triple extraction, and both reference-based and LLM-as-a-Judge\nevaluation. We also create the first HMA ontology and a curated dataset of\nreal-world demining reports. Experiments show ontology-aligned prompts boost\nextraction accuracy by 44.2%, cut hallucinations by 22.5%, and improve format\nconformance by 20.9% over baselines. While validated on Cambodian reports,\nTextMine can adapt to global demining efforts or other domains, transforming\nunstructured data into structured knowledge."
                },
                "authors": [
                    {
                        "name": "Chenyue Zhou"
                    },
                    {
                        "name": "Gürkan Solmaz"
                    },
                    {
                        "name": "Flavio Cirillo"
                    },
                    {
                        "name": "Kiril Gashteovski"
                    },
                    {
                        "name": "Jonathan Fürst"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Fürst"
                },
                "author": "Jonathan Fürst",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15097v1",
                "updated": "2025-09-18T15:54:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    54,
                    15,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:54:15Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    54,
                    15,
                    3,
                    261,
                    0
                ],
                "title": "The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based\n  Incremental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based\n  Incremental Learning"
                },
                "summary": "The rising computational and energy demands of deep learning, particularly in\nlarge-scale architectures such as foundation models and large language models\n(LLMs), pose significant challenges to sustainability. Traditional\ngradient-based training methods are inefficient, requiring numerous iterative\nupdates and high power consumption. To address these limitations, we propose a\nhybrid framework that combines hierarchical decomposition with FPGA-based\ndirect equation solving and incremental learning. Our method divides the neural\nnetwork into two functional tiers: lower layers are optimized via single-step\nequation solving on FPGAs for efficient and parallelizable feature extraction,\nwhile higher layers employ adaptive incremental learning to support continual\nupdates without full retraining. Building upon this foundation, we introduce\nthe Compound LLM framework, which explicitly deploys LLM modules across both\nhierarchy levels. The lower-level LLM handles reusable representation learning\nwith minimal energy overhead, while the upper-level LLM performs adaptive\ndecision-making through energy-aware updates. This integrated design enhances\nscalability, reduces redundant computation, and aligns with the principles of\nsustainable AI. Theoretical analysis and architectural insights demonstrate\nthat our method reduces computational costs significantly while preserving high\nmodel performance, making it well-suited for edge deployment and real-time\nadaptation in energy-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rising computational and energy demands of deep learning, particularly in\nlarge-scale architectures such as foundation models and large language models\n(LLMs), pose significant challenges to sustainability. Traditional\ngradient-based training methods are inefficient, requiring numerous iterative\nupdates and high power consumption. To address these limitations, we propose a\nhybrid framework that combines hierarchical decomposition with FPGA-based\ndirect equation solving and incremental learning. Our method divides the neural\nnetwork into two functional tiers: lower layers are optimized via single-step\nequation solving on FPGAs for efficient and parallelizable feature extraction,\nwhile higher layers employ adaptive incremental learning to support continual\nupdates without full retraining. Building upon this foundation, we introduce\nthe Compound LLM framework, which explicitly deploys LLM modules across both\nhierarchy levels. The lower-level LLM handles reusable representation learning\nwith minimal energy overhead, while the upper-level LLM performs adaptive\ndecision-making through energy-aware updates. This integrated design enhances\nscalability, reduces redundant computation, and aligns with the principles of\nsustainable AI. Theoretical analysis and architectural insights demonstrate\nthat our method reduces computational costs significantly while preserving high\nmodel performance, making it well-suited for edge deployment and real-time\nadaptation in energy-constrained environments."
                },
                "authors": [
                    {
                        "name": "Mohammad Saleh Vahdatpour"
                    },
                    {
                        "name": "Huaiyuan Chu"
                    },
                    {
                        "name": "Yanqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yanqing Zhang"
                },
                "author": "Yanqing Zhang",
                "arxiv_comment": "Published at IJCNN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15095v1",
                "updated": "2025-09-18T15:50:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    50,
                    54,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:50:54Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    50,
                    54,
                    3,
                    261,
                    0
                ],
                "title": "Listening, Imagining \\& Refining: A Heuristic Optimized ASR Correction\n  Framework with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Listening, Imagining \\& Refining: A Heuristic Optimized ASR Correction\n  Framework with LLMs"
                },
                "summary": "Automatic Speech Recognition (ASR) systems remain prone to errors that affect\ndownstream applications. In this paper, we propose LIR-ASR, a heuristic\noptimized iterative correction framework using LLMs, inspired by human auditory\nperception. LIR-ASR applies a \"Listening-Imagining-Refining\" strategy,\ngenerating phonetic variants and refining them in context. A heuristic\noptimization with finite state machine (FSM) is introduced to prevent the\ncorrection process from being trapped in local optima and rule-based\nconstraints help maintain semantic fidelity. Experiments on both English and\nChinese ASR outputs show that LIR-ASR achieves average reductions in CER/WER of\nup to 1.5 percentage points compared to baselines, demonstrating substantial\naccuracy gains in transcription.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Speech Recognition (ASR) systems remain prone to errors that affect\ndownstream applications. In this paper, we propose LIR-ASR, a heuristic\noptimized iterative correction framework using LLMs, inspired by human auditory\nperception. LIR-ASR applies a \"Listening-Imagining-Refining\" strategy,\ngenerating phonetic variants and refining them in context. A heuristic\noptimization with finite state machine (FSM) is introduced to prevent the\ncorrection process from being trapped in local optima and rule-based\nconstraints help maintain semantic fidelity. Experiments on both English and\nChinese ASR outputs show that LIR-ASR achieves average reductions in CER/WER of\nup to 1.5 percentage points compared to baselines, demonstrating substantial\naccuracy gains in transcription."
                },
                "authors": [
                    {
                        "name": "Yutong Liu"
                    },
                    {
                        "name": "Ziyue Zhang"
                    },
                    {
                        "name": "Yongbin Yu"
                    },
                    {
                        "name": "Xiangxiang Wang"
                    },
                    {
                        "name": "Yuqing Cai"
                    },
                    {
                        "name": "Nyima Tashi"
                    }
                ],
                "author_detail": {
                    "name": "Nyima Tashi"
                },
                "author": "Nyima Tashi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15089v1",
                "updated": "2025-09-18T15:46:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    46,
                    40,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:46:40Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    46,
                    40,
                    3,
                    261,
                    0
                ],
                "title": "LLM-OREF: An Open Relation Extraction Framework Based on Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-OREF: An Open Relation Extraction Framework Based on Large Language\n  Models"
                },
                "summary": "The goal of open relation extraction (OpenRE) is to develop an RE model that\ncan generalize to new relations not encountered during training. Existing\nstudies primarily formulate OpenRE as a clustering task. They first cluster all\ntest instances based on the similarity between the instances, and then manually\nassign a new relation to each cluster. However, their reliance on human\nannotation limits their practicality. In this paper, we propose an OpenRE\nframework based on large language models (LLMs), which directly predicts new\nrelations for test instances by leveraging their strong language understanding\nand generation abilities, without human intervention. Specifically, our\nframework consists of two core components: (1) a relation discoverer (RD),\ndesigned to predict new relations for test instances based on\n\\textit{demonstrations} formed by training instances with known relations; and\n(2) a relation predictor (RP), used to select the most likely relation for a\ntest instance from $n$ candidate relations, guided by \\textit{demonstrations}\ncomposed of their instances. To enhance the ability of our framework to predict\nnew relations, we design a self-correcting inference strategy composed of three\nstages: relation discovery, relation denoising, and relation prediction. In the\nfirst stage, we use RD to preliminarily predict new relations for all test\ninstances. Next, we apply RP to select some high-reliability test instances for\neach new relation from the prediction results of RD through a cross-validation\nmethod. During the third stage, we employ RP to re-predict the relations of all\ntest instances based on the demonstrations constructed from these reliable test\ninstances. Extensive experiments on three OpenRE datasets demonstrate the\neffectiveness of our framework. We release our code at\nhttps://github.com/XMUDeepLIT/LLM-OREF.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The goal of open relation extraction (OpenRE) is to develop an RE model that\ncan generalize to new relations not encountered during training. Existing\nstudies primarily formulate OpenRE as a clustering task. They first cluster all\ntest instances based on the similarity between the instances, and then manually\nassign a new relation to each cluster. However, their reliance on human\nannotation limits their practicality. In this paper, we propose an OpenRE\nframework based on large language models (LLMs), which directly predicts new\nrelations for test instances by leveraging their strong language understanding\nand generation abilities, without human intervention. Specifically, our\nframework consists of two core components: (1) a relation discoverer (RD),\ndesigned to predict new relations for test instances based on\n\\textit{demonstrations} formed by training instances with known relations; and\n(2) a relation predictor (RP), used to select the most likely relation for a\ntest instance from $n$ candidate relations, guided by \\textit{demonstrations}\ncomposed of their instances. To enhance the ability of our framework to predict\nnew relations, we design a self-correcting inference strategy composed of three\nstages: relation discovery, relation denoising, and relation prediction. In the\nfirst stage, we use RD to preliminarily predict new relations for all test\ninstances. Next, we apply RP to select some high-reliability test instances for\neach new relation from the prediction results of RD through a cross-validation\nmethod. During the third stage, we employ RP to re-predict the relations of all\ntest instances based on the demonstrations constructed from these reliable test\ninstances. Extensive experiments on three OpenRE datasets demonstrate the\neffectiveness of our framework. We release our code at\nhttps://github.com/XMUDeepLIT/LLM-OREF.git."
                },
                "authors": [
                    {
                        "name": "Hongyao Tu"
                    },
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Yujie Lin"
                    },
                    {
                        "name": "Xin Lin"
                    },
                    {
                        "name": "Haibo Zhang"
                    },
                    {
                        "name": "Long Zhang"
                    },
                    {
                        "name": "Jinsong Su"
                    }
                ],
                "author_detail": {
                    "name": "Jinsong Su"
                },
                "author": "Jinsong Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13835v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13835v2",
                "updated": "2025-09-18T15:44:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    44,
                    33,
                    3,
                    261,
                    0
                ],
                "published": "2024-12-18T13:25:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    25,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "RAcQUEt: Unveiling the Dangers of Overlooked Referential Ambiguity in\n  Visual LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAcQUEt: Unveiling the Dangers of Overlooked Referential Ambiguity in\n  Visual LLMs"
                },
                "summary": "Ambiguity resolution is key to effective communication. While humans\neffortlessly address ambiguity through conversational grounding strategies, the\nextent to which current language models can emulate these strategies remains\nunclear. In this work, we examine referential ambiguity in image-based question\nanswering by introducing RACQUET, a carefully curated dataset targeting\ndistinct aspects of ambiguity. Through a series of evaluations, we reveal\nsignificant limitations and problems of overconfidence of state-of-the-art\nlarge multimodal language models in addressing ambiguity in their responses.\nThe overconfidence issue becomes particularly relevant for RACQUET-BIAS, a\nsubset designed to analyze a critical yet underexplored problem: failing to\naddress ambiguity leads to stereotypical, socially biased responses. Our\nresults underscore the urgency of equipping models with robust strategies to\ndeal with uncertainty without resorting to undesirable stereotypes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ambiguity resolution is key to effective communication. While humans\neffortlessly address ambiguity through conversational grounding strategies, the\nextent to which current language models can emulate these strategies remains\nunclear. In this work, we examine referential ambiguity in image-based question\nanswering by introducing RACQUET, a carefully curated dataset targeting\ndistinct aspects of ambiguity. Through a series of evaluations, we reveal\nsignificant limitations and problems of overconfidence of state-of-the-art\nlarge multimodal language models in addressing ambiguity in their responses.\nThe overconfidence issue becomes particularly relevant for RACQUET-BIAS, a\nsubset designed to analyze a critical yet underexplored problem: failing to\naddress ambiguity leads to stereotypical, socially biased responses. Our\nresults underscore the urgency of equipping models with robust strategies to\ndeal with uncertainty without resorting to undesirable stereotypes."
                },
                "authors": [
                    {
                        "name": "Alberto Testoni"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "Raquel Fernández"
                    }
                ],
                "author_detail": {
                    "name": "Raquel Fernández"
                },
                "author": "Raquel Fernández",
                "arxiv_comment": "Accepted to EMNLP 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13835v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13835v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15087v1",
                "updated": "2025-09-18T15:43:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    43,
                    33,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:43:33Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    43,
                    33,
                    3,
                    261,
                    0
                ],
                "title": "Adaptive LoRA Experts Allocation and Selection for Federated Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive LoRA Experts Allocation and Selection for Federated Fine-Tuning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\nvarious tasks, but fine-tuning them for domain-specific applications often\nrequires substantial domain-specific data that may be distributed across\nmultiple organizations. Federated Learning (FL) offers a privacy-preserving\nsolution, but faces challenges with computational constraints when applied to\nLLMs. Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient\nfine-tuning approach, though a single LoRA module often struggles with\nheterogeneous data across diverse domains. This paper addresses two critical\nchallenges in federated LoRA fine-tuning: 1. determining the optimal number and\nallocation of LoRA experts across heterogeneous clients, and 2. enabling\nclients to selectively utilize these experts based on their specific data\ncharacteristics. We propose FedLEASE (Federated adaptive LoRA Expert Allocation\nand SElection), a novel framework that adaptively clusters clients based on\nrepresentation similarity to allocate and train domain-specific LoRA experts.\nIt also introduces an adaptive top-$M$ Mixture-of-Experts mechanism that allows\neach client to select the optimal number of utilized experts. Our extensive\nexperiments on diverse benchmark datasets demonstrate that FedLEASE\nsignificantly outperforms existing federated fine-tuning approaches in\nheterogeneous client settings while maintaining communication efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities across\nvarious tasks, but fine-tuning them for domain-specific applications often\nrequires substantial domain-specific data that may be distributed across\nmultiple organizations. Federated Learning (FL) offers a privacy-preserving\nsolution, but faces challenges with computational constraints when applied to\nLLMs. Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient\nfine-tuning approach, though a single LoRA module often struggles with\nheterogeneous data across diverse domains. This paper addresses two critical\nchallenges in federated LoRA fine-tuning: 1. determining the optimal number and\nallocation of LoRA experts across heterogeneous clients, and 2. enabling\nclients to selectively utilize these experts based on their specific data\ncharacteristics. We propose FedLEASE (Federated adaptive LoRA Expert Allocation\nand SElection), a novel framework that adaptively clusters clients based on\nrepresentation similarity to allocate and train domain-specific LoRA experts.\nIt also introduces an adaptive top-$M$ Mixture-of-Experts mechanism that allows\neach client to select the optimal number of utilized experts. Our extensive\nexperiments on diverse benchmark datasets demonstrate that FedLEASE\nsignificantly outperforms existing federated fine-tuning approaches in\nheterogeneous client settings while maintaining communication efficiency."
                },
                "authors": [
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Jieming Bian"
                    },
                    {
                        "name": "Letian Zhang"
                    },
                    {
                        "name": "Jie Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Xu"
                },
                "author": "Jie Xu",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15082v1",
                "updated": "2025-09-18T15:41:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    41,
                    58,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:41:58Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    41,
                    58,
                    3,
                    261,
                    0
                ],
                "title": "From Who Said What to Who They Are: Modular Training-free Identity-Aware\n  LLM Refinement of Speaker Diarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Who Said What to Who They Are: Modular Training-free Identity-Aware\n  LLM Refinement of Speaker Diarization"
                },
                "summary": "Speaker diarization (SD) struggles in real-world scenarios due to dynamic\nenvironments and unknown speaker counts. SD is rarely used alone and is often\npaired with automatic speech recognition (ASR), but non-modular methods that\njointly train on domain-specific data have limited flexibility. Moreover, many\napplications require true speaker identities rather than SD's pseudo labels. We\npropose a training-free modular pipeline combining off-the-shelf SD, ASR, and a\nlarge language model (LLM) to determine who spoke, what was said, and who they\nare. Using structured LLM prompting on reconciled SD and ASR outputs, our\nmethod leverages semantic continuity in conversational context to refine\nlow-confidence speaker labels and assigns role identities while correcting\nsplit speakers. On a real-world patient-clinician dataset, our approach\nachieves a 29.7% relative error reduction over baseline reconciled SD and ASR.\nIt enhances diarization performance without additional training and delivers a\ncomplete pipeline for SD, ASR, and speaker identity detection in practical\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speaker diarization (SD) struggles in real-world scenarios due to dynamic\nenvironments and unknown speaker counts. SD is rarely used alone and is often\npaired with automatic speech recognition (ASR), but non-modular methods that\njointly train on domain-specific data have limited flexibility. Moreover, many\napplications require true speaker identities rather than SD's pseudo labels. We\npropose a training-free modular pipeline combining off-the-shelf SD, ASR, and a\nlarge language model (LLM) to determine who spoke, what was said, and who they\nare. Using structured LLM prompting on reconciled SD and ASR outputs, our\nmethod leverages semantic continuity in conversational context to refine\nlow-confidence speaker labels and assigns role identities while correcting\nsplit speakers. On a real-world patient-clinician dataset, our approach\nachieves a 29.7% relative error reduction over baseline reconciled SD and ASR.\nIt enhances diarization performance without additional training and delivers a\ncomplete pipeline for SD, ASR, and speaker identity detection in practical\napplications."
                },
                "authors": [
                    {
                        "name": "Yu-Wen Chen"
                    },
                    {
                        "name": "William Ho"
                    },
                    {
                        "name": "Maxim Topaz"
                    },
                    {
                        "name": "Julia Hirschberg"
                    },
                    {
                        "name": "Zoran Kostic"
                    }
                ],
                "author_detail": {
                    "name": "Zoran Kostic"
                },
                "author": "Zoran Kostic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15076v1",
                "updated": "2025-09-18T15:36:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    36,
                    38,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:36:38Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    36,
                    38,
                    3,
                    261,
                    0
                ],
                "title": "Forecasting and Visualizing Air Quality from Sky Images with\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting and Visualizing Air Quality from Sky Images with\n  Vision-Language Models"
                },
                "summary": "Air pollution remains a critical threat to public health and environmental\nsustainability, yet conventional monitoring systems are often constrained by\nlimited spatial coverage and accessibility. This paper proposes an AI-driven\nagent that predicts ambient air pollution levels from sky images and\nsynthesizes realistic visualizations of pollution scenarios using generative\nmodeling. Our approach combines statistical texture analysis with supervised\nlearning for pollution classification, and leverages vision-language model\n(VLM)-guided image generation to produce interpretable representations of air\nquality conditions. The generated visuals simulate varying degrees of\npollution, offering a foundation for user-facing interfaces that improve\ntransparency and support informed environmental decision-making. These outputs\ncan be seamlessly integrated into intelligent applications aimed at enhancing\nsituational awareness and encouraging behavioral responses based on real-time\nforecasts. We validate our method using a dataset of urban sky images and\ndemonstrate its effectiveness in both pollution level estimation and\nsemantically consistent visual synthesis. The system design further\nincorporates human-centered user experience principles to ensure accessibility,\nclarity, and public engagement in air quality forecasting. To support scalable\nand energy-efficient deployment, future iterations will incorporate a green CNN\narchitecture enhanced with FPGA-based incremental learning, enabling real-time\ninference on edge platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Air pollution remains a critical threat to public health and environmental\nsustainability, yet conventional monitoring systems are often constrained by\nlimited spatial coverage and accessibility. This paper proposes an AI-driven\nagent that predicts ambient air pollution levels from sky images and\nsynthesizes realistic visualizations of pollution scenarios using generative\nmodeling. Our approach combines statistical texture analysis with supervised\nlearning for pollution classification, and leverages vision-language model\n(VLM)-guided image generation to produce interpretable representations of air\nquality conditions. The generated visuals simulate varying degrees of\npollution, offering a foundation for user-facing interfaces that improve\ntransparency and support informed environmental decision-making. These outputs\ncan be seamlessly integrated into intelligent applications aimed at enhancing\nsituational awareness and encouraging behavioral responses based on real-time\nforecasts. We validate our method using a dataset of urban sky images and\ndemonstrate its effectiveness in both pollution level estimation and\nsemantically consistent visual synthesis. The system design further\nincorporates human-centered user experience principles to ensure accessibility,\nclarity, and public engagement in air quality forecasting. To support scalable\nand energy-efficient deployment, future iterations will incorporate a green CNN\narchitecture enhanced with FPGA-based incremental learning, enabling real-time\ninference on edge platforms."
                },
                "authors": [
                    {
                        "name": "Mohammad Saleh Vahdatpour"
                    },
                    {
                        "name": "Maryam Eyvazi"
                    },
                    {
                        "name": "Yanqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yanqing Zhang"
                },
                "author": "Yanqing Zhang",
                "arxiv_comment": "Published at ICCVW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15074v1",
                "updated": "2025-09-18T15:35:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    35,
                    40,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:35:40Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    35,
                    40,
                    3,
                    261,
                    0
                ],
                "title": "Weighted Automata for Exact Inference in Discrete Probabilistic Programs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weighted Automata for Exact Inference in Discrete Probabilistic Programs"
                },
                "summary": "In probabilistic programming, the inference problem asks to determine a\nprogram's posterior distribution conditioned on its \"observe\" instructions.\nInference is challenging, especially when exact rather than approximate results\nare required. Inspired by recent work on probability generating functions\n(PGFs), we propose encoding distributions on $\\mathbb{N}^k$ as weighted\nautomata over a commutative alphabet with $k$ symbols. Based on this, we map\nthe semantics of various imperative programming statements to\nautomata-theoretic constructions. For a rich class of programs, this results in\nan effective translation from prior to posterior distribution, both encoded as\nautomata. We prove that our approach is sound with respect to a standard\noperational program semantics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In probabilistic programming, the inference problem asks to determine a\nprogram's posterior distribution conditioned on its \"observe\" instructions.\nInference is challenging, especially when exact rather than approximate results\nare required. Inspired by recent work on probability generating functions\n(PGFs), we propose encoding distributions on $\\mathbb{N}^k$ as weighted\nautomata over a commutative alphabet with $k$ symbols. Based on this, we map\nthe semantics of various imperative programming statements to\nautomata-theoretic constructions. For a rich class of programs, this results in\nan effective translation from prior to posterior distribution, both encoded as\nautomata. We prove that our approach is sound with respect to a standard\noperational program semantics."
                },
                "authors": [
                    {
                        "name": "Dominik Geißler"
                    },
                    {
                        "name": "Tobias Winkler"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Winkler"
                },
                "author": "Tobias Winkler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13557v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13557v2",
                "updated": "2025-09-18T15:34:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    34,
                    44,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-16T21:52:04Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    21,
                    52,
                    4,
                    1,
                    259,
                    0
                ],
                "title": "MACO: A Multi-Agent LLM-Based Hardware/Software Co-Design Framework for\n  CGRAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MACO: A Multi-Agent LLM-Based Hardware/Software Co-Design Framework for\n  CGRAs"
                },
                "summary": "Coarse-grained Reconfigurable Arrays (CGRAs) are a promising computing\narchitecture that can deliver high-performance, energy-efficient acceleration\nacross diverse domains. By supporting reconfiguration at the functional unit\nlevel, CGRAs efficiently adapt to varying computational patterns and optimize\nresource utilization. However, designing CGRAs is highly challenging due to the\nvast design space, independent architectural parameters, and the time-consuming\nnature of manual design. Fortunately, the rapid advancement of large language\nmodels (LLMs) presents new opportunities to automate this process.\n  In this work, we propose MACO -- an open-source multi-agent LLM-based\nframework for Hardware/Software (HW/SW) co-design of CGRAs. The framework\nemploys LLM reasoning to generate CGRAs across four stages: HW/SW co-design,\nDesign error correction, Best design selection, and Evaluation & Feedback.\nFurthermore, MACO iteratively optimizes the generated CGRAs, leveraging agent\nreasoning and feedback to achieve higher PPA (that is, power, performance, and\narea) design points for a given domain. In addition, we introduce an LLM\nself-learning mechanism that employs LLM-driven decision making to select the\noptimal CGRA to accelerate the design process.\n  We evaluate the framework with state-of-the-art LLM-based methods and manual\nCGRA design, in terms of performance, power consumption, and area. Experimental\nresults show that MACO efficiently generates high-quality CGRA architectures,\nsignificantly reducing manual design effort and demonstrating the potential of\nour framework for real-world CGRA design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-grained Reconfigurable Arrays (CGRAs) are a promising computing\narchitecture that can deliver high-performance, energy-efficient acceleration\nacross diverse domains. By supporting reconfiguration at the functional unit\nlevel, CGRAs efficiently adapt to varying computational patterns and optimize\nresource utilization. However, designing CGRAs is highly challenging due to the\nvast design space, independent architectural parameters, and the time-consuming\nnature of manual design. Fortunately, the rapid advancement of large language\nmodels (LLMs) presents new opportunities to automate this process.\n  In this work, we propose MACO -- an open-source multi-agent LLM-based\nframework for Hardware/Software (HW/SW) co-design of CGRAs. The framework\nemploys LLM reasoning to generate CGRAs across four stages: HW/SW co-design,\nDesign error correction, Best design selection, and Evaluation & Feedback.\nFurthermore, MACO iteratively optimizes the generated CGRAs, leveraging agent\nreasoning and feedback to achieve higher PPA (that is, power, performance, and\narea) design points for a given domain. In addition, we introduce an LLM\nself-learning mechanism that employs LLM-driven decision making to select the\noptimal CGRA to accelerate the design process.\n  We evaluate the framework with state-of-the-art LLM-based methods and manual\nCGRA design, in terms of performance, power consumption, and area. Experimental\nresults show that MACO efficiently generates high-quality CGRA architectures,\nsignificantly reducing manual design effort and demonstrating the potential of\nour framework for real-world CGRA design."
                },
                "authors": [
                    {
                        "name": "Zesong Jiang"
                    },
                    {
                        "name": "Yuqi Sun"
                    },
                    {
                        "name": "Qing Zhong"
                    },
                    {
                        "name": "Mahathi Krishna"
                    },
                    {
                        "name": "Deepak Patil"
                    },
                    {
                        "name": "Cheng Tan"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    },
                    {
                        "name": "Jeff Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Zhang"
                },
                "author": "Jeff Zhang",
                "arxiv_comment": "Due to certain confidentiality requirements, this article needs to be\n  withdrawn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13557v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13557v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15068v1",
                "updated": "2025-09-18T15:30:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    30,
                    38,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:30:38Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    30,
                    38,
                    3,
                    261,
                    0
                ],
                "title": "Learning in Context: Personalizing Educational Content with Large\n  Language Models to Enhance Student Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning in Context: Personalizing Educational Content with Large\n  Language Models to Enhance Student Learning"
                },
                "summary": "Standardized, one-size-fits-all educational content often fails to connect\nwith students' individual backgrounds and interests, leading to disengagement\nand a perceived lack of relevance. To address this challenge, we introduce\nPAGE, a novel framework that leverages large language models (LLMs) to\nautomatically personalize educational materials by adapting them to each\nstudent's unique context, such as their major and personal interests. To\nvalidate our approach, we deployed PAGE in a semester-long intelligent tutoring\nsystem and conducted a user study to evaluate its impact in an authentic\neducational setting. Our findings show that students who received personalized\ncontent demonstrated significantly improved learning outcomes and reported\nhigher levels of engagement, perceived relevance, and trust compared to those\nwho used standardized materials. This work demonstrates the practical value of\nLLM-powered personalization and offers key design implications for creating\nmore effective, engaging, and trustworthy educational experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standardized, one-size-fits-all educational content often fails to connect\nwith students' individual backgrounds and interests, leading to disengagement\nand a perceived lack of relevance. To address this challenge, we introduce\nPAGE, a novel framework that leverages large language models (LLMs) to\nautomatically personalize educational materials by adapting them to each\nstudent's unique context, such as their major and personal interests. To\nvalidate our approach, we deployed PAGE in a semester-long intelligent tutoring\nsystem and conducted a user study to evaluate its impact in an authentic\neducational setting. Our findings show that students who received personalized\ncontent demonstrated significantly improved learning outcomes and reported\nhigher levels of engagement, perceived relevance, and trust compared to those\nwho used standardized materials. This work demonstrates the practical value of\nLLM-powered personalization and offers key design implications for creating\nmore effective, engaging, and trustworthy educational experiences."
                },
                "authors": [
                    {
                        "name": "Joy Jia Yin Lim"
                    },
                    {
                        "name": "Daniel Zhang-Li"
                    },
                    {
                        "name": "Jifan Yu"
                    },
                    {
                        "name": "Xin Cong"
                    },
                    {
                        "name": "Ye He"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Huiqin Liu"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    },
                    {
                        "name": "Bin Xu"
                    }
                ],
                "author_detail": {
                    "name": "Bin Xu"
                },
                "author": "Bin Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08827v2",
                "updated": "2025-09-18T15:28:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    28,
                    2,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-10T17:59:43Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    59,
                    43,
                    2,
                    253,
                    0
                ],
                "title": "A Survey of Reinforcement Learning for Large Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Reinforcement Learning for Large Reasoning Models"
                },
                "summary": "In this paper, we survey recent advances in Reinforcement Learning (RL) for\nreasoning with Large Language Models (LLMs). RL has achieved remarkable success\nin advancing the frontier of LLM capabilities, particularly in addressing\ncomplex logical tasks such as mathematics and coding. As a result, RL has\nemerged as a foundational methodology for transforming LLMs into LRMs. With the\nrapid progress of the field, further scaling of RL for LRMs now faces\nfoundational challenges not only in computational resources but also in\nalgorithm design, training data, and infrastructure. To this end, it is timely\nto revisit the development of this domain, reassess its trajectory, and explore\nstrategies to enhance the scalability of RL toward Artificial SuperIntelligence\n(ASI). In particular, we examine research applying RL to LLMs and LRMs for\nreasoning abilities, especially since the release of DeepSeek-R1, including\nfoundational components, core problems, training resources, and downstream\napplications, to identify future opportunities and directions for this rapidly\nevolving area. We hope this review will promote future research on RL for\nbroader reasoning models. Github:\nhttps://github.com/TsinghuaC3I/Awesome-RL-for-LRMs",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we survey recent advances in Reinforcement Learning (RL) for\nreasoning with Large Language Models (LLMs). RL has achieved remarkable success\nin advancing the frontier of LLM capabilities, particularly in addressing\ncomplex logical tasks such as mathematics and coding. As a result, RL has\nemerged as a foundational methodology for transforming LLMs into LRMs. With the\nrapid progress of the field, further scaling of RL for LRMs now faces\nfoundational challenges not only in computational resources but also in\nalgorithm design, training data, and infrastructure. To this end, it is timely\nto revisit the development of this domain, reassess its trajectory, and explore\nstrategies to enhance the scalability of RL toward Artificial SuperIntelligence\n(ASI). In particular, we examine research applying RL to LLMs and LRMs for\nreasoning abilities, especially since the release of DeepSeek-R1, including\nfoundational components, core problems, training resources, and downstream\napplications, to identify future opportunities and directions for this rapidly\nevolving area. We hope this review will promote future research on RL for\nbroader reasoning models. Github:\nhttps://github.com/TsinghuaC3I/Awesome-RL-for-LRMs"
                },
                "authors": [
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Yuxin Zuo"
                    },
                    {
                        "name": "Bingxiang He"
                    },
                    {
                        "name": "Youbang Sun"
                    },
                    {
                        "name": "Runze Liu"
                    },
                    {
                        "name": "Che Jiang"
                    },
                    {
                        "name": "Yuchen Fan"
                    },
                    {
                        "name": "Kai Tian"
                    },
                    {
                        "name": "Guoli Jia"
                    },
                    {
                        "name": "Pengfei Li"
                    },
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Xingtai Lv"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Sihang Zeng"
                    },
                    {
                        "name": "Shang Qu"
                    },
                    {
                        "name": "Haozhan Li"
                    },
                    {
                        "name": "Shijie Wang"
                    },
                    {
                        "name": "Yuru Wang"
                    },
                    {
                        "name": "Xinwei Long"
                    },
                    {
                        "name": "Fangfu Liu"
                    },
                    {
                        "name": "Xiang Xu"
                    },
                    {
                        "name": "Jiaze Ma"
                    },
                    {
                        "name": "Xuekai Zhu"
                    },
                    {
                        "name": "Ermo Hua"
                    },
                    {
                        "name": "Yihao Liu"
                    },
                    {
                        "name": "Zonglin Li"
                    },
                    {
                        "name": "Huayu Chen"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Yafu Li"
                    },
                    {
                        "name": "Weize Chen"
                    },
                    {
                        "name": "Zhenzhao Yuan"
                    },
                    {
                        "name": "Junqi Gao"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Zhiyuan Ma"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "arxiv_comment": "Fixed typos; added missing and recent citations (114 -> 117 pages)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15061v1",
                "updated": "2025-09-18T15:25:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    25,
                    31,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:25:31Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    25,
                    31,
                    3,
                    261,
                    0
                ],
                "title": "Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn\n  Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn\n  Dialogue"
                },
                "summary": "The ultimate goal of embodied agents is to create collaborators that can\ninteract with humans, not mere executors that passively follow instructions.\nThis requires agents to communicate, coordinate, and adapt their actions based\non human feedback. Recently, advances in VLAs have offered a path toward this\ngoal. However, most current VLA-based embodied agents operate in a one-way\nmode: they receive an instruction and execute it without feedback. This\napproach fails in real-world scenarios where instructions are often ambiguous.\nIn this paper, we address this problem with the Ask-to-Clarify framework. Our\nframework first resolves ambiguous instructions by asking questions in a\nmulti-turn dialogue. Then it generates low-level actions end-to-end.\nSpecifically, the Ask-to-Clarify framework consists of two components, one VLM\nfor collaboration and one diffusion for action. We also introduce a connection\nmodule that generates conditions for the diffusion based on the output of the\nVLM. This module adjusts the observation by instructions to create reliable\nconditions. We train our framework with a two-stage knowledge-insulation\nstrategy. First, we fine-tune the collaboration component using\nambiguity-solving dialogue data to handle ambiguity. Then, we integrate the\naction component while freezing the collaboration one. This preserves the\ninteraction abilities while fine-tuning the diffusion to generate actions. The\ntraining strategy guarantees our framework can first ask questions, then\ngenerate actions. During inference, a signal detector functions as a router\nthat helps our framework switch between asking questions and taking actions. We\nevaluate the Ask-to-Clarify framework in 8 real-world tasks, where it\noutperforms existing state-of-the-art VLAs. The results suggest that our\nproposed framework, along with the training strategy, provides a path toward\ncollaborative embodied agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ultimate goal of embodied agents is to create collaborators that can\ninteract with humans, not mere executors that passively follow instructions.\nThis requires agents to communicate, coordinate, and adapt their actions based\non human feedback. Recently, advances in VLAs have offered a path toward this\ngoal. However, most current VLA-based embodied agents operate in a one-way\nmode: they receive an instruction and execute it without feedback. This\napproach fails in real-world scenarios where instructions are often ambiguous.\nIn this paper, we address this problem with the Ask-to-Clarify framework. Our\nframework first resolves ambiguous instructions by asking questions in a\nmulti-turn dialogue. Then it generates low-level actions end-to-end.\nSpecifically, the Ask-to-Clarify framework consists of two components, one VLM\nfor collaboration and one diffusion for action. We also introduce a connection\nmodule that generates conditions for the diffusion based on the output of the\nVLM. This module adjusts the observation by instructions to create reliable\nconditions. We train our framework with a two-stage knowledge-insulation\nstrategy. First, we fine-tune the collaboration component using\nambiguity-solving dialogue data to handle ambiguity. Then, we integrate the\naction component while freezing the collaboration one. This preserves the\ninteraction abilities while fine-tuning the diffusion to generate actions. The\ntraining strategy guarantees our framework can first ask questions, then\ngenerate actions. During inference, a signal detector functions as a router\nthat helps our framework switch between asking questions and taking actions. We\nevaluate the Ask-to-Clarify framework in 8 real-world tasks, where it\noutperforms existing state-of-the-art VLAs. The results suggest that our\nproposed framework, along with the training strategy, provides a path toward\ncollaborative embodied agents."
                },
                "authors": [
                    {
                        "name": "Xingyao Lin"
                    },
                    {
                        "name": "Xinghao Zhu"
                    },
                    {
                        "name": "Tianyi Lu"
                    },
                    {
                        "name": "Sicheng Xie"
                    },
                    {
                        "name": "Hui Zhang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Zuxuan Wu"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Gang Jiang"
                },
                "author": "Yu-Gang Jiang",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15059v1",
                "updated": "2025-09-18T15:22:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    22,
                    33,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:22:33Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    22,
                    33,
                    3,
                    261,
                    0
                ],
                "title": "QuizRank: Picking Images by Quizzing VLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuizRank: Picking Images by Quizzing VLMs"
                },
                "summary": "Images play a vital role in improving the readability and comprehension of\nWikipedia articles by serving as `illustrative aids.' However, not all images\nare equally effective and not all Wikipedia editors are trained in their\nselection. We propose QuizRank, a novel method of image selection that\nleverages large language models (LLMs) and vision language models (VLMs) to\nrank images as learning interventions. Our approach transforms textual\ndescriptions of the article's subject into multiple-choice questions about\nimportant visual characteristics of the concept. We utilize these questions to\nquiz the VLM: the better an image can help answer questions, the higher it is\nranked. To further improve discrimination between visually similar items, we\nintroduce a Contrastive QuizRank that leverages differences in the features of\ntarget (e.g., a Western Bluebird) and distractor concepts (e.g., Mountain\nBluebird) to generate questions. We demonstrate the potential of VLMs as\neffective visual evaluators by showing a high congruence with human quiz-takers\nand an effective discriminative ranking of images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Images play a vital role in improving the readability and comprehension of\nWikipedia articles by serving as `illustrative aids.' However, not all images\nare equally effective and not all Wikipedia editors are trained in their\nselection. We propose QuizRank, a novel method of image selection that\nleverages large language models (LLMs) and vision language models (VLMs) to\nrank images as learning interventions. Our approach transforms textual\ndescriptions of the article's subject into multiple-choice questions about\nimportant visual characteristics of the concept. We utilize these questions to\nquiz the VLM: the better an image can help answer questions, the higher it is\nranked. To further improve discrimination between visually similar items, we\nintroduce a Contrastive QuizRank that leverages differences in the features of\ntarget (e.g., a Western Bluebird) and distractor concepts (e.g., Mountain\nBluebird) to generate questions. We demonstrate the potential of VLMs as\neffective visual evaluators by showing a high congruence with human quiz-takers\nand an effective discriminative ranking of images."
                },
                "authors": [
                    {
                        "name": "Tenghao Ji"
                    },
                    {
                        "name": "Eytan Adar"
                    }
                ],
                "author_detail": {
                    "name": "Eytan Adar"
                },
                "author": "Eytan Adar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18106v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18106v3",
                "updated": "2025-09-18T15:18:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    18,
                    10,
                    3,
                    261,
                    0
                ],
                "published": "2025-08-25T15:11:11Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    11,
                    11,
                    0,
                    237,
                    0
                ],
                "title": "A.S.E: A Repository-Level Benchmark for Evaluating Security in\n  AI-Generated Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A.S.E: A Repository-Level Benchmark for Evaluating Security in\n  AI-Generated Code"
                },
                "summary": "The increasing adoption of large language models (LLMs) in software\nengineering necessitates rigorous security evaluation of their generated code.\nHowever, existing benchmarks often lack relevance to real-world AI-assisted\nprogramming scenarios, making them inadequate for assessing the practical\nsecurity risks associated with AI-generated code in production environments. To\naddress this gap, we introduce A.S.E (AI Code Generation Security Evaluation),\na repository-level evaluation benchmark designed to closely mirror real-world\nAI programming tasks, offering a comprehensive and reliable framework for\nassessing the security of AI-generated code. Our evaluation of leading LLMs on\nA.S.E reveals several key findings. In particular, current LLMs still struggle\nwith secure coding. The complexity in repository-level scenarios presents\nchallenges for LLMs that typically perform well on snippet-level tasks.\nMoreover, a larger reasoning budget does not necessarily lead to better code\ngeneration. These observations offer valuable insights into the current state\nof AI code generation and help developers identify the most suitable models for\npractical tasks. They also lay the groundwork for refining LLMs to generate\nsecure and efficient code in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of large language models (LLMs) in software\nengineering necessitates rigorous security evaluation of their generated code.\nHowever, existing benchmarks often lack relevance to real-world AI-assisted\nprogramming scenarios, making them inadequate for assessing the practical\nsecurity risks associated with AI-generated code in production environments. To\naddress this gap, we introduce A.S.E (AI Code Generation Security Evaluation),\na repository-level evaluation benchmark designed to closely mirror real-world\nAI programming tasks, offering a comprehensive and reliable framework for\nassessing the security of AI-generated code. Our evaluation of leading LLMs on\nA.S.E reveals several key findings. In particular, current LLMs still struggle\nwith secure coding. The complexity in repository-level scenarios presents\nchallenges for LLMs that typically perform well on snippet-level tasks.\nMoreover, a larger reasoning budget does not necessarily lead to better code\ngeneration. These observations offer valuable insights into the current state\nof AI code generation and help developers identify the most suitable models for\npractical tasks. They also lay the groundwork for refining LLMs to generate\nsecure and efficient code in real-world applications."
                },
                "authors": [
                    {
                        "name": "Keke Lian"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Libo Chen"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Ziming Zhao"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Miaoqian Lin"
                    },
                    {
                        "name": "Haotong Duan"
                    },
                    {
                        "name": "Haoran Zhao"
                    },
                    {
                        "name": "Shuang Liao"
                    },
                    {
                        "name": "Mingda Guo"
                    },
                    {
                        "name": "Jiazheng Quan"
                    },
                    {
                        "name": "Yilu Zhong"
                    },
                    {
                        "name": "Chenhao He"
                    },
                    {
                        "name": "Zichuan Chen"
                    },
                    {
                        "name": "Jie Wu"
                    },
                    {
                        "name": "Haoling Li"
                    },
                    {
                        "name": "Zhaoxuan Li"
                    },
                    {
                        "name": "Jiongchi Yu"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Dong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dong Zhang"
                },
                "author": "Dong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18106v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18106v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15038v1",
                "updated": "2025-09-18T15:04:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    4,
                    6,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:04:06Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    4,
                    6,
                    3,
                    261,
                    0
                ],
                "title": "Value-Guided KV Compression for LLMs via Approximated CUR Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-Guided KV Compression for LLMs via Approximated CUR Decomposition"
                },
                "summary": "Key-value (KV) cache compression has emerged as a critical technique for\nreducing the memory and latency overhead of autoregressive language models\nduring inference. Prior approaches predominantly rely on query-key attention\nscores to rank and evict cached tokens, assuming that attention intensity\ncorrelates with semantic importance. However, this heuristic overlooks the\ncontribution of value vectors, which directly influence the attention output.\nIn this paper, we propose CurDKV, a novel, value-centric KV compression method\nthat selects keys and values based on leverage scores computed from CUR matrix\ndecomposition. Our approach approximates the dominant subspace of the attention\noutput $softmax(QK^T)V$, ensuring that the retained tokens best preserve the\nmodel's predictive behavior. Theoretically, we show that attention score\napproximation does not guarantee output preservation, and demonstrate that\nCUR-based selection minimizes end-to-end attention reconstruction loss.\nEmpirically, CurDKV achieves up to 9.6% higher accuracy than state-of-the-art\nmethods like SnapKV and ChunkKV under aggressive compression budgets on LLaMA\nand Mistral, while maintaining compatibility with FlashAttention and Grouped\nQuery Attention. In addition to improved accuracy, CurDKV reduces generation\nlatency by up to 40% at high compression, offering a practical speed-accuracy\ntradeoff.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) cache compression has emerged as a critical technique for\nreducing the memory and latency overhead of autoregressive language models\nduring inference. Prior approaches predominantly rely on query-key attention\nscores to rank and evict cached tokens, assuming that attention intensity\ncorrelates with semantic importance. However, this heuristic overlooks the\ncontribution of value vectors, which directly influence the attention output.\nIn this paper, we propose CurDKV, a novel, value-centric KV compression method\nthat selects keys and values based on leverage scores computed from CUR matrix\ndecomposition. Our approach approximates the dominant subspace of the attention\noutput $softmax(QK^T)V$, ensuring that the retained tokens best preserve the\nmodel's predictive behavior. Theoretically, we show that attention score\napproximation does not guarantee output preservation, and demonstrate that\nCUR-based selection minimizes end-to-end attention reconstruction loss.\nEmpirically, CurDKV achieves up to 9.6% higher accuracy than state-of-the-art\nmethods like SnapKV and ChunkKV under aggressive compression budgets on LLaMA\nand Mistral, while maintaining compatibility with FlashAttention and Grouped\nQuery Attention. In addition to improved accuracy, CurDKV reduces generation\nlatency by up to 40% at high compression, offering a practical speed-accuracy\ntradeoff."
                },
                "authors": [
                    {
                        "name": "Ayan Sengupta"
                    },
                    {
                        "name": "Siddhant Chaudhary"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21448v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21448v2",
                "updated": "2025-09-18T15:02:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    2,
                    49,
                    3,
                    261,
                    0
                ],
                "published": "2025-05-27T17:20:38Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    20,
                    38,
                    1,
                    147,
                    0
                ],
                "title": "OmniSync: Towards Universal Lip Synchronization via Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniSync: Towards Universal Lip Synchronization via Diffusion\n  Transformers"
                },
                "summary": "Lip synchronization is the task of aligning a speaker's lip movements in\nvideo with corresponding speech audio, and it is essential for creating\nrealistic, expressive video content. However, existing methods often rely on\nreference frames and masked-frame inpainting, which limit their robustness to\nidentity consistency, pose variations, facial occlusions, and stylized content.\nIn addition, since audio signals provide weaker conditioning than visual cues,\nlip shape leakage from the original video will affect lip sync quality. In this\npaper, we present OmniSync, a universal lip synchronization framework for\ndiverse visual scenarios. Our approach introduces a mask-free training paradigm\nusing Diffusion Transformer models for direct frame editing without explicit\nmasks, enabling unlimited-duration inference while maintaining natural facial\ndynamics and preserving character identity. During inference, we propose a\nflow-matching-based progressive noise initialization to ensure pose and\nidentity consistency, while allowing precise mouth-region editing. To address\nthe weak conditioning signal of audio, we develop a Dynamic Spatiotemporal\nClassifier-Free Guidance (DS-CFG) mechanism that adaptively adjusts guidance\nstrength over time and space. We also establish the AIGC-LipSync Benchmark, the\nfirst evaluation suite for lip synchronization in diverse AI-generated videos.\nExtensive experiments demonstrate that OmniSync significantly outperforms prior\nmethods in both visual quality and lip sync accuracy, achieving superior\nresults in both real-world and AI-generated videos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lip synchronization is the task of aligning a speaker's lip movements in\nvideo with corresponding speech audio, and it is essential for creating\nrealistic, expressive video content. However, existing methods often rely on\nreference frames and masked-frame inpainting, which limit their robustness to\nidentity consistency, pose variations, facial occlusions, and stylized content.\nIn addition, since audio signals provide weaker conditioning than visual cues,\nlip shape leakage from the original video will affect lip sync quality. In this\npaper, we present OmniSync, a universal lip synchronization framework for\ndiverse visual scenarios. Our approach introduces a mask-free training paradigm\nusing Diffusion Transformer models for direct frame editing without explicit\nmasks, enabling unlimited-duration inference while maintaining natural facial\ndynamics and preserving character identity. During inference, we propose a\nflow-matching-based progressive noise initialization to ensure pose and\nidentity consistency, while allowing precise mouth-region editing. To address\nthe weak conditioning signal of audio, we develop a Dynamic Spatiotemporal\nClassifier-Free Guidance (DS-CFG) mechanism that adaptively adjusts guidance\nstrength over time and space. We also establish the AIGC-LipSync Benchmark, the\nfirst evaluation suite for lip synchronization in diverse AI-generated videos.\nExtensive experiments demonstrate that OmniSync significantly outperforms prior\nmethods in both visual quality and lip sync accuracy, achieving superior\nresults in both real-world and AI-generated videos."
                },
                "authors": [
                    {
                        "name": "Ziqiao Peng"
                    },
                    {
                        "name": "Jiwen Liu"
                    },
                    {
                        "name": "Haoxian Zhang"
                    },
                    {
                        "name": "Xiaoqiang Liu"
                    },
                    {
                        "name": "Songlin Tang"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Hongyan Liu"
                    },
                    {
                        "name": "Jun He"
                    }
                ],
                "author_detail": {
                    "name": "Jun He"
                },
                "author": "Jun He",
                "arxiv_comment": "Accepted as NeurIPS 2025 spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21448v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21448v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15027v1",
                "updated": "2025-09-18T14:53:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    53,
                    41,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T14:53:41Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    53,
                    41,
                    3,
                    261,
                    0
                ],
                "title": "CLEAR: A Comprehensive Linguistic Evaluation of Argument Rewriting by\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLEAR: A Comprehensive Linguistic Evaluation of Argument Rewriting by\n  Large Language Models"
                },
                "summary": "While LLMs have been extensively studied on general text generation tasks,\nthere is less research on text rewriting, a task related to general text\ngeneration, and particularly on the behavior of models on this task. In this\npaper we analyze what changes LLMs make in a text rewriting setting. We focus\nspecifically on argumentative texts and their improvement, a task named\nArgument Improvement (ArgImp). We present CLEAR: an evaluation pipeline\nconsisting of 57 metrics mapped to four linguistic levels: lexical, syntactic,\nsemantic and pragmatic. This pipeline is used to examine the qualities of\nLLM-rewritten arguments on a broad set of argumentation corpora and compare the\nbehavior of different LLMs on this task and analyze the behavior of different\nLLMs on this task in terms of linguistic levels. By taking all four linguistic\nlevels into consideration, we find that the models perform ArgImp by shortening\nthe texts while simultaneously increasing average word length and merging\nsentences. Overall we note an increase in the persuasion and coherence\ndimensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While LLMs have been extensively studied on general text generation tasks,\nthere is less research on text rewriting, a task related to general text\ngeneration, and particularly on the behavior of models on this task. In this\npaper we analyze what changes LLMs make in a text rewriting setting. We focus\nspecifically on argumentative texts and their improvement, a task named\nArgument Improvement (ArgImp). We present CLEAR: an evaluation pipeline\nconsisting of 57 metrics mapped to four linguistic levels: lexical, syntactic,\nsemantic and pragmatic. This pipeline is used to examine the qualities of\nLLM-rewritten arguments on a broad set of argumentation corpora and compare the\nbehavior of different LLMs on this task and analyze the behavior of different\nLLMs on this task in terms of linguistic levels. By taking all four linguistic\nlevels into consideration, we find that the models perform ArgImp by shortening\nthe texts while simultaneously increasing average word length and merging\nsentences. Overall we note an increase in the persuasion and coherence\ndimensions."
                },
                "authors": [
                    {
                        "name": "Thomas Huber"
                    },
                    {
                        "name": "Christina Niklaus"
                    }
                ],
                "author_detail": {
                    "name": "Christina Niklaus"
                },
                "author": "Christina Niklaus",
                "arxiv_comment": "Accepted at EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15020v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15020v1",
                "updated": "2025-09-18T14:47:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    47,
                    58,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T14:47:58Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    47,
                    58,
                    3,
                    261,
                    0
                ],
                "title": "Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question\n  Answering with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question\n  Answering with LLMs"
                },
                "summary": "When evaluating large language models (LLMs) with multiple-choice question\nanswering (MCQA), it is common to end the prompt with the string \"Answer:\" to\nfacilitate automated answer extraction via next-token probabilities. However,\nthere is no consensus on how to tokenize the space following the colon, often\noverlooked as a trivial choice. In this paper, we uncover accuracy differences\nof up to 11% due to this (seemingly irrelevant) tokenization variation as well\nas reshuffled model rankings, raising concerns about the reliability of LLM\ncomparisons in prior work. Surprisingly, we are able to recommend one specific\nstrategy -- tokenizing the space together with the answer letter -- as we\nobserve consistent and statistically significant performance improvements.\nAdditionally, it improves model calibration, enhancing the reliability of the\nmodel's confidence estimates. Our findings underscore the importance of careful\nevaluation design and highlight the need for standardized, transparent\nevaluation protocols to ensure reliable and comparable results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When evaluating large language models (LLMs) with multiple-choice question\nanswering (MCQA), it is common to end the prompt with the string \"Answer:\" to\nfacilitate automated answer extraction via next-token probabilities. However,\nthere is no consensus on how to tokenize the space following the colon, often\noverlooked as a trivial choice. In this paper, we uncover accuracy differences\nof up to 11% due to this (seemingly irrelevant) tokenization variation as well\nas reshuffled model rankings, raising concerns about the reliability of LLM\ncomparisons in prior work. Surprisingly, we are able to recommend one specific\nstrategy -- tokenizing the space together with the answer letter -- as we\nobserve consistent and statistically significant performance improvements.\nAdditionally, it improves model calibration, enhancing the reliability of the\nmodel's confidence estimates. Our findings underscore the importance of careful\nevaluation design and highlight the need for standardized, transparent\nevaluation protocols to ensure reliable and comparable results."
                },
                "authors": [
                    {
                        "name": "Mario Sanz-Guerrero"
                    },
                    {
                        "name": "Minh Duc Bui"
                    },
                    {
                        "name": "Katharina von der Wense"
                    }
                ],
                "author_detail": {
                    "name": "Katharina von der Wense"
                },
                "author": "Katharina von der Wense",
                "arxiv_comment": "Accepted to EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15020v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15012v1",
                "updated": "2025-09-18T14:42:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    42,
                    25,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T14:42:25Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    42,
                    25,
                    3,
                    261,
                    0
                ],
                "title": "Constraining Cosmology with Double-Source-Plane Strong Gravitational\n  Lenses From the AGEL Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraining Cosmology with Double-Source-Plane Strong Gravitational\n  Lenses From the AGEL Survey"
                },
                "summary": "Double-source-plane strong gravitational lenses (DSPLs), with two sources at\ndifferent redshifts, are independent cosmological probes of the dark energy\nequation of state parameter $w$ and the matter density parameter $\\Omega_{\\rm\nm}$. We present the lens model for the DSPL AGEL035346$-$170639 and infer\ncosmological constraints from this system for flat $\\Lambda$CDM and flat $w$CDM\ncosmologies. From the joint posterior of $w$ and $\\Omega_{\\rm m}$ in the flat\n$w$CDM cosmology, we extract the following median values and 1$\\sigma$\nuncertainties: $w = -1.52^{+0.49}_{-0.33}$ and $\\Omega_{\\rm m} =\n0.192^{+0.305}_{-0.131}$ from AGEL0353 alone. Combining our measurements with\ntwo previously analyzed DSPLs, we present the joint constraint on these\nparameters from a sample of three, the largest galaxy-scale DSPL sample used\nfor cosmological measurement to date. The combined precision of $w$ from three\nDSPLs is higher by 15% over AGEL0353 alone. Combining DSPL and cosmic microwave\nbackground (CMB) measurements improves the precision of $w$ from CMB-only\nconstraints by 39%, demonstrating the complementarity of DSPLs with the CMB.\nDespite their promising constraining power, DSPLs are limited by sample size,\nwith only a handful discovered so far. Although ongoing and near-future\nwide-area sky surveys will increase the number of known DSPLs by up to two\norders of magnitude, these systems will still require dedicated high-resolution\nimaging and spectroscopic follow-ups like those presented in this paper. Our\nASTRO 3D Galaxy Evolution with Lenses (AGEL) collaboration is undertaking such\nfollow-up campaigns for several newly discovered DSPLs and will provide\ncosmological measurements from larger samples of DSPLs in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Double-source-plane strong gravitational lenses (DSPLs), with two sources at\ndifferent redshifts, are independent cosmological probes of the dark energy\nequation of state parameter $w$ and the matter density parameter $\\Omega_{\\rm\nm}$. We present the lens model for the DSPL AGEL035346$-$170639 and infer\ncosmological constraints from this system for flat $\\Lambda$CDM and flat $w$CDM\ncosmologies. From the joint posterior of $w$ and $\\Omega_{\\rm m}$ in the flat\n$w$CDM cosmology, we extract the following median values and 1$\\sigma$\nuncertainties: $w = -1.52^{+0.49}_{-0.33}$ and $\\Omega_{\\rm m} =\n0.192^{+0.305}_{-0.131}$ from AGEL0353 alone. Combining our measurements with\ntwo previously analyzed DSPLs, we present the joint constraint on these\nparameters from a sample of three, the largest galaxy-scale DSPL sample used\nfor cosmological measurement to date. The combined precision of $w$ from three\nDSPLs is higher by 15% over AGEL0353 alone. Combining DSPL and cosmic microwave\nbackground (CMB) measurements improves the precision of $w$ from CMB-only\nconstraints by 39%, demonstrating the complementarity of DSPLs with the CMB.\nDespite their promising constraining power, DSPLs are limited by sample size,\nwith only a handful discovered so far. Although ongoing and near-future\nwide-area sky surveys will increase the number of known DSPLs by up to two\norders of magnitude, these systems will still require dedicated high-resolution\nimaging and spectroscopic follow-ups like those presented in this paper. Our\nASTRO 3D Galaxy Evolution with Lenses (AGEL) collaboration is undertaking such\nfollow-up campaigns for several newly discovered DSPLs and will provide\ncosmological measurements from larger samples of DSPLs in the future."
                },
                "authors": [
                    {
                        "name": "Duncan J. Bowden"
                    },
                    {
                        "name": "Nandini Sahu"
                    },
                    {
                        "name": "Anowar J. Shajib"
                    },
                    {
                        "name": "Kim-Vy Tran"
                    },
                    {
                        "name": "Tania M. Barone"
                    },
                    {
                        "name": "Keerthi Vasan G. C."
                    },
                    {
                        "name": "Daniel J. Ballard"
                    },
                    {
                        "name": "Thomas E. Collett"
                    },
                    {
                        "name": "Faith Dalessandro"
                    },
                    {
                        "name": "Giovanni Ferrami"
                    },
                    {
                        "name": "Karl Glazebrook"
                    },
                    {
                        "name": "William J. Gottemoller"
                    },
                    {
                        "name": "Leena Iwamoto"
                    },
                    {
                        "name": "Tucker Jones"
                    },
                    {
                        "name": "Glenn G. Kacprzak"
                    },
                    {
                        "name": "Geraint F. Lewis"
                    },
                    {
                        "name": "Haven McIntosh-Lombardo"
                    },
                    {
                        "name": "Hannah Skobe"
                    },
                    {
                        "name": "Sherry H. Suyu"
                    },
                    {
                        "name": "Sarah M. Sweet"
                    }
                ],
                "author_detail": {
                    "name": "Sarah M. Sweet"
                },
                "author": "Sarah M. Sweet",
                "arxiv_comment": "19 pages, 10 figures, Accepted for publication in the Astrophysical\n  Journal (ApJ)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14998v1",
                "updated": "2025-09-18T14:33:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    33,
                    36,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T14:33:36Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    33,
                    36,
                    3,
                    261,
                    0
                ],
                "title": "A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical\n  Decision-making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical\n  Decision-making"
                },
                "summary": "Medical decision-making often involves integrating knowledge from multiple\nclinical specialties, typically achieved through multidisciplinary teams.\nInspired by this collaborative process, recent work has leveraged large\nlanguage models (LLMs) in multi-agent collaboration frameworks to emulate\nexpert teamwork. While these approaches improve reasoning through agent\ninteraction, they are limited by static, pre-assigned roles, which hinder\nadaptability and dynamic knowledge integration. To address these limitations,\nwe propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration\nframework that enables LLM agents to dynamically form and expand expert teams\nbased on the evolving diagnostic context. KAMAC begins with one or more expert\nagents and then conducts a knowledge-driven discussion to identify and fill\nknowledge gaps by recruiting additional specialists as needed. This supports\nflexible, scalable collaboration in complex clinical scenarios, with decisions\nfinalized through reviewing updated agent comments. Experiments on two\nreal-world medical benchmarks demonstrate that KAMAC significantly outperforms\nboth single-agent and advanced multi-agent methods, particularly in complex\nclinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty\nexpertise. Our code is publicly available at:\nhttps://github.com/XiaoXiao-Woo/KAMAC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical decision-making often involves integrating knowledge from multiple\nclinical specialties, typically achieved through multidisciplinary teams.\nInspired by this collaborative process, recent work has leveraged large\nlanguage models (LLMs) in multi-agent collaboration frameworks to emulate\nexpert teamwork. While these approaches improve reasoning through agent\ninteraction, they are limited by static, pre-assigned roles, which hinder\nadaptability and dynamic knowledge integration. To address these limitations,\nwe propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration\nframework that enables LLM agents to dynamically form and expand expert teams\nbased on the evolving diagnostic context. KAMAC begins with one or more expert\nagents and then conducts a knowledge-driven discussion to identify and fill\nknowledge gaps by recruiting additional specialists as needed. This supports\nflexible, scalable collaboration in complex clinical scenarios, with decisions\nfinalized through reviewing updated agent comments. Experiments on two\nreal-world medical benchmarks demonstrate that KAMAC significantly outperforms\nboth single-agent and advanced multi-agent methods, particularly in complex\nclinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty\nexpertise. Our code is publicly available at:\nhttps://github.com/XiaoXiao-Woo/KAMAC."
                },
                "authors": [
                    {
                        "name": "Xiao Wu"
                    },
                    {
                        "name": "Ting-Zhu Huang"
                    },
                    {
                        "name": "Liang-Jian Deng"
                    },
                    {
                        "name": "Yanyuan Qiao"
                    },
                    {
                        "name": "Imran Razzak"
                    },
                    {
                        "name": "Yutong Xie"
                    }
                ],
                "author_detail": {
                    "name": "Yutong Xie"
                },
                "author": "Yutong Xie",
                "arxiv_comment": "The paper has been accepted to the EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14997v1",
                "updated": "2025-09-18T14:32:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    32,
                    20,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T14:32:20Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    32,
                    20,
                    3,
                    261,
                    0
                ],
                "title": "No evidence for $H_0$ anisotropy from Tully-Fisher or supernova\n  distances",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No evidence for $H_0$ anisotropy from Tully-Fisher or supernova\n  distances"
                },
                "summary": "Claims of anisotropy in the Hubble constant have been made based on direct\ndistance tracers such as Tully-Fisher galaxies and Type Ia supernovae. We\nrevisit these using the CosmicFlows-4 Tully-Fisher W1 subsample, 2MTF and SFI++\nTully-Fisher catalogues, and the Pantheon+ supernova compilation (restricted to\n$z < 0.05$), including a dipole in either the Tully-Fisher zero-point or the\nstandardised supernova absolute magnitude. Our forward-modelling framework\njointly calibrates the distance relation, marginalises over distances, and\naccounts for peculiar velocities using a linear-theory reconstruction. We\ncompare the anisotropic and isotropic model using the Bayesian evidence. In the\nCosmicFlows-4 sample, we infer a zero-point dipole of amplitude $0.087 \\pm\n0.019$ mag, or $4.1\\pm0.9$ per cent when expressed as a dipole in the Hubble\nparameter. This is consistent with previous estimates but at higher\nsignificance: model comparison yields odds of $877\\!:\\!1$ in favour of\nincluding the zero-point dipole. In Pantheon+ we infer zero-point dipole\namplitude of $0.049 \\pm 0.013$ mag, or $2.3\\pm 0.6$ per cent when expressed as\na dipole in the Hubble parameter. However, by allowing for a radially varying\nvelocity dipole, we show that the anisotropic model captures local flow\nfeatures (or possibly systematics) in the data rather than an actual linearly\ngrowing effective bulk flow caused by anisotropy in the zero-point or expansion\nrate. Inferring a more general bulk flow curve we find results fully consistent\nwith expectations from the standard cosmological model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Claims of anisotropy in the Hubble constant have been made based on direct\ndistance tracers such as Tully-Fisher galaxies and Type Ia supernovae. We\nrevisit these using the CosmicFlows-4 Tully-Fisher W1 subsample, 2MTF and SFI++\nTully-Fisher catalogues, and the Pantheon+ supernova compilation (restricted to\n$z < 0.05$), including a dipole in either the Tully-Fisher zero-point or the\nstandardised supernova absolute magnitude. Our forward-modelling framework\njointly calibrates the distance relation, marginalises over distances, and\naccounts for peculiar velocities using a linear-theory reconstruction. We\ncompare the anisotropic and isotropic model using the Bayesian evidence. In the\nCosmicFlows-4 sample, we infer a zero-point dipole of amplitude $0.087 \\pm\n0.019$ mag, or $4.1\\pm0.9$ per cent when expressed as a dipole in the Hubble\nparameter. This is consistent with previous estimates but at higher\nsignificance: model comparison yields odds of $877\\!:\\!1$ in favour of\nincluding the zero-point dipole. In Pantheon+ we infer zero-point dipole\namplitude of $0.049 \\pm 0.013$ mag, or $2.3\\pm 0.6$ per cent when expressed as\na dipole in the Hubble parameter. However, by allowing for a radially varying\nvelocity dipole, we show that the anisotropic model captures local flow\nfeatures (or possibly systematics) in the data rather than an actual linearly\ngrowing effective bulk flow caused by anisotropy in the zero-point or expansion\nrate. Inferring a more general bulk flow curve we find results fully consistent\nwith expectations from the standard cosmological model."
                },
                "authors": [
                    {
                        "name": "Richard Stiskalek"
                    },
                    {
                        "name": "Harry Desmond"
                    },
                    {
                        "name": "Guilhem Lavaux"
                    }
                ],
                "author_detail": {
                    "name": "Guilhem Lavaux"
                },
                "author": "Guilhem Lavaux",
                "arxiv_comment": "18 pages, 8 figures. To be submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18309v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18309v2",
                "updated": "2025-09-18T14:30:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    30,
                    9,
                    3,
                    261,
                    0
                ],
                "published": "2025-02-25T15:53:18Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    53,
                    18,
                    1,
                    56,
                    0
                ],
                "title": "GCDance: Genre-Controlled 3D Full Body Dance Generation Driven By Music",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GCDance: Genre-Controlled 3D Full Body Dance Generation Driven By Music"
                },
                "summary": "Generating high-quality full-body dance sequences from music is a challenging\ntask as it requires strict adherence to genre-specific choreography. Moreover,\nthe generated sequences must be both physically realistic and precisely\nsynchronized with the beats and rhythm of the music. To overcome these\nchallenges, we propose GCDance, a classifier-free diffusion framework for\ngenerating genre-specific dance motions conditioned on both music and textual\nprompts. Specifically, our approach extracts music features by combining\nhigh-level pre-trained music foundation model features with hand-crafted\nfeatures for multi-granularity feature fusion. To achieve genre\ncontrollability, we leverage CLIP to efficiently embed genre-based textual\nprompt representations at each time step within our dance generation pipeline.\nOur GCDance framework can generate diverse dance styles from the same piece of\nmusic while ensuring coherence with the rhythm and melody of the music.\nExtensive experimental results obtained on the FineDance dataset demonstrate\nthat GCDance significantly outperforms the existing state-of-the-art\napproaches, which also achieve competitive results on the AIST++ dataset. Our\nablation and inference time analysis demonstrate that GCDance provides an\neffective solution for high-quality music-driven dance generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating high-quality full-body dance sequences from music is a challenging\ntask as it requires strict adherence to genre-specific choreography. Moreover,\nthe generated sequences must be both physically realistic and precisely\nsynchronized with the beats and rhythm of the music. To overcome these\nchallenges, we propose GCDance, a classifier-free diffusion framework for\ngenerating genre-specific dance motions conditioned on both music and textual\nprompts. Specifically, our approach extracts music features by combining\nhigh-level pre-trained music foundation model features with hand-crafted\nfeatures for multi-granularity feature fusion. To achieve genre\ncontrollability, we leverage CLIP to efficiently embed genre-based textual\nprompt representations at each time step within our dance generation pipeline.\nOur GCDance framework can generate diverse dance styles from the same piece of\nmusic while ensuring coherence with the rhythm and melody of the music.\nExtensive experimental results obtained on the FineDance dataset demonstrate\nthat GCDance significantly outperforms the existing state-of-the-art\napproaches, which also achieve competitive results on the AIST++ dataset. Our\nablation and inference time analysis demonstrate that GCDance provides an\neffective solution for high-quality music-driven dance generation."
                },
                "authors": [
                    {
                        "name": "Xinran Liu"
                    },
                    {
                        "name": "Xu Dong"
                    },
                    {
                        "name": "Diptesh Kanojia"
                    },
                    {
                        "name": "Wenwu Wang"
                    },
                    {
                        "name": "Zhenhua Feng"
                    }
                ],
                "author_detail": {
                    "name": "Zhenhua Feng"
                },
                "author": "Zhenhua Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18309v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18309v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20869v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20869v2",
                "updated": "2025-09-18T14:12:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    12,
                    45,
                    3,
                    261,
                    0
                ],
                "published": "2025-06-25T22:40:00Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    22,
                    40,
                    0,
                    2,
                    176,
                    0
                ],
                "title": "Engineering RAG Systems for Real-World Applications: Design,\n  Development, and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engineering RAG Systems for Real-World Applications: Design,\n  Development, and Evaluation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems are emerging as a key approach\nfor grounding Large Language Models (LLMs) in external knowledge, addressing\nlimitations in factual accuracy and contextual relevance. However, there is a\nlack of empirical studies that report on the development of RAG-based\nimplementations grounded in real-world use cases, evaluated through general\nuser involvement, and accompanied by systematic documentation of lessons\nlearned. This paper presents five domain-specific RAG applications developed\nfor real-world scenarios across governance, cybersecurity, agriculture,\nindustrial research, and medical diagnostics. Each system incorporates\nmultilingual OCR, semantic retrieval via vector embeddings, and domain-adapted\nLLMs, deployed through local servers or cloud APIs to meet distinct user needs.\nA web-based evaluation involving a total of 100 participants assessed the\nsystems across six dimensions: (i) Ease of Use, (ii) Relevance, (iii)\nTransparency, (iv) Responsiveness, (v) Accuracy, and (vi) Likelihood of\nRecommendation. Based on user feedback and our development experience, we\ndocumented twelve key lessons learned, highlighting technical, operational, and\nethical challenges affecting the reliability and usability of RAG systems in\npractice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems are emerging as a key approach\nfor grounding Large Language Models (LLMs) in external knowledge, addressing\nlimitations in factual accuracy and contextual relevance. However, there is a\nlack of empirical studies that report on the development of RAG-based\nimplementations grounded in real-world use cases, evaluated through general\nuser involvement, and accompanied by systematic documentation of lessons\nlearned. This paper presents five domain-specific RAG applications developed\nfor real-world scenarios across governance, cybersecurity, agriculture,\nindustrial research, and medical diagnostics. Each system incorporates\nmultilingual OCR, semantic retrieval via vector embeddings, and domain-adapted\nLLMs, deployed through local servers or cloud APIs to meet distinct user needs.\nA web-based evaluation involving a total of 100 participants assessed the\nsystems across six dimensions: (i) Ease of Use, (ii) Relevance, (iii)\nTransparency, (iv) Responsiveness, (v) Accuracy, and (vi) Likelihood of\nRecommendation. Based on user feedback and our development experience, we\ndocumented twelve key lessons learned, highlighting technical, operational, and\nethical challenges affecting the reliability and usability of RAG systems in\npractice."
                },
                "authors": [
                    {
                        "name": "Md Toufique Hasan"
                    },
                    {
                        "name": "Muhammad Waseem"
                    },
                    {
                        "name": "Kai-Kristian Kemell"
                    },
                    {
                        "name": "Ayman Asad Khan"
                    },
                    {
                        "name": "Mika Saari"
                    },
                    {
                        "name": "Pekka Abrahamsson"
                    }
                ],
                "author_detail": {
                    "name": "Pekka Abrahamsson"
                },
                "author": "Pekka Abrahamsson",
                "arxiv_doi": "10.1007/978-3-032-04200-2_10",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-032-04200-2_10",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.20869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20869v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the Proceedings of the 51st Euromicro Conference on\n  Software Engineering and Advanced Applications, SEAA 2025. Lecture Notes in\n  Computer Science, volume 16082, pages 143-158. Springer, 2026",
                "arxiv_journal_ref": "LNCS 16082, 143-158, 2026",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.11; I.2.6; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14979v1",
                "updated": "2025-09-18T14:08:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    8,
                    45,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T14:08:45Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    8,
                    45,
                    3,
                    261,
                    0
                ],
                "title": "What Matters in LLM-Based Feature Extractor for Recommender? A\n  Systematic Analysis of Prompts, Models, and Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Matters in LLM-Based Feature Extractor for Recommender? A\n  Systematic Analysis of Prompts, Models, and Adaptation"
                },
                "summary": "Using Large Language Models (LLMs) to generate semantic features has been\ndemonstrated as a powerful paradigm for enhancing Sequential Recommender\nSystems (SRS). This typically involves three stages: processing item text,\nextracting features with LLMs, and adapting them for downstream models.\nHowever, existing methods vary widely in prompting, architecture, and\nadaptation strategies, making it difficult to fairly compare design choices and\nidentify what truly drives performance. In this work, we propose RecXplore, a\nmodular analytical framework that decomposes the LLM-as-feature-extractor\npipeline into four modules: data processing, semantic feature extraction,\nfeature adaptation, and sequential modeling. Instead of proposing new\ntechniques, RecXplore revisits and organizes established methods, enabling\nsystematic exploration of each module in isolation. Experiments on four public\ndatasets show that simply combining the best designs from existing techniques\nwithout exhaustive search yields up to 18.7% relative improvement in NDCG@5 and\n12.7% in HR@5 over strong baselines. These results underscore the utility of\nmodular benchmarking for identifying effective design patterns and promoting\nstandardized research in LLM-enhanced recommendation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models (LLMs) to generate semantic features has been\ndemonstrated as a powerful paradigm for enhancing Sequential Recommender\nSystems (SRS). This typically involves three stages: processing item text,\nextracting features with LLMs, and adapting them for downstream models.\nHowever, existing methods vary widely in prompting, architecture, and\nadaptation strategies, making it difficult to fairly compare design choices and\nidentify what truly drives performance. In this work, we propose RecXplore, a\nmodular analytical framework that decomposes the LLM-as-feature-extractor\npipeline into four modules: data processing, semantic feature extraction,\nfeature adaptation, and sequential modeling. Instead of proposing new\ntechniques, RecXplore revisits and organizes established methods, enabling\nsystematic exploration of each module in isolation. Experiments on four public\ndatasets show that simply combining the best designs from existing techniques\nwithout exhaustive search yields up to 18.7% relative improvement in NDCG@5 and\n12.7% in HR@5 over strong baselines. These results underscore the utility of\nmodular benchmarking for identifying effective design patterns and promoting\nstandardized research in LLM-enhanced recommendation."
                },
                "authors": [
                    {
                        "name": "Kainan Shi"
                    },
                    {
                        "name": "Peilin Zhou"
                    },
                    {
                        "name": "Ge Wang"
                    },
                    {
                        "name": "Han Ding"
                    },
                    {
                        "name": "Fei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wang"
                },
                "arxiv_affiliation": "Xi'an Jiaotong University",
                "author": "Fei Wang",
                "arxiv_comment": "9 pages. Keywords: Recommender Systems, Large Language Models,\n  Sequential Recommendation, Feature Extraction",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3; I.2.6; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10987v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10987v2",
                "updated": "2025-09-18T14:08:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    8,
                    11,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-13T21:33:48Z",
                "published_parsed": [
                    2025,
                    9,
                    13,
                    21,
                    33,
                    48,
                    5,
                    256,
                    0
                ],
                "title": "A High-Order Cumulant Extension of Quasi-Linkage Equilibrium",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A High-Order Cumulant Extension of Quasi-Linkage Equilibrium"
                },
                "summary": "A central question in evolutionary biology is how to quantitatively\nunderstand the dynamics of genetically diverse populations. Modeling the\ngenotype distribution is challenging, as it ultimately requires tracking all\ncorrelations (or cumulants) among alleles at different loci. The quasi-linkage\nequilibrium (QLE) approximation simplifies this by assuming that correlations\nbetween alleles at different loci are weak -- i.e., low linkage disequilibrium\n-- allowing their dynamics to be modeled perturbatively. However, QLE breaks\ndown under strong selection, significant epistatic interactions, or weak\nrecombination. We extend the multilocus QLE framework to allow cumulants up to\norder $K$ to evolve dynamically, while higher-order cumulants ($>K$) are\nassumed to equilibrate rapidly. This extended QLE (exQLE) framework yields a\ngeneral equation of motion for cumulants up to order $K$, which parallels the\nstandard QLE dynamics (recovered when $K = 1$). In this formulation, cumulant\ndynamics are driven by the gradient of average fitness, mediated by a\ngeometrically interpretable matrix that stems from competition among genotypes.\nOur analysis shows that the exQLE with $K=2$ accurately captures cumulant\ndynamics even when the fitness function includes higher-order (e.g., third- or\nfourth-order) epistatic interactions, capabilities that standard QLE lacks. We\nalso applied the exQLE framework to infer fitness parameters from temporal\nsequence data. Overall, exQLE provides a systematic and interpretable\napproximation scheme, leveraging analytical cumulant dynamics and reducing\ncomplexity by progressively truncating higher-order cumulants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A central question in evolutionary biology is how to quantitatively\nunderstand the dynamics of genetically diverse populations. Modeling the\ngenotype distribution is challenging, as it ultimately requires tracking all\ncorrelations (or cumulants) among alleles at different loci. The quasi-linkage\nequilibrium (QLE) approximation simplifies this by assuming that correlations\nbetween alleles at different loci are weak -- i.e., low linkage disequilibrium\n-- allowing their dynamics to be modeled perturbatively. However, QLE breaks\ndown under strong selection, significant epistatic interactions, or weak\nrecombination. We extend the multilocus QLE framework to allow cumulants up to\norder $K$ to evolve dynamically, while higher-order cumulants ($>K$) are\nassumed to equilibrate rapidly. This extended QLE (exQLE) framework yields a\ngeneral equation of motion for cumulants up to order $K$, which parallels the\nstandard QLE dynamics (recovered when $K = 1$). In this formulation, cumulant\ndynamics are driven by the gradient of average fitness, mediated by a\ngeometrically interpretable matrix that stems from competition among genotypes.\nOur analysis shows that the exQLE with $K=2$ accurately captures cumulant\ndynamics even when the fitness function includes higher-order (e.g., third- or\nfourth-order) epistatic interactions, capabilities that standard QLE lacks. We\nalso applied the exQLE framework to infer fitness parameters from temporal\nsequence data. Overall, exQLE provides a systematic and interpretable\napproximation scheme, leveraging analytical cumulant dynamics and reducing\ncomplexity by progressively truncating higher-order cumulants."
                },
                "authors": [
                    {
                        "name": "Kai S. Shimagaki"
                    },
                    {
                        "name": "Jorge Fernandez-de-Cossio-Diaz"
                    },
                    {
                        "name": "Mauro Pastore"
                    },
                    {
                        "name": "Rémi Monasson"
                    },
                    {
                        "name": "Simona Cocco"
                    },
                    {
                        "name": "John P. Barton"
                    }
                ],
                "author_detail": {
                    "name": "John P. Barton"
                },
                "author": "John P. Barton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10987v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10987v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14971v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14971v1",
                "updated": "2025-09-18T14:03:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    3,
                    58,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T14:03:58Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    3,
                    58,
                    3,
                    261,
                    0
                ],
                "title": "Alternative Likelihood Approximations for High-Dimensional Intervals for\n  Lasso",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alternative Likelihood Approximations for High-Dimensional Intervals for\n  Lasso"
                },
                "summary": "Classical frequentist approaches to inference for the lasso emphasize exact\ncoverage for each feature, which requires debiasing and severs the connection\nbetween confidence intervals and the original lasso estimates. To address this,\nin earlier work we introduced the idea of average coverage, allowing for biased\nintervals that align with the lasso point estimates, and proposed the Relaxed\nLasso Posterior (RL-P) intervals, which leverage the Bayesian interpretation of\nthe lasso penalty as a Laplace prior together with a Normal likelihood\nconditional on the selected features. While RL-P achieves approximate average\ncoverage, its intervals need not contain the lasso estimates. In this work, we\npropose alternative constructions based on different likelihood approximations\nto the full high-dimensional likelihood, yielding intervals that remain\ncentered on the lasso estimates while still achieving average coverage. Our\nresults continue to demonstrate that intentionally biased intervals provide a\nprincipled and practically useful framework for inference in high-dimensional\nregression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classical frequentist approaches to inference for the lasso emphasize exact\ncoverage for each feature, which requires debiasing and severs the connection\nbetween confidence intervals and the original lasso estimates. To address this,\nin earlier work we introduced the idea of average coverage, allowing for biased\nintervals that align with the lasso point estimates, and proposed the Relaxed\nLasso Posterior (RL-P) intervals, which leverage the Bayesian interpretation of\nthe lasso penalty as a Laplace prior together with a Normal likelihood\nconditional on the selected features. While RL-P achieves approximate average\ncoverage, its intervals need not contain the lasso estimates. In this work, we\npropose alternative constructions based on different likelihood approximations\nto the full high-dimensional likelihood, yielding intervals that remain\ncentered on the lasso estimates while still achieving average coverage. Our\nresults continue to demonstrate that intentionally biased intervals provide a\nprincipled and practically useful framework for inference in high-dimensional\nregression."
                },
                "authors": [
                    {
                        "name": "Logan Harris"
                    },
                    {
                        "name": "Patrick Breheny"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Breheny"
                },
                "author": "Patrick Breheny",
                "arxiv_comment": "17 pages, 5 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14971v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14971v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13484v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13484v2",
                "updated": "2025-09-18T14:03:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    3,
                    41,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-16T19:31:40Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    19,
                    31,
                    40,
                    1,
                    259,
                    0
                ],
                "title": "MINGLE: VLMs for Semantically Complex Region Detection in Urban Scenes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MINGLE: VLMs for Semantically Complex Region Detection in Urban Scenes"
                },
                "summary": "Understanding group-level social interactions in public spaces is crucial for\nurban planning, informing the design of socially vibrant and inclusive\nenvironments. Detecting such interactions from images involves interpreting\nsubtle visual cues such as relations, proximity, and co-movement - semantically\ncomplex signals that go beyond traditional object detection. To address this\nchallenge, we introduce a social group region detection task, which requires\ninferring and spatially grounding visual regions defined by abstract\ninterpersonal relations. We propose MINGLE (Modeling INterpersonal Group-Level\nEngagement), a modular three-stage pipeline that integrates: (1) off-the-shelf\nhuman detection and depth estimation, (2) VLM-based reasoning to classify\npairwise social affiliation, and (3) a lightweight spatial aggregation\nalgorithm to localize socially connected groups. To support this task and\nencourage future research, we present a new dataset of 100K urban street-view\nimages annotated with bounding boxes and labels for both individuals and\nsocially interacting groups. The annotations combine human-created labels and\noutputs from the MINGLE pipeline, ensuring semantic richness and broad coverage\nof real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding group-level social interactions in public spaces is crucial for\nurban planning, informing the design of socially vibrant and inclusive\nenvironments. Detecting such interactions from images involves interpreting\nsubtle visual cues such as relations, proximity, and co-movement - semantically\ncomplex signals that go beyond traditional object detection. To address this\nchallenge, we introduce a social group region detection task, which requires\ninferring and spatially grounding visual regions defined by abstract\ninterpersonal relations. We propose MINGLE (Modeling INterpersonal Group-Level\nEngagement), a modular three-stage pipeline that integrates: (1) off-the-shelf\nhuman detection and depth estimation, (2) VLM-based reasoning to classify\npairwise social affiliation, and (3) a lightweight spatial aggregation\nalgorithm to localize socially connected groups. To support this task and\nencourage future research, we present a new dataset of 100K urban street-view\nimages annotated with bounding boxes and labels for both individuals and\nsocially interacting groups. The annotations combine human-created labels and\noutputs from the MINGLE pipeline, ensuring semantic richness and broad coverage\nof real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Alexandra Kudaeva"
                    },
                    {
                        "name": "Marco Cipriano"
                    },
                    {
                        "name": "Fatimeh Al Ghannam"
                    },
                    {
                        "name": "Freya Tan"
                    },
                    {
                        "name": "Gerard de Melo"
                    },
                    {
                        "name": "Andres Sevtsuk"
                    }
                ],
                "author_detail": {
                    "name": "Andres Sevtsuk"
                },
                "author": "Andres Sevtsuk",
                "arxiv_comment": "13 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13484v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13484v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14968v1",
                "updated": "2025-09-18T14:01:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    1,
                    14,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T14:01:14Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    1,
                    14,
                    3,
                    261,
                    0
                ],
                "title": "FAWN: A MultiEncoder Fusion-Attention Wave Network for Integrated\n  Sensing and Communication Indoor Scene Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAWN: A MultiEncoder Fusion-Attention Wave Network for Integrated\n  Sensing and Communication Indoor Scene Inference"
                },
                "summary": "The upcoming generations of wireless technologies promise an era where\neverything is interconnected and intelligent. As the need for intelligence\ngrows, networks must learn to better understand the physical world. However,\ndeploying dedicated hardware to perceive the environment is not always\nfeasible, mainly due to costs and/or complexity. Integrated Sensing and\nCommunication (ISAC) has made a step forward in addressing this challenge.\nWithin ISAC, passive sensing emerges as a cost-effective solution that reuses\nwireless communications to sense the environment, without interfering with\nexisting communications. Nevertheless, the majority of current solutions are\nlimited to one technology (mostly Wi-Fi or 5G), constraining the maximum\naccuracy reachable. As different technologies work with different spectrums, we\nsee a necessity in integrating more than one technology to augment the coverage\narea. Hence, we take the advantage of ISAC passive sensing, to present FAWN, a\nMultiEncoder Fusion-Attention Wave Network for ISAC indoor scene inference.\nFAWN is based on the original transformers architecture, to fuse information\nfrom Wi-Fi and 5G, making the network capable of understanding the physical\nworld without interfering with the current communication. To test our solution,\nwe have built a prototype and integrated it in a real scenario. Results show\nerrors below 0.6 m around 84% of times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The upcoming generations of wireless technologies promise an era where\neverything is interconnected and intelligent. As the need for intelligence\ngrows, networks must learn to better understand the physical world. However,\ndeploying dedicated hardware to perceive the environment is not always\nfeasible, mainly due to costs and/or complexity. Integrated Sensing and\nCommunication (ISAC) has made a step forward in addressing this challenge.\nWithin ISAC, passive sensing emerges as a cost-effective solution that reuses\nwireless communications to sense the environment, without interfering with\nexisting communications. Nevertheless, the majority of current solutions are\nlimited to one technology (mostly Wi-Fi or 5G), constraining the maximum\naccuracy reachable. As different technologies work with different spectrums, we\nsee a necessity in integrating more than one technology to augment the coverage\narea. Hence, we take the advantage of ISAC passive sensing, to present FAWN, a\nMultiEncoder Fusion-Attention Wave Network for ISAC indoor scene inference.\nFAWN is based on the original transformers architecture, to fuse information\nfrom Wi-Fi and 5G, making the network capable of understanding the physical\nworld without interfering with the current communication. To test our solution,\nwe have built a prototype and integrated it in a real scenario. Results show\nerrors below 0.6 m around 84% of times."
                },
                "authors": [
                    {
                        "name": "Carlos Barroso-Fernández"
                    },
                    {
                        "name": "Alejandro Calvillo-Fernandez"
                    },
                    {
                        "name": "Antonio de la Oliva"
                    },
                    {
                        "name": "Carlos J. Bernardos"
                    }
                ],
                "author_detail": {
                    "name": "Carlos J. Bernardos"
                },
                "author": "Carlos J. Bernardos",
                "arxiv_comment": "7 pages, 6 figures and tables, less than 5500 words. Under revision\n  at IEEE Communication Magazine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01566v2",
                "updated": "2025-09-18T13:42:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    13,
                    42,
                    32,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-01T15:51:30Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    15,
                    51,
                    30,
                    0,
                    244,
                    0
                ],
                "title": "CSRM-LLM: Embracing Multilingual LLMs for Cold-Start Relevance Matching\n  in Emerging E-commerce Markets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSRM-LLM: Embracing Multilingual LLMs for Cold-Start Relevance Matching\n  in Emerging E-commerce Markets"
                },
                "summary": "As global e-commerce platforms continue to expand, companies are entering new\nmarkets where they encounter cold-start challenges due to limited human labels\nand user behaviors. In this paper, we share our experiences in Coupang to\nprovide a competitive cold-start performance of relevance matching for emerging\ne-commerce markets. Specifically, we present a Cold-Start Relevance Matching\n(CSRM) framework, utilizing a multilingual Large Language Model (LLM) to\naddress three challenges: (1) activating cross-lingual transfer learning\nabilities of LLMs through machine translation tasks; (2) enhancing query\nunderstanding and incorporating e-commerce knowledge by retrieval-based query\naugmentation; (3) mitigating the impact of training label errors through a\nmulti-round self-distillation training strategy. Our experiments demonstrate\nthe effectiveness of CSRM-LLM and the proposed techniques, resulting in\nsuccessful real-world deployment and significant online gains, with a 45.8%\nreduction in defect ratio and a 0.866% uplift in session purchase rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As global e-commerce platforms continue to expand, companies are entering new\nmarkets where they encounter cold-start challenges due to limited human labels\nand user behaviors. In this paper, we share our experiences in Coupang to\nprovide a competitive cold-start performance of relevance matching for emerging\ne-commerce markets. Specifically, we present a Cold-Start Relevance Matching\n(CSRM) framework, utilizing a multilingual Large Language Model (LLM) to\naddress three challenges: (1) activating cross-lingual transfer learning\nabilities of LLMs through machine translation tasks; (2) enhancing query\nunderstanding and incorporating e-commerce knowledge by retrieval-based query\naugmentation; (3) mitigating the impact of training label errors through a\nmulti-round self-distillation training strategy. Our experiments demonstrate\nthe effectiveness of CSRM-LLM and the proposed techniques, resulting in\nsuccessful real-world deployment and significant online gains, with a 45.8%\nreduction in defect ratio and a 0.866% uplift in session purchase rate."
                },
                "authors": [
                    {
                        "name": "Yujing Wang"
                    },
                    {
                        "name": "Yiren Chen"
                    },
                    {
                        "name": "Huoran Li"
                    },
                    {
                        "name": "Chunxu Xu"
                    },
                    {
                        "name": "Yuchong Luo"
                    },
                    {
                        "name": "Xianghui Mao"
                    },
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Chunyang Ma"
                    },
                    {
                        "name": "Qiqi Jiang"
                    },
                    {
                        "name": "Yin Wang"
                    },
                    {
                        "name": "Fan Gao"
                    },
                    {
                        "name": "Wenting Mo"
                    },
                    {
                        "name": "Pei Wen"
                    },
                    {
                        "name": "Shantanu Kumar"
                    },
                    {
                        "name": "Taejin Park"
                    },
                    {
                        "name": "Yiwei Song"
                    },
                    {
                        "name": "Vijay Rajaram"
                    },
                    {
                        "name": "Tao Cheng"
                    },
                    {
                        "name": "Sonu Durgia"
                    },
                    {
                        "name": "Pranam Kolari"
                    }
                ],
                "author_detail": {
                    "name": "Pranam Kolari"
                },
                "author": "Pranam Kolari",
                "arxiv_comment": "7 pages, 3 figures, accepted by CIKM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14956v1",
                "updated": "2025-09-18T13:39:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    13,
                    39,
                    59,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T13:39:59Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    13,
                    39,
                    59,
                    3,
                    261,
                    0
                ],
                "title": "Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent\n  Systems"
                },
                "summary": "This paper proposes a novel architectural framework aimed at enhancing\nsecurity and reliability in multi-agent systems (MAS). A central component of\nthis framework is a network of Sentinel Agents, functioning as a distributed\nsecurity layer that integrates techniques such as semantic analysis via large\nlanguage models (LLMs), behavioral analytics, retrieval-augmented verification,\nand cross-agent anomaly detection. Such agents can potentially oversee\ninter-agent communications, identify potential threats, enforce privacy and\naccess controls, and maintain comprehensive audit records. Complementary to the\nidea of Sentinel Agents is the use of a Coordinator Agent. The Coordinator\nAgent supervises policy implementation, and manages agent participation. In\naddition, the Coordinator also ingests alerts from Sentinel Agents. Based on\nthese alerts, it can adapt policies, isolate or quarantine misbehaving agents,\nand contain threats to maintain the integrity of the MAS ecosystem. This\ndual-layered security approach, combining the continuous monitoring of Sentinel\nAgents with the governance functions of Coordinator Agents, supports dynamic\nand adaptive defense mechanisms against a range of threats, including prompt\ninjection, collusive agent behavior, hallucinations generated by LLMs, privacy\nbreaches, and coordinated multi-agent attacks. In addition to the architectural\ndesign, we present a simulation study where 162 synthetic attacks of different\nfamilies (prompt injection, hallucination, and data exfiltration) were injected\ninto a multi-agent conversational environment. The Sentinel Agents successfully\ndetected the attack attempts, confirming the practical feasibility of the\nproposed monitoring approach. The framework also offers enhanced system\nobservability, supports regulatory compliance, and enables policy evolution\nover time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a novel architectural framework aimed at enhancing\nsecurity and reliability in multi-agent systems (MAS). A central component of\nthis framework is a network of Sentinel Agents, functioning as a distributed\nsecurity layer that integrates techniques such as semantic analysis via large\nlanguage models (LLMs), behavioral analytics, retrieval-augmented verification,\nand cross-agent anomaly detection. Such agents can potentially oversee\ninter-agent communications, identify potential threats, enforce privacy and\naccess controls, and maintain comprehensive audit records. Complementary to the\nidea of Sentinel Agents is the use of a Coordinator Agent. The Coordinator\nAgent supervises policy implementation, and manages agent participation. In\naddition, the Coordinator also ingests alerts from Sentinel Agents. Based on\nthese alerts, it can adapt policies, isolate or quarantine misbehaving agents,\nand contain threats to maintain the integrity of the MAS ecosystem. This\ndual-layered security approach, combining the continuous monitoring of Sentinel\nAgents with the governance functions of Coordinator Agents, supports dynamic\nand adaptive defense mechanisms against a range of threats, including prompt\ninjection, collusive agent behavior, hallucinations generated by LLMs, privacy\nbreaches, and coordinated multi-agent attacks. In addition to the architectural\ndesign, we present a simulation study where 162 synthetic attacks of different\nfamilies (prompt injection, hallucination, and data exfiltration) were injected\ninto a multi-agent conversational environment. The Sentinel Agents successfully\ndetected the attack attempts, confirming the practical feasibility of the\nproposed monitoring approach. The framework also offers enhanced system\nobservability, supports regulatory compliance, and enables policy evolution\nover time."
                },
                "authors": [
                    {
                        "name": "Diego Gosmar"
                    },
                    {
                        "name": "Deborah A. Dahl"
                    }
                ],
                "author_detail": {
                    "name": "Deborah A. Dahl"
                },
                "author": "Deborah A. Dahl",
                "arxiv_comment": "25 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14943v1",
                "updated": "2025-09-18T13:30:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    13,
                    30,
                    31,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T13:30:31Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    13,
                    30,
                    31,
                    3,
                    261,
                    0
                ],
                "title": "Explicit vs. Implicit Biographies: Evaluating and Adapting LLM\n  Information Extraction on Wikidata-Derived Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explicit vs. Implicit Biographies: Evaluating and Adapting LLM\n  Information Extraction on Wikidata-Derived Texts"
                },
                "summary": "Text Implicitness has always been challenging in Natural Language Processing\n(NLP), with traditional methods relying on explicit statements to identify\nentities and their relationships. From the sentence \"Zuhdi attends church every\nSunday\", the relationship between Zuhdi and Christianity is evident for a human\nreader, but it presents a challenge when it must be inferred automatically.\nLarge language models (LLMs) have proven effective in NLP downstream tasks such\nas text comprehension and information extraction (IE).\n  This study examines how textual implicitness affects IE tasks in pre-trained\nLLMs: LLaMA 2.3, DeepSeekV1, and Phi1.5. We generate two synthetic datasets of\n10k implicit and explicit verbalization of biographic information to measure\nthe impact on LLM performance and analyze whether fine-tuning implicit data\nimproves their ability to generalize in implicit reasoning tasks.\n  This research presents an experiment on the internal reasoning processes of\nLLMs in IE, particularly in dealing with implicit and explicit contexts. The\nresults demonstrate that fine-tuning LLM models with LoRA (low-rank adaptation)\nimproves their performance in extracting information from implicit texts,\ncontributing to better model interpretability and reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Implicitness has always been challenging in Natural Language Processing\n(NLP), with traditional methods relying on explicit statements to identify\nentities and their relationships. From the sentence \"Zuhdi attends church every\nSunday\", the relationship between Zuhdi and Christianity is evident for a human\nreader, but it presents a challenge when it must be inferred automatically.\nLarge language models (LLMs) have proven effective in NLP downstream tasks such\nas text comprehension and information extraction (IE).\n  This study examines how textual implicitness affects IE tasks in pre-trained\nLLMs: LLaMA 2.3, DeepSeekV1, and Phi1.5. We generate two synthetic datasets of\n10k implicit and explicit verbalization of biographic information to measure\nthe impact on LLM performance and analyze whether fine-tuning implicit data\nimproves their ability to generalize in implicit reasoning tasks.\n  This research presents an experiment on the internal reasoning processes of\nLLMs in IE, particularly in dealing with implicit and explicit contexts. The\nresults demonstrate that fine-tuning LLM models with LoRA (low-rank adaptation)\nimproves their performance in extracting information from implicit texts,\ncontributing to better model interpretability and reliability."
                },
                "authors": [
                    {
                        "name": "Alessandra Stramiglio"
                    },
                    {
                        "name": "Andrea Schimmenti"
                    },
                    {
                        "name": "Valentina Pasqual"
                    },
                    {
                        "name": "Marieke van Erp"
                    },
                    {
                        "name": "Francesco Sovrano"
                    },
                    {
                        "name": "Fabio Vitali"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Vitali"
                },
                "author": "Fabio Vitali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04287v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04287v2",
                "updated": "2025-09-18T13:18:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    13,
                    18,
                    52,
                    3,
                    261,
                    0
                ],
                "published": "2025-08-06T10:19:11Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    10,
                    19,
                    11,
                    2,
                    218,
                    0
                ],
                "title": "Parameter Estimation for Weakly Interacting Hypoelliptic Diffusions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter Estimation for Weakly Interacting Hypoelliptic Diffusions"
                },
                "summary": "We study parameter estimation for interacting particle systems (IPSs)\nconsisting of $N$ weakly interacting multivariate hypoelliptic SDEs. We propose\na locally Gaussian approximation of the transition dynamics, carefully designed\nto address the degenerate structure of the noise (diffusion matrix), thus\nleading to the formation of a well-defined full likelihood. Our approach\npermits carrying out statistical inference for a wide class of hypoelliptic\nIPSs that are not covered by recent works as the latter rely on the\nEuler-Maruyama scheme. We analyze a contrast estimator based on the developed\nlikelihood with $n$ high-frequency particle observations over a fixed period\n$[0,T]$ and show its asymptotic normality as $n, N \\to \\infty$ with a\nrequirement that the step-size $\\Delta_n = T/n$ is such that\n$N\\Delta_n\\rightarrow 0$, assuming that all particle coordinates (e.g.~position\nand velocity) are observed. In practical situations where only partial\nobservations (e.g. particle positions but not velocities) are available, the\nproposed locally Gaussian approximation offers greater flexibility for\ninference, when combined with established Bayesian techniques. In particular,\nunlike the Euler-Maruyama-based approaches, we do not have to impose\nrestrictive structures on the hypoelliptic IPSs. We present numerical\nexperiments that illustrate the effectiveness of our approach, both with\ncomplete and partial particle observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study parameter estimation for interacting particle systems (IPSs)\nconsisting of $N$ weakly interacting multivariate hypoelliptic SDEs. We propose\na locally Gaussian approximation of the transition dynamics, carefully designed\nto address the degenerate structure of the noise (diffusion matrix), thus\nleading to the formation of a well-defined full likelihood. Our approach\npermits carrying out statistical inference for a wide class of hypoelliptic\nIPSs that are not covered by recent works as the latter rely on the\nEuler-Maruyama scheme. We analyze a contrast estimator based on the developed\nlikelihood with $n$ high-frequency particle observations over a fixed period\n$[0,T]$ and show its asymptotic normality as $n, N \\to \\infty$ with a\nrequirement that the step-size $\\Delta_n = T/n$ is such that\n$N\\Delta_n\\rightarrow 0$, assuming that all particle coordinates (e.g.~position\nand velocity) are observed. In practical situations where only partial\nobservations (e.g. particle positions but not velocities) are available, the\nproposed locally Gaussian approximation offers greater flexibility for\ninference, when combined with established Bayesian techniques. In particular,\nunlike the Euler-Maruyama-based approaches, we do not have to impose\nrestrictive structures on the hypoelliptic IPSs. We present numerical\nexperiments that illustrate the effectiveness of our approach, both with\ncomplete and partial particle observations."
                },
                "authors": [
                    {
                        "name": "Yuga Iguchi"
                    },
                    {
                        "name": "Alexandros Beskos"
                    },
                    {
                        "name": "Grigorios A. Pavliotis"
                    }
                ],
                "author_detail": {
                    "name": "Grigorios A. Pavliotis"
                },
                "author": "Grigorios A. Pavliotis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04287v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04287v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01153v2",
                "updated": "2025-09-18T13:15:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    13,
                    15,
                    39,
                    3,
                    261,
                    0
                ],
                "published": "2025-07-01T19:27:22Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    19,
                    27,
                    22,
                    1,
                    182,
                    0
                ],
                "title": "A Frequentist Simulation-Based Inference Treatment of Sterile Neutrino\n  Global Fits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Frequentist Simulation-Based Inference Treatment of Sterile Neutrino\n  Global Fits"
                },
                "summary": "A critical challenge in particle physics is combining results from diverse\nexperimental setups that measure the same physical quantity to enhance\nprecision and statistical power, a process known as a global fit. Global fits\nof sterile neutrino searches, hunts for additional neutrino oscillation\nfrequencies and amplitudes, present an intriguing case study. In such a\nscenario, the key assumptions underlying Wilks' theorem, a cornerstone of most\nclassic frequentist analyses, do not hold. The method of Feldman and Cousins, a\ntrials-based approach which does not assume Wilks' theorem, becomes\ncomputationally prohibitive for complex or intractable likelihoods. To bypass\nthis limitation, we borrow a technique from simulation-based inference (SBI) to\nestimate likelihood ratios for use in building trials-based confidence\nintervals, speeding up test statistic evaluations by a factor $>10^4$ per grid\npoint, resulting in a faster, but approximate, frequentist fitting framework.\nApplied to a subset of sterile neutrino search data involving the disappearance\nof muon-flavor (anti)neutrinos, our method leverages machine learning to\ncompute frequentist confidence intervals while significantly reducing\ncomputational expense. In addition, the SBI-based approach holds additional\nvalue by recognizing underlying systematic uncertainties that the Wilks\napproach does not. Thus, our method allows for more robust machine\nlearning-based analyses critical to performing accurate but computationally\nfeasible global fits. This allows, for the first time, a global fit to sterile\nneutrino data without assuming Wilks' theorem. While we demonstrate the utility\nof such a technique studying sterile neutrino searches, it is applicable to\nboth single-experiment and global fits of all kinds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical challenge in particle physics is combining results from diverse\nexperimental setups that measure the same physical quantity to enhance\nprecision and statistical power, a process known as a global fit. Global fits\nof sterile neutrino searches, hunts for additional neutrino oscillation\nfrequencies and amplitudes, present an intriguing case study. In such a\nscenario, the key assumptions underlying Wilks' theorem, a cornerstone of most\nclassic frequentist analyses, do not hold. The method of Feldman and Cousins, a\ntrials-based approach which does not assume Wilks' theorem, becomes\ncomputationally prohibitive for complex or intractable likelihoods. To bypass\nthis limitation, we borrow a technique from simulation-based inference (SBI) to\nestimate likelihood ratios for use in building trials-based confidence\nintervals, speeding up test statistic evaluations by a factor $>10^4$ per grid\npoint, resulting in a faster, but approximate, frequentist fitting framework.\nApplied to a subset of sterile neutrino search data involving the disappearance\nof muon-flavor (anti)neutrinos, our method leverages machine learning to\ncompute frequentist confidence intervals while significantly reducing\ncomputational expense. In addition, the SBI-based approach holds additional\nvalue by recognizing underlying systematic uncertainties that the Wilks\napproach does not. Thus, our method allows for more robust machine\nlearning-based analyses critical to performing accurate but computationally\nfeasible global fits. This allows, for the first time, a global fit to sterile\nneutrino data without assuming Wilks' theorem. While we demonstrate the utility\nof such a technique studying sterile neutrino searches, it is applicable to\nboth single-experiment and global fits of all kinds."
                },
                "authors": [
                    {
                        "name": "Joshua Villarreal"
                    },
                    {
                        "name": "Julia Woodward"
                    },
                    {
                        "name": "John Hardin"
                    },
                    {
                        "name": "Janet Conrad"
                    }
                ],
                "author_detail": {
                    "name": "Janet Conrad"
                },
                "author": "Janet Conrad",
                "arxiv_doi": "10.1088/2632-2153/ae040c",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/2632-2153/ae040c",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.01153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Mach. Learn.: Sci. Technol. 6 035053 (2025)",
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14934v1",
                "updated": "2025-09-18T13:14:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    13,
                    14,
                    25,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T13:14:25Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    13,
                    14,
                    25,
                    3,
                    261,
                    0
                ],
                "title": "Mitigating data replication in text-to-audio generative diffusion models\n  through anti-memorization guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating data replication in text-to-audio generative diffusion models\n  through anti-memorization guidance"
                },
                "summary": "A persistent challenge in generative audio models is data replication, where\nthe model unintentionally generates parts of its training data during\ninference. In this work, we address this issue in text-to-audio diffusion\nmodels by exploring the use of anti-memorization strategies. We adopt\nAnti-Memorization Guidance (AMG), a technique that modifies the sampling\nprocess of pre-trained diffusion models to discourage memorization. Our study\nexplores three types of guidance within AMG, each designed to reduce\nreplication while preserving generation quality. We use Stable Audio Open as\nour backbone, leveraging its fully open-source architecture and training\ndataset. Our comprehensive experimental analysis suggests that AMG\nsignificantly mitigates memorization in diffusion-based text-to-audio\ngeneration without compromising audio fidelity or semantic alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A persistent challenge in generative audio models is data replication, where\nthe model unintentionally generates parts of its training data during\ninference. In this work, we address this issue in text-to-audio diffusion\nmodels by exploring the use of anti-memorization strategies. We adopt\nAnti-Memorization Guidance (AMG), a technique that modifies the sampling\nprocess of pre-trained diffusion models to discourage memorization. Our study\nexplores three types of guidance within AMG, each designed to reduce\nreplication while preserving generation quality. We use Stable Audio Open as\nour backbone, leveraging its fully open-source architecture and training\ndataset. Our comprehensive experimental analysis suggests that AMG\nsignificantly mitigates memorization in diffusion-based text-to-audio\ngeneration without compromising audio fidelity or semantic alignment."
                },
                "authors": [
                    {
                        "name": "Francisco Messina"
                    },
                    {
                        "name": "Francesca Ronchini"
                    },
                    {
                        "name": "Luca Comanducci"
                    },
                    {
                        "name": "Paolo Bestagini"
                    },
                    {
                        "name": "Fabio Antonacci"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Antonacci"
                },
                "author": "Fabio Antonacci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14930v1",
                "updated": "2025-09-18T13:07:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    13,
                    7,
                    53,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T13:07:53Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    13,
                    7,
                    53,
                    3,
                    261,
                    0
                ],
                "title": "Cross-Modal Knowledge Distillation for Speech Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Modal Knowledge Distillation for Speech Large Language Models"
                },
                "summary": "In this work, we present the first systematic evaluation of catastrophic\nforgetting and modality inequivalence in speech large language models, showing\nthat introducing speech capabilities can degrade knowledge and reasoning even\nwhen inputs remain textual, and performance further decreases with spoken\nqueries. To address these challenges, we propose a cross-modal knowledge\ndistillation framework that leverages both text-to-text and speech-to-text\nchannels to transfer knowledge from a text-based teacher model to a speech LLM.\nExtensive experiments on dialogue and audio understanding tasks validate the\neffectiveness of our approach in preserving textual knowledge, improving\ncross-modal alignment, and enhancing reasoning in speech-based interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present the first systematic evaluation of catastrophic\nforgetting and modality inequivalence in speech large language models, showing\nthat introducing speech capabilities can degrade knowledge and reasoning even\nwhen inputs remain textual, and performance further decreases with spoken\nqueries. To address these challenges, we propose a cross-modal knowledge\ndistillation framework that leverages both text-to-text and speech-to-text\nchannels to transfer knowledge from a text-based teacher model to a speech LLM.\nExtensive experiments on dialogue and audio understanding tasks validate the\neffectiveness of our approach in preserving textual knowledge, improving\ncross-modal alignment, and enhancing reasoning in speech-based interactions."
                },
                "authors": [
                    {
                        "name": "Enzhi Wang"
                    },
                    {
                        "name": "Qicheng Li"
                    },
                    {
                        "name": "Zhiyuan Tang"
                    },
                    {
                        "name": "Yuhang Jia"
                    }
                ],
                "author_detail": {
                    "name": "Yuhang Jia"
                },
                "author": "Yuhang Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14928v1",
                "updated": "2025-09-18T13:07:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    13,
                    7,
                    14,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T13:07:14Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    13,
                    7,
                    14,
                    3,
                    261,
                    0
                ],
                "title": "Radial pulsation runaway in massive red supergiants in late evolutionary\n  stage and implications to hydrogen-rich supernovae",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radial pulsation runaway in massive red supergiants in late evolutionary\n  stage and implications to hydrogen-rich supernovae"
                },
                "summary": "Performing a series of hydrodynamic stellar evolutionary simulations with\n\\textsc{Mesa} (Module for Experiments in Stellar Astrophysics), we investigate\nthe excitation and growth of radial pulsations of massive red supergiants\n(RSGs) with the initial mass range of\n$M_\\mathrm{ini}=13$--$18\\,\\mathrm{M}_\\odot$. We show that strong radial\npulsations develop in the hydrogen-rich envelope in their late evolutionary\nstages, and eventually the surface radial velocity exceeds the escape velocity\nfor higher-mass models. On the other hand, lower-mass models exhibit more\nmoderate pulsations with finite velocity amplitudes and are expected to keep\nmassive hydrogen-rich envelopes when they evolve toward the gravitational\ncollapse of the iron core. While the latter group ends up as a familiar\ntransient population of exploding RSGs, i.e., type IIP supernovae (SNe), the\nformer group may expel a part of their envelopes and explode as different\ntransients population. We investigate how the energy of the oscillating\nenvelope is dissipated and released as radiation. We also empirically determine\nthe condition for the pulsation-driven mass ejection in terms of the\nluminosity-to-mass ratio, $L/M>10^{3.9}\\mathrm{L}_\\odot/\\mathrm{M}_\\odot$. The\ncorresponding luminosity threshold for the explored mass range may explain the\nobservationally inferred constraints on type IIP SN progenitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performing a series of hydrodynamic stellar evolutionary simulations with\n\\textsc{Mesa} (Module for Experiments in Stellar Astrophysics), we investigate\nthe excitation and growth of radial pulsations of massive red supergiants\n(RSGs) with the initial mass range of\n$M_\\mathrm{ini}=13$--$18\\,\\mathrm{M}_\\odot$. We show that strong radial\npulsations develop in the hydrogen-rich envelope in their late evolutionary\nstages, and eventually the surface radial velocity exceeds the escape velocity\nfor higher-mass models. On the other hand, lower-mass models exhibit more\nmoderate pulsations with finite velocity amplitudes and are expected to keep\nmassive hydrogen-rich envelopes when they evolve toward the gravitational\ncollapse of the iron core. While the latter group ends up as a familiar\ntransient population of exploding RSGs, i.e., type IIP supernovae (SNe), the\nformer group may expel a part of their envelopes and explode as different\ntransients population. We investigate how the energy of the oscillating\nenvelope is dissipated and released as radiation. We also empirically determine\nthe condition for the pulsation-driven mass ejection in terms of the\nluminosity-to-mass ratio, $L/M>10^{3.9}\\mathrm{L}_\\odot/\\mathrm{M}_\\odot$. The\ncorresponding luminosity threshold for the explored mass range may explain the\nobservationally inferred constraints on type IIP SN progenitors."
                },
                "authors": [
                    {
                        "name": "Akihiro Suzuki"
                    },
                    {
                        "name": "Toshikazu Shigeyama"
                    }
                ],
                "author_detail": {
                    "name": "Toshikazu Shigeyama"
                },
                "author": "Toshikazu Shigeyama",
                "arxiv_comment": "23 pages, 15 figures, accepted for publication in MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14926v1",
                "updated": "2025-09-18T13:04:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    13,
                    4,
                    30,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T13:04:30Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    13,
                    4,
                    30,
                    3,
                    261,
                    0
                ],
                "title": "Patent Language Model Pretraining with ModernBERT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patent Language Model Pretraining with ModernBERT"
                },
                "summary": "Transformer-based language models such as BERT have become foundational in\nNLP, yet their performance degrades in specialized domains like patents, which\ncontain long, technical, and legally structured text. Prior approaches to\npatent NLP have primarily relied on fine-tuning general-purpose models or\ndomain-adapted variants pretrained with limited data. In this work, we pretrain\n3 domain-specific masked language models for patents, using the ModernBERT\narchitecture and a curated corpus of over 60 million patent records. Our\napproach incorporates architectural optimizations, including FlashAttention,\nrotary embeddings, and GLU feed-forward layers. We evaluate our models on four\ndownstream patent classification tasks. Our model, ModernBERT-base-PT,\nconsistently outperforms the general-purpose ModernBERT baseline on three out\nof four datasets and achieves competitive performance with a baseline\nPatentBERT. Additional experiments with ModernBERT-base-VX and\nMosaic-BERT-large demonstrate that scaling the model size and customizing the\ntokenizer further enhance performance on selected tasks. Notably, all\nModernBERT variants retain substantially faster inference over - 3x that of\nPatentBERT - underscoring their suitability for time-sensitive applications.\nThese results underscore the benefits of domain-specific pretraining and\narchitectural improvements for patent-focused NLP tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based language models such as BERT have become foundational in\nNLP, yet their performance degrades in specialized domains like patents, which\ncontain long, technical, and legally structured text. Prior approaches to\npatent NLP have primarily relied on fine-tuning general-purpose models or\ndomain-adapted variants pretrained with limited data. In this work, we pretrain\n3 domain-specific masked language models for patents, using the ModernBERT\narchitecture and a curated corpus of over 60 million patent records. Our\napproach incorporates architectural optimizations, including FlashAttention,\nrotary embeddings, and GLU feed-forward layers. We evaluate our models on four\ndownstream patent classification tasks. Our model, ModernBERT-base-PT,\nconsistently outperforms the general-purpose ModernBERT baseline on three out\nof four datasets and achieves competitive performance with a baseline\nPatentBERT. Additional experiments with ModernBERT-base-VX and\nMosaic-BERT-large demonstrate that scaling the model size and customizing the\ntokenizer further enhance performance on selected tasks. Notably, all\nModernBERT variants retain substantially faster inference over - 3x that of\nPatentBERT - underscoring their suitability for time-sensitive applications.\nThese results underscore the benefits of domain-specific pretraining and\narchitectural improvements for patent-focused NLP tasks."
                },
                "authors": [
                    {
                        "name": "Amirhossein Yousefiramandi"
                    },
                    {
                        "name": "Ciaran Cooney"
                    }
                ],
                "author_detail": {
                    "name": "Ciaran Cooney"
                },
                "author": "Ciaran Cooney",
                "arxiv_comment": "7 pages, 2 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14922v1",
                "updated": "2025-09-18T12:59:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    12,
                    59,
                    7,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T12:59:07Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    12,
                    59,
                    7,
                    3,
                    261,
                    0
                ],
                "title": "A Comparative Evaluation of Large Language Models for Persian Sentiment\n  Analysis and Emotion Detection in Social Media Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparative Evaluation of Large Language Models for Persian Sentiment\n  Analysis and Emotion Detection in Social Media Texts"
                },
                "summary": "This study presents a comprehensive comparative evaluation of four\nstate-of-the-art Large Language Models (LLMs)--Claude 3.7 Sonnet, DeepSeek-V3,\nGemini 2.0 Flash, and GPT-4o--for sentiment analysis and emotion detection in\nPersian social media texts. Comparative analysis among LLMs has witnessed a\nsignificant rise in recent years, however, most of these analyses have been\nconducted on English language tasks, creating gaps in understanding\ncross-linguistic performance patterns. This research addresses these gaps\nthrough rigorous experimental design using balanced Persian datasets containing\n900 texts for sentiment analysis (positive, negative, neutral) and 1,800 texts\nfor emotion detection (anger, fear, happiness, hate, sadness, surprise). The\nmain focus was to allow for a direct and fair comparison among different\nmodels, by using consistent prompts, uniform processing parameters, and by\nanalyzing the performance metrics such as precision, recall, F1-scores, along\nwith misclassification patterns. The results show that all models reach an\nacceptable level of performance, and a statistical comparison of the best three\nmodels indicates no significant differences among them. However, GPT-4o\ndemonstrated a marginally higher raw accuracy value for both tasks, while\nGemini 2.0 Flash proved to be the most cost-efficient. The findings indicate\nthat the emotion detection task is more challenging for all models compared to\nthe sentiment analysis task, and the misclassification patterns can represent\nsome challenges in Persian language texts. These findings establish performance\nbenchmarks for Persian NLP applications and offer practical guidance for model\nselection based on accuracy, efficiency, and cost considerations, while\nrevealing cultural and linguistic challenges that require consideration in\nmultilingual AI system deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a comprehensive comparative evaluation of four\nstate-of-the-art Large Language Models (LLMs)--Claude 3.7 Sonnet, DeepSeek-V3,\nGemini 2.0 Flash, and GPT-4o--for sentiment analysis and emotion detection in\nPersian social media texts. Comparative analysis among LLMs has witnessed a\nsignificant rise in recent years, however, most of these analyses have been\nconducted on English language tasks, creating gaps in understanding\ncross-linguistic performance patterns. This research addresses these gaps\nthrough rigorous experimental design using balanced Persian datasets containing\n900 texts for sentiment analysis (positive, negative, neutral) and 1,800 texts\nfor emotion detection (anger, fear, happiness, hate, sadness, surprise). The\nmain focus was to allow for a direct and fair comparison among different\nmodels, by using consistent prompts, uniform processing parameters, and by\nanalyzing the performance metrics such as precision, recall, F1-scores, along\nwith misclassification patterns. The results show that all models reach an\nacceptable level of performance, and a statistical comparison of the best three\nmodels indicates no significant differences among them. However, GPT-4o\ndemonstrated a marginally higher raw accuracy value for both tasks, while\nGemini 2.0 Flash proved to be the most cost-efficient. The findings indicate\nthat the emotion detection task is more challenging for all models compared to\nthe sentiment analysis task, and the misclassification patterns can represent\nsome challenges in Persian language texts. These findings establish performance\nbenchmarks for Persian NLP applications and offer practical guidance for model\nselection based on accuracy, efficiency, and cost considerations, while\nrevealing cultural and linguistic challenges that require consideration in\nmultilingual AI system deployment."
                },
                "authors": [
                    {
                        "name": "Kian Tohidi"
                    },
                    {
                        "name": "Kia Dashtipour"
                    },
                    {
                        "name": "Simone Rebora"
                    },
                    {
                        "name": "Sevda Pourfaramarz"
                    }
                ],
                "author_detail": {
                    "name": "Sevda Pourfaramarz"
                },
                "author": "Sevda Pourfaramarz",
                "arxiv_comment": "19 pages, 8 Figures, 9 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19026v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19026v3",
                "updated": "2025-09-18T12:56:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    12,
                    56,
                    38,
                    3,
                    261,
                    0
                ],
                "published": "2025-08-26T13:43:45Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    43,
                    45,
                    1,
                    238,
                    0
                ],
                "title": "MovieCORE: COgnitive REasoning in Movies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MovieCORE: COgnitive REasoning in Movies"
                },
                "summary": "This paper introduces MovieCORE, a novel video question answering (VQA)\ndataset designed to probe deeper cognitive understanding of movie content.\nUnlike existing datasets that focus on surface-level comprehension, MovieCORE\nemphasizes questions that engage System-2 thinking while remaining specific to\nthe video material. We present an innovative agentic brainstorming approach,\nutilizing multiple large language models (LLMs) as thought agents to generate\nand refine high-quality question-answer pairs. To evaluate dataset quality, we\ndevelop a set of cognitive tests assessing depth, thought-provocation\npotential, and syntactic complexity. We also propose a comprehensive evaluation\nscheme for assessing VQA model performance on deeper cognitive tasks. To\naddress the limitations of existing video-language models (VLMs), we introduce\nan agentic enhancement module, Agentic Choice Enhancement (ACE), which improves\nmodel reasoning capabilities post-training by up to 25%. Our work contributes\nto advancing movie understanding in AI systems and provides valuable insights\ninto the capabilities and limitations of current VQA models when faced with\nmore challenging, nuanced questions about cinematic content. Our project page,\ndataset and code can be found at\nhttps://joslefaure.github.io/assets/html/moviecore.html.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces MovieCORE, a novel video question answering (VQA)\ndataset designed to probe deeper cognitive understanding of movie content.\nUnlike existing datasets that focus on surface-level comprehension, MovieCORE\nemphasizes questions that engage System-2 thinking while remaining specific to\nthe video material. We present an innovative agentic brainstorming approach,\nutilizing multiple large language models (LLMs) as thought agents to generate\nand refine high-quality question-answer pairs. To evaluate dataset quality, we\ndevelop a set of cognitive tests assessing depth, thought-provocation\npotential, and syntactic complexity. We also propose a comprehensive evaluation\nscheme for assessing VQA model performance on deeper cognitive tasks. To\naddress the limitations of existing video-language models (VLMs), we introduce\nan agentic enhancement module, Agentic Choice Enhancement (ACE), which improves\nmodel reasoning capabilities post-training by up to 25%. Our work contributes\nto advancing movie understanding in AI systems and provides valuable insights\ninto the capabilities and limitations of current VQA models when faced with\nmore challenging, nuanced questions about cinematic content. Our project page,\ndataset and code can be found at\nhttps://joslefaure.github.io/assets/html/moviecore.html."
                },
                "authors": [
                    {
                        "name": "Gueter Josmy Faure"
                    },
                    {
                        "name": "Min-Hung Chen"
                    },
                    {
                        "name": "Jia-Fong Yeh"
                    },
                    {
                        "name": "Ying Cheng"
                    },
                    {
                        "name": "Hung-Ting Su"
                    },
                    {
                        "name": "Yung-Hao Tang"
                    },
                    {
                        "name": "Shang-Hong Lai"
                    },
                    {
                        "name": "Winston H. Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Winston H. Hsu"
                },
                "author": "Winston H. Hsu",
                "arxiv_comment": "Accepted for EMNLP'2025 Main Conference (Oral Presentation). Project\n  Page: https://joslefaure.github.io/assets/html/moviecore.html",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19026v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19026v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13456v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13456v3",
                "updated": "2025-09-18T12:48:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    12,
                    48,
                    34,
                    3,
                    261,
                    0
                ],
                "published": "2024-10-17T11:34:07Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    11,
                    34,
                    7,
                    3,
                    291,
                    0
                ],
                "title": "Unlocking Legal Knowledge: A Multilingual Dataset for Judicial\n  Summarization in Switzerland",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Legal Knowledge: A Multilingual Dataset for Judicial\n  Summarization in Switzerland"
                },
                "summary": "Legal research depends on headnotes: concise summaries that help lawyers\nquickly identify relevant cases. Yet, many court decisions lack them due to the\nhigh cost of manual annotation. To address this gap, we introduce the Swiss\nLandmark Decisions Summarization (SLDS) dataset containing 20K rulings from the\nSwiss Federal Supreme Court, each with headnotes in German, French, and\nItalian. SLDS has the potential to significantly improve access to legal\ninformation and transform legal research in Switzerland. We fine-tune open\nmodels (Qwen2.5, Llama 3.2, Phi-3.5) and compare them to larger general-purpose\nand reasoning-tuned LLMs, including GPT-4o, Claude 3.5 Sonnet, and the\nopen-source DeepSeek R1. Using an LLM-as-a-Judge framework, we find that\nfine-tuned models perform well in terms of lexical similarity, while larger\nmodels generate more legally accurate and coherent summaries. Interestingly,\nreasoning-focused models show no consistent benefit, suggesting that factual\nprecision is more important than deep reasoning in this task. We release SLDS\nunder a CC BY 4.0 license to support future research in cross-lingual legal\nsummarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal research depends on headnotes: concise summaries that help lawyers\nquickly identify relevant cases. Yet, many court decisions lack them due to the\nhigh cost of manual annotation. To address this gap, we introduce the Swiss\nLandmark Decisions Summarization (SLDS) dataset containing 20K rulings from the\nSwiss Federal Supreme Court, each with headnotes in German, French, and\nItalian. SLDS has the potential to significantly improve access to legal\ninformation and transform legal research in Switzerland. We fine-tune open\nmodels (Qwen2.5, Llama 3.2, Phi-3.5) and compare them to larger general-purpose\nand reasoning-tuned LLMs, including GPT-4o, Claude 3.5 Sonnet, and the\nopen-source DeepSeek R1. Using an LLM-as-a-Judge framework, we find that\nfine-tuned models perform well in terms of lexical similarity, while larger\nmodels generate more legally accurate and coherent summaries. Interestingly,\nreasoning-focused models show no consistent benefit, suggesting that factual\nprecision is more important than deep reasoning in this task. We release SLDS\nunder a CC BY 4.0 license to support future research in cross-lingual legal\nsummarization."
                },
                "authors": [
                    {
                        "name": "Luca Rolshoven"
                    },
                    {
                        "name": "Vishvaksenan Rasiah"
                    },
                    {
                        "name": "Srinanda Brügger Bose"
                    },
                    {
                        "name": "Sarah Hostettler"
                    },
                    {
                        "name": "Lara Burkhalter"
                    },
                    {
                        "name": "Matthias Stürmer"
                    },
                    {
                        "name": "Joel Niklaus"
                    }
                ],
                "author_detail": {
                    "name": "Joel Niklaus"
                },
                "author": "Joel Niklaus",
                "arxiv_comment": "Accepted to EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13456v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13456v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14911v1",
                "updated": "2025-09-18T12:40:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    12,
                    40,
                    35,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T12:40:35Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    12,
                    40,
                    35,
                    3,
                    261,
                    0
                ],
                "title": "Learning Informed Prior Distributions with Normalizing Flows for\n  Bayesian Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Informed Prior Distributions with Normalizing Flows for\n  Bayesian Analysis"
                },
                "summary": "We investigate the use of normalizing flow (NF) models as flexible priors in\nBayesian inference with Markov Chain Monte Carlo (MCMC) sampling. Trained on\nposteriors from previous analyses, these models can be used as informative\npriors, capturing non-trivial distributions and correlations, in subsequent\ninference tasks. We compare different training strategies and loss functions,\nfinding that training based on Kullback-Leibler (KL) divergence and\nunsupervised learning consistently yield the most accurate reproductions of\nreference distributions. Applied in sequential Bayesian workflows, MCMC with\nthe NF-based priors reproduces the results of one-shot joint inferences well,\nprovided the target distributions are unimodal. In cases with pronounced\nmulti-modality or dataset tension, distortions may arise, underscoring the need\nfor caution in multi-stage Bayesian inference. A comparison between the pocoMC\nMCMC sampler and the standard emcee sampler further demonstrates the importance\nof advanced and robust algorithms for exploring the posterior space. Overall,\nour results establish NF-based priors as a practical and efficient tool for\nsequential Bayesian inference in high-dimensional parameter spaces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the use of normalizing flow (NF) models as flexible priors in\nBayesian inference with Markov Chain Monte Carlo (MCMC) sampling. Trained on\nposteriors from previous analyses, these models can be used as informative\npriors, capturing non-trivial distributions and correlations, in subsequent\ninference tasks. We compare different training strategies and loss functions,\nfinding that training based on Kullback-Leibler (KL) divergence and\nunsupervised learning consistently yield the most accurate reproductions of\nreference distributions. Applied in sequential Bayesian workflows, MCMC with\nthe NF-based priors reproduces the results of one-shot joint inferences well,\nprovided the target distributions are unimodal. In cases with pronounced\nmulti-modality or dataset tension, distortions may arise, underscoring the need\nfor caution in multi-stage Bayesian inference. A comparison between the pocoMC\nMCMC sampler and the standard emcee sampler further demonstrates the importance\nof advanced and robust algorithms for exploring the posterior space. Overall,\nour results establish NF-based priors as a practical and efficient tool for\nsequential Bayesian inference in high-dimensional parameter spaces."
                },
                "authors": [
                    {
                        "name": "Hendrik Roch"
                    },
                    {
                        "name": "Chun Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chun Shen"
                },
                "author": "Chun Shen",
                "arxiv_comment": "13 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06217v2",
                "updated": "2025-09-18T12:31:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    12,
                    31,
                    12,
                    3,
                    261,
                    0
                ],
                "published": "2025-02-10T07:49:35Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    7,
                    49,
                    35,
                    0,
                    41,
                    0
                ],
                "title": "Examining False Positives under Inference Scaling for Mathematical\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining False Positives under Inference Scaling for Mathematical\n  Reasoning"
                },
                "summary": "Recent advancements in language models have led to significant improvements\nin mathematical reasoning across various benchmarks. However, most of these\nbenchmarks rely on automatic evaluation methods that only compare final answers\nusing heuristics, without verifying the underlying reasoning steps. This\nlimitation results in false positive solutions, where models may produce\ncorrect final answers but with flawed deduction paths. In this paper, we\nsystematically examine the prevalence of false positive solutions in\nmathematical problem solving for language models. We analyze the\ncharacteristics and extent of this issue across different open-source models,\ndatasets of varying difficulty levels, and decoding strategies. Specifically,\nwe explore how false positives influence the inference time scaling behavior of\nlanguage models. Our experimental results reveal that: (1) false positive\nsolutions persist across different models, datasets, and decoding methods, (2)\nsampling-based inference time scaling methods do not alleviate the problem, and\n(3) the pass@N evaluation metric is more susceptible to false positives,\nsuggesting a significantly lower scaling ceiling than what automatic\nevaluations indicate. Additionally, we analyze specific instances of false\npositives and discuss potential limitations in self-improvement techniques and\nsynthetic data generation under such conditions. Our data and code are publicly\navailable at https://github.com/Wloner0809/False-Positives-in-Math.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in language models have led to significant improvements\nin mathematical reasoning across various benchmarks. However, most of these\nbenchmarks rely on automatic evaluation methods that only compare final answers\nusing heuristics, without verifying the underlying reasoning steps. This\nlimitation results in false positive solutions, where models may produce\ncorrect final answers but with flawed deduction paths. In this paper, we\nsystematically examine the prevalence of false positive solutions in\nmathematical problem solving for language models. We analyze the\ncharacteristics and extent of this issue across different open-source models,\ndatasets of varying difficulty levels, and decoding strategies. Specifically,\nwe explore how false positives influence the inference time scaling behavior of\nlanguage models. Our experimental results reveal that: (1) false positive\nsolutions persist across different models, datasets, and decoding methods, (2)\nsampling-based inference time scaling methods do not alleviate the problem, and\n(3) the pass@N evaluation metric is more susceptible to false positives,\nsuggesting a significantly lower scaling ceiling than what automatic\nevaluations indicate. Additionally, we analyze specific instances of false\npositives and discuss potential limitations in self-improvement techniques and\nsynthetic data generation under such conditions. Our data and code are publicly\navailable at https://github.com/Wloner0809/False-Positives-in-Math."
                },
                "authors": [
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Nan Yang"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Furu Wei"
                    },
                    {
                        "name": "Fuli Feng"
                    }
                ],
                "author_detail": {
                    "name": "Fuli Feng"
                },
                "author": "Fuli Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14902v1",
                "updated": "2025-09-18T12:27:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    12,
                    27,
                    16,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T12:27:16Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    12,
                    27,
                    16,
                    3,
                    261,
                    0
                ],
                "title": "Magnetic activity and differential rotation of HIP12653",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetic activity and differential rotation of HIP12653"
                },
                "summary": "We present a spectroscopic and photometric study of HIP12653 to investigate\nits magnetic cycle and differential rotation. Using HARPS archival spectra\nmatched with MARCS-AMBER theoretical templates, we derive the stellar\nparameters (Teff, logg, FeH, and vsini) of the target. The S-index, an activity\nindicator based on the emission of the CaII H&K lines, is fitted to determine\nthe magnetic cycle and rotation periods. We refine the magnetic cycle period to\n5799.20 \\pm 0.88 d and suggest the existence of a secondary, shorter cycle of\n674.6922 \\pm 0.0098 d, making HIP12653 the youngest star known to exhibit such\na short activity cycle. During the minimum activity phase, a rotation period of\n4.8 d is estimated. This is notably different from the 7-day period obtained\nwhen measurements during minimum activity are excluded, suggesting that these\ntwo periods are rotation periods at different latitudes. To explore this\nhypothesis, we introduce a novel light curve fitting method that incorporates\nmultiple harmonics to model different spot configurations. Applied to synthetic\nlight curves, the method recovers at least two rotation periods close to the\ntrue input values in 92.1% of cases. The inferred rotation shear shows a median\ndeviation of 0.0011 \\pm 0.0003 and a standard deviation of 0.0177 \\pm 0.0002\nfrom the true value. Applying this approach to TESS photometric data from 2018\nto 2023, we detect three distinct rotation periods, 4.8 d, 5.7 d, and 7.7 d,\n(along with a signal at 3.75 d interpreted as its first harmonic), consistent\nwith spots located at different latitudes. Assuming a solar-like differential\nrotation, we estimate an inclination of 34.0 \\pm 1.8^\\circ and a rotational\nshear of \\alpha = 0.38 \\pm 0.01. These results confirm the 4.8-d period and\ndemonstrate that differential rotation can be constrained by tracking rotation\nperiod changes across different phases of the magnetic cycle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a spectroscopic and photometric study of HIP12653 to investigate\nits magnetic cycle and differential rotation. Using HARPS archival spectra\nmatched with MARCS-AMBER theoretical templates, we derive the stellar\nparameters (Teff, logg, FeH, and vsini) of the target. The S-index, an activity\nindicator based on the emission of the CaII H&K lines, is fitted to determine\nthe magnetic cycle and rotation periods. We refine the magnetic cycle period to\n5799.20 \\pm 0.88 d and suggest the existence of a secondary, shorter cycle of\n674.6922 \\pm 0.0098 d, making HIP12653 the youngest star known to exhibit such\na short activity cycle. During the minimum activity phase, a rotation period of\n4.8 d is estimated. This is notably different from the 7-day period obtained\nwhen measurements during minimum activity are excluded, suggesting that these\ntwo periods are rotation periods at different latitudes. To explore this\nhypothesis, we introduce a novel light curve fitting method that incorporates\nmultiple harmonics to model different spot configurations. Applied to synthetic\nlight curves, the method recovers at least two rotation periods close to the\ntrue input values in 92.1% of cases. The inferred rotation shear shows a median\ndeviation of 0.0011 \\pm 0.0003 and a standard deviation of 0.0177 \\pm 0.0002\nfrom the true value. Applying this approach to TESS photometric data from 2018\nto 2023, we detect three distinct rotation periods, 4.8 d, 5.7 d, and 7.7 d,\n(along with a signal at 3.75 d interpreted as its first harmonic), consistent\nwith spots located at different latitudes. Assuming a solar-like differential\nrotation, we estimate an inclination of 34.0 \\pm 1.8^\\circ and a rotational\nshear of \\alpha = 0.38 \\pm 0.01. These results confirm the 4.8-d period and\ndemonstrate that differential rotation can be constrained by tracking rotation\nperiod changes across different phases of the magnetic cycle."
                },
                "authors": [
                    {
                        "name": "Amina Boulkaboul"
                    },
                    {
                        "name": "Lotfi Yelles Chaouche"
                    },
                    {
                        "name": "Alessandro Carmelo Lanzafame"
                    },
                    {
                        "name": "Yassine Damerdji"
                    }
                ],
                "author_detail": {
                    "name": "Yassine Damerdji"
                },
                "author": "Yassine Damerdji",
                "arxiv_doi": "10.1088/1674-4527/ae05fb",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1674-4527/ae05fb",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.14902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication in Research in Astronomy and Astrophysics",
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19176v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19176v3",
                "updated": "2025-09-18T12:24:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    12,
                    24,
                    53,
                    3,
                    261,
                    0
                ],
                "published": "2025-05-25T14:48:49Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    14,
                    48,
                    49,
                    6,
                    145,
                    0
                ],
                "title": "Assistant-Guided Mitigation of Teacher Preference Bias in LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assistant-Guided Mitigation of Teacher Preference Bias in LLM-as-a-Judge"
                },
                "summary": "LLM-as-a-Judge employs large language models (LLMs), such as GPT-4, to\nevaluate the quality of LLM-generated responses, gaining popularity for its\ncost-effectiveness and strong alignment with human evaluations. However,\ntraining proxy judge models using evaluation data generated by powerful teacher\nmodels introduces a critical yet previously overlooked issue: teacher\npreference bias, where the proxy judge model learns a biased preference for\nresponses from the teacher model. To tackle this problem, we propose a novel\nsetting that incorporates an additional assistant model, which is not biased\ntoward the teacher model's responses, to complement the training data. Building\non this setup, we introduce AGDe-Judge, a three-stage framework designed to\ndebias from both the labels and feedbacks in the training data. Extensive\nexperiments demonstrate that AGDe-Judge effectively reduces teacher preference\nbias while maintaining strong performance across six evaluation benchmarks.\nCode is available at https://github.com/Liuz233/AGDe-Judge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge employs large language models (LLMs), such as GPT-4, to\nevaluate the quality of LLM-generated responses, gaining popularity for its\ncost-effectiveness and strong alignment with human evaluations. However,\ntraining proxy judge models using evaluation data generated by powerful teacher\nmodels introduces a critical yet previously overlooked issue: teacher\npreference bias, where the proxy judge model learns a biased preference for\nresponses from the teacher model. To tackle this problem, we propose a novel\nsetting that incorporates an additional assistant model, which is not biased\ntoward the teacher model's responses, to complement the training data. Building\non this setup, we introduce AGDe-Judge, a three-stage framework designed to\ndebias from both the labels and feedbacks in the training data. Extensive\nexperiments demonstrate that AGDe-Judge effectively reduces teacher preference\nbias while maintaining strong performance across six evaluation benchmarks.\nCode is available at https://github.com/Liuz233/AGDe-Judge."
                },
                "authors": [
                    {
                        "name": "Zhuo Liu"
                    },
                    {
                        "name": "Moxin Li"
                    },
                    {
                        "name": "Xun Deng"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Fuli Feng"
                    }
                ],
                "author_detail": {
                    "name": "Fuli Feng"
                },
                "author": "Fuli Feng",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19176v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19176v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14901v1",
                "updated": "2025-09-18T12:23:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    12,
                    23,
                    51,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T12:23:51Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    12,
                    23,
                    51,
                    3,
                    261,
                    0
                ],
                "title": "Pseudo-Label Enhanced Cascaded Framework: 2nd Technical Report for LSVOS\n  2025 VOS Track",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pseudo-Label Enhanced Cascaded Framework: 2nd Technical Report for LSVOS\n  2025 VOS Track"
                },
                "summary": "Complex Video Object Segmentation (VOS) presents significant challenges in\naccurately segmenting objects across frames, especially in the presence of\nsmall and similar targets, frequent occlusions, rapid motion, and complex\ninteractions. In this report, we present our solution for the LSVOS 2025 VOS\nTrack based on the SAM2 framework. We adopt a pseudo-labeling strategy during\ntraining: a trained SAM2 checkpoint is deployed within the SAM2Long framework\nto generate pseudo labels for the MOSE test set, which are then combined with\nexisting data for further training. For inference, the SAM2Long framework is\nemployed to obtain our primary segmentation results, while an open-source SeC\nmodel runs in parallel to produce complementary predictions. A cascaded\ndecision mechanism dynamically integrates outputs from both models, exploiting\nthe temporal stability of SAM2Long and the concept-level robustness of SeC.\nBenefiting from pseudo-label training and cascaded multi-model inference, our\napproach achieves a J\\&F score of 0.8616 on the MOSE test set -- +1.4 points\nover our SAM2Long baseline -- securing the 2nd place in the LSVOS 2025 VOS\nTrack, and demonstrating strong robustness and accuracy in long, complex video\nsegmentation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex Video Object Segmentation (VOS) presents significant challenges in\naccurately segmenting objects across frames, especially in the presence of\nsmall and similar targets, frequent occlusions, rapid motion, and complex\ninteractions. In this report, we present our solution for the LSVOS 2025 VOS\nTrack based on the SAM2 framework. We adopt a pseudo-labeling strategy during\ntraining: a trained SAM2 checkpoint is deployed within the SAM2Long framework\nto generate pseudo labels for the MOSE test set, which are then combined with\nexisting data for further training. For inference, the SAM2Long framework is\nemployed to obtain our primary segmentation results, while an open-source SeC\nmodel runs in parallel to produce complementary predictions. A cascaded\ndecision mechanism dynamically integrates outputs from both models, exploiting\nthe temporal stability of SAM2Long and the concept-level robustness of SeC.\nBenefiting from pseudo-label training and cascaded multi-model inference, our\napproach achieves a J\\&F score of 0.8616 on the MOSE test set -- +1.4 points\nover our SAM2Long baseline -- securing the 2nd place in the LSVOS 2025 VOS\nTrack, and demonstrating strong robustness and accuracy in long, complex video\nsegmentation scenarios."
                },
                "authors": [
                    {
                        "name": "An Yan"
                    },
                    {
                        "name": "Leilei Cao"
                    },
                    {
                        "name": "Feng Lu"
                    },
                    {
                        "name": "Ran Hong"
                    },
                    {
                        "name": "Youhai Jiang"
                    },
                    {
                        "name": "Fengjie Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Fengjie Zhu"
                },
                "author": "Fengjie Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14900v1",
                "updated": "2025-09-18T12:22:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    12,
                    22,
                    32,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T12:22:32Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    12,
                    22,
                    32,
                    3,
                    261,
                    0
                ],
                "title": "FURINA: Free from Unmergeable Router via LINear Aggregation of mixed\n  experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FURINA: Free from Unmergeable Router via LINear Aggregation of mixed\n  experts"
                },
                "summary": "The Mixture of Experts (MoE) paradigm has been successfully integrated into\nLow-Rank Adaptation (LoRA) for parameter-efficient fine-tuning (PEFT),\ndelivering performance gains with minimal parameter overhead. However, a key\nlimitation of existing MoE-LoRA methods is their reliance on a discrete router,\nwhich prevents the integration of the MoE components into the backbone model.\nTo overcome this, we propose FURINA, a novel Free from Unmergeable Router\nframework based on the LINear Aggregation of experts. FURINA eliminates the\nrouter by introducing a Self-Routing mechanism. This is achieved through three\ncore innovations: (1) decoupled learning of the direction and magnitude for\nLoRA adapters, (2) a shared learnable magnitude vector for consistent\nactivation scaling, and (3) expert selection loss that encourages divergent\nexpert activation. The proposed mechanism leverages the angular similarity\nbetween the input and each adapter's directional component to activate experts,\nwhich are then scaled by the shared magnitude vector. This design allows the\noutput norm to naturally reflect the importance of each expert, thereby\nenabling dynamic, router-free routing. The expert selection loss further\nsharpens this behavior by encouraging sparsity and aligning it with standard\nMoE activation patterns. We also introduce a shared expert within the MoE-LoRA\nblock that provides stable, foundational knowledge. To the best of our\nknowledge, FURINA is the first router-free, MoE-enhanced LoRA method that can\nbe fully merged into the backbone model, introducing zero additional\ninference-time cost or complexity. Extensive experiments demonstrate that\nFURINA not only significantly outperforms standard LoRA but also matches or\nsurpasses the performance of existing MoE-LoRA methods, while eliminating the\nextra inference-time overhead of MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) paradigm has been successfully integrated into\nLow-Rank Adaptation (LoRA) for parameter-efficient fine-tuning (PEFT),\ndelivering performance gains with minimal parameter overhead. However, a key\nlimitation of existing MoE-LoRA methods is their reliance on a discrete router,\nwhich prevents the integration of the MoE components into the backbone model.\nTo overcome this, we propose FURINA, a novel Free from Unmergeable Router\nframework based on the LINear Aggregation of experts. FURINA eliminates the\nrouter by introducing a Self-Routing mechanism. This is achieved through three\ncore innovations: (1) decoupled learning of the direction and magnitude for\nLoRA adapters, (2) a shared learnable magnitude vector for consistent\nactivation scaling, and (3) expert selection loss that encourages divergent\nexpert activation. The proposed mechanism leverages the angular similarity\nbetween the input and each adapter's directional component to activate experts,\nwhich are then scaled by the shared magnitude vector. This design allows the\noutput norm to naturally reflect the importance of each expert, thereby\nenabling dynamic, router-free routing. The expert selection loss further\nsharpens this behavior by encouraging sparsity and aligning it with standard\nMoE activation patterns. We also introduce a shared expert within the MoE-LoRA\nblock that provides stable, foundational knowledge. To the best of our\nknowledge, FURINA is the first router-free, MoE-enhanced LoRA method that can\nbe fully merged into the backbone model, introducing zero additional\ninference-time cost or complexity. Extensive experiments demonstrate that\nFURINA not only significantly outperforms standard LoRA but also matches or\nsurpasses the performance of existing MoE-LoRA methods, while eliminating the\nextra inference-time overhead of MoE."
                },
                "authors": [
                    {
                        "name": "Jiayi Han"
                    },
                    {
                        "name": "Liang Du"
                    },
                    {
                        "name": "Yinda Chen"
                    },
                    {
                        "name": "Xiao Kang"
                    },
                    {
                        "name": "Weiyang Ding"
                    },
                    {
                        "name": "Donghong Han"
                    }
                ],
                "author_detail": {
                    "name": "Donghong Han"
                },
                "author": "Donghong Han",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14899v1",
                "updated": "2025-09-18T12:21:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    12,
                    21,
                    30,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T12:21:30Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    12,
                    21,
                    30,
                    3,
                    261,
                    0
                ],
                "title": "CARGO: A Framework for Confidence-Aware Routing of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARGO: A Framework for Confidence-Aware Routing of Large Language Models"
                },
                "summary": "As large language models (LLMs) proliferate in scale, specialization, and\nlatency profiles, the challenge of routing user prompts to the most appropriate\nmodel has become increasingly critical for balancing performance and cost. We\nintroduce CARGO (Category-Aware Routing with Gap-based Optimization), a\nlightweight, confidence-aware framework for dynamic LLM selection. CARGO\nemploys a single embedding-based regressor trained on LLM-judged pairwise\ncomparisons to predict model performance, with an optional binary classifier\ninvoked when predictions are uncertain. This two-stage design enables precise,\ncost-aware routing without the need for human-annotated supervision. To capture\ndomain-specific behavior, CARGO also supports category-specific regressors\ntrained across five task groups: mathematics, coding, reasoning, summarization,\nand creative writing. Evaluated on four competitive LLMs (GPT-4o, Claude 3.5\nSonnet, DeepSeek V3, and Perplexity Sonar), CARGO achieves a top-1 routing\naccuracy of 76.4% and win rates ranging from 72% to 89% against individual\nexperts. These results demonstrate that confidence-guided, lightweight routing\ncan achieve expert-level performance with minimal overhead, offering a\npractical solution for real-world, multi-model LLM deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) proliferate in scale, specialization, and\nlatency profiles, the challenge of routing user prompts to the most appropriate\nmodel has become increasingly critical for balancing performance and cost. We\nintroduce CARGO (Category-Aware Routing with Gap-based Optimization), a\nlightweight, confidence-aware framework for dynamic LLM selection. CARGO\nemploys a single embedding-based regressor trained on LLM-judged pairwise\ncomparisons to predict model performance, with an optional binary classifier\ninvoked when predictions are uncertain. This two-stage design enables precise,\ncost-aware routing without the need for human-annotated supervision. To capture\ndomain-specific behavior, CARGO also supports category-specific regressors\ntrained across five task groups: mathematics, coding, reasoning, summarization,\nand creative writing. Evaluated on four competitive LLMs (GPT-4o, Claude 3.5\nSonnet, DeepSeek V3, and Perplexity Sonar), CARGO achieves a top-1 routing\naccuracy of 76.4% and win rates ranging from 72% to 89% against individual\nexperts. These results demonstrate that confidence-guided, lightweight routing\ncan achieve expert-level performance with minimal overhead, offering a\npractical solution for real-world, multi-model LLM deployments."
                },
                "authors": [
                    {
                        "name": "Amine Barrak"
                    },
                    {
                        "name": "Yosr Fourati"
                    },
                    {
                        "name": "Michael Olchawa"
                    },
                    {
                        "name": "Emna Ksontini"
                    },
                    {
                        "name": "Khalil Zoghlami"
                    }
                ],
                "author_detail": {
                    "name": "Khalil Zoghlami"
                },
                "author": "Khalil Zoghlami",
                "arxiv_journal_ref": "35th IEEE International Conference on Collaborative Advances in\n  Software and Computing 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14882v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14882v1",
                "updated": "2025-09-18T12:00:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    12,
                    0,
                    7,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T12:00:07Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    12,
                    0,
                    7,
                    3,
                    261,
                    0
                ],
                "title": "Llama-Mimi: Speech Language Models with Interleaved Semantic and\n  Acoustic Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Llama-Mimi: Speech Language Models with Interleaved Semantic and\n  Acoustic Tokens"
                },
                "summary": "We propose Llama-Mimi, a speech language model that uses a unified tokenizer\nand a single Transformer decoder to jointly model sequences of interleaved\nsemantic and acoustic tokens. Comprehensive evaluation shows that Llama-Mimi\nachieves state-of-the-art performance in acoustic consistency and possesses the\nability to preserve speaker identity. Our analysis further demonstrates that\nincreasing the number of quantizers improves acoustic fidelity but degrades\nlinguistic performance, highlighting the inherent challenge of maintaining\nlong-term coherence. We additionally introduce an LLM-as-a-Judge-based\nevaluation to assess the spoken content quality of generated outputs. Our\nmodels, code, and speech samples are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Llama-Mimi, a speech language model that uses a unified tokenizer\nand a single Transformer decoder to jointly model sequences of interleaved\nsemantic and acoustic tokens. Comprehensive evaluation shows that Llama-Mimi\nachieves state-of-the-art performance in acoustic consistency and possesses the\nability to preserve speaker identity. Our analysis further demonstrates that\nincreasing the number of quantizers improves acoustic fidelity but degrades\nlinguistic performance, highlighting the inherent challenge of maintaining\nlong-term coherence. We additionally introduce an LLM-as-a-Judge-based\nevaluation to assess the spoken content quality of generated outputs. Our\nmodels, code, and speech samples are publicly available."
                },
                "authors": [
                    {
                        "name": "Issa Sugiura"
                    },
                    {
                        "name": "Shuhei Kurita"
                    },
                    {
                        "name": "Yusuke Oda"
                    },
                    {
                        "name": "Ryuichiro Higashinaka"
                    }
                ],
                "author_detail": {
                    "name": "Ryuichiro Higashinaka"
                },
                "author": "Ryuichiro Higashinaka",
                "arxiv_comment": "5 pages, 1 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14882v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14882v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18650v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18650v2",
                "updated": "2025-09-18T11:58:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    58,
                    13,
                    3,
                    261,
                    0
                ],
                "published": "2025-02-25T21:19:27Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    21,
                    19,
                    27,
                    1,
                    56,
                    0
                ],
                "title": "Single- vs. Dual-Prompt Dialogue Generation with LLMs for Job Interviews\n  in Human Resources",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single- vs. Dual-Prompt Dialogue Generation with LLMs for Job Interviews\n  in Human Resources"
                },
                "summary": "Optimizing language models for use in conversational agents requires large\nquantities of example dialogues. Increasingly, these dialogues are\nsynthetically generated by using powerful large language models (LLMs),\nespecially in domains where obtaining authentic human data is challenging. One\nsuch domain is human resources (HR). In this context, we compare two LLM-based\ndialogue generation methods for producing HR job interviews, and assess which\nmethod generates higher-quality dialogues, i.e., those more difficult to\ndistinguish from genuine human discourse. The first method uses a single prompt\nto generate the complete interview dialogue. The second method uses two agents\nthat converse with each other. To evaluate dialogue quality under each method,\nwe ask a judge LLM to determine whether AI was used for interview generation,\nusing pairwise interview comparisons. We empirically find that, at the expense\nof a sixfold increase in token count, interviews generated with the dual-prompt\nmethod achieve a win rate 2 to 10 times higher than those generated with the\nsingle-prompt method. This difference remains consistent regardless of whether\nGPT-4o or Llama 3.3 70B is used for either interview generation or quality\njudging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing language models for use in conversational agents requires large\nquantities of example dialogues. Increasingly, these dialogues are\nsynthetically generated by using powerful large language models (LLMs),\nespecially in domains where obtaining authentic human data is challenging. One\nsuch domain is human resources (HR). In this context, we compare two LLM-based\ndialogue generation methods for producing HR job interviews, and assess which\nmethod generates higher-quality dialogues, i.e., those more difficult to\ndistinguish from genuine human discourse. The first method uses a single prompt\nto generate the complete interview dialogue. The second method uses two agents\nthat converse with each other. To evaluate dialogue quality under each method,\nwe ask a judge LLM to determine whether AI was used for interview generation,\nusing pairwise interview comparisons. We empirically find that, at the expense\nof a sixfold increase in token count, interviews generated with the dual-prompt\nmethod achieve a win rate 2 to 10 times higher than those generated with the\nsingle-prompt method. This difference remains consistent regardless of whether\nGPT-4o or Llama 3.3 70B is used for either interview generation or quality\njudging."
                },
                "authors": [
                    {
                        "name": "Joachim De Baer"
                    },
                    {
                        "name": "A. Seza Doğruöz"
                    },
                    {
                        "name": "Thomas Demeester"
                    },
                    {
                        "name": "Chris Develder"
                    }
                ],
                "author_detail": {
                    "name": "Chris Develder"
                },
                "author": "Chris Develder",
                "arxiv_comment": "12 pages. Accepted to the Fourth Workshop on Generation, Evaluation\n  and Metrics (GEM^2) at ACL 2025. ACL Anthology version available at\n  https://aclanthology.org/2025.gem-1.74",
                "arxiv_journal_ref": "Proceedings of the Fourth Workshop on Generation, Evaluation and\n  Metrics (GEM^2), pages 947-957, Vienna, Austria and virtual meeting, August\n  2025. Association for Computational Linguistics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18650v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18650v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14880v1",
                "updated": "2025-09-18T11:51:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    51,
                    20,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T11:51:20Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    51,
                    20,
                    3,
                    261,
                    0
                ],
                "title": "From Hype to Insight: Rethinking Large Language Model Integration in\n  Visual Speech Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Hype to Insight: Rethinking Large Language Model Integration in\n  Visual Speech Recognition"
                },
                "summary": "Advances in self-supervised encoders have improved Visual Speech Recognition\n(VSR). Recent approaches integrating these encoders with LLM decoders improves\ntranscription accuracy; however, it remains unclear whether these gains stem\nfrom visual understanding or stronger language modeling. In this work, we\nsystematically evaluate LLM decoders by freezing or selectively updating the\nvisual encoder, scaling decoder size, comparing adaptation strategies and\narchitectures, and varying training data across LRS2, LRS3, and their\ncombination. Evaluation on LRS2, LRS3, and WildVSR shows that scaling and\nadaptation yield limited improvements, while combining datasets enhances\ngeneralization. Semantic analysis reveals that gains arise primarily from\nlexical rather than semantic processing. Our Llama-2-13B model trained on the\ncombined set achieves 24.7\\% WER on LRS3 and 47.0\\% on WildVSR, establishing\nSOTA among models trained without additional supervision. Our findings indicate\nLLM decoders refine contextual reasoning rather than visual features,\nemphasizing the need for stronger visual encoders to drive meaningful progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in self-supervised encoders have improved Visual Speech Recognition\n(VSR). Recent approaches integrating these encoders with LLM decoders improves\ntranscription accuracy; however, it remains unclear whether these gains stem\nfrom visual understanding or stronger language modeling. In this work, we\nsystematically evaluate LLM decoders by freezing or selectively updating the\nvisual encoder, scaling decoder size, comparing adaptation strategies and\narchitectures, and varying training data across LRS2, LRS3, and their\ncombination. Evaluation on LRS2, LRS3, and WildVSR shows that scaling and\nadaptation yield limited improvements, while combining datasets enhances\ngeneralization. Semantic analysis reveals that gains arise primarily from\nlexical rather than semantic processing. Our Llama-2-13B model trained on the\ncombined set achieves 24.7\\% WER on LRS3 and 47.0\\% on WildVSR, establishing\nSOTA among models trained without additional supervision. Our findings indicate\nLLM decoders refine contextual reasoning rather than visual features,\nemphasizing the need for stronger visual encoders to drive meaningful progress."
                },
                "authors": [
                    {
                        "name": "Rishabh Jain"
                    },
                    {
                        "name": "Naomi Harte"
                    }
                ],
                "author_detail": {
                    "name": "Naomi Harte"
                },
                "author": "Naomi Harte",
                "arxiv_comment": "submitted to ICASSP 2026. This work has been submitted to the IEEE\n  for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14875v1",
                "updated": "2025-09-18T11:44:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    44,
                    10,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T11:44:10Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    44,
                    10,
                    3,
                    261,
                    0
                ],
                "title": "Beyond Spherical geometry: Unraveling complex features of objects\n  orbiting around stars from its transit light curve using deep learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Spherical geometry: Unraveling complex features of objects\n  orbiting around stars from its transit light curve using deep learning"
                },
                "summary": "Characterizing the geometry of an object orbiting around a star from its\ntransit light curve is a powerful tool to uncover various complex phenomena.\nThis problem is inherently ill-posed, since similar or identical light curves\ncan be produced by multiple different shapes. In this study, we investigate the\nextent to which the features of a shape can be embedded in a transit light\ncurve. We generate a library of two-dimensional random shapes and simulate\ntheir transit light curves with light curve simulator, Yuti. Each shape is\ndecomposed into a series of elliptical components expressed in the form of\nFourier coefficients that adds increasingly diminishing perturbations to an\nideal ellipse. We train deep neural networks to predict these Fourier\ncoefficients directly from simulated light curves. Our results demonstrate that\nthe neural network can successfully reconstruct the low-order ellipses, which\ndescribe overall shape, orientation and large-scale perturbations. For higher\norder ellipses the scale is successfully determined but the inference of\neccentricity and orientation is limited, demonstrating the extent of shape\ninformation in the light curve. We explore the impact of non-convex shape\nfeatures in reconstruction, and show its dependence on shape orientation. The\nlevel of reconstruction achieved by the neural network underscores the utility\nof using light curves as a means to extract geometric information from\ntransiting systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing the geometry of an object orbiting around a star from its\ntransit light curve is a powerful tool to uncover various complex phenomena.\nThis problem is inherently ill-posed, since similar or identical light curves\ncan be produced by multiple different shapes. In this study, we investigate the\nextent to which the features of a shape can be embedded in a transit light\ncurve. We generate a library of two-dimensional random shapes and simulate\ntheir transit light curves with light curve simulator, Yuti. Each shape is\ndecomposed into a series of elliptical components expressed in the form of\nFourier coefficients that adds increasingly diminishing perturbations to an\nideal ellipse. We train deep neural networks to predict these Fourier\ncoefficients directly from simulated light curves. Our results demonstrate that\nthe neural network can successfully reconstruct the low-order ellipses, which\ndescribe overall shape, orientation and large-scale perturbations. For higher\norder ellipses the scale is successfully determined but the inference of\neccentricity and orientation is limited, demonstrating the extent of shape\ninformation in the light curve. We explore the impact of non-convex shape\nfeatures in reconstruction, and show its dependence on shape orientation. The\nlevel of reconstruction achieved by the neural network underscores the utility\nof using light curves as a means to extract geometric information from\ntransiting systems."
                },
                "authors": [
                    {
                        "name": "Ushasi Bhowmick"
                    },
                    {
                        "name": "Shivam Kumaran"
                    }
                ],
                "author_detail": {
                    "name": "Shivam Kumaran"
                },
                "author": "Shivam Kumaran",
                "arxiv_comment": "16 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11686v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11686v2",
                "updated": "2025-09-18T11:44:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    44,
                    9,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-15T08:38:01Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    8,
                    38,
                    1,
                    0,
                    258,
                    0
                ],
                "title": "Do Code Semantics Help? A Comprehensive Study on Execution Trace-Based\n  Information for Code Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Code Semantics Help? A Comprehensive Study on Execution Trace-Based\n  Information for Code Large Language Models"
                },
                "summary": "Code Large Language Models (Code LLMs) have opened a new era in programming\nwith their impressive capabilities. However, recent research has revealed\ncritical limitations in their ability to reason about runtime behavior and\nunderstand the actual functionality of programs, which poses significant\nchallenges for their post-training and practical deployment. Specifically, Code\nLLMs encounter two principal issues: (1) a lack of proficiency in reasoning\nabout program execution behavior, as they struggle to interpret what programs\nactually do during runtime, and (2) the inconsistent and fragmented\nrepresentation of semantic information, such as execution traces, across\nexisting methods, which hinders their ability to generalize and reason\neffectively. These challenges underscore the necessity for more systematic\napproaches to enhance the reasoning capabilities of Code LLMs. To address these\nissues, we introduce a generic framework to support integrating semantic\ninformation~(e.g., execution trace) to code task-relevant prompts, and conduct\na comprehensive study to explore the role of semantic information in enhancing\nthe reasoning ability of Code LLMs accordingly. Specifically, we focus on\ninvestigating the usefulness of trace-based semantic information in boosting\nsupervised fine-tuning~(SFT) and post-phase inference of Code LLMs. The\nexperimental results surprisingly disagree with previous works and demonstrate\nthat semantic information has limited usefulness for SFT and test time scaling\nof Code LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Large Language Models (Code LLMs) have opened a new era in programming\nwith their impressive capabilities. However, recent research has revealed\ncritical limitations in their ability to reason about runtime behavior and\nunderstand the actual functionality of programs, which poses significant\nchallenges for their post-training and practical deployment. Specifically, Code\nLLMs encounter two principal issues: (1) a lack of proficiency in reasoning\nabout program execution behavior, as they struggle to interpret what programs\nactually do during runtime, and (2) the inconsistent and fragmented\nrepresentation of semantic information, such as execution traces, across\nexisting methods, which hinders their ability to generalize and reason\neffectively. These challenges underscore the necessity for more systematic\napproaches to enhance the reasoning capabilities of Code LLMs. To address these\nissues, we introduce a generic framework to support integrating semantic\ninformation~(e.g., execution trace) to code task-relevant prompts, and conduct\na comprehensive study to explore the role of semantic information in enhancing\nthe reasoning ability of Code LLMs accordingly. Specifically, we focus on\ninvestigating the usefulness of trace-based semantic information in boosting\nsupervised fine-tuning~(SFT) and post-phase inference of Code LLMs. The\nexperimental results surprisingly disagree with previous works and demonstrate\nthat semantic information has limited usefulness for SFT and test time scaling\nof Code LLM."
                },
                "authors": [
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Xiaofei Xie"
                    },
                    {
                        "name": "Qiang Hu"
                    },
                    {
                        "name": "Shangqing Liu"
                    },
                    {
                        "name": "Yi Li"
                    }
                ],
                "author_detail": {
                    "name": "Yi Li"
                },
                "author": "Yi Li",
                "arxiv_comment": "EMNLP2025-findings https://openreview.net/forum?id=d4ICISW2T4",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11686v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11686v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07856v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07856v2",
                "updated": "2025-09-18T11:36:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    36,
                    58,
                    3,
                    261,
                    0
                ],
                "published": "2025-07-10T15:38:51Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    15,
                    38,
                    51,
                    3,
                    191,
                    0
                ],
                "title": "A Bayesian Framework for UHECR Source Association and Parameter\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bayesian Framework for UHECR Source Association and Parameter\n  Inference"
                },
                "summary": "The identification of potential sources of ultra-high-energy cosmic rays\n(UHECRs) remains challenging due to magnetic deflections and propagation\nlosses, which are particularly strong for nuclei. In previous iterations of\nthis work, we proposed an approach for UHECR astronomy based on Bayesian\ninference through explicit modelling of propagation and magnetic deflection\neffects. The event-by-event mass information is expected to provide tighter\nconstraints on these parameters and to help identify unknown sources. However,\nthe measurements of the average mass through observations from the surface\ndetectors at the Pierre Auger Observatory already indicate that the UHECR\nmasses are well represented through its statistical average. In this\ncontribution, we present our framework which uses energy and mass moments of\n$\\ln A$ to infer the source parameters of UHECRs, including the mass\ncomposition at the source. We demonstrate the performance of our model using\nsimulated datasets based on the Pierre Auger Observatory and Telescope Array\nProject. Our model can be readily applied to currently available data, and we\ndiscuss the implications of our results for UHECR source identification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The identification of potential sources of ultra-high-energy cosmic rays\n(UHECRs) remains challenging due to magnetic deflections and propagation\nlosses, which are particularly strong for nuclei. In previous iterations of\nthis work, we proposed an approach for UHECR astronomy based on Bayesian\ninference through explicit modelling of propagation and magnetic deflection\neffects. The event-by-event mass information is expected to provide tighter\nconstraints on these parameters and to help identify unknown sources. However,\nthe measurements of the average mass through observations from the surface\ndetectors at the Pierre Auger Observatory already indicate that the UHECR\nmasses are well represented through its statistical average. In this\ncontribution, we present our framework which uses energy and mass moments of\n$\\ln A$ to infer the source parameters of UHECRs, including the mass\ncomposition at the source. We demonstrate the performance of our model using\nsimulated datasets based on the Pierre Auger Observatory and Telescope Array\nProject. Our model can be readily applied to currently available data, and we\ndiscuss the implications of our results for UHECR source identification."
                },
                "authors": [
                    {
                        "name": "Keito Watanabe"
                    },
                    {
                        "name": "Anatoli Fedynitch"
                    },
                    {
                        "name": "Francesca Capel"
                    },
                    {
                        "name": "Hiroyuki Sagawa"
                    }
                ],
                "author_detail": {
                    "name": "Hiroyuki Sagawa"
                },
                "author": "Hiroyuki Sagawa",
                "arxiv_comment": "Presented at the 39th International Cosmic Ray Conference (ICRC\n  2025). 8 pages, 2 figures (v2: Updated to match final version)",
                "arxiv_journal_ref": "PoS(ICRC2025)435",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07856v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07856v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15213v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15213v2",
                "updated": "2025-09-18T11:35:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    35,
                    1,
                    3,
                    261,
                    0
                ],
                "published": "2025-08-21T03:53:35Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    53,
                    35,
                    3,
                    233,
                    0
                ],
                "title": "Select to Know: An Internal-External Knowledge Self-Selection Framework\n  for Domain-Specific Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Select to Know: An Internal-External Knowledge Self-Selection Framework\n  for Domain-Specific Question Answering"
                },
                "summary": "Large Language Models (LLMs) perform well in general QA but often struggle in\ndomain-specific scenarios. Retrieval-Augmented Generation (RAG) introduces\nexternal knowledge but suffers from hallucinations and latency due to noisy\nretrievals. Continued pretraining internalizes domain knowledge but is costly\nand lacks cross-domain flexibility. We attribute this challenge to the\nlong-tail distribution of domain knowledge, which leaves partial yet useful\ninternal knowledge underutilized. We further argue that knowledge acquisition\nshould be progressive, mirroring human learning: first understanding concepts,\nthen applying them to complex reasoning. To address this, we propose Selct2Know\n(S2K), a cost-effective framework that internalizes domain knowledge through an\ninternal-external knowledge self-selection strategy and selective supervised\nfine-tuning. We also introduce a structured reasoning data generation pipeline\nand integrate GRPO to enhance reasoning ability. Experiments on medical, legal,\nand financial QA benchmarks show that S2K consistently outperforms existing\nmethods and matches domain-pretrained LLMs with significantly lower cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) perform well in general QA but often struggle in\ndomain-specific scenarios. Retrieval-Augmented Generation (RAG) introduces\nexternal knowledge but suffers from hallucinations and latency due to noisy\nretrievals. Continued pretraining internalizes domain knowledge but is costly\nand lacks cross-domain flexibility. We attribute this challenge to the\nlong-tail distribution of domain knowledge, which leaves partial yet useful\ninternal knowledge underutilized. We further argue that knowledge acquisition\nshould be progressive, mirroring human learning: first understanding concepts,\nthen applying them to complex reasoning. To address this, we propose Selct2Know\n(S2K), a cost-effective framework that internalizes domain knowledge through an\ninternal-external knowledge self-selection strategy and selective supervised\nfine-tuning. We also introduce a structured reasoning data generation pipeline\nand integrate GRPO to enhance reasoning ability. Experiments on medical, legal,\nand financial QA benchmarks show that S2K consistently outperforms existing\nmethods and matches domain-pretrained LLMs with significantly lower cost."
                },
                "authors": [
                    {
                        "name": "Bolei He"
                    },
                    {
                        "name": "Xinran He"
                    },
                    {
                        "name": "Run Shao"
                    },
                    {
                        "name": "Shanfu Shu"
                    },
                    {
                        "name": "Xianwei Xue"
                    },
                    {
                        "name": "Mingquan Cheng"
                    },
                    {
                        "name": "Haifeng Li"
                    },
                    {
                        "name": "Zhenhua Ling"
                    }
                ],
                "author_detail": {
                    "name": "Zhenhua Ling"
                },
                "author": "Zhenhua Ling",
                "arxiv_comment": "EMNLP2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15213v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15213v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20013v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20013v2",
                "updated": "2025-09-18T11:32:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    32,
                    15,
                    3,
                    261,
                    0
                ],
                "published": "2025-05-26T14:03:37Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    14,
                    3,
                    37,
                    0,
                    146,
                    0
                ],
                "title": "WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought\n  in Reflection, Branching, and Rollback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought\n  in Reflection, Branching, and Rollback"
                },
                "summary": "Web agents powered by Large Language Models (LLMs) show promise for\nnext-generation AI, but their limited reasoning in uncertain, dynamic web\nenvironments hinders robust deployment. In this paper, we identify key\nreasoning skills essential for effective web agents, i.e., reflection &\nlookahead, branching, and rollback, and curate trajectory data that exemplifies\nthese abilities by reconstructing the agent's (inference-time) reasoning\nalgorithms into chain-of-thought rationales. We conduct experiments in the\nagent self-improving benchmark, OpenWebVoyager, and demonstrate that distilling\nsalient reasoning patterns into the backbone LLM via simple fine-tuning can\nsubstantially enhance its performance. Our approach yields significant\nimprovements across multiple benchmarks, including WebVoyager, Mind2web-live,\nand SimpleQA (web search), highlighting the potential of targeted reasoning\nskill enhancement for web agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web agents powered by Large Language Models (LLMs) show promise for\nnext-generation AI, but their limited reasoning in uncertain, dynamic web\nenvironments hinders robust deployment. In this paper, we identify key\nreasoning skills essential for effective web agents, i.e., reflection &\nlookahead, branching, and rollback, and curate trajectory data that exemplifies\nthese abilities by reconstructing the agent's (inference-time) reasoning\nalgorithms into chain-of-thought rationales. We conduct experiments in the\nagent self-improving benchmark, OpenWebVoyager, and demonstrate that distilling\nsalient reasoning patterns into the backbone LLM via simple fine-tuning can\nsubstantially enhance its performance. Our approach yields significant\nimprovements across multiple benchmarks, including WebVoyager, Mind2web-live,\nand SimpleQA (web search), highlighting the potential of targeted reasoning\nskill enhancement for web agents."
                },
                "authors": [
                    {
                        "name": "Minda Hu"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Jianshu Zhang"
                    },
                    {
                        "name": "Junyu Ma"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Jingyan Zhou"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Irwin King"
                    }
                ],
                "author_detail": {
                    "name": "Irwin King"
                },
                "author": "Irwin King",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20013v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20013v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14858v1",
                "updated": "2025-09-18T11:24:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    24,
                    47,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T11:24:47Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    24,
                    47,
                    3,
                    261,
                    0
                ],
                "title": "MeanFlowSE: one-step generative speech enhancement via conditional mean\n  flow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeanFlowSE: one-step generative speech enhancement via conditional mean\n  flow"
                },
                "summary": "Multistep inference is a bottleneck for real-time generative speech\nenhancement because flow- and diffusion-based systems learn an instantaneous\nvelocity field and therefore rely on iterative ordinary differential equation\n(ODE) solvers. We introduce MeanFlowSE, a conditional generative model that\nlearns the average velocity over finite intervals along a trajectory. Using a\nJacobian-vector product (JVP) to instantiate the MeanFlow identity, we derive a\nlocal training objective that directly supervises finite-interval displacement\nwhile remaining consistent with the instantaneous-field constraint on the\ndiagonal. At inference, MeanFlowSE performs single-step generation via a\nbackward-in-time displacement, removing the need for multistep solvers; an\noptional few-step variant offers additional refinement. On VoiceBank-DEMAND,\nthe single-step model achieves strong intelligibility, fidelity, and perceptual\nquality with substantially lower computational cost than multistep baselines.\nThe method requires no knowledge distillation or external teachers, providing\nan efficient, high-fidelity framework for real-time generative speech\nenhancement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multistep inference is a bottleneck for real-time generative speech\nenhancement because flow- and diffusion-based systems learn an instantaneous\nvelocity field and therefore rely on iterative ordinary differential equation\n(ODE) solvers. We introduce MeanFlowSE, a conditional generative model that\nlearns the average velocity over finite intervals along a trajectory. Using a\nJacobian-vector product (JVP) to instantiate the MeanFlow identity, we derive a\nlocal training objective that directly supervises finite-interval displacement\nwhile remaining consistent with the instantaneous-field constraint on the\ndiagonal. At inference, MeanFlowSE performs single-step generation via a\nbackward-in-time displacement, removing the need for multistep solvers; an\noptional few-step variant offers additional refinement. On VoiceBank-DEMAND,\nthe single-step model achieves strong intelligibility, fidelity, and perceptual\nquality with substantially lower computational cost than multistep baselines.\nThe method requires no knowledge distillation or external teachers, providing\nan efficient, high-fidelity framework for real-time generative speech\nenhancement."
                },
                "authors": [
                    {
                        "name": "Duojia Li"
                    },
                    {
                        "name": "Shenghui Lu"
                    },
                    {
                        "name": "Hongchen Pan"
                    },
                    {
                        "name": "Zongyi Zhan"
                    },
                    {
                        "name": "Qingyang Hong"
                    },
                    {
                        "name": "Lin Li"
                    }
                ],
                "author_detail": {
                    "name": "Lin Li"
                },
                "author": "Lin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14856v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14856v1",
                "updated": "2025-09-18T11:24:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    24,
                    9,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T11:24:09Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    24,
                    9,
                    3,
                    261,
                    0
                ],
                "title": "CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End\n  Code Review Evaluation in Python Projects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End\n  Code Review Evaluation in Python Projects"
                },
                "summary": "Automated code review (CR) is a key application for Large Language Models\n(LLMs), but progress is hampered by a \"reality gap\": existing benchmarks\nevaluate models on isolated sub-tasks using simplified, context-poor data. This\nfails to reflect the holistic context-rich nature of real-world CR. To bridge\nthis gap, we introduce CodeFuse-CR-Bench, the first comprehensiveness-aware\nbenchmark for repository-level CR evaluation. CodeFuse-CR-Bench comprises 601\nhigh-quality instances from 70 Python projects covering nine Pull-Request (PR)\nproblem domains, where each instance provides rich, multi-faceted context\nincluding the associated issue, PR details, and repository state, enabling\nend-to-end evaluation. Beyond superficial metrics, we also propose a novel\nevaluation framework that combines rule-based checks for location and syntax\nwith model-based judgments of review quality. We present the first large-scale\nassessment of state-of-the-art LLMs on this comprehensive CR task. Our results\nestablish crucial baselines and reveal that (1) no single LLM dominates all\naspects of CR; (2) Gemini 2.5 Pro achieves the highest comprehensive\nperformance; and (3) different LLMs exhibit varying robustness to redundant\ncontext. These findings highlight the necessity of holistic, multi-dimensional\nevaluation and provide actionable insights for advancing truly intelligent yet\npractical CR assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated code review (CR) is a key application for Large Language Models\n(LLMs), but progress is hampered by a \"reality gap\": existing benchmarks\nevaluate models on isolated sub-tasks using simplified, context-poor data. This\nfails to reflect the holistic context-rich nature of real-world CR. To bridge\nthis gap, we introduce CodeFuse-CR-Bench, the first comprehensiveness-aware\nbenchmark for repository-level CR evaluation. CodeFuse-CR-Bench comprises 601\nhigh-quality instances from 70 Python projects covering nine Pull-Request (PR)\nproblem domains, where each instance provides rich, multi-faceted context\nincluding the associated issue, PR details, and repository state, enabling\nend-to-end evaluation. Beyond superficial metrics, we also propose a novel\nevaluation framework that combines rule-based checks for location and syntax\nwith model-based judgments of review quality. We present the first large-scale\nassessment of state-of-the-art LLMs on this comprehensive CR task. Our results\nestablish crucial baselines and reveal that (1) no single LLM dominates all\naspects of CR; (2) Gemini 2.5 Pro achieves the highest comprehensive\nperformance; and (3) different LLMs exhibit varying robustness to redundant\ncontext. These findings highlight the necessity of holistic, multi-dimensional\nevaluation and provide actionable insights for advancing truly intelligent yet\npractical CR assistants."
                },
                "authors": [
                    {
                        "name": "Hanyang Guo"
                    },
                    {
                        "name": "Xunjin Zheng"
                    },
                    {
                        "name": "Zihan Liao"
                    },
                    {
                        "name": "Hang Yu"
                    },
                    {
                        "name": "Peng DI"
                    },
                    {
                        "name": "Ziyin Zhang"
                    },
                    {
                        "name": "Hong-Ning Dai"
                    }
                ],
                "author_detail": {
                    "name": "Hong-Ning Dai"
                },
                "author": "Hong-Ning Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14856v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14856v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14851v1",
                "updated": "2025-09-18T11:16:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    16,
                    9,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T11:16:09Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    16,
                    9,
                    3,
                    261,
                    0
                ],
                "title": "Empathy-R1: A Chain-of-Empathy and Reinforcement Learning Framework for\n  Long-Form Mental Health Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empathy-R1: A Chain-of-Empathy and Reinforcement Learning Framework for\n  Long-Form Mental Health Support"
                },
                "summary": "Empathy is critical for effective mental health support, especially when\naddressing Long Counseling Texts (LCTs). However, existing Large Language\nModels (LLMs) often generate replies that are semantically fluent but lack the\nstructured reasoning necessary for genuine psychological support, particularly\nin a Chinese context. To bridge this gap, we introduce Empathy-R1, a novel\nframework that integrates a Chain-of-Empathy (CoE) reasoning process with\nReinforcement Learning (RL) to enhance response quality for LCTs. Inspired by\ncognitive-behavioral therapy, our CoE paradigm guides the model to sequentially\nreason about a help-seeker's emotions, causes, and intentions, making its\nthinking process both transparent and interpretable. Our framework is empowered\nby a new large-scale Chinese dataset, Empathy-QA, and a two-stage training\nprocess. First, Supervised Fine-Tuning instills the CoE's reasoning structure.\nSubsequently, RL, guided by a dedicated reward model, refines the therapeutic\nrelevance and contextual appropriateness of the final responses. Experiments\nshow that Empathy-R1 achieves strong performance on key automatic metrics. More\nimportantly, human evaluations confirm its superiority, showing a clear\npreference over strong baselines and achieving a Win@1 rate of 44.30% on our\nnew benchmark. By enabling interpretable and contextually nuanced responses,\nEmpathy-R1 represents a significant advancement in developing responsible and\ngenuinely beneficial AI for mental health support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empathy is critical for effective mental health support, especially when\naddressing Long Counseling Texts (LCTs). However, existing Large Language\nModels (LLMs) often generate replies that are semantically fluent but lack the\nstructured reasoning necessary for genuine psychological support, particularly\nin a Chinese context. To bridge this gap, we introduce Empathy-R1, a novel\nframework that integrates a Chain-of-Empathy (CoE) reasoning process with\nReinforcement Learning (RL) to enhance response quality for LCTs. Inspired by\ncognitive-behavioral therapy, our CoE paradigm guides the model to sequentially\nreason about a help-seeker's emotions, causes, and intentions, making its\nthinking process both transparent and interpretable. Our framework is empowered\nby a new large-scale Chinese dataset, Empathy-QA, and a two-stage training\nprocess. First, Supervised Fine-Tuning instills the CoE's reasoning structure.\nSubsequently, RL, guided by a dedicated reward model, refines the therapeutic\nrelevance and contextual appropriateness of the final responses. Experiments\nshow that Empathy-R1 achieves strong performance on key automatic metrics. More\nimportantly, human evaluations confirm its superiority, showing a clear\npreference over strong baselines and achieving a Win@1 rate of 44.30% on our\nnew benchmark. By enabling interpretable and contextually nuanced responses,\nEmpathy-R1 represents a significant advancement in developing responsible and\ngenuinely beneficial AI for mental health support."
                },
                "authors": [
                    {
                        "name": "Xianrong Yao"
                    },
                    {
                        "name": "Dong She"
                    },
                    {
                        "name": "Chenxu Zhang"
                    },
                    {
                        "name": "Yimeng Zhang"
                    },
                    {
                        "name": "Yueru Sun"
                    },
                    {
                        "name": "Noman Ahmed"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Zhanpeng Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhanpeng Jin"
                },
                "author": "Zhanpeng Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14849v1",
                "updated": "2025-09-18T11:14:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    14,
                    14,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T11:14:14Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    14,
                    14,
                    3,
                    261,
                    0
                ],
                "title": "A Comprehensive Framework for F-statistic-based Parameter Estimation of\n  Binary Black Hole Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Framework for F-statistic-based Parameter Estimation of\n  Binary Black Hole Signals"
                },
                "summary": "We present a comprehensive investigation of the F-statistic method for\nparameter estimation of gravitational wave (GW) signals from binary black hole\nmergers. By analytically maximizing the likelihood over the luminosity distance\nand polarization angle, this approach reduces the dimensionality of the\nparameter space to enhance computational efficiency. We also introduce a novel\nformulation for calculating the Bayes factor for the F-statistic, enabling a\nquantitative assessment of its performance against standard full\nfrequency-domain (FFD) Bayesian inference. Using the benchmark event GW150914,\nwe demonstrate that the F-statistic method is not only approximately $70\\%$\nfaster than FFD but is also statistically stable across different sampler\nconfigurations, with a log-Bayes factor between runs smaller than $0.1$.\nFurthermore, the F-statistic exhibits superior stability against changes in\nsampler configuration, yielding consistently lower Jensen-Shannon divergence\nvalues between analysis runs. While the F-statistic produces slightly broader\nconstraints on some parameters, we argue this represents a more honest\nuncertainty quantification, particularly in high-dimensional parameter spaces\nwith complex posterior structures. These results highlight the significant\nadvantages of the F-statistic method for GW data analysis, positioning it as a\npowerful tool for the era of high-rate detections with future observatories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive investigation of the F-statistic method for\nparameter estimation of gravitational wave (GW) signals from binary black hole\nmergers. By analytically maximizing the likelihood over the luminosity distance\nand polarization angle, this approach reduces the dimensionality of the\nparameter space to enhance computational efficiency. We also introduce a novel\nformulation for calculating the Bayes factor for the F-statistic, enabling a\nquantitative assessment of its performance against standard full\nfrequency-domain (FFD) Bayesian inference. Using the benchmark event GW150914,\nwe demonstrate that the F-statistic method is not only approximately $70\\%$\nfaster than FFD but is also statistically stable across different sampler\nconfigurations, with a log-Bayes factor between runs smaller than $0.1$.\nFurthermore, the F-statistic exhibits superior stability against changes in\nsampler configuration, yielding consistently lower Jensen-Shannon divergence\nvalues between analysis runs. While the F-statistic produces slightly broader\nconstraints on some parameters, we argue this represents a more honest\nuncertainty quantification, particularly in high-dimensional parameter spaces\nwith complex posterior structures. These results highlight the significant\nadvantages of the F-statistic method for GW data analysis, positioning it as a\npowerful tool for the era of high-rate detections with future observatories."
                },
                "authors": [
                    {
                        "name": "Hai-Tian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hai-Tian Wang"
                },
                "author": "Hai-Tian Wang",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11289v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11289v3",
                "updated": "2025-09-18T11:11:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    11,
                    1,
                    3,
                    261,
                    0
                ],
                "published": "2025-06-12T20:46:41Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    20,
                    46,
                    41,
                    3,
                    163,
                    0
                ],
                "title": "Inferring Quantum Network Topologies using Genetic Optimisation of\n  Indirect Measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Quantum Network Topologies using Genetic Optimisation of\n  Indirect Measurements"
                },
                "summary": "The characterisation of quantum networks is fundamental to understanding how\nenergy and information propagates through complex systems, with applications in\ncontrol, communication, error mitigation and energy transfer. In this work, we\nexplore the use of external probes to infer the network topology in the context\nof continuous-time quantum walks, where a single excitation traverses the\nnetwork with a pattern strongly influenced by its topology. The probes act as\ndecay channels for the excitation, and can be interpreted as performing an\nindirect measurement on the network dynamics. By making use of a Genetic\nOptimisation algorithm, we demonstrate that the data collected by the probes\ncan be used to successfully reconstruct the topology of any quantum network\nwith high success rates, where performance is limited only by computational\nresources for large network sizes. Moreover, we show that increasing the number\nof probes significantly simplifies the reconstruction task, revealing a\ntradeoff between the number of probes and the required computational power.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The characterisation of quantum networks is fundamental to understanding how\nenergy and information propagates through complex systems, with applications in\ncontrol, communication, error mitigation and energy transfer. In this work, we\nexplore the use of external probes to infer the network topology in the context\nof continuous-time quantum walks, where a single excitation traverses the\nnetwork with a pattern strongly influenced by its topology. The probes act as\ndecay channels for the excitation, and can be interpreted as performing an\nindirect measurement on the network dynamics. By making use of a Genetic\nOptimisation algorithm, we demonstrate that the data collected by the probes\ncan be used to successfully reconstruct the topology of any quantum network\nwith high success rates, where performance is limited only by computational\nresources for large network sizes. Moreover, we show that increasing the number\nof probes significantly simplifies the reconstruction task, revealing a\ntradeoff between the number of probes and the required computational power."
                },
                "authors": [
                    {
                        "name": "Conall J. Campbell"
                    },
                    {
                        "name": "Matthew Mackinnon"
                    },
                    {
                        "name": "Mauro Paternostro"
                    },
                    {
                        "name": "Diana A. Chisholm"
                    }
                ],
                "author_detail": {
                    "name": "Diana A. Chisholm"
                },
                "author": "Diana A. Chisholm",
                "arxiv_doi": "10.1116/5.0287733",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1116/5.0287733",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.11289v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11289v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 8 figures, comments welcome",
                "arxiv_journal_ref": "AVS Quantum Sci. 7, 034403 (2025)",
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.15218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15218v1",
                "updated": "2025-09-18T17:59:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    59,
                    16,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:59:16Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    59,
                    16,
                    3,
                    261,
                    0
                ],
                "title": "LNE-Blocking: An Efficient Framework for Contamination Mitigation\n  Evaluation on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LNE-Blocking: An Efficient Framework for Contamination Mitigation\n  Evaluation on Large Language Models"
                },
                "summary": "The problem of data contamination is now almost inevitable during the\ndevelopment of large language models (LLMs), with the training data commonly\nintegrating those evaluation benchmarks even unintentionally. This problem\nsubsequently makes it hard to benchmark LLMs fairly. Instead of constructing\ncontamination-free datasets (quite hard), we propose a novel framework,\n\\textbf{LNE-Blocking}, to restore model performance prior to contamination on\npotentially leaked datasets. Our framework consists of two components:\ncontamination detection and disruption operation. For the prompt, the framework\nfirst uses the contamination detection method, \\textbf{LNE}, to assess the\nextent of contamination in the model. Based on this, it adjusts the intensity\nof the disruption operation, \\textbf{Blocking}, to elicit non-memorized\nresponses from the model. Our framework is the first to efficiently restore the\nmodel's greedy decoding performance. This comes with a strong performance on\nmultiple datasets with potential leakage risks, and it consistently achieves\nstable recovery results across different models and varying levels of data\ncontamination. We release the code at https://github.com/RuijieH/LNE-Blocking\nto facilitate research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The problem of data contamination is now almost inevitable during the\ndevelopment of large language models (LLMs), with the training data commonly\nintegrating those evaluation benchmarks even unintentionally. This problem\nsubsequently makes it hard to benchmark LLMs fairly. Instead of constructing\ncontamination-free datasets (quite hard), we propose a novel framework,\n\\textbf{LNE-Blocking}, to restore model performance prior to contamination on\npotentially leaked datasets. Our framework consists of two components:\ncontamination detection and disruption operation. For the prompt, the framework\nfirst uses the contamination detection method, \\textbf{LNE}, to assess the\nextent of contamination in the model. Based on this, it adjusts the intensity\nof the disruption operation, \\textbf{Blocking}, to elicit non-memorized\nresponses from the model. Our framework is the first to efficiently restore the\nmodel's greedy decoding performance. This comes with a strong performance on\nmultiple datasets with potential leakage risks, and it consistently achieves\nstable recovery results across different models and varying levels of data\ncontamination. We release the code at https://github.com/RuijieH/LNE-Blocking\nto facilitate research."
                },
                "authors": [
                    {
                        "name": "Ruijie Hou"
                    },
                    {
                        "name": "Yueyang Jiao"
                    },
                    {
                        "name": "Hanxu Hu"
                    },
                    {
                        "name": "Yingming Li"
                    },
                    {
                        "name": "Wai Lam"
                    },
                    {
                        "name": "Huajian Zhang"
                    },
                    {
                        "name": "Hongyuan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Hongyuan Lu"
                },
                "author": "Hongyuan Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15216v1",
                "updated": "2025-09-18T17:59:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    59,
                    5,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:59:05Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    59,
                    5,
                    3,
                    261,
                    0
                ],
                "title": "Assessing Historical Structural Oppression Worldwide via Rule-Guided\n  Prompting of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Historical Structural Oppression Worldwide via Rule-Guided\n  Prompting of Large Language Models"
                },
                "summary": "Traditional efforts to measure historical structural oppression struggle with\ncross-national validity due to the unique, locally specified histories of\nexclusion, colonization, and social status in each country, and often have\nrelied on structured indices that privilege material resources while\noverlooking lived, identity-based exclusion. We introduce a novel framework for\noppression measurement that leverages Large Language Models (LLMs) to generate\ncontext-sensitive scores of lived historical disadvantage across diverse\ngeopolitical settings. Using unstructured self-identified ethnicity utterances\nfrom a multilingual COVID-19 global study, we design rule-guided prompting\nstrategies that encourage models to produce interpretable, theoretically\ngrounded estimations of oppression. We systematically evaluate these strategies\nacross multiple state-of-the-art LLMs. Our results demonstrate that LLMs, when\nguided by explicit rules, can capture nuanced forms of identity-based\nhistorical oppression within nations. This approach provides a complementary\nmeasurement tool that highlights dimensions of systemic exclusion, offering a\nscalable, cross-cultural lens for understanding how oppression manifests in\ndata-driven research and public health contexts. To support reproducible\nevaluation, we release an open-sourced benchmark dataset for assessing LLMs on\noppression measurement\n(https://github.com/chattergpt/llm-oppression-benchmark).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional efforts to measure historical structural oppression struggle with\ncross-national validity due to the unique, locally specified histories of\nexclusion, colonization, and social status in each country, and often have\nrelied on structured indices that privilege material resources while\noverlooking lived, identity-based exclusion. We introduce a novel framework for\noppression measurement that leverages Large Language Models (LLMs) to generate\ncontext-sensitive scores of lived historical disadvantage across diverse\ngeopolitical settings. Using unstructured self-identified ethnicity utterances\nfrom a multilingual COVID-19 global study, we design rule-guided prompting\nstrategies that encourage models to produce interpretable, theoretically\ngrounded estimations of oppression. We systematically evaluate these strategies\nacross multiple state-of-the-art LLMs. Our results demonstrate that LLMs, when\nguided by explicit rules, can capture nuanced forms of identity-based\nhistorical oppression within nations. This approach provides a complementary\nmeasurement tool that highlights dimensions of systemic exclusion, offering a\nscalable, cross-cultural lens for understanding how oppression manifests in\ndata-driven research and public health contexts. To support reproducible\nevaluation, we release an open-sourced benchmark dataset for assessing LLMs on\noppression measurement\n(https://github.com/chattergpt/llm-oppression-benchmark)."
                },
                "authors": [
                    {
                        "name": "Sreejato Chatterjee"
                    },
                    {
                        "name": "Linh Tran"
                    },
                    {
                        "name": "Quoc Duy Nguyen"
                    },
                    {
                        "name": "Roni Kirson"
                    },
                    {
                        "name": "Drue Hamlin"
                    },
                    {
                        "name": "Harvest Aquino"
                    },
                    {
                        "name": "Hanjia Lyu"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Timothy Dye"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Dye"
                },
                "author": "Timothy Dye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15213v1",
                "updated": "2025-09-18T17:58:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    58,
                    15,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:58:15Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    58,
                    15,
                    3,
                    261,
                    0
                ],
                "title": "Evil Vizier: Vulnerabilities of LLM-Integrated XR Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evil Vizier: Vulnerabilities of LLM-Integrated XR Systems"
                },
                "summary": "Extended reality (XR) applications increasingly integrate Large Language\nModels (LLMs) to enhance user experience, scene understanding, and even\ngenerate executable XR content, and are often called \"AI glasses\". Despite\nthese potential benefits, the integrated XR-LLM pipeline makes XR applications\nvulnerable to new forms of attacks. In this paper, we analyze LLM-Integated XR\nsystems in the literature and in practice and categorize them along different\ndimensions from a systems perspective. Building on this categorization, we\nidentify a common threat model and demonstrate a series of proof-of-concept\nattacks on multiple XR platforms that employ various LLM models (Meta Quest 3,\nMeta Ray-Ban, Android, and Microsoft HoloLens 2 running Llama and GPT models).\nAlthough these platforms each implement LLM integration differently, they share\nvulnerabilities where an attacker can modify the public context surrounding a\nlegitimate LLM query, resulting in erroneous visual or auditory feedback to\nusers, thus compromising their safety or privacy, sowing confusion, or other\nharmful effects. To defend against these threats, we discuss mitigation\nstrategies and best practices for developers, including an initial defense\nprototype, and call on the community to develop new protection mechanisms to\nmitigate these risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extended reality (XR) applications increasingly integrate Large Language\nModels (LLMs) to enhance user experience, scene understanding, and even\ngenerate executable XR content, and are often called \"AI glasses\". Despite\nthese potential benefits, the integrated XR-LLM pipeline makes XR applications\nvulnerable to new forms of attacks. In this paper, we analyze LLM-Integated XR\nsystems in the literature and in practice and categorize them along different\ndimensions from a systems perspective. Building on this categorization, we\nidentify a common threat model and demonstrate a series of proof-of-concept\nattacks on multiple XR platforms that employ various LLM models (Meta Quest 3,\nMeta Ray-Ban, Android, and Microsoft HoloLens 2 running Llama and GPT models).\nAlthough these platforms each implement LLM integration differently, they share\nvulnerabilities where an attacker can modify the public context surrounding a\nlegitimate LLM query, resulting in erroneous visual or auditory feedback to\nusers, thus compromising their safety or privacy, sowing confusion, or other\nharmful effects. To defend against these threats, we discuss mitigation\nstrategies and best practices for developers, including an initial defense\nprototype, and call on the community to develop new protection mechanisms to\nmitigate these risks."
                },
                "authors": [
                    {
                        "name": "Yicheng Zhang"
                    },
                    {
                        "name": "Zijian Huang"
                    },
                    {
                        "name": "Sophie Chen"
                    },
                    {
                        "name": "Erfan Shayegani"
                    },
                    {
                        "name": "Jiasi Chen"
                    },
                    {
                        "name": "Nael Abu-Ghazaleh"
                    }
                ],
                "author_detail": {
                    "name": "Nael Abu-Ghazaleh"
                },
                "author": "Nael Abu-Ghazaleh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15207v1",
                "updated": "2025-09-18T17:56:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    56,
                    36,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:56:36Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    56,
                    36,
                    3,
                    261,
                    0
                ],
                "title": "FlowRL: Matching Reward Distributions for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowRL: Matching Reward Distributions for LLM Reasoning"
                },
                "summary": "We propose FlowRL: matching the full reward distribution via flow balancing\ninstead of maximizing rewards in large language model (LLM) reinforcement\nlearning (RL). Recent advanced reasoning models adopt reward-maximizing methods\n(\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while\nneglecting less frequent but valid reasoning paths, thus reducing diversity. In\ncontrast, we transform scalar rewards into a normalized target distribution\nusing a learnable partition function, and then minimize the reverse KL\ndivergence between the policy and the target distribution. We implement this\nidea as a flow-balanced optimization method that promotes diverse exploration\nand generalizable reasoning trajectories. We conduct experiments on math and\ncode reasoning tasks: FlowRL achieves a significant average improvement of\n$10.0\\%$ over GRPO and $5.1\\%$ over PPO on math benchmarks, and performs\nconsistently better on code reasoning tasks. These results highlight reward\ndistribution-matching as a key step toward efficient exploration and diverse\nreasoning in LLM reinforcement learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose FlowRL: matching the full reward distribution via flow balancing\ninstead of maximizing rewards in large language model (LLM) reinforcement\nlearning (RL). Recent advanced reasoning models adopt reward-maximizing methods\n(\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while\nneglecting less frequent but valid reasoning paths, thus reducing diversity. In\ncontrast, we transform scalar rewards into a normalized target distribution\nusing a learnable partition function, and then minimize the reverse KL\ndivergence between the policy and the target distribution. We implement this\nidea as a flow-balanced optimization method that promotes diverse exploration\nand generalizable reasoning trajectories. We conduct experiments on math and\ncode reasoning tasks: FlowRL achieves a significant average improvement of\n$10.0\\%$ over GRPO and $5.1\\%$ over PPO on math benchmarks, and performs\nconsistently better on code reasoning tasks. These results highlight reward\ndistribution-matching as a key step toward efficient exploration and diverse\nreasoning in LLM reinforcement learning."
                },
                "authors": [
                    {
                        "name": "Xuekai Zhu"
                    },
                    {
                        "name": "Daixuan Cheng"
                    },
                    {
                        "name": "Dinghuai Zhang"
                    },
                    {
                        "name": "Hengli Li"
                    },
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Che Jiang"
                    },
                    {
                        "name": "Youbang Sun"
                    },
                    {
                        "name": "Ermo Hua"
                    },
                    {
                        "name": "Yuxin Zuo"
                    },
                    {
                        "name": "Xingtai Lv"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Lin Chen"
                    },
                    {
                        "name": "Fanghao Shao"
                    },
                    {
                        "name": "Bo Xue"
                    },
                    {
                        "name": "Yunchong Song"
                    },
                    {
                        "name": "Zhenjie Yang"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Bowen Zhou"
                    },
                    {
                        "name": "Hongyuan Mei"
                    },
                    {
                        "name": "Zhouhan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouhan Lin"
                },
                "author": "Zhouhan Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15202v1",
                "updated": "2025-09-18T17:54:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    54,
                    31,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:54:31Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    54,
                    31,
                    3,
                    261,
                    0
                ],
                "title": "Beyond Surface Alignment: Rebuilding LLMs Safety Mechanism via\n  Probabilistically Ablating Refusal Direction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Surface Alignment: Rebuilding LLMs Safety Mechanism via\n  Probabilistically Ablating Refusal Direction"
                },
                "summary": "Jailbreak attacks pose persistent threats to large language models (LLMs).\nCurrent safety alignment methods have attempted to address these issues, but\nthey experience two significant limitations: insufficient safety alignment\ndepth and unrobust internal defense mechanisms. These limitations make them\nvulnerable to adversarial attacks such as prefilling and refusal direction\nmanipulation. We introduce DeepRefusal, a robust safety alignment framework\nthat overcomes these issues. DeepRefusal forces the model to dynamically\nrebuild its refusal mechanisms from jailbreak states. This is achieved by\nprobabilistically ablating the refusal direction across layers and token depths\nduring fine-tuning. Our method not only defends against prefilling and refusal\ndirection attacks but also demonstrates strong resilience against other unseen\njailbreak strategies. Extensive evaluations on four open-source LLM families\nand six representative attacks show that DeepRefusal reduces attack success\nrates by approximately 95%, while maintaining model capabilities with minimal\nperformance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak attacks pose persistent threats to large language models (LLMs).\nCurrent safety alignment methods have attempted to address these issues, but\nthey experience two significant limitations: insufficient safety alignment\ndepth and unrobust internal defense mechanisms. These limitations make them\nvulnerable to adversarial attacks such as prefilling and refusal direction\nmanipulation. We introduce DeepRefusal, a robust safety alignment framework\nthat overcomes these issues. DeepRefusal forces the model to dynamically\nrebuild its refusal mechanisms from jailbreak states. This is achieved by\nprobabilistically ablating the refusal direction across layers and token depths\nduring fine-tuning. Our method not only defends against prefilling and refusal\ndirection attacks but also demonstrates strong resilience against other unseen\njailbreak strategies. Extensive evaluations on four open-source LLM families\nand six representative attacks show that DeepRefusal reduces attack success\nrates by approximately 95%, while maintaining model capabilities with minimal\nperformance degradation."
                },
                "authors": [
                    {
                        "name": "Yuanbo Xie"
                    },
                    {
                        "name": "Yingjie Zhang"
                    },
                    {
                        "name": "Tianyun Liu"
                    },
                    {
                        "name": "Duohe Ma"
                    },
                    {
                        "name": "Tingwen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tingwen Liu"
                },
                "author": "Tingwen Liu",
                "arxiv_comment": "Accepted by EMNLP2025 Finding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15195v1",
                "updated": "2025-09-18T17:52:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    52,
                    6,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:52:06Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    52,
                    6,
                    3,
                    261,
                    0
                ],
                "title": "Orion: Fuzzing Workflow Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orion: Fuzzing Workflow Automation"
                },
                "summary": "Fuzz testing is one of the most effective techniques for finding software\nvulnerabilities. While modern fuzzers can generate inputs and monitor\nexecutions automatically, the overall workflow, from analyzing a codebase, to\nconfiguring harnesses, to triaging results, still requires substantial manual\neffort. Prior attempts focused on single stages such as harness synthesis or\ninput minimization, leaving researchers to manually connect the pieces into a\ncomplete fuzzing campaign.\n  We introduce Orion, a framework that automates the the manual bottlenecks of\nfuzzing by integrating LLM reasoning with traditional tools, allowing campaigns\nto scale to settings where human effort alone was impractical. Orion uses LLMs\nfor code reasoning and semantic guidance, while relying on deterministic tools\nfor verification, iterative refinement, and tasks that require precision.\nAcross our benchmark suite, Orion reduces human effort by 46-204x depending on\nthe workflow stage, and we demonstrate its effectiveness through the discovery\nof two previously unknown vulnerabilities in the widely used open-source clib\nlibrary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fuzz testing is one of the most effective techniques for finding software\nvulnerabilities. While modern fuzzers can generate inputs and monitor\nexecutions automatically, the overall workflow, from analyzing a codebase, to\nconfiguring harnesses, to triaging results, still requires substantial manual\neffort. Prior attempts focused on single stages such as harness synthesis or\ninput minimization, leaving researchers to manually connect the pieces into a\ncomplete fuzzing campaign.\n  We introduce Orion, a framework that automates the the manual bottlenecks of\nfuzzing by integrating LLM reasoning with traditional tools, allowing campaigns\nto scale to settings where human effort alone was impractical. Orion uses LLMs\nfor code reasoning and semantic guidance, while relying on deterministic tools\nfor verification, iterative refinement, and tasks that require precision.\nAcross our benchmark suite, Orion reduces human effort by 46-204x depending on\nthe workflow stage, and we demonstrate its effectiveness through the discovery\nof two previously unknown vulnerabilities in the widely used open-source clib\nlibrary."
                },
                "authors": [
                    {
                        "name": "Max Bazalii"
                    },
                    {
                        "name": "Marius Fleischer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Fleischer"
                },
                "author": "Marius Fleischer",
                "arxiv_comment": "11 pages, 3 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.6; I.2.2; D.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15194v1",
                "updated": "2025-09-18T17:50:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    50,
                    4,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:50:04Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    50,
                    4,
                    3,
                    261,
                    0
                ],
                "title": "Evolving Language Models without Labels: Majority Drives Selection,\n  Novelty Promotes Variation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolving Language Models without Labels: Majority Drives Selection,\n  Novelty Promotes Variation"
                },
                "summary": "Large language models (LLMs) are increasingly trained with reinforcement\nlearning from verifiable rewards (RLVR), yet real-world deployment demands\nmodels that can self-improve without labels or external judges. Existing\nlabel-free methods, confidence minimization, self-consistency, or majority-vote\nobjectives, stabilize learning but steadily shrink exploration, causing an\nentropy collapse: generations become shorter, less diverse, and brittle. Unlike\nprior approaches such as Test-Time Reinforcement Learning (TTRL), which\nprimarily adapt models to the immediate unlabeled dataset at hand, our goal is\nbroader: to enable general improvements without sacrificing the model's\ninherent exploration capacity and generalization ability, i.e., evolving. We\nformalize this issue and propose EVolution-Oriented and Label-free\nReinforcement Learning (EVOL-RL), a simple rule that couples stability with\nvariation under a label-free setting. EVOL-RL keeps the majority-voted answer\nas a stable anchor (selection) while adding a novelty-aware reward that favors\nresponses whose reasoning differs from what has already been produced\n(variation), measured in semantic space. Implemented with GRPO, EVOL-RL also\nuses asymmetric clipping to preserve strong signals and an entropy regularizer\nto sustain search. This majority-for-selection + novelty-for-variation design\nprevents collapse, maintains longer and more informative chains of thought, and\nimproves both pass@1 and pass@n. EVOL-RL consistently outperforms the\nmajority-only TTRL baseline; e.g., training on label-free AIME24 lifts\nQwen3-4B-Base AIME25 pass@1 from TTRL's 4.6% to 16.4%, and pass@16 from 18.5%\nto 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks\nstronger generalization across domains (e.g., GPQA). Furthermore, we\ndemonstrate that EVOL-RL also boosts performance in the RLVR setting,\nhighlighting its broad applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly trained with reinforcement\nlearning from verifiable rewards (RLVR), yet real-world deployment demands\nmodels that can self-improve without labels or external judges. Existing\nlabel-free methods, confidence minimization, self-consistency, or majority-vote\nobjectives, stabilize learning but steadily shrink exploration, causing an\nentropy collapse: generations become shorter, less diverse, and brittle. Unlike\nprior approaches such as Test-Time Reinforcement Learning (TTRL), which\nprimarily adapt models to the immediate unlabeled dataset at hand, our goal is\nbroader: to enable general improvements without sacrificing the model's\ninherent exploration capacity and generalization ability, i.e., evolving. We\nformalize this issue and propose EVolution-Oriented and Label-free\nReinforcement Learning (EVOL-RL), a simple rule that couples stability with\nvariation under a label-free setting. EVOL-RL keeps the majority-voted answer\nas a stable anchor (selection) while adding a novelty-aware reward that favors\nresponses whose reasoning differs from what has already been produced\n(variation), measured in semantic space. Implemented with GRPO, EVOL-RL also\nuses asymmetric clipping to preserve strong signals and an entropy regularizer\nto sustain search. This majority-for-selection + novelty-for-variation design\nprevents collapse, maintains longer and more informative chains of thought, and\nimproves both pass@1 and pass@n. EVOL-RL consistently outperforms the\nmajority-only TTRL baseline; e.g., training on label-free AIME24 lifts\nQwen3-4B-Base AIME25 pass@1 from TTRL's 4.6% to 16.4%, and pass@16 from 18.5%\nto 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks\nstronger generalization across domains (e.g., GPQA). Furthermore, we\ndemonstrate that EVOL-RL also boosts performance in the RLVR setting,\nhighlighting its broad applicability."
                },
                "authors": [
                    {
                        "name": "Yujun Zhou"
                    },
                    {
                        "name": "Zhenwen Liang"
                    },
                    {
                        "name": "Haolin Liu"
                    },
                    {
                        "name": "Wenhao Yu"
                    },
                    {
                        "name": "Kishan Panaganti"
                    },
                    {
                        "name": "Linfeng Song"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05755v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05755v3",
                "updated": "2025-09-18T17:38:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    38,
                    28,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-06T15:48:49Z",
                "published_parsed": [
                    2025,
                    9,
                    6,
                    15,
                    48,
                    49,
                    5,
                    249,
                    0
                ],
                "title": "Exploit Tool Invocation Prompt for Tool Behavior Hijacking in LLM-Based\n  Agentic System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploit Tool Invocation Prompt for Tool Behavior Hijacking in LLM-Based\n  Agentic System"
                },
                "summary": "LLM-based agentic systems leverage large language models to handle user\nqueries, make decisions, and execute external tools for complex tasks across\ndomains like chatbots, customer service, and software engineering. A critical\ncomponent of these systems is the Tool Invocation Prompt (TIP), which defines\ntool interaction protocols and guides LLMs to ensure the security and\ncorrectness of tool usage. Despite its importance, TIP security has been\nlargely overlooked. This work investigates TIP-related security risks,\nrevealing that major LLM-based systems like Cursor, Claude Code, and others are\nvulnerable to attacks such as remote code execution (RCE) and denial of service\n(DoS). Through a systematic TIP exploitation workflow (TEW), we demonstrate\nexternal tool behavior hijacking via manipulated tool invocations. We also\npropose defense mechanisms to enhance TIP security in LLM-based agentic\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agentic systems leverage large language models to handle user\nqueries, make decisions, and execute external tools for complex tasks across\ndomains like chatbots, customer service, and software engineering. A critical\ncomponent of these systems is the Tool Invocation Prompt (TIP), which defines\ntool interaction protocols and guides LLMs to ensure the security and\ncorrectness of tool usage. Despite its importance, TIP security has been\nlargely overlooked. This work investigates TIP-related security risks,\nrevealing that major LLM-based systems like Cursor, Claude Code, and others are\nvulnerable to attacks such as remote code execution (RCE) and denial of service\n(DoS). Through a systematic TIP exploitation workflow (TEW), we demonstrate\nexternal tool behavior hijacking via manipulated tool invocations. We also\npropose defense mechanisms to enhance TIP security in LLM-based agentic\nsystems."
                },
                "authors": [
                    {
                        "name": "Yuchong Xie"
                    },
                    {
                        "name": "Mingyu Luo"
                    },
                    {
                        "name": "Zesen Liu"
                    },
                    {
                        "name": "Zhixiang Zhang"
                    },
                    {
                        "name": "Kaikai Zhang"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Zongjie Li"
                    },
                    {
                        "name": "Ping Chen"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Dongdong She"
                    }
                ],
                "author_detail": {
                    "name": "Dongdong She"
                },
                "author": "Dongdong She",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05755v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05755v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08123v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08123v3",
                "updated": "2025-09-18T17:36:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    36,
                    13,
                    3,
                    261,
                    0
                ],
                "published": "2025-06-09T18:24:57Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    18,
                    24,
                    57,
                    0,
                    160,
                    0
                ],
                "title": "QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA"
                },
                "summary": "Alignment of large language models (LLMs) with principles like helpfulness,\nhonesty, and harmlessness typically relies on scalar rewards that obscure which\nobjectives drive the training signal. We introduce QA-LIGN, which decomposes\nmonolithic rewards into interpretable principle-specific evaluations through\nstructured natural language programs. Models learn through a draft, critique,\nand revise pipeline, where symbolic evaluation against the rubrics provides\ntransparent feedback for both initial and revised responses during GRPO\ntraining. Applied to uncensored Llama-3.1-8B-Instruct, QA-LIGN reduces attack\nsuccess rates by up to 68.7% while maintaining a 0.67% false refusal rate,\nachieving Pareto optimal safety-helpfulness performance and outperforming both\nDPO and GRPO with state-of-the-art reward models given equivalent training.\nThese results demonstrate that making reward signals interpretable and modular\nimproves alignment effectiveness, suggesting transparency enhances LLM safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment of large language models (LLMs) with principles like helpfulness,\nhonesty, and harmlessness typically relies on scalar rewards that obscure which\nobjectives drive the training signal. We introduce QA-LIGN, which decomposes\nmonolithic rewards into interpretable principle-specific evaluations through\nstructured natural language programs. Models learn through a draft, critique,\nand revise pipeline, where symbolic evaluation against the rubrics provides\ntransparent feedback for both initial and revised responses during GRPO\ntraining. Applied to uncensored Llama-3.1-8B-Instruct, QA-LIGN reduces attack\nsuccess rates by up to 68.7% while maintaining a 0.67% false refusal rate,\nachieving Pareto optimal safety-helpfulness performance and outperforming both\nDPO and GRPO with state-of-the-art reward models given equivalent training.\nThese results demonstrate that making reward signals interpretable and modular\nimproves alignment effectiveness, suggesting transparency enhances LLM safety."
                },
                "authors": [
                    {
                        "name": "Jacob Dineen"
                    },
                    {
                        "name": "Aswin RRV"
                    },
                    {
                        "name": "Qin Liu"
                    },
                    {
                        "name": "Zhikun Xu"
                    },
                    {
                        "name": "Xiao Ye"
                    },
                    {
                        "name": "Ming Shen"
                    },
                    {
                        "name": "Zhaonan Li"
                    },
                    {
                        "name": "Shijie Lu"
                    },
                    {
                        "name": "Chitta Baral"
                    },
                    {
                        "name": "Muhao Chen"
                    },
                    {
                        "name": "Ben Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Ben Zhou"
                },
                "arxiv_affiliation": "Arizona State University",
                "author": "Ben Zhou",
                "arxiv_comment": "Accepted to Findings of EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08123v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08123v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15178v1",
                "updated": "2025-09-18T17:35:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    35,
                    50,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:35:50Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    35,
                    50,
                    3,
                    261,
                    0
                ],
                "title": "Unleashing the Potential of Multimodal LLMs for Zero-Shot\n  Spatio-Temporal Video Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of Multimodal LLMs for Zero-Shot\n  Spatio-Temporal Video Grounding"
                },
                "summary": "Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal\ntube of a video, as specified by the input text query. In this paper, we\nutilize multimodal large language models (MLLMs) to explore a zero-shot\nsolution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to\ndynamically assign special tokens, referred to as \\textit{grounding tokens},\nfor grounding the text query; and (2) MLLMs often suffer from suboptimal\ngrounding due to the inability to fully integrate the cues in the text query\n(\\textit{e.g.}, attributes, actions) for inference. Based on these insights, we\npropose a MLLM-based zero-shot framework for STVG, which includes novel\ndecomposed spatio-temporal highlighting (DSTH) and temporal-augmented\nassembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH\nstrategy first decouples the original query into attribute and action\nsub-queries for inquiring the existence of the target both spatially and\ntemporally. It then uses a novel logit-guided re-attention (LRA) module to\nlearn latent variables as spatial and temporal prompts, by regularizing token\npredictions for each sub-query. These prompts highlight attribute and action\ncues, respectively, directing the model's attention to reliable spatial and\ntemporal related visual regions. In addition, as the spatial grounding by the\nattribute sub-query should be temporally consistent, we introduce the TAS\nstrategy to assemble the predictions using the original video frames and the\ntemporal-augmented frames as inputs to help improve temporal consistency. We\nevaluate our method on various MLLMs, and show that it outperforms SOTA methods\non three common STVG benchmarks.\n  The code will be available at https://github.com/zaiquanyang/LLaVA_Next_STVG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal\ntube of a video, as specified by the input text query. In this paper, we\nutilize multimodal large language models (MLLMs) to explore a zero-shot\nsolution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to\ndynamically assign special tokens, referred to as \\textit{grounding tokens},\nfor grounding the text query; and (2) MLLMs often suffer from suboptimal\ngrounding due to the inability to fully integrate the cues in the text query\n(\\textit{e.g.}, attributes, actions) for inference. Based on these insights, we\npropose a MLLM-based zero-shot framework for STVG, which includes novel\ndecomposed spatio-temporal highlighting (DSTH) and temporal-augmented\nassembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH\nstrategy first decouples the original query into attribute and action\nsub-queries for inquiring the existence of the target both spatially and\ntemporally. It then uses a novel logit-guided re-attention (LRA) module to\nlearn latent variables as spatial and temporal prompts, by regularizing token\npredictions for each sub-query. These prompts highlight attribute and action\ncues, respectively, directing the model's attention to reliable spatial and\ntemporal related visual regions. In addition, as the spatial grounding by the\nattribute sub-query should be temporally consistent, we introduce the TAS\nstrategy to assemble the predictions using the original video frames and the\ntemporal-augmented frames as inputs to help improve temporal consistency. We\nevaluate our method on various MLLMs, and show that it outperforms SOTA methods\non three common STVG benchmarks.\n  The code will be available at https://github.com/zaiquanyang/LLaVA_Next_STVG."
                },
                "authors": [
                    {
                        "name": "Zaiquan Yang"
                    },
                    {
                        "name": "Yuhao Liu"
                    },
                    {
                        "name": "Gerhard Hancke"
                    },
                    {
                        "name": "Rynson W. H. Lau"
                    }
                ],
                "author_detail": {
                    "name": "Rynson W. H. Lau"
                },
                "author": "Rynson W. H. Lau",
                "arxiv_journal_ref": "NeurIPS2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15174v1",
                "updated": "2025-09-18T17:30:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    30,
                    36,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:30:36Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    30,
                    36,
                    3,
                    261,
                    0
                ],
                "title": "SMARTER: A Data-efficient Framework to Improve Toxicity Detection with\n  Explanation via Self-augmenting Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMARTER: A Data-efficient Framework to Improve Toxicity Detection with\n  Explanation via Self-augmenting Large Language Models"
                },
                "summary": "WARNING: This paper contains examples of offensive materials. Toxic content\nhas become pervasive on social media platforms. We introduce SMARTER, a\ndata-efficient two-stage framework for explainable content moderation using\nLarge Language Models (LLMs). In Stage 1, we leverage LLMs' own outputs to\ngenerate synthetic explanations for both correct and incorrect labels, enabling\nalignment via preference optimization with minimal human supervision. In Stage\n2, we refine explanation quality through cross-model training, allowing weaker\nmodels to align stylistically and semantically with stronger ones. Experiments\non three benchmark tasks -- HateXplain, Latent Hate, and Implicit Hate --\ndemonstrate that SMARTER enables LLMs to achieve up to a 13.5% macro-F1\nimprovement over standard few-shot baselines while using only a fraction of the\nfull training data. Our framework offers a scalable strategy for low-resource\nsettings by harnessing LLMs' self-improving capabilities for both\nclassification and explanation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WARNING: This paper contains examples of offensive materials. Toxic content\nhas become pervasive on social media platforms. We introduce SMARTER, a\ndata-efficient two-stage framework for explainable content moderation using\nLarge Language Models (LLMs). In Stage 1, we leverage LLMs' own outputs to\ngenerate synthetic explanations for both correct and incorrect labels, enabling\nalignment via preference optimization with minimal human supervision. In Stage\n2, we refine explanation quality through cross-model training, allowing weaker\nmodels to align stylistically and semantically with stronger ones. Experiments\non three benchmark tasks -- HateXplain, Latent Hate, and Implicit Hate --\ndemonstrate that SMARTER enables LLMs to achieve up to a 13.5% macro-F1\nimprovement over standard few-shot baselines while using only a fraction of the\nfull training data. Our framework offers a scalable strategy for low-resource\nsettings by harnessing LLMs' self-improving capabilities for both\nclassification and explanation."
                },
                "authors": [
                    {
                        "name": "Huy Nghiem"
                    },
                    {
                        "name": "Advik Sachdeva"
                    },
                    {
                        "name": "Hal Daumé III"
                    }
                ],
                "author_detail": {
                    "name": "Hal Daumé III"
                },
                "author": "Hal Daumé III",
                "arxiv_comment": "NLP, Hate speech detection, explanation, LLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15159v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15159v1",
                "updated": "2025-09-18T17:06:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    6,
                    53,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:06:53Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    6,
                    53,
                    3,
                    261,
                    0
                ],
                "title": "AIP: Subverting Retrieval-Augmented Generation via Adversarial\n  Instructional Prompt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIP: Subverting Retrieval-Augmented Generation via Adversarial\n  Instructional Prompt"
                },
                "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nretrieving relevant documents from external sources to improve factual accuracy\nand verifiability. However, this reliance introduces new attack surfaces within\nthe retrieval pipeline, beyond the LLM itself. While prior RAG attacks have\nexposed such vulnerabilities, they largely rely on manipulating user queries,\nwhich is often infeasible in practice due to fixed or protected user inputs.\nThis narrow focus overlooks a more realistic and stealthy vector: instructional\nprompts, which are widely reused, publicly shared, and rarely audited. Their\nimplicit trust makes them a compelling target for adversaries to manipulate RAG\nbehavior covertly.\n  We introduce a novel attack for Adversarial Instructional Prompt (AIP) that\nexploits adversarial instructional prompts to manipulate RAG outputs by subtly\naltering retrieval behavior. By shifting the attack surface to the\ninstructional prompts, AIP reveals how trusted yet seemingly benign interface\ncomponents can be weaponized to degrade system integrity. The attack is crafted\nto achieve three goals: (1) naturalness, to evade user detection; (2) utility,\nto encourage use of prompts; and (3) robustness, to remain effective across\ndiverse query variations. We propose a diverse query generation strategy that\nsimulates realistic linguistic variation in user queries, enabling the\ndiscovery of prompts that generalize across paraphrases and rephrasings.\nBuilding on this, a genetic algorithm-based joint optimization is developed to\nevolve adversarial prompts by balancing attack success, clean-task utility, and\nstealthiness. Experimental results show that AIP achieves up to 95.23% ASR\nwhile preserving benign functionality. These findings uncover a critical and\npreviously overlooked vulnerability in RAG systems, emphasizing the need to\nreassess the shared instructional prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nretrieving relevant documents from external sources to improve factual accuracy\nand verifiability. However, this reliance introduces new attack surfaces within\nthe retrieval pipeline, beyond the LLM itself. While prior RAG attacks have\nexposed such vulnerabilities, they largely rely on manipulating user queries,\nwhich is often infeasible in practice due to fixed or protected user inputs.\nThis narrow focus overlooks a more realistic and stealthy vector: instructional\nprompts, which are widely reused, publicly shared, and rarely audited. Their\nimplicit trust makes them a compelling target for adversaries to manipulate RAG\nbehavior covertly.\n  We introduce a novel attack for Adversarial Instructional Prompt (AIP) that\nexploits adversarial instructional prompts to manipulate RAG outputs by subtly\naltering retrieval behavior. By shifting the attack surface to the\ninstructional prompts, AIP reveals how trusted yet seemingly benign interface\ncomponents can be weaponized to degrade system integrity. The attack is crafted\nto achieve three goals: (1) naturalness, to evade user detection; (2) utility,\nto encourage use of prompts; and (3) robustness, to remain effective across\ndiverse query variations. We propose a diverse query generation strategy that\nsimulates realistic linguistic variation in user queries, enabling the\ndiscovery of prompts that generalize across paraphrases and rephrasings.\nBuilding on this, a genetic algorithm-based joint optimization is developed to\nevolve adversarial prompts by balancing attack success, clean-task utility, and\nstealthiness. Experimental results show that AIP achieves up to 95.23% ASR\nwhile preserving benign functionality. These findings uncover a critical and\npreviously overlooked vulnerability in RAG systems, emphasizing the need to\nreassess the shared instructional prompts."
                },
                "authors": [
                    {
                        "name": "Saket S. Chaturvedi"
                    },
                    {
                        "name": "Gaurav Bagwe"
                    },
                    {
                        "name": "Lan Zhang"
                    },
                    {
                        "name": "Xiaoyong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyong Yuan"
                },
                "author": "Xiaoyong Yuan",
                "arxiv_comment": "Accepted at EMNLP 2025 Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15159v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15148v1",
                "updated": "2025-09-18T16:55:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    55,
                    9,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T16:55:09Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    55,
                    9,
                    3,
                    261,
                    0
                ],
                "title": "A1: Asynchronous Test-Time Scaling via Conformal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A1: Asynchronous Test-Time Scaling via Conformal Prediction"
                },
                "summary": "Large language models (LLMs) benefit from test-time scaling, but existing\nmethods face significant challenges, including severe synchronization overhead,\nmemory bottlenecks, and latency, especially during speculative decoding with\nlong reasoning chains. We introduce A1 (Asynchronous Test-Time Scaling), a\nstatistically guaranteed adaptive inference framework that addresses these\nchallenges. A1 refines arithmetic intensity to identify synchronization as the\ndominant bottleneck, proposes an online calibration strategy to enable\nasynchronous inference, and designs a three-stage rejection sampling pipeline\nthat supports both sequential and parallel scaling. Through experiments on the\nMATH, AMC23, AIME24, and AIME25 datasets, across various draft-target model\nfamilies, we demonstrate that A1 achieves a remarkable 56.7x speedup in\ntest-time scaling and a 4.14x improvement in throughput, all while maintaining\naccurate rejection-rate control, reducing latency and memory overhead, and no\naccuracy loss compared to using target model scaling alone. These results\nposition A1 as an efficient and principled solution for scalable LLM inference.\nWe have released the code at\nhttps://github.com/menik1126/asynchronous-test-time-scaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) benefit from test-time scaling, but existing\nmethods face significant challenges, including severe synchronization overhead,\nmemory bottlenecks, and latency, especially during speculative decoding with\nlong reasoning chains. We introduce A1 (Asynchronous Test-Time Scaling), a\nstatistically guaranteed adaptive inference framework that addresses these\nchallenges. A1 refines arithmetic intensity to identify synchronization as the\ndominant bottleneck, proposes an online calibration strategy to enable\nasynchronous inference, and designs a three-stage rejection sampling pipeline\nthat supports both sequential and parallel scaling. Through experiments on the\nMATH, AMC23, AIME24, and AIME25 datasets, across various draft-target model\nfamilies, we demonstrate that A1 achieves a remarkable 56.7x speedup in\ntest-time scaling and a 4.14x improvement in throughput, all while maintaining\naccurate rejection-rate control, reducing latency and memory overhead, and no\naccuracy loss compared to using target model scaling alone. These results\nposition A1 as an efficient and principled solution for scalable LLM inference.\nWe have released the code at\nhttps://github.com/menik1126/asynchronous-test-time-scaling."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Qiujiang Chen"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Chenyang Zhao"
                    },
                    {
                        "name": "Hui Shen"
                    },
                    {
                        "name": "Alexander Hanbo Li"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Haochen Tan"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "Tech Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11277v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11277v2",
                "updated": "2025-09-18T16:45:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    45,
                    16,
                    3,
                    261,
                    0
                ],
                "published": "2025-08-15T07:29:42Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    29,
                    42,
                    4,
                    227,
                    0
                ],
                "title": "Probing the Representational Power of Sparse Autoencoders in Vision\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing the Representational Power of Sparse Autoencoders in Vision\n  Models"
                },
                "summary": "Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting\nthe hidden states of large language models (LLMs). By learning to reconstruct\nactivations from a sparse bottleneck layer, SAEs discover interpretable\nfeatures from the high-dimensional internal representations of LLMs. Despite\ntheir popularity with language models, SAEs remain understudied in the visual\ndomain. In this work, we provide an extensive evaluation the representational\npower of SAEs for vision models using a broad range of image-based tasks. Our\nexperimental results demonstrate that SAE features are semantically meaningful,\nimprove out-of-distribution generalization, and enable controllable generation\nacross three vision model architectures: vision embedding models, multi-modal\nLMMs and diffusion models. In vision embedding models, we find that learned SAE\nfeatures can be used for OOD detection and provide evidence that they recover\nthe ontological structure of the underlying model. For diffusion models, we\ndemonstrate that SAEs enable semantic steering through text encoder\nmanipulation and develop an automated pipeline for discovering\nhuman-interpretable attributes. Finally, we conduct exploratory experiments on\nmulti-modal LLMs, finding evidence that SAE features reveal shared\nrepresentations across vision and language modalities. Our study provides a\nfoundation for SAE evaluation in vision models, highlighting their strong\npotential improving interpretability, generalization, and steerability in the\nvisual domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting\nthe hidden states of large language models (LLMs). By learning to reconstruct\nactivations from a sparse bottleneck layer, SAEs discover interpretable\nfeatures from the high-dimensional internal representations of LLMs. Despite\ntheir popularity with language models, SAEs remain understudied in the visual\ndomain. In this work, we provide an extensive evaluation the representational\npower of SAEs for vision models using a broad range of image-based tasks. Our\nexperimental results demonstrate that SAE features are semantically meaningful,\nimprove out-of-distribution generalization, and enable controllable generation\nacross three vision model architectures: vision embedding models, multi-modal\nLMMs and diffusion models. In vision embedding models, we find that learned SAE\nfeatures can be used for OOD detection and provide evidence that they recover\nthe ontological structure of the underlying model. For diffusion models, we\ndemonstrate that SAEs enable semantic steering through text encoder\nmanipulation and develop an automated pipeline for discovering\nhuman-interpretable attributes. Finally, we conduct exploratory experiments on\nmulti-modal LLMs, finding evidence that SAE features reveal shared\nrepresentations across vision and language modalities. Our study provides a\nfoundation for SAE evaluation in vision models, highlighting their strong\npotential improving interpretability, generalization, and steerability in the\nvisual domain."
                },
                "authors": [
                    {
                        "name": "Matthew Lyle Olson"
                    },
                    {
                        "name": "Musashi Hinck"
                    },
                    {
                        "name": "Neale Ratzlaff"
                    },
                    {
                        "name": "Changbai Li"
                    },
                    {
                        "name": "Phillip Howard"
                    },
                    {
                        "name": "Vasudev Lal"
                    },
                    {
                        "name": "Shao-Yen Tseng"
                    }
                ],
                "author_detail": {
                    "name": "Shao-Yen Tseng"
                },
                "author": "Shao-Yen Tseng",
                "arxiv_comment": "ICCV 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11277v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11277v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15132v1",
                "updated": "2025-09-18T16:42:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    42,
                    1,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T16:42:01Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    42,
                    1,
                    3,
                    261,
                    0
                ],
                "title": "From Pixels to Urban Policy-Intelligence: Recovering Legacy Effects of\n  Redlining with a Multimodal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Pixels to Urban Policy-Intelligence: Recovering Legacy Effects of\n  Redlining with a Multimodal LLM"
                },
                "summary": "This paper shows how a multimodal large language model (MLLM) can expand\nurban measurement capacity and support tracking of place-based policy\ninterventions. Using a structured, reason-then-estimate pipeline on street-view\nimagery, GPT-4o infers neighborhood poverty and tree canopy, which we embed in\na quasi-experimental design evaluating the legacy of 1930s redlining. GPT-4o\nrecovers the expected adverse socio-environmental legacy effects of redlining,\nwith estimates statistically indistinguishable from authoritative sources, and\nit outperforms a conventional pixel-based segmentation baseline-consistent with\nthe idea that holistic scene reasoning extracts higher-order information beyond\nobject counts alone. These results position MLLMs as policy-grade instruments\nfor neighborhood measurement and motivate broader validation across\npolicy-evaluation settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper shows how a multimodal large language model (MLLM) can expand\nurban measurement capacity and support tracking of place-based policy\ninterventions. Using a structured, reason-then-estimate pipeline on street-view\nimagery, GPT-4o infers neighborhood poverty and tree canopy, which we embed in\na quasi-experimental design evaluating the legacy of 1930s redlining. GPT-4o\nrecovers the expected adverse socio-environmental legacy effects of redlining,\nwith estimates statistically indistinguishable from authoritative sources, and\nit outperforms a conventional pixel-based segmentation baseline-consistent with\nthe idea that holistic scene reasoning extracts higher-order information beyond\nobject counts alone. These results position MLLMs as policy-grade instruments\nfor neighborhood measurement and motivate broader validation across\npolicy-evaluation settings."
                },
                "authors": [
                    {
                        "name": "Anthony Howell"
                    },
                    {
                        "name": "Nancy Wu"
                    },
                    {
                        "name": "Sharmistha Bagchi"
                    },
                    {
                        "name": "Yushim Kim"
                    },
                    {
                        "name": "Chayn Sun"
                    }
                ],
                "author_detail": {
                    "name": "Chayn Sun"
                },
                "author": "Chayn Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15129v1",
                "updated": "2025-09-18T16:40:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    40,
                    14,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T16:40:14Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    40,
                    14,
                    3,
                    261,
                    0
                ],
                "title": "Doppler Radiance Field-Guided Antenna Selection for Improved\n  Generalization in Multi-Antenna Wi-Fi-based Human Activity Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doppler Radiance Field-Guided Antenna Selection for Improved\n  Generalization in Multi-Antenna Wi-Fi-based Human Activity Recognition"
                },
                "summary": "With the IEEE 802.11bf Task Group introducing amendments to the WLAN standard\nfor advanced sensing, interest in using Wi-Fi Channel State Information (CSI)\nfor remote sensing has surged. Recent findings indicate that learning a unified\nthree-dimensional motion representation through Doppler Radiance Fields (DoRFs)\nderived from CSI significantly improves the generalization capabilities of\nWi-Fi-based human activity recognition (HAR). Despite this progress, CSI\nsignals remain affected by asynchronous access point (AP) clocks and additive\nnoise from environmental and hardware sources. Consequently, even with existing\npreprocessing techniques, both the CSI data and Doppler velocity projections\nused in DoRFs are still susceptible to noise and outliers, limiting HAR\nperformance. To address this challenge, we propose a novel framework for\nmulti-antenna APs to suppress noise and identify the most informative antennas\nbased on DoRF fitting errors, which capture inconsistencies among Doppler\nvelocity projections. Experimental results on a challenging small-scale hand\ngesture recognition dataset demonstrate that the proposed DoRF-guided\nWi-Fi-based HAR approach significantly improves generalization capability,\npaving the way for robust real-world sensing deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the IEEE 802.11bf Task Group introducing amendments to the WLAN standard\nfor advanced sensing, interest in using Wi-Fi Channel State Information (CSI)\nfor remote sensing has surged. Recent findings indicate that learning a unified\nthree-dimensional motion representation through Doppler Radiance Fields (DoRFs)\nderived from CSI significantly improves the generalization capabilities of\nWi-Fi-based human activity recognition (HAR). Despite this progress, CSI\nsignals remain affected by asynchronous access point (AP) clocks and additive\nnoise from environmental and hardware sources. Consequently, even with existing\npreprocessing techniques, both the CSI data and Doppler velocity projections\nused in DoRFs are still susceptible to noise and outliers, limiting HAR\nperformance. To address this challenge, we propose a novel framework for\nmulti-antenna APs to suppress noise and identify the most informative antennas\nbased on DoRF fitting errors, which capture inconsistencies among Doppler\nvelocity projections. Experimental results on a challenging small-scale hand\ngesture recognition dataset demonstrate that the proposed DoRF-guided\nWi-Fi-based HAR approach significantly improves generalization capability,\npaving the way for robust real-world sensing deployments."
                },
                "authors": [
                    {
                        "name": "Navid Hasanzadeh"
                    },
                    {
                        "name": "Shahrokh Valaee"
                    }
                ],
                "author_detail": {
                    "name": "Shahrokh Valaee"
                },
                "author": "Shahrokh Valaee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20020v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20020v2",
                "updated": "2025-09-18T16:34:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    34,
                    54,
                    3,
                    261,
                    0
                ],
                "published": "2025-04-28T17:42:02Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    42,
                    2,
                    0,
                    118,
                    0
                ],
                "title": "Modular Machine Learning: An Indispensable Path towards New-Generation\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modular Machine Learning: An Indispensable Path towards New-Generation\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) have substantially advanced machine learning\nresearch, including natural language processing, computer vision, data mining,\netc., yet they still exhibit critical limitations in explainability,\nreliability, adaptability, and extensibility. In this paper, we overview a\npromising learning paradigm, i.e., Modular Machine Learning (MML), as an\nessential approach toward new-generation LLMs capable of addressing these\nissues. We begin by systematically and comprehensively surveying the existing\nliterature on modular machine learning, with a particular focus on modular data\nrepresentation and modular models. Then, we propose a unified MML framework for\nLLMs, which decomposes the complex structure of LLMs into three interdependent\ncomponents: modular representation, modular model, and modular reasoning.\nSpecifically, the MML paradigm discussed in this article is able to: i) clarify\nthe internal working mechanism of LLMs through the disentanglement of semantic\ncomponents; ii) allow for flexible and task-adaptive model design; iii) enable\nan interpretable and logic-driven decision-making process. We further elaborate\na feasible implementation of MML-based LLMs via leveraging advanced techniques\nsuch as disentangled representation learning, neural architecture search and\nneuro-symbolic learning. Last but not least, we critically identify the\nremaining key challenges, such as the integration of continuous neural and\ndiscrete symbolic processes, joint optimization, and computational scalability,\npresent promising future research directions that deserve further exploration.\nUltimately, we believe the integration of the MML with LLMs has the potential\nto bridge the gap between statistical (deep) learning and formal (logical)\nreasoning, thereby paving the way for robust, adaptable, and trustworthy AI\nsystems across a wide range of real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have substantially advanced machine learning\nresearch, including natural language processing, computer vision, data mining,\netc., yet they still exhibit critical limitations in explainability,\nreliability, adaptability, and extensibility. In this paper, we overview a\npromising learning paradigm, i.e., Modular Machine Learning (MML), as an\nessential approach toward new-generation LLMs capable of addressing these\nissues. We begin by systematically and comprehensively surveying the existing\nliterature on modular machine learning, with a particular focus on modular data\nrepresentation and modular models. Then, we propose a unified MML framework for\nLLMs, which decomposes the complex structure of LLMs into three interdependent\ncomponents: modular representation, modular model, and modular reasoning.\nSpecifically, the MML paradigm discussed in this article is able to: i) clarify\nthe internal working mechanism of LLMs through the disentanglement of semantic\ncomponents; ii) allow for flexible and task-adaptive model design; iii) enable\nan interpretable and logic-driven decision-making process. We further elaborate\na feasible implementation of MML-based LLMs via leveraging advanced techniques\nsuch as disentangled representation learning, neural architecture search and\nneuro-symbolic learning. Last but not least, we critically identify the\nremaining key challenges, such as the integration of continuous neural and\ndiscrete symbolic processes, joint optimization, and computational scalability,\npresent promising future research directions that deserve further exploration.\nUltimately, we believe the integration of the MML with LLMs has the potential\nto bridge the gap between statistical (deep) learning and formal (logical)\nreasoning, thereby paving the way for robust, adaptable, and trustworthy AI\nsystems across a wide range of real-world applications."
                },
                "authors": [
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Haibo Chen"
                    },
                    {
                        "name": "Zeyang Zhang"
                    },
                    {
                        "name": "Wenwu Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Wenwu Zhu"
                },
                "author": "Wenwu Zhu",
                "arxiv_comment": "20 pages, 4 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20020v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20020v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05129v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05129v2",
                "updated": "2025-09-18T16:29:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    29,
                    0,
                    3,
                    261,
                    0
                ],
                "published": "2025-07-07T15:41:38Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    41,
                    38,
                    0,
                    188,
                    0
                ],
                "title": "SMART: Simulated Students Aligned with Item Response Theory for Question\n  Difficulty Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMART: Simulated Students Aligned with Item Response Theory for Question\n  Difficulty Prediction"
                },
                "summary": "Item (question) difficulties play a crucial role in educational assessments,\nenabling accurate and efficient assessment of student abilities and\npersonalization to maximize learning outcomes. Traditionally, estimating item\ndifficulties can be costly, requiring real students to respond to items,\nfollowed by fitting an item response theory (IRT) model to get difficulty\nestimates. This approach cannot be applied to the cold-start setting for\npreviously unseen items either. In this work, we present SMART (Simulated\nStudents Aligned with IRT), a novel method for aligning simulated students with\ninstructed ability, which can then be used in simulations to predict the\ndifficulty of open-ended items. We achieve this alignment using direct\npreference optimization (DPO), where we form preference pairs based on how\nlikely responses are under a ground-truth IRT model. We perform a simulation by\ngenerating thousands of responses, evaluating them with a large language model\n(LLM)-based scoring model, and fit the resulting data to an IRT model to obtain\nitem difficulty estimates. Through extensive experiments on two real-world\nstudent response datasets, we show that SMART outperforms other item difficulty\nprediction methods by leveraging its improved ability alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Item (question) difficulties play a crucial role in educational assessments,\nenabling accurate and efficient assessment of student abilities and\npersonalization to maximize learning outcomes. Traditionally, estimating item\ndifficulties can be costly, requiring real students to respond to items,\nfollowed by fitting an item response theory (IRT) model to get difficulty\nestimates. This approach cannot be applied to the cold-start setting for\npreviously unseen items either. In this work, we present SMART (Simulated\nStudents Aligned with IRT), a novel method for aligning simulated students with\ninstructed ability, which can then be used in simulations to predict the\ndifficulty of open-ended items. We achieve this alignment using direct\npreference optimization (DPO), where we form preference pairs based on how\nlikely responses are under a ground-truth IRT model. We perform a simulation by\ngenerating thousands of responses, evaluating them with a large language model\n(LLM)-based scoring model, and fit the resulting data to an IRT model to obtain\nitem difficulty estimates. Through extensive experiments on two real-world\nstudent response datasets, we show that SMART outperforms other item difficulty\nprediction methods by leveraging its improved ability alignment."
                },
                "authors": [
                    {
                        "name": "Alexander Scarlatos"
                    },
                    {
                        "name": "Nigel Fernandez"
                    },
                    {
                        "name": "Christopher Ormerod"
                    },
                    {
                        "name": "Susan Lottridge"
                    },
                    {
                        "name": "Andrew Lan"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Lan"
                },
                "author": "Andrew Lan",
                "arxiv_comment": "Published in EMNLP 2025: The 2025 Conference on Empirical Methods in\n  Natural Language Processing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05129v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05129v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15122v1",
                "updated": "2025-09-18T16:28:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    28,
                    19,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T16:28:19Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    28,
                    19,
                    3,
                    261,
                    0
                ],
                "title": "Prestige over merit: An adapted audit of LLM bias in peer review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prestige over merit: An adapted audit of LLM bias in peer review"
                },
                "summary": "Large language models (LLMs) are playing an increasingly integral, though\nlargely informal, role in scholarly peer review. Yet it remains unclear whether\nLLMs reproduce the biases observed in human decision-making. We adapt a\nresume-style audit to scientific publishing, developing a multi-role LLM\nsimulation (editor/reviewer) that evaluates a representative set of\nhigh-quality manuscripts across the physical, biological, and social sciences\nunder randomized author identities (institutional prestige, gender, race). The\naudit reveals a strong and consistent institutional-prestige bias: identical\npapers attributed to low-prestige affiliations face a significantly higher risk\nof rejection, despite only modest differences in LLM-assessed quality. To probe\nmechanisms, we generate synthetic CVs for the same author profiles; these\nencode large prestige-linked disparities and an inverted prestige-tenure\ngradient relative to national benchmarks. The results suggest that both domain\nnorms and prestige-linked priors embedded in training data shape paper-level\noutcomes once identity is visible, converting affiliation into a decisive\nstatus cue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are playing an increasingly integral, though\nlargely informal, role in scholarly peer review. Yet it remains unclear whether\nLLMs reproduce the biases observed in human decision-making. We adapt a\nresume-style audit to scientific publishing, developing a multi-role LLM\nsimulation (editor/reviewer) that evaluates a representative set of\nhigh-quality manuscripts across the physical, biological, and social sciences\nunder randomized author identities (institutional prestige, gender, race). The\naudit reveals a strong and consistent institutional-prestige bias: identical\npapers attributed to low-prestige affiliations face a significantly higher risk\nof rejection, despite only modest differences in LLM-assessed quality. To probe\nmechanisms, we generate synthetic CVs for the same author profiles; these\nencode large prestige-linked disparities and an inverted prestige-tenure\ngradient relative to national benchmarks. The results suggest that both domain\nnorms and prestige-linked priors embedded in training data shape paper-level\noutcomes once identity is visible, converting affiliation into a decisive\nstatus cue."
                },
                "authors": [
                    {
                        "name": "Anthony Howell"
                    },
                    {
                        "name": "Jieshu Wang"
                    },
                    {
                        "name": "Luyu Du"
                    },
                    {
                        "name": "Julia Melkers"
                    },
                    {
                        "name": "Varshil Shah"
                    }
                ],
                "author_detail": {
                    "name": "Varshil Shah"
                },
                "author": "Varshil Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16815v2",
                "updated": "2025-09-18T16:26:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    26,
                    53,
                    3,
                    261,
                    0
                ],
                "published": "2025-07-22T17:59:46Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    59,
                    46,
                    1,
                    203,
                    0
                ],
                "title": "ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent\n  Planning"
                },
                "summary": "Vision-language-action (VLA) reasoning tasks require agents to interpret\nmultimodal instructions, perform long-horizon planning, and act adaptively in\ndynamic environments. Existing approaches typically train VLA models in an\nend-to-end fashion, directly mapping inputs to actions without explicit\nreasoning, which hinders their ability to plan over multiple steps or adapt to\ncomplex task variations. In this paper, we propose ThinkAct, a dual-system\nframework that bridges high-level reasoning with low-level action execution via\nreinforced visual latent planning. ThinkAct trains a multimodal LLM to generate\nembodied reasoning plans guided by reinforcing action-aligned visual rewards\nbased on goal completion and trajectory consistency. These reasoning plans are\ncompressed into a visual plan latent that conditions a downstream action model\nfor robust action execution on target environments. Extensive experiments on\nembodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct\nenables few-shot adaptation, long-horizon planning, and self-correction\nbehaviors in complex embodied AI tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language-action (VLA) reasoning tasks require agents to interpret\nmultimodal instructions, perform long-horizon planning, and act adaptively in\ndynamic environments. Existing approaches typically train VLA models in an\nend-to-end fashion, directly mapping inputs to actions without explicit\nreasoning, which hinders their ability to plan over multiple steps or adapt to\ncomplex task variations. In this paper, we propose ThinkAct, a dual-system\nframework that bridges high-level reasoning with low-level action execution via\nreinforced visual latent planning. ThinkAct trains a multimodal LLM to generate\nembodied reasoning plans guided by reinforcing action-aligned visual rewards\nbased on goal completion and trajectory consistency. These reasoning plans are\ncompressed into a visual plan latent that conditions a downstream action model\nfor robust action execution on target environments. Extensive experiments on\nembodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct\nenables few-shot adaptation, long-horizon planning, and self-correction\nbehaviors in complex embodied AI tasks."
                },
                "authors": [
                    {
                        "name": "Chi-Pin Huang"
                    },
                    {
                        "name": "Yueh-Hua Wu"
                    },
                    {
                        "name": "Min-Hung Chen"
                    },
                    {
                        "name": "Yu-Chiang Frank Wang"
                    },
                    {
                        "name": "Fu-En Yang"
                    }
                ],
                "author_detail": {
                    "name": "Fu-En Yang"
                },
                "author": "Fu-En Yang",
                "arxiv_comment": "NeurIPS 2025. Project page:\n  https://jasper0314-huang.github.io/thinkact-vla/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10943v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10943v2",
                "updated": "2025-09-18T16:17:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    17,
                    30,
                    3,
                    261,
                    0
                ],
                "published": "2025-06-12T17:48:13Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    17,
                    48,
                    13,
                    3,
                    163,
                    0
                ],
                "title": "Self-Adapting Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Adapting Language Models"
                },
                "summary": "Large language models (LLMs) are powerful but static; they lack mechanisms to\nadapt their weights in response to new tasks, knowledge, or examples. We\nintroduce Self-Adapting LLMs (SEAL), a framework that enables LLMs to\nself-adapt by generating their own finetuning data and update directives. Given\na new input, the model produces a self-edit-a generation that may restructure\nthe information in different ways, specify optimization hyperparameters, or\ninvoke tools for data augmentation and gradient-based updates. Through\nsupervised finetuning (SFT), these self-edits result in persistent weight\nupdates, enabling lasting adaptation. To train the model to produce effective\nself-edits, we use a reinforcement learning loop with the downstream\nperformance of the updated model as the reward signal. Unlike prior approaches\nthat rely on separate adaptation modules or auxiliary networks, SEAL directly\nuses the model's own generation to control its adaptation process. Experiments\non knowledge incorporation and few-shot generalization show that SEAL is a\npromising step toward language models capable of self-directed adaptation. Our\nwebsite and code is available at https://jyopari.github.io/posts/seal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are powerful but static; they lack mechanisms to\nadapt their weights in response to new tasks, knowledge, or examples. We\nintroduce Self-Adapting LLMs (SEAL), a framework that enables LLMs to\nself-adapt by generating their own finetuning data and update directives. Given\na new input, the model produces a self-edit-a generation that may restructure\nthe information in different ways, specify optimization hyperparameters, or\ninvoke tools for data augmentation and gradient-based updates. Through\nsupervised finetuning (SFT), these self-edits result in persistent weight\nupdates, enabling lasting adaptation. To train the model to produce effective\nself-edits, we use a reinforcement learning loop with the downstream\nperformance of the updated model as the reward signal. Unlike prior approaches\nthat rely on separate adaptation modules or auxiliary networks, SEAL directly\nuses the model's own generation to control its adaptation process. Experiments\non knowledge incorporation and few-shot generalization show that SEAL is a\npromising step toward language models capable of self-directed adaptation. Our\nwebsite and code is available at https://jyopari.github.io/posts/seal."
                },
                "authors": [
                    {
                        "name": "Adam Zweiger"
                    },
                    {
                        "name": "Jyothish Pari"
                    },
                    {
                        "name": "Han Guo"
                    },
                    {
                        "name": "Ekin Akyürek"
                    },
                    {
                        "name": "Yoon Kim"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    }
                ],
                "author_detail": {
                    "name": "Pulkit Agrawal"
                },
                "author": "Pulkit Agrawal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10943v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15110v1",
                "updated": "2025-09-18T16:14:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    14,
                    34,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T16:14:34Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    14,
                    34,
                    3,
                    261,
                    0
                ],
                "title": "TDRM: Smooth Reward Models with Temporal Difference for LLM RL and\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TDRM: Smooth Reward Models with Temporal Difference for LLM RL and\n  Inference"
                },
                "summary": "Reward models are central to both reinforcement learning (RL) with language\nmodels and inference-time verification. However, existing reward models often\nlack temporal consistency, leading to ineffective policy updates and unstable\nRL training. We introduce TDRM, a method for learning smoother and more\nreliable reward models by minimizing temporal differences during training. This\ntemporal-difference (TD) regularization produces smooth rewards and improves\nalignment with long-term objectives. Incorporating TDRM into the actor-critic\nstyle online RL loop yields consistent empirical gains. It is worth noting that\nTDRM is a supplement to verifiable reward methods, and both can be used in\nseries. Experiments show that TD-trained process reward models (PRMs) improve\nperformance across Best-of-N (up to 6.6%) and tree-search (up to 23.7%)\nsettings. When combined with Reinforcement Learning with Verifiable Rewards\n(RLVR), TD-trained PRMs lead to more data-efficient RL -- achieving comparable\nperformance with just 2.5k data to what baseline methods require 50.1k data to\nattain -- and yield higher-quality language model policies on 8 model variants\n(5 series), e.g., Qwen2.5-(0.5B, 1,5B), GLM4-9B-0414, GLM-Z1-9B-0414,\nQwen2.5-Math-(1.5B, 7B), and DeepSeek-R1-Distill-Qwen-(1.5B, 7B). We release\nall code at https://github.com/THUDM/TDRM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models are central to both reinforcement learning (RL) with language\nmodels and inference-time verification. However, existing reward models often\nlack temporal consistency, leading to ineffective policy updates and unstable\nRL training. We introduce TDRM, a method for learning smoother and more\nreliable reward models by minimizing temporal differences during training. This\ntemporal-difference (TD) regularization produces smooth rewards and improves\nalignment with long-term objectives. Incorporating TDRM into the actor-critic\nstyle online RL loop yields consistent empirical gains. It is worth noting that\nTDRM is a supplement to verifiable reward methods, and both can be used in\nseries. Experiments show that TD-trained process reward models (PRMs) improve\nperformance across Best-of-N (up to 6.6%) and tree-search (up to 23.7%)\nsettings. When combined with Reinforcement Learning with Verifiable Rewards\n(RLVR), TD-trained PRMs lead to more data-efficient RL -- achieving comparable\nperformance with just 2.5k data to what baseline methods require 50.1k data to\nattain -- and yield higher-quality language model policies on 8 model variants\n(5 series), e.g., Qwen2.5-(0.5B, 1,5B), GLM4-9B-0414, GLM-Z1-9B-0414,\nQwen2.5-Math-(1.5B, 7B), and DeepSeek-R1-Distill-Qwen-(1.5B, 7B). We release\nall code at https://github.com/THUDM/TDRM."
                },
                "authors": [
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Min Cai"
                    },
                    {
                        "name": "Jonathan Li"
                    },
                    {
                        "name": "Ziniu Hu"
                    },
                    {
                        "name": "Yisong Yue"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "arxiv_comment": "9 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15108v1",
                "updated": "2025-09-18T16:13:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    13,
                    23,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T16:13:23Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    16,
                    13,
                    23,
                    3,
                    261,
                    0
                ],
                "title": "Design and characterization of the Flat-Field Calibration of the\n  NectarCAM Camera",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and characterization of the Flat-Field Calibration of the\n  NectarCAM Camera"
                },
                "summary": "The NectarCAM flat-field flasher is a calibration device designed for the\ncamera that will equip the Medium-Sized Telescopes (MSTs) of the northern site\nof the Cherenkov Telescope Array Observatory (CTAO). Positioned in the centre\nof the MST dish, 16 meters in front of the camera, the flasher emits short\n(FWHM $\\approx$ 5 ns), uniform (2$-$4%) light pulses to illuminate the entire\nfocal plane.\n  Accurate calibration is crucial for the optimal operation of the NectarCAM,\nensuring precise gain computation and mitigating differences in\nlight-collection efficiency of the pixels of the camera. Using the flat-field\nflasher, two informations are obtained : the pixel gain and the relative\nefficiency between pixels. In addition, the flasher is used to probe the\ndynamic range over which the camera operates effectively.\n  In this study, we report on the performance characterisation of the\nflat-field flasher using a dedicated test bench. We report on the results of\ntests conducted on several flasher units, evaluating their reliability.\nFurthermore, we describe how the flat-field coefficients are applied within the\ncamera to ensure uniformity of response of few percent level across all 1855\npixels.\n  As the deployment of the first MST at the CTAO northern site is scheduled for\n2027, this work represents a significant contribution to the collaboration`s\nefforts to finalize camera calibration systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NectarCAM flat-field flasher is a calibration device designed for the\ncamera that will equip the Medium-Sized Telescopes (MSTs) of the northern site\nof the Cherenkov Telescope Array Observatory (CTAO). Positioned in the centre\nof the MST dish, 16 meters in front of the camera, the flasher emits short\n(FWHM $\\approx$ 5 ns), uniform (2$-$4%) light pulses to illuminate the entire\nfocal plane.\n  Accurate calibration is crucial for the optimal operation of the NectarCAM,\nensuring precise gain computation and mitigating differences in\nlight-collection efficiency of the pixels of the camera. Using the flat-field\nflasher, two informations are obtained : the pixel gain and the relative\nefficiency between pixels. In addition, the flasher is used to probe the\ndynamic range over which the camera operates effectively.\n  In this study, we report on the performance characterisation of the\nflat-field flasher using a dedicated test bench. We report on the results of\ntests conducted on several flasher units, evaluating their reliability.\nFurthermore, we describe how the flat-field coefficients are applied within the\ncamera to ensure uniformity of response of few percent level across all 1855\npixels.\n  As the deployment of the first MST at the CTAO northern site is scheduled for\n2027, this work represents a significant contribution to the collaboration`s\nefforts to finalize camera calibration systems."
                },
                "authors": [
                    {
                        "name": "Anastasiia Mikhno"
                    },
                    {
                        "name": "Federica Bradascio"
                    },
                    {
                        "name": "Jonathan Biteau"
                    },
                    {
                        "name": "François Brun"
                    },
                    {
                        "name": "Patrick Brun"
                    },
                    {
                        "name": "Hossam Boutalha"
                    },
                    {
                        "name": "Justine Devin"
                    },
                    {
                        "name": "Armelle Jardin-Blicq"
                    },
                    {
                        "name": "Pierre Jean"
                    },
                    {
                        "name": "Michael Josselin"
                    },
                    {
                        "name": "Jean-Philippe Lenain"
                    },
                    {
                        "name": "Quentin Luce"
                    },
                    {
                        "name": "Vincent Marandon"
                    },
                    {
                        "name": "Kevin Pressard"
                    },
                    {
                        "name": "Georges Vasileiadis"
                    },
                    {
                        "name": "CTAO NectarCAM Collaboration"
                    }
                ],
                "author_detail": {
                    "name": "CTAO NectarCAM Collaboration"
                },
                "author": "CTAO NectarCAM Collaboration",
                "arxiv_comment": "Presented at the 39th International Cosmic Ray Conference (ICRC\n  2025), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15099v1",
                "updated": "2025-09-18T15:56:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    56,
                    20,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:56:20Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    56,
                    20,
                    3,
                    261,
                    0
                ],
                "title": "Digital Twin-based Cooperative Autonomous Driving in Smart\n  Intersections: A Multi-Agent Reinforcement Learning Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Twin-based Cooperative Autonomous Driving in Smart\n  Intersections: A Multi-Agent Reinforcement Learning Approach"
                },
                "summary": "Unsignalized intersections pose safety and efficiency challenges due to\ncomplex traffic flows and blind spots. In this paper, a digital twin (DT)-based\ncooperative driving system with roadside unit (RSU)-centric architecture is\nproposed for enhancing safety and efficiency at unsignalized intersections. The\nsystem leverages comprehensive bird-eye-view (BEV) perception to eliminate\nblind spots and employs a hybrid reinforcement learning (RL) framework\ncombining offline pre-training with online fine-tuning. Specifically, driving\npolicies are initially trained using conservative Q-learning (CQL) with\nbehavior cloning (BC) on real datasets, then fine-tuned using multi-agent\nproximal policy optimization (MAPPO) with self-attention mechanisms to handle\ndynamic multi-agent coordination. The RSU implements real-time commands via\nvehicle-to-infrastructure (V2I) communications. Experimental results show that\nthe proposed method yields failure rates below 0.03\\% coordinating up to three\nconnected autonomous vehicles (CAVs), significantly outperforming traditional\nmethods. In addition, the system exhibits sub-linear computational scaling with\ninference times under 40 ms. Furthermore, it demonstrates robust generalization\nacross diverse unsignalized intersection scenarios, indicating its practicality\nand readiness for real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsignalized intersections pose safety and efficiency challenges due to\ncomplex traffic flows and blind spots. In this paper, a digital twin (DT)-based\ncooperative driving system with roadside unit (RSU)-centric architecture is\nproposed for enhancing safety and efficiency at unsignalized intersections. The\nsystem leverages comprehensive bird-eye-view (BEV) perception to eliminate\nblind spots and employs a hybrid reinforcement learning (RL) framework\ncombining offline pre-training with online fine-tuning. Specifically, driving\npolicies are initially trained using conservative Q-learning (CQL) with\nbehavior cloning (BC) on real datasets, then fine-tuned using multi-agent\nproximal policy optimization (MAPPO) with self-attention mechanisms to handle\ndynamic multi-agent coordination. The RSU implements real-time commands via\nvehicle-to-infrastructure (V2I) communications. Experimental results show that\nthe proposed method yields failure rates below 0.03\\% coordinating up to three\nconnected autonomous vehicles (CAVs), significantly outperforming traditional\nmethods. In addition, the system exhibits sub-linear computational scaling with\ninference times under 40 ms. Furthermore, it demonstrates robust generalization\nacross diverse unsignalized intersection scenarios, indicating its practicality\nand readiness for real-world deployment."
                },
                "authors": [
                    {
                        "name": "Taoyuan Yu"
                    },
                    {
                        "name": "Kui Wang"
                    },
                    {
                        "name": "Zongdian Li"
                    },
                    {
                        "name": "Tao Yu"
                    },
                    {
                        "name": "Kei Sakaguchi"
                    },
                    {
                        "name": "Walid Saad"
                    }
                ],
                "author_detail": {
                    "name": "Walid Saad"
                },
                "author": "Walid Saad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15098v1",
                "updated": "2025-09-18T15:55:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    55,
                    19,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:55:19Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    55,
                    19,
                    3,
                    261,
                    0
                ],
                "title": "TextMine: LLM-Powered Knowledge Extraction for Humanitarian Mine Action",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextMine: LLM-Powered Knowledge Extraction for Humanitarian Mine Action"
                },
                "summary": "Humanitarian Mine Action has generated extensive best-practice knowledge, but\nmuch remains locked in unstructured reports. We introduce TextMine, an\nontology-guided pipeline that uses Large Language Models to extract knowledge\ntriples from HMA texts. TextMine integrates document chunking, domain-aware\nprompting, triple extraction, and both reference-based and LLM-as-a-Judge\nevaluation. We also create the first HMA ontology and a curated dataset of\nreal-world demining reports. Experiments show ontology-aligned prompts boost\nextraction accuracy by 44.2%, cut hallucinations by 22.5%, and improve format\nconformance by 20.9% over baselines. While validated on Cambodian reports,\nTextMine can adapt to global demining efforts or other domains, transforming\nunstructured data into structured knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humanitarian Mine Action has generated extensive best-practice knowledge, but\nmuch remains locked in unstructured reports. We introduce TextMine, an\nontology-guided pipeline that uses Large Language Models to extract knowledge\ntriples from HMA texts. TextMine integrates document chunking, domain-aware\nprompting, triple extraction, and both reference-based and LLM-as-a-Judge\nevaluation. We also create the first HMA ontology and a curated dataset of\nreal-world demining reports. Experiments show ontology-aligned prompts boost\nextraction accuracy by 44.2%, cut hallucinations by 22.5%, and improve format\nconformance by 20.9% over baselines. While validated on Cambodian reports,\nTextMine can adapt to global demining efforts or other domains, transforming\nunstructured data into structured knowledge."
                },
                "authors": [
                    {
                        "name": "Chenyue Zhou"
                    },
                    {
                        "name": "Gürkan Solmaz"
                    },
                    {
                        "name": "Flavio Cirillo"
                    },
                    {
                        "name": "Kiril Gashteovski"
                    },
                    {
                        "name": "Jonathan Fürst"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Fürst"
                },
                "author": "Jonathan Fürst",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15097v1",
                "updated": "2025-09-18T15:54:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    54,
                    15,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:54:15Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    54,
                    15,
                    3,
                    261,
                    0
                ],
                "title": "The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based\n  Incremental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based\n  Incremental Learning"
                },
                "summary": "The rising computational and energy demands of deep learning, particularly in\nlarge-scale architectures such as foundation models and large language models\n(LLMs), pose significant challenges to sustainability. Traditional\ngradient-based training methods are inefficient, requiring numerous iterative\nupdates and high power consumption. To address these limitations, we propose a\nhybrid framework that combines hierarchical decomposition with FPGA-based\ndirect equation solving and incremental learning. Our method divides the neural\nnetwork into two functional tiers: lower layers are optimized via single-step\nequation solving on FPGAs for efficient and parallelizable feature extraction,\nwhile higher layers employ adaptive incremental learning to support continual\nupdates without full retraining. Building upon this foundation, we introduce\nthe Compound LLM framework, which explicitly deploys LLM modules across both\nhierarchy levels. The lower-level LLM handles reusable representation learning\nwith minimal energy overhead, while the upper-level LLM performs adaptive\ndecision-making through energy-aware updates. This integrated design enhances\nscalability, reduces redundant computation, and aligns with the principles of\nsustainable AI. Theoretical analysis and architectural insights demonstrate\nthat our method reduces computational costs significantly while preserving high\nmodel performance, making it well-suited for edge deployment and real-time\nadaptation in energy-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rising computational and energy demands of deep learning, particularly in\nlarge-scale architectures such as foundation models and large language models\n(LLMs), pose significant challenges to sustainability. Traditional\ngradient-based training methods are inefficient, requiring numerous iterative\nupdates and high power consumption. To address these limitations, we propose a\nhybrid framework that combines hierarchical decomposition with FPGA-based\ndirect equation solving and incremental learning. Our method divides the neural\nnetwork into two functional tiers: lower layers are optimized via single-step\nequation solving on FPGAs for efficient and parallelizable feature extraction,\nwhile higher layers employ adaptive incremental learning to support continual\nupdates without full retraining. Building upon this foundation, we introduce\nthe Compound LLM framework, which explicitly deploys LLM modules across both\nhierarchy levels. The lower-level LLM handles reusable representation learning\nwith minimal energy overhead, while the upper-level LLM performs adaptive\ndecision-making through energy-aware updates. This integrated design enhances\nscalability, reduces redundant computation, and aligns with the principles of\nsustainable AI. Theoretical analysis and architectural insights demonstrate\nthat our method reduces computational costs significantly while preserving high\nmodel performance, making it well-suited for edge deployment and real-time\nadaptation in energy-constrained environments."
                },
                "authors": [
                    {
                        "name": "Mohammad Saleh Vahdatpour"
                    },
                    {
                        "name": "Huaiyuan Chu"
                    },
                    {
                        "name": "Yanqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yanqing Zhang"
                },
                "author": "Yanqing Zhang",
                "arxiv_comment": "Published at IJCNN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15095v1",
                "updated": "2025-09-18T15:50:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    50,
                    54,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:50:54Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    50,
                    54,
                    3,
                    261,
                    0
                ],
                "title": "Listening, Imagining \\& Refining: A Heuristic Optimized ASR Correction\n  Framework with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Listening, Imagining \\& Refining: A Heuristic Optimized ASR Correction\n  Framework with LLMs"
                },
                "summary": "Automatic Speech Recognition (ASR) systems remain prone to errors that affect\ndownstream applications. In this paper, we propose LIR-ASR, a heuristic\noptimized iterative correction framework using LLMs, inspired by human auditory\nperception. LIR-ASR applies a \"Listening-Imagining-Refining\" strategy,\ngenerating phonetic variants and refining them in context. A heuristic\noptimization with finite state machine (FSM) is introduced to prevent the\ncorrection process from being trapped in local optima and rule-based\nconstraints help maintain semantic fidelity. Experiments on both English and\nChinese ASR outputs show that LIR-ASR achieves average reductions in CER/WER of\nup to 1.5 percentage points compared to baselines, demonstrating substantial\naccuracy gains in transcription.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Speech Recognition (ASR) systems remain prone to errors that affect\ndownstream applications. In this paper, we propose LIR-ASR, a heuristic\noptimized iterative correction framework using LLMs, inspired by human auditory\nperception. LIR-ASR applies a \"Listening-Imagining-Refining\" strategy,\ngenerating phonetic variants and refining them in context. A heuristic\noptimization with finite state machine (FSM) is introduced to prevent the\ncorrection process from being trapped in local optima and rule-based\nconstraints help maintain semantic fidelity. Experiments on both English and\nChinese ASR outputs show that LIR-ASR achieves average reductions in CER/WER of\nup to 1.5 percentage points compared to baselines, demonstrating substantial\naccuracy gains in transcription."
                },
                "authors": [
                    {
                        "name": "Yutong Liu"
                    },
                    {
                        "name": "Ziyue Zhang"
                    },
                    {
                        "name": "Yongbin Yu"
                    },
                    {
                        "name": "Xiangxiang Wang"
                    },
                    {
                        "name": "Yuqing Cai"
                    },
                    {
                        "name": "Nyima Tashi"
                    }
                ],
                "author_detail": {
                    "name": "Nyima Tashi"
                },
                "author": "Nyima Tashi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15089v1",
                "updated": "2025-09-18T15:46:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    46,
                    40,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:46:40Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    46,
                    40,
                    3,
                    261,
                    0
                ],
                "title": "LLM-OREF: An Open Relation Extraction Framework Based on Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-OREF: An Open Relation Extraction Framework Based on Large Language\n  Models"
                },
                "summary": "The goal of open relation extraction (OpenRE) is to develop an RE model that\ncan generalize to new relations not encountered during training. Existing\nstudies primarily formulate OpenRE as a clustering task. They first cluster all\ntest instances based on the similarity between the instances, and then manually\nassign a new relation to each cluster. However, their reliance on human\nannotation limits their practicality. In this paper, we propose an OpenRE\nframework based on large language models (LLMs), which directly predicts new\nrelations for test instances by leveraging their strong language understanding\nand generation abilities, without human intervention. Specifically, our\nframework consists of two core components: (1) a relation discoverer (RD),\ndesigned to predict new relations for test instances based on\n\\textit{demonstrations} formed by training instances with known relations; and\n(2) a relation predictor (RP), used to select the most likely relation for a\ntest instance from $n$ candidate relations, guided by \\textit{demonstrations}\ncomposed of their instances. To enhance the ability of our framework to predict\nnew relations, we design a self-correcting inference strategy composed of three\nstages: relation discovery, relation denoising, and relation prediction. In the\nfirst stage, we use RD to preliminarily predict new relations for all test\ninstances. Next, we apply RP to select some high-reliability test instances for\neach new relation from the prediction results of RD through a cross-validation\nmethod. During the third stage, we employ RP to re-predict the relations of all\ntest instances based on the demonstrations constructed from these reliable test\ninstances. Extensive experiments on three OpenRE datasets demonstrate the\neffectiveness of our framework. We release our code at\nhttps://github.com/XMUDeepLIT/LLM-OREF.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The goal of open relation extraction (OpenRE) is to develop an RE model that\ncan generalize to new relations not encountered during training. Existing\nstudies primarily formulate OpenRE as a clustering task. They first cluster all\ntest instances based on the similarity between the instances, and then manually\nassign a new relation to each cluster. However, their reliance on human\nannotation limits their practicality. In this paper, we propose an OpenRE\nframework based on large language models (LLMs), which directly predicts new\nrelations for test instances by leveraging their strong language understanding\nand generation abilities, without human intervention. Specifically, our\nframework consists of two core components: (1) a relation discoverer (RD),\ndesigned to predict new relations for test instances based on\n\\textit{demonstrations} formed by training instances with known relations; and\n(2) a relation predictor (RP), used to select the most likely relation for a\ntest instance from $n$ candidate relations, guided by \\textit{demonstrations}\ncomposed of their instances. To enhance the ability of our framework to predict\nnew relations, we design a self-correcting inference strategy composed of three\nstages: relation discovery, relation denoising, and relation prediction. In the\nfirst stage, we use RD to preliminarily predict new relations for all test\ninstances. Next, we apply RP to select some high-reliability test instances for\neach new relation from the prediction results of RD through a cross-validation\nmethod. During the third stage, we employ RP to re-predict the relations of all\ntest instances based on the demonstrations constructed from these reliable test\ninstances. Extensive experiments on three OpenRE datasets demonstrate the\neffectiveness of our framework. We release our code at\nhttps://github.com/XMUDeepLIT/LLM-OREF.git."
                },
                "authors": [
                    {
                        "name": "Hongyao Tu"
                    },
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Yujie Lin"
                    },
                    {
                        "name": "Xin Lin"
                    },
                    {
                        "name": "Haibo Zhang"
                    },
                    {
                        "name": "Long Zhang"
                    },
                    {
                        "name": "Jinsong Su"
                    }
                ],
                "author_detail": {
                    "name": "Jinsong Su"
                },
                "author": "Jinsong Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13835v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13835v2",
                "updated": "2025-09-18T15:44:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    44,
                    33,
                    3,
                    261,
                    0
                ],
                "published": "2024-12-18T13:25:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    25,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "RAcQUEt: Unveiling the Dangers of Overlooked Referential Ambiguity in\n  Visual LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAcQUEt: Unveiling the Dangers of Overlooked Referential Ambiguity in\n  Visual LLMs"
                },
                "summary": "Ambiguity resolution is key to effective communication. While humans\neffortlessly address ambiguity through conversational grounding strategies, the\nextent to which current language models can emulate these strategies remains\nunclear. In this work, we examine referential ambiguity in image-based question\nanswering by introducing RACQUET, a carefully curated dataset targeting\ndistinct aspects of ambiguity. Through a series of evaluations, we reveal\nsignificant limitations and problems of overconfidence of state-of-the-art\nlarge multimodal language models in addressing ambiguity in their responses.\nThe overconfidence issue becomes particularly relevant for RACQUET-BIAS, a\nsubset designed to analyze a critical yet underexplored problem: failing to\naddress ambiguity leads to stereotypical, socially biased responses. Our\nresults underscore the urgency of equipping models with robust strategies to\ndeal with uncertainty without resorting to undesirable stereotypes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ambiguity resolution is key to effective communication. While humans\neffortlessly address ambiguity through conversational grounding strategies, the\nextent to which current language models can emulate these strategies remains\nunclear. In this work, we examine referential ambiguity in image-based question\nanswering by introducing RACQUET, a carefully curated dataset targeting\ndistinct aspects of ambiguity. Through a series of evaluations, we reveal\nsignificant limitations and problems of overconfidence of state-of-the-art\nlarge multimodal language models in addressing ambiguity in their responses.\nThe overconfidence issue becomes particularly relevant for RACQUET-BIAS, a\nsubset designed to analyze a critical yet underexplored problem: failing to\naddress ambiguity leads to stereotypical, socially biased responses. Our\nresults underscore the urgency of equipping models with robust strategies to\ndeal with uncertainty without resorting to undesirable stereotypes."
                },
                "authors": [
                    {
                        "name": "Alberto Testoni"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "Raquel Fernández"
                    }
                ],
                "author_detail": {
                    "name": "Raquel Fernández"
                },
                "author": "Raquel Fernández",
                "arxiv_comment": "Accepted to EMNLP 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13835v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13835v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15087v1",
                "updated": "2025-09-18T15:43:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    43,
                    33,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:43:33Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    43,
                    33,
                    3,
                    261,
                    0
                ],
                "title": "Adaptive LoRA Experts Allocation and Selection for Federated Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive LoRA Experts Allocation and Selection for Federated Fine-Tuning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\nvarious tasks, but fine-tuning them for domain-specific applications often\nrequires substantial domain-specific data that may be distributed across\nmultiple organizations. Federated Learning (FL) offers a privacy-preserving\nsolution, but faces challenges with computational constraints when applied to\nLLMs. Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient\nfine-tuning approach, though a single LoRA module often struggles with\nheterogeneous data across diverse domains. This paper addresses two critical\nchallenges in federated LoRA fine-tuning: 1. determining the optimal number and\nallocation of LoRA experts across heterogeneous clients, and 2. enabling\nclients to selectively utilize these experts based on their specific data\ncharacteristics. We propose FedLEASE (Federated adaptive LoRA Expert Allocation\nand SElection), a novel framework that adaptively clusters clients based on\nrepresentation similarity to allocate and train domain-specific LoRA experts.\nIt also introduces an adaptive top-$M$ Mixture-of-Experts mechanism that allows\neach client to select the optimal number of utilized experts. Our extensive\nexperiments on diverse benchmark datasets demonstrate that FedLEASE\nsignificantly outperforms existing federated fine-tuning approaches in\nheterogeneous client settings while maintaining communication efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities across\nvarious tasks, but fine-tuning them for domain-specific applications often\nrequires substantial domain-specific data that may be distributed across\nmultiple organizations. Federated Learning (FL) offers a privacy-preserving\nsolution, but faces challenges with computational constraints when applied to\nLLMs. Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient\nfine-tuning approach, though a single LoRA module often struggles with\nheterogeneous data across diverse domains. This paper addresses two critical\nchallenges in federated LoRA fine-tuning: 1. determining the optimal number and\nallocation of LoRA experts across heterogeneous clients, and 2. enabling\nclients to selectively utilize these experts based on their specific data\ncharacteristics. We propose FedLEASE (Federated adaptive LoRA Expert Allocation\nand SElection), a novel framework that adaptively clusters clients based on\nrepresentation similarity to allocate and train domain-specific LoRA experts.\nIt also introduces an adaptive top-$M$ Mixture-of-Experts mechanism that allows\neach client to select the optimal number of utilized experts. Our extensive\nexperiments on diverse benchmark datasets demonstrate that FedLEASE\nsignificantly outperforms existing federated fine-tuning approaches in\nheterogeneous client settings while maintaining communication efficiency."
                },
                "authors": [
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Jieming Bian"
                    },
                    {
                        "name": "Letian Zhang"
                    },
                    {
                        "name": "Jie Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Xu"
                },
                "author": "Jie Xu",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15082v1",
                "updated": "2025-09-18T15:41:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    41,
                    58,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:41:58Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    41,
                    58,
                    3,
                    261,
                    0
                ],
                "title": "From Who Said What to Who They Are: Modular Training-free Identity-Aware\n  LLM Refinement of Speaker Diarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Who Said What to Who They Are: Modular Training-free Identity-Aware\n  LLM Refinement of Speaker Diarization"
                },
                "summary": "Speaker diarization (SD) struggles in real-world scenarios due to dynamic\nenvironments and unknown speaker counts. SD is rarely used alone and is often\npaired with automatic speech recognition (ASR), but non-modular methods that\njointly train on domain-specific data have limited flexibility. Moreover, many\napplications require true speaker identities rather than SD's pseudo labels. We\npropose a training-free modular pipeline combining off-the-shelf SD, ASR, and a\nlarge language model (LLM) to determine who spoke, what was said, and who they\nare. Using structured LLM prompting on reconciled SD and ASR outputs, our\nmethod leverages semantic continuity in conversational context to refine\nlow-confidence speaker labels and assigns role identities while correcting\nsplit speakers. On a real-world patient-clinician dataset, our approach\nachieves a 29.7% relative error reduction over baseline reconciled SD and ASR.\nIt enhances diarization performance without additional training and delivers a\ncomplete pipeline for SD, ASR, and speaker identity detection in practical\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speaker diarization (SD) struggles in real-world scenarios due to dynamic\nenvironments and unknown speaker counts. SD is rarely used alone and is often\npaired with automatic speech recognition (ASR), but non-modular methods that\njointly train on domain-specific data have limited flexibility. Moreover, many\napplications require true speaker identities rather than SD's pseudo labels. We\npropose a training-free modular pipeline combining off-the-shelf SD, ASR, and a\nlarge language model (LLM) to determine who spoke, what was said, and who they\nare. Using structured LLM prompting on reconciled SD and ASR outputs, our\nmethod leverages semantic continuity in conversational context to refine\nlow-confidence speaker labels and assigns role identities while correcting\nsplit speakers. On a real-world patient-clinician dataset, our approach\nachieves a 29.7% relative error reduction over baseline reconciled SD and ASR.\nIt enhances diarization performance without additional training and delivers a\ncomplete pipeline for SD, ASR, and speaker identity detection in practical\napplications."
                },
                "authors": [
                    {
                        "name": "Yu-Wen Chen"
                    },
                    {
                        "name": "William Ho"
                    },
                    {
                        "name": "Maxim Topaz"
                    },
                    {
                        "name": "Julia Hirschberg"
                    },
                    {
                        "name": "Zoran Kostic"
                    }
                ],
                "author_detail": {
                    "name": "Zoran Kostic"
                },
                "author": "Zoran Kostic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15076v1",
                "updated": "2025-09-18T15:36:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    36,
                    38,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:36:38Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    36,
                    38,
                    3,
                    261,
                    0
                ],
                "title": "Forecasting and Visualizing Air Quality from Sky Images with\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting and Visualizing Air Quality from Sky Images with\n  Vision-Language Models"
                },
                "summary": "Air pollution remains a critical threat to public health and environmental\nsustainability, yet conventional monitoring systems are often constrained by\nlimited spatial coverage and accessibility. This paper proposes an AI-driven\nagent that predicts ambient air pollution levels from sky images and\nsynthesizes realistic visualizations of pollution scenarios using generative\nmodeling. Our approach combines statistical texture analysis with supervised\nlearning for pollution classification, and leverages vision-language model\n(VLM)-guided image generation to produce interpretable representations of air\nquality conditions. The generated visuals simulate varying degrees of\npollution, offering a foundation for user-facing interfaces that improve\ntransparency and support informed environmental decision-making. These outputs\ncan be seamlessly integrated into intelligent applications aimed at enhancing\nsituational awareness and encouraging behavioral responses based on real-time\nforecasts. We validate our method using a dataset of urban sky images and\ndemonstrate its effectiveness in both pollution level estimation and\nsemantically consistent visual synthesis. The system design further\nincorporates human-centered user experience principles to ensure accessibility,\nclarity, and public engagement in air quality forecasting. To support scalable\nand energy-efficient deployment, future iterations will incorporate a green CNN\narchitecture enhanced with FPGA-based incremental learning, enabling real-time\ninference on edge platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Air pollution remains a critical threat to public health and environmental\nsustainability, yet conventional monitoring systems are often constrained by\nlimited spatial coverage and accessibility. This paper proposes an AI-driven\nagent that predicts ambient air pollution levels from sky images and\nsynthesizes realistic visualizations of pollution scenarios using generative\nmodeling. Our approach combines statistical texture analysis with supervised\nlearning for pollution classification, and leverages vision-language model\n(VLM)-guided image generation to produce interpretable representations of air\nquality conditions. The generated visuals simulate varying degrees of\npollution, offering a foundation for user-facing interfaces that improve\ntransparency and support informed environmental decision-making. These outputs\ncan be seamlessly integrated into intelligent applications aimed at enhancing\nsituational awareness and encouraging behavioral responses based on real-time\nforecasts. We validate our method using a dataset of urban sky images and\ndemonstrate its effectiveness in both pollution level estimation and\nsemantically consistent visual synthesis. The system design further\nincorporates human-centered user experience principles to ensure accessibility,\nclarity, and public engagement in air quality forecasting. To support scalable\nand energy-efficient deployment, future iterations will incorporate a green CNN\narchitecture enhanced with FPGA-based incremental learning, enabling real-time\ninference on edge platforms."
                },
                "authors": [
                    {
                        "name": "Mohammad Saleh Vahdatpour"
                    },
                    {
                        "name": "Maryam Eyvazi"
                    },
                    {
                        "name": "Yanqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yanqing Zhang"
                },
                "author": "Yanqing Zhang",
                "arxiv_comment": "Published at ICCVW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13557v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13557v2",
                "updated": "2025-09-18T15:34:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    34,
                    44,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-16T21:52:04Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    21,
                    52,
                    4,
                    1,
                    259,
                    0
                ],
                "title": "MACO: A Multi-Agent LLM-Based Hardware/Software Co-Design Framework for\n  CGRAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MACO: A Multi-Agent LLM-Based Hardware/Software Co-Design Framework for\n  CGRAs"
                },
                "summary": "Coarse-grained Reconfigurable Arrays (CGRAs) are a promising computing\narchitecture that can deliver high-performance, energy-efficient acceleration\nacross diverse domains. By supporting reconfiguration at the functional unit\nlevel, CGRAs efficiently adapt to varying computational patterns and optimize\nresource utilization. However, designing CGRAs is highly challenging due to the\nvast design space, independent architectural parameters, and the time-consuming\nnature of manual design. Fortunately, the rapid advancement of large language\nmodels (LLMs) presents new opportunities to automate this process.\n  In this work, we propose MACO -- an open-source multi-agent LLM-based\nframework for Hardware/Software (HW/SW) co-design of CGRAs. The framework\nemploys LLM reasoning to generate CGRAs across four stages: HW/SW co-design,\nDesign error correction, Best design selection, and Evaluation & Feedback.\nFurthermore, MACO iteratively optimizes the generated CGRAs, leveraging agent\nreasoning and feedback to achieve higher PPA (that is, power, performance, and\narea) design points for a given domain. In addition, we introduce an LLM\nself-learning mechanism that employs LLM-driven decision making to select the\noptimal CGRA to accelerate the design process.\n  We evaluate the framework with state-of-the-art LLM-based methods and manual\nCGRA design, in terms of performance, power consumption, and area. Experimental\nresults show that MACO efficiently generates high-quality CGRA architectures,\nsignificantly reducing manual design effort and demonstrating the potential of\nour framework for real-world CGRA design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-grained Reconfigurable Arrays (CGRAs) are a promising computing\narchitecture that can deliver high-performance, energy-efficient acceleration\nacross diverse domains. By supporting reconfiguration at the functional unit\nlevel, CGRAs efficiently adapt to varying computational patterns and optimize\nresource utilization. However, designing CGRAs is highly challenging due to the\nvast design space, independent architectural parameters, and the time-consuming\nnature of manual design. Fortunately, the rapid advancement of large language\nmodels (LLMs) presents new opportunities to automate this process.\n  In this work, we propose MACO -- an open-source multi-agent LLM-based\nframework for Hardware/Software (HW/SW) co-design of CGRAs. The framework\nemploys LLM reasoning to generate CGRAs across four stages: HW/SW co-design,\nDesign error correction, Best design selection, and Evaluation & Feedback.\nFurthermore, MACO iteratively optimizes the generated CGRAs, leveraging agent\nreasoning and feedback to achieve higher PPA (that is, power, performance, and\narea) design points for a given domain. In addition, we introduce an LLM\nself-learning mechanism that employs LLM-driven decision making to select the\noptimal CGRA to accelerate the design process.\n  We evaluate the framework with state-of-the-art LLM-based methods and manual\nCGRA design, in terms of performance, power consumption, and area. Experimental\nresults show that MACO efficiently generates high-quality CGRA architectures,\nsignificantly reducing manual design effort and demonstrating the potential of\nour framework for real-world CGRA design."
                },
                "authors": [
                    {
                        "name": "Zesong Jiang"
                    },
                    {
                        "name": "Yuqi Sun"
                    },
                    {
                        "name": "Qing Zhong"
                    },
                    {
                        "name": "Mahathi Krishna"
                    },
                    {
                        "name": "Deepak Patil"
                    },
                    {
                        "name": "Cheng Tan"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    },
                    {
                        "name": "Jeff Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Zhang"
                },
                "author": "Jeff Zhang",
                "arxiv_comment": "Due to certain confidentiality requirements, this article needs to be\n  withdrawn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13557v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13557v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15068v1",
                "updated": "2025-09-18T15:30:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    30,
                    38,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:30:38Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    30,
                    38,
                    3,
                    261,
                    0
                ],
                "title": "Learning in Context: Personalizing Educational Content with Large\n  Language Models to Enhance Student Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning in Context: Personalizing Educational Content with Large\n  Language Models to Enhance Student Learning"
                },
                "summary": "Standardized, one-size-fits-all educational content often fails to connect\nwith students' individual backgrounds and interests, leading to disengagement\nand a perceived lack of relevance. To address this challenge, we introduce\nPAGE, a novel framework that leverages large language models (LLMs) to\nautomatically personalize educational materials by adapting them to each\nstudent's unique context, such as their major and personal interests. To\nvalidate our approach, we deployed PAGE in a semester-long intelligent tutoring\nsystem and conducted a user study to evaluate its impact in an authentic\neducational setting. Our findings show that students who received personalized\ncontent demonstrated significantly improved learning outcomes and reported\nhigher levels of engagement, perceived relevance, and trust compared to those\nwho used standardized materials. This work demonstrates the practical value of\nLLM-powered personalization and offers key design implications for creating\nmore effective, engaging, and trustworthy educational experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standardized, one-size-fits-all educational content often fails to connect\nwith students' individual backgrounds and interests, leading to disengagement\nand a perceived lack of relevance. To address this challenge, we introduce\nPAGE, a novel framework that leverages large language models (LLMs) to\nautomatically personalize educational materials by adapting them to each\nstudent's unique context, such as their major and personal interests. To\nvalidate our approach, we deployed PAGE in a semester-long intelligent tutoring\nsystem and conducted a user study to evaluate its impact in an authentic\neducational setting. Our findings show that students who received personalized\ncontent demonstrated significantly improved learning outcomes and reported\nhigher levels of engagement, perceived relevance, and trust compared to those\nwho used standardized materials. This work demonstrates the practical value of\nLLM-powered personalization and offers key design implications for creating\nmore effective, engaging, and trustworthy educational experiences."
                },
                "authors": [
                    {
                        "name": "Joy Jia Yin Lim"
                    },
                    {
                        "name": "Daniel Zhang-Li"
                    },
                    {
                        "name": "Jifan Yu"
                    },
                    {
                        "name": "Xin Cong"
                    },
                    {
                        "name": "Ye He"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Huiqin Liu"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    },
                    {
                        "name": "Bin Xu"
                    }
                ],
                "author_detail": {
                    "name": "Bin Xu"
                },
                "author": "Bin Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08827v2",
                "updated": "2025-09-18T15:28:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    28,
                    2,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-10T17:59:43Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    59,
                    43,
                    2,
                    253,
                    0
                ],
                "title": "A Survey of Reinforcement Learning for Large Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Reinforcement Learning for Large Reasoning Models"
                },
                "summary": "In this paper, we survey recent advances in Reinforcement Learning (RL) for\nreasoning with Large Language Models (LLMs). RL has achieved remarkable success\nin advancing the frontier of LLM capabilities, particularly in addressing\ncomplex logical tasks such as mathematics and coding. As a result, RL has\nemerged as a foundational methodology for transforming LLMs into LRMs. With the\nrapid progress of the field, further scaling of RL for LRMs now faces\nfoundational challenges not only in computational resources but also in\nalgorithm design, training data, and infrastructure. To this end, it is timely\nto revisit the development of this domain, reassess its trajectory, and explore\nstrategies to enhance the scalability of RL toward Artificial SuperIntelligence\n(ASI). In particular, we examine research applying RL to LLMs and LRMs for\nreasoning abilities, especially since the release of DeepSeek-R1, including\nfoundational components, core problems, training resources, and downstream\napplications, to identify future opportunities and directions for this rapidly\nevolving area. We hope this review will promote future research on RL for\nbroader reasoning models. Github:\nhttps://github.com/TsinghuaC3I/Awesome-RL-for-LRMs",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we survey recent advances in Reinforcement Learning (RL) for\nreasoning with Large Language Models (LLMs). RL has achieved remarkable success\nin advancing the frontier of LLM capabilities, particularly in addressing\ncomplex logical tasks such as mathematics and coding. As a result, RL has\nemerged as a foundational methodology for transforming LLMs into LRMs. With the\nrapid progress of the field, further scaling of RL for LRMs now faces\nfoundational challenges not only in computational resources but also in\nalgorithm design, training data, and infrastructure. To this end, it is timely\nto revisit the development of this domain, reassess its trajectory, and explore\nstrategies to enhance the scalability of RL toward Artificial SuperIntelligence\n(ASI). In particular, we examine research applying RL to LLMs and LRMs for\nreasoning abilities, especially since the release of DeepSeek-R1, including\nfoundational components, core problems, training resources, and downstream\napplications, to identify future opportunities and directions for this rapidly\nevolving area. We hope this review will promote future research on RL for\nbroader reasoning models. Github:\nhttps://github.com/TsinghuaC3I/Awesome-RL-for-LRMs"
                },
                "authors": [
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Yuxin Zuo"
                    },
                    {
                        "name": "Bingxiang He"
                    },
                    {
                        "name": "Youbang Sun"
                    },
                    {
                        "name": "Runze Liu"
                    },
                    {
                        "name": "Che Jiang"
                    },
                    {
                        "name": "Yuchen Fan"
                    },
                    {
                        "name": "Kai Tian"
                    },
                    {
                        "name": "Guoli Jia"
                    },
                    {
                        "name": "Pengfei Li"
                    },
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Xingtai Lv"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Sihang Zeng"
                    },
                    {
                        "name": "Shang Qu"
                    },
                    {
                        "name": "Haozhan Li"
                    },
                    {
                        "name": "Shijie Wang"
                    },
                    {
                        "name": "Yuru Wang"
                    },
                    {
                        "name": "Xinwei Long"
                    },
                    {
                        "name": "Fangfu Liu"
                    },
                    {
                        "name": "Xiang Xu"
                    },
                    {
                        "name": "Jiaze Ma"
                    },
                    {
                        "name": "Xuekai Zhu"
                    },
                    {
                        "name": "Ermo Hua"
                    },
                    {
                        "name": "Yihao Liu"
                    },
                    {
                        "name": "Zonglin Li"
                    },
                    {
                        "name": "Huayu Chen"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Yafu Li"
                    },
                    {
                        "name": "Weize Chen"
                    },
                    {
                        "name": "Zhenzhao Yuan"
                    },
                    {
                        "name": "Junqi Gao"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Zhiyuan Ma"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "arxiv_comment": "Fixed typos; added missing and recent citations (114 -> 117 pages)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15059v1",
                "updated": "2025-09-18T15:22:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    22,
                    33,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:22:33Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    22,
                    33,
                    3,
                    261,
                    0
                ],
                "title": "QuizRank: Picking Images by Quizzing VLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuizRank: Picking Images by Quizzing VLMs"
                },
                "summary": "Images play a vital role in improving the readability and comprehension of\nWikipedia articles by serving as `illustrative aids.' However, not all images\nare equally effective and not all Wikipedia editors are trained in their\nselection. We propose QuizRank, a novel method of image selection that\nleverages large language models (LLMs) and vision language models (VLMs) to\nrank images as learning interventions. Our approach transforms textual\ndescriptions of the article's subject into multiple-choice questions about\nimportant visual characteristics of the concept. We utilize these questions to\nquiz the VLM: the better an image can help answer questions, the higher it is\nranked. To further improve discrimination between visually similar items, we\nintroduce a Contrastive QuizRank that leverages differences in the features of\ntarget (e.g., a Western Bluebird) and distractor concepts (e.g., Mountain\nBluebird) to generate questions. We demonstrate the potential of VLMs as\neffective visual evaluators by showing a high congruence with human quiz-takers\nand an effective discriminative ranking of images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Images play a vital role in improving the readability and comprehension of\nWikipedia articles by serving as `illustrative aids.' However, not all images\nare equally effective and not all Wikipedia editors are trained in their\nselection. We propose QuizRank, a novel method of image selection that\nleverages large language models (LLMs) and vision language models (VLMs) to\nrank images as learning interventions. Our approach transforms textual\ndescriptions of the article's subject into multiple-choice questions about\nimportant visual characteristics of the concept. We utilize these questions to\nquiz the VLM: the better an image can help answer questions, the higher it is\nranked. To further improve discrimination between visually similar items, we\nintroduce a Contrastive QuizRank that leverages differences in the features of\ntarget (e.g., a Western Bluebird) and distractor concepts (e.g., Mountain\nBluebird) to generate questions. We demonstrate the potential of VLMs as\neffective visual evaluators by showing a high congruence with human quiz-takers\nand an effective discriminative ranking of images."
                },
                "authors": [
                    {
                        "name": "Tenghao Ji"
                    },
                    {
                        "name": "Eytan Adar"
                    }
                ],
                "author_detail": {
                    "name": "Eytan Adar"
                },
                "author": "Eytan Adar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18106v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18106v3",
                "updated": "2025-09-18T15:18:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    18,
                    10,
                    3,
                    261,
                    0
                ],
                "published": "2025-08-25T15:11:11Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    11,
                    11,
                    0,
                    237,
                    0
                ],
                "title": "A.S.E: A Repository-Level Benchmark for Evaluating Security in\n  AI-Generated Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A.S.E: A Repository-Level Benchmark for Evaluating Security in\n  AI-Generated Code"
                },
                "summary": "The increasing adoption of large language models (LLMs) in software\nengineering necessitates rigorous security evaluation of their generated code.\nHowever, existing benchmarks often lack relevance to real-world AI-assisted\nprogramming scenarios, making them inadequate for assessing the practical\nsecurity risks associated with AI-generated code in production environments. To\naddress this gap, we introduce A.S.E (AI Code Generation Security Evaluation),\na repository-level evaluation benchmark designed to closely mirror real-world\nAI programming tasks, offering a comprehensive and reliable framework for\nassessing the security of AI-generated code. Our evaluation of leading LLMs on\nA.S.E reveals several key findings. In particular, current LLMs still struggle\nwith secure coding. The complexity in repository-level scenarios presents\nchallenges for LLMs that typically perform well on snippet-level tasks.\nMoreover, a larger reasoning budget does not necessarily lead to better code\ngeneration. These observations offer valuable insights into the current state\nof AI code generation and help developers identify the most suitable models for\npractical tasks. They also lay the groundwork for refining LLMs to generate\nsecure and efficient code in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of large language models (LLMs) in software\nengineering necessitates rigorous security evaluation of their generated code.\nHowever, existing benchmarks often lack relevance to real-world AI-assisted\nprogramming scenarios, making them inadequate for assessing the practical\nsecurity risks associated with AI-generated code in production environments. To\naddress this gap, we introduce A.S.E (AI Code Generation Security Evaluation),\na repository-level evaluation benchmark designed to closely mirror real-world\nAI programming tasks, offering a comprehensive and reliable framework for\nassessing the security of AI-generated code. Our evaluation of leading LLMs on\nA.S.E reveals several key findings. In particular, current LLMs still struggle\nwith secure coding. The complexity in repository-level scenarios presents\nchallenges for LLMs that typically perform well on snippet-level tasks.\nMoreover, a larger reasoning budget does not necessarily lead to better code\ngeneration. These observations offer valuable insights into the current state\nof AI code generation and help developers identify the most suitable models for\npractical tasks. They also lay the groundwork for refining LLMs to generate\nsecure and efficient code in real-world applications."
                },
                "authors": [
                    {
                        "name": "Keke Lian"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Libo Chen"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Ziming Zhao"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Miaoqian Lin"
                    },
                    {
                        "name": "Haotong Duan"
                    },
                    {
                        "name": "Haoran Zhao"
                    },
                    {
                        "name": "Shuang Liao"
                    },
                    {
                        "name": "Mingda Guo"
                    },
                    {
                        "name": "Jiazheng Quan"
                    },
                    {
                        "name": "Yilu Zhong"
                    },
                    {
                        "name": "Chenhao He"
                    },
                    {
                        "name": "Zichuan Chen"
                    },
                    {
                        "name": "Jie Wu"
                    },
                    {
                        "name": "Haoling Li"
                    },
                    {
                        "name": "Zhaoxuan Li"
                    },
                    {
                        "name": "Jiongchi Yu"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Dong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dong Zhang"
                },
                "author": "Dong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18106v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18106v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15038v1",
                "updated": "2025-09-18T15:04:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    4,
                    6,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:04:06Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    4,
                    6,
                    3,
                    261,
                    0
                ],
                "title": "Value-Guided KV Compression for LLMs via Approximated CUR Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-Guided KV Compression for LLMs via Approximated CUR Decomposition"
                },
                "summary": "Key-value (KV) cache compression has emerged as a critical technique for\nreducing the memory and latency overhead of autoregressive language models\nduring inference. Prior approaches predominantly rely on query-key attention\nscores to rank and evict cached tokens, assuming that attention intensity\ncorrelates with semantic importance. However, this heuristic overlooks the\ncontribution of value vectors, which directly influence the attention output.\nIn this paper, we propose CurDKV, a novel, value-centric KV compression method\nthat selects keys and values based on leverage scores computed from CUR matrix\ndecomposition. Our approach approximates the dominant subspace of the attention\noutput $softmax(QK^T)V$, ensuring that the retained tokens best preserve the\nmodel's predictive behavior. Theoretically, we show that attention score\napproximation does not guarantee output preservation, and demonstrate that\nCUR-based selection minimizes end-to-end attention reconstruction loss.\nEmpirically, CurDKV achieves up to 9.6% higher accuracy than state-of-the-art\nmethods like SnapKV and ChunkKV under aggressive compression budgets on LLaMA\nand Mistral, while maintaining compatibility with FlashAttention and Grouped\nQuery Attention. In addition to improved accuracy, CurDKV reduces generation\nlatency by up to 40% at high compression, offering a practical speed-accuracy\ntradeoff.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) cache compression has emerged as a critical technique for\nreducing the memory and latency overhead of autoregressive language models\nduring inference. Prior approaches predominantly rely on query-key attention\nscores to rank and evict cached tokens, assuming that attention intensity\ncorrelates with semantic importance. However, this heuristic overlooks the\ncontribution of value vectors, which directly influence the attention output.\nIn this paper, we propose CurDKV, a novel, value-centric KV compression method\nthat selects keys and values based on leverage scores computed from CUR matrix\ndecomposition. Our approach approximates the dominant subspace of the attention\noutput $softmax(QK^T)V$, ensuring that the retained tokens best preserve the\nmodel's predictive behavior. Theoretically, we show that attention score\napproximation does not guarantee output preservation, and demonstrate that\nCUR-based selection minimizes end-to-end attention reconstruction loss.\nEmpirically, CurDKV achieves up to 9.6% higher accuracy than state-of-the-art\nmethods like SnapKV and ChunkKV under aggressive compression budgets on LLaMA\nand Mistral, while maintaining compatibility with FlashAttention and Grouped\nQuery Attention. In addition to improved accuracy, CurDKV reduces generation\nlatency by up to 40% at high compression, offering a practical speed-accuracy\ntradeoff."
                },
                "authors": [
                    {
                        "name": "Ayan Sengupta"
                    },
                    {
                        "name": "Siddhant Chaudhary"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15031v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15031v1",
                "updated": "2025-09-18T14:56:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    56,
                    50,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T14:56:50Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    56,
                    50,
                    3,
                    261,
                    0
                ],
                "title": "AutoEdit: Automatic Hyperparameter Tuning for Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoEdit: Automatic Hyperparameter Tuning for Image Editing"
                },
                "summary": "Recent advances in diffusion models have revolutionized text-guided image\nediting, yet existing editing methods face critical challenges in\nhyperparameter identification. To get the reasonable editing performance, these\nmethods often require the user to brute-force tune multiple interdependent\nhyperparameters, such as inversion timesteps and attention modification,\n\\textit{etc.} This process incurs high computational costs due to the huge\nhyperparameter search space. We consider searching optimal editing's\nhyperparameters as a sequential decision-making task within the diffusion\ndenoising process. Specifically, we propose a reinforcement learning framework,\nwhich establishes a Markov Decision Process that dynamically adjusts\nhyperparameters across denoising steps, integrating editing objectives into a\nreward function. The method achieves time efficiency through proximal policy\noptimization while maintaining optimal hyperparameter configurations.\nExperiments demonstrate significant reduction in search time and computational\noverhead compared to existing brute-force approaches, advancing the practical\ndeployment of a diffusion-based image editing framework in the real world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion models have revolutionized text-guided image\nediting, yet existing editing methods face critical challenges in\nhyperparameter identification. To get the reasonable editing performance, these\nmethods often require the user to brute-force tune multiple interdependent\nhyperparameters, such as inversion timesteps and attention modification,\n\\textit{etc.} This process incurs high computational costs due to the huge\nhyperparameter search space. We consider searching optimal editing's\nhyperparameters as a sequential decision-making task within the diffusion\ndenoising process. Specifically, we propose a reinforcement learning framework,\nwhich establishes a Markov Decision Process that dynamically adjusts\nhyperparameters across denoising steps, integrating editing objectives into a\nreward function. The method achieves time efficiency through proximal policy\noptimization while maintaining optimal hyperparameter configurations.\nExperiments demonstrate significant reduction in search time and computational\noverhead compared to existing brute-force approaches, advancing the practical\ndeployment of a diffusion-based image editing framework in the real world."
                },
                "authors": [
                    {
                        "name": "Chau Pham"
                    },
                    {
                        "name": "Quan Dao"
                    },
                    {
                        "name": "Mahesh Bhosale"
                    },
                    {
                        "name": "Yunjie Tian"
                    },
                    {
                        "name": "Dimitris Metaxas"
                    },
                    {
                        "name": "David Doermann"
                    }
                ],
                "author_detail": {
                    "name": "David Doermann"
                },
                "author": "David Doermann",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15031v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15031v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15027v1",
                "updated": "2025-09-18T14:53:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    53,
                    41,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T14:53:41Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    53,
                    41,
                    3,
                    261,
                    0
                ],
                "title": "CLEAR: A Comprehensive Linguistic Evaluation of Argument Rewriting by\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLEAR: A Comprehensive Linguistic Evaluation of Argument Rewriting by\n  Large Language Models"
                },
                "summary": "While LLMs have been extensively studied on general text generation tasks,\nthere is less research on text rewriting, a task related to general text\ngeneration, and particularly on the behavior of models on this task. In this\npaper we analyze what changes LLMs make in a text rewriting setting. We focus\nspecifically on argumentative texts and their improvement, a task named\nArgument Improvement (ArgImp). We present CLEAR: an evaluation pipeline\nconsisting of 57 metrics mapped to four linguistic levels: lexical, syntactic,\nsemantic and pragmatic. This pipeline is used to examine the qualities of\nLLM-rewritten arguments on a broad set of argumentation corpora and compare the\nbehavior of different LLMs on this task and analyze the behavior of different\nLLMs on this task in terms of linguistic levels. By taking all four linguistic\nlevels into consideration, we find that the models perform ArgImp by shortening\nthe texts while simultaneously increasing average word length and merging\nsentences. Overall we note an increase in the persuasion and coherence\ndimensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While LLMs have been extensively studied on general text generation tasks,\nthere is less research on text rewriting, a task related to general text\ngeneration, and particularly on the behavior of models on this task. In this\npaper we analyze what changes LLMs make in a text rewriting setting. We focus\nspecifically on argumentative texts and their improvement, a task named\nArgument Improvement (ArgImp). We present CLEAR: an evaluation pipeline\nconsisting of 57 metrics mapped to four linguistic levels: lexical, syntactic,\nsemantic and pragmatic. This pipeline is used to examine the qualities of\nLLM-rewritten arguments on a broad set of argumentation corpora and compare the\nbehavior of different LLMs on this task and analyze the behavior of different\nLLMs on this task in terms of linguistic levels. By taking all four linguistic\nlevels into consideration, we find that the models perform ArgImp by shortening\nthe texts while simultaneously increasing average word length and merging\nsentences. Overall we note an increase in the persuasion and coherence\ndimensions."
                },
                "authors": [
                    {
                        "name": "Thomas Huber"
                    },
                    {
                        "name": "Christina Niklaus"
                    }
                ],
                "author_detail": {
                    "name": "Christina Niklaus"
                },
                "author": "Christina Niklaus",
                "arxiv_comment": "Accepted at EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15020v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15020v1",
                "updated": "2025-09-18T14:47:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    47,
                    58,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T14:47:58Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    47,
                    58,
                    3,
                    261,
                    0
                ],
                "title": "Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question\n  Answering with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question\n  Answering with LLMs"
                },
                "summary": "When evaluating large language models (LLMs) with multiple-choice question\nanswering (MCQA), it is common to end the prompt with the string \"Answer:\" to\nfacilitate automated answer extraction via next-token probabilities. However,\nthere is no consensus on how to tokenize the space following the colon, often\noverlooked as a trivial choice. In this paper, we uncover accuracy differences\nof up to 11% due to this (seemingly irrelevant) tokenization variation as well\nas reshuffled model rankings, raising concerns about the reliability of LLM\ncomparisons in prior work. Surprisingly, we are able to recommend one specific\nstrategy -- tokenizing the space together with the answer letter -- as we\nobserve consistent and statistically significant performance improvements.\nAdditionally, it improves model calibration, enhancing the reliability of the\nmodel's confidence estimates. Our findings underscore the importance of careful\nevaluation design and highlight the need for standardized, transparent\nevaluation protocols to ensure reliable and comparable results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When evaluating large language models (LLMs) with multiple-choice question\nanswering (MCQA), it is common to end the prompt with the string \"Answer:\" to\nfacilitate automated answer extraction via next-token probabilities. However,\nthere is no consensus on how to tokenize the space following the colon, often\noverlooked as a trivial choice. In this paper, we uncover accuracy differences\nof up to 11% due to this (seemingly irrelevant) tokenization variation as well\nas reshuffled model rankings, raising concerns about the reliability of LLM\ncomparisons in prior work. Surprisingly, we are able to recommend one specific\nstrategy -- tokenizing the space together with the answer letter -- as we\nobserve consistent and statistically significant performance improvements.\nAdditionally, it improves model calibration, enhancing the reliability of the\nmodel's confidence estimates. Our findings underscore the importance of careful\nevaluation design and highlight the need for standardized, transparent\nevaluation protocols to ensure reliable and comparable results."
                },
                "authors": [
                    {
                        "name": "Mario Sanz-Guerrero"
                    },
                    {
                        "name": "Minh Duc Bui"
                    },
                    {
                        "name": "Katharina von der Wense"
                    }
                ],
                "author_detail": {
                    "name": "Katharina von der Wense"
                },
                "author": "Katharina von der Wense",
                "arxiv_comment": "Accepted to EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15020v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14998v1",
                "updated": "2025-09-18T14:33:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    33,
                    36,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T14:33:36Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    33,
                    36,
                    3,
                    261,
                    0
                ],
                "title": "A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical\n  Decision-making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical\n  Decision-making"
                },
                "summary": "Medical decision-making often involves integrating knowledge from multiple\nclinical specialties, typically achieved through multidisciplinary teams.\nInspired by this collaborative process, recent work has leveraged large\nlanguage models (LLMs) in multi-agent collaboration frameworks to emulate\nexpert teamwork. While these approaches improve reasoning through agent\ninteraction, they are limited by static, pre-assigned roles, which hinder\nadaptability and dynamic knowledge integration. To address these limitations,\nwe propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration\nframework that enables LLM agents to dynamically form and expand expert teams\nbased on the evolving diagnostic context. KAMAC begins with one or more expert\nagents and then conducts a knowledge-driven discussion to identify and fill\nknowledge gaps by recruiting additional specialists as needed. This supports\nflexible, scalable collaboration in complex clinical scenarios, with decisions\nfinalized through reviewing updated agent comments. Experiments on two\nreal-world medical benchmarks demonstrate that KAMAC significantly outperforms\nboth single-agent and advanced multi-agent methods, particularly in complex\nclinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty\nexpertise. Our code is publicly available at:\nhttps://github.com/XiaoXiao-Woo/KAMAC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical decision-making often involves integrating knowledge from multiple\nclinical specialties, typically achieved through multidisciplinary teams.\nInspired by this collaborative process, recent work has leveraged large\nlanguage models (LLMs) in multi-agent collaboration frameworks to emulate\nexpert teamwork. While these approaches improve reasoning through agent\ninteraction, they are limited by static, pre-assigned roles, which hinder\nadaptability and dynamic knowledge integration. To address these limitations,\nwe propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration\nframework that enables LLM agents to dynamically form and expand expert teams\nbased on the evolving diagnostic context. KAMAC begins with one or more expert\nagents and then conducts a knowledge-driven discussion to identify and fill\nknowledge gaps by recruiting additional specialists as needed. This supports\nflexible, scalable collaboration in complex clinical scenarios, with decisions\nfinalized through reviewing updated agent comments. Experiments on two\nreal-world medical benchmarks demonstrate that KAMAC significantly outperforms\nboth single-agent and advanced multi-agent methods, particularly in complex\nclinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty\nexpertise. Our code is publicly available at:\nhttps://github.com/XiaoXiao-Woo/KAMAC."
                },
                "authors": [
                    {
                        "name": "Xiao Wu"
                    },
                    {
                        "name": "Ting-Zhu Huang"
                    },
                    {
                        "name": "Liang-Jian Deng"
                    },
                    {
                        "name": "Yanyuan Qiao"
                    },
                    {
                        "name": "Imran Razzak"
                    },
                    {
                        "name": "Yutong Xie"
                    }
                ],
                "author_detail": {
                    "name": "Yutong Xie"
                },
                "author": "Yutong Xie",
                "arxiv_comment": "The paper has been accepted to the EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14992v1",
                "updated": "2025-09-18T14:23:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    23,
                    36,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T14:23:36Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    23,
                    36,
                    3,
                    261,
                    0
                ],
                "title": "ExT: Towards Scalable Autonomous Excavation via Large-Scale Multi-Task\n  Pretraining and Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExT: Towards Scalable Autonomous Excavation via Large-Scale Multi-Task\n  Pretraining and Fine-Tuning"
                },
                "summary": "Scaling up the deployment of autonomous excavators is of great economic and\nsocietal importance. Yet it remains a challenging problem, as effective systems\nmust robustly handle unseen worksite conditions and new hardware\nconfigurations. Current state-of-the-art approaches rely on highly engineered,\ntask-specific controllers, which require extensive manual tuning for each new\nscenario. In contrast, recent advances in large-scale pretrained models have\nshown remarkable adaptability across tasks and embodiments in domains such as\nmanipulation and navigation, but their applicability to heavy construction\nmachinery remains largely unexplored. In this work, we introduce ExT, a unified\nopen-source framework for large-scale demonstration collection, pretraining,\nand fine-tuning of multitask excavation policies. ExT policies are first\ntrained on large-scale demonstrations collected from a mix of experts, then\nfine-tuned either with supervised fine-tuning (SFT) or reinforcement learning\nfine-tuning (RLFT) to specialize to new tasks or operating conditions. Through\nboth simulation and real-world experiments, we show that pretrained ExT\npolicies can execute complete excavation cycles with centimeter-level accuracy,\nsuccessfully transferring from simulation to real machine with performance\ncomparable to specialized single-task controllers. Furthermore, in simulation,\nwe demonstrate that ExT's fine-tuning pipelines allow rapid adaptation to new\ntasks, out-of-distribution conditions, and machine configurations, while\nmaintaining strong performance on previously learned tasks. These results\nhighlight the potential of ExT to serve as a foundation for scalable and\ngeneralizable autonomous excavation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling up the deployment of autonomous excavators is of great economic and\nsocietal importance. Yet it remains a challenging problem, as effective systems\nmust robustly handle unseen worksite conditions and new hardware\nconfigurations. Current state-of-the-art approaches rely on highly engineered,\ntask-specific controllers, which require extensive manual tuning for each new\nscenario. In contrast, recent advances in large-scale pretrained models have\nshown remarkable adaptability across tasks and embodiments in domains such as\nmanipulation and navigation, but their applicability to heavy construction\nmachinery remains largely unexplored. In this work, we introduce ExT, a unified\nopen-source framework for large-scale demonstration collection, pretraining,\nand fine-tuning of multitask excavation policies. ExT policies are first\ntrained on large-scale demonstrations collected from a mix of experts, then\nfine-tuned either with supervised fine-tuning (SFT) or reinforcement learning\nfine-tuning (RLFT) to specialize to new tasks or operating conditions. Through\nboth simulation and real-world experiments, we show that pretrained ExT\npolicies can execute complete excavation cycles with centimeter-level accuracy,\nsuccessfully transferring from simulation to real machine with performance\ncomparable to specialized single-task controllers. Furthermore, in simulation,\nwe demonstrate that ExT's fine-tuning pipelines allow rapid adaptation to new\ntasks, out-of-distribution conditions, and machine configurations, while\nmaintaining strong performance on previously learned tasks. These results\nhighlight the potential of ExT to serve as a foundation for scalable and\ngeneralizable autonomous excavation."
                },
                "authors": [
                    {
                        "name": "Yifan Zhai"
                    },
                    {
                        "name": "Lorenzo Terenzi"
                    },
                    {
                        "name": "Patrick Frey"
                    },
                    {
                        "name": "Diego Garcia Soto"
                    },
                    {
                        "name": "Pascal Egli"
                    },
                    {
                        "name": "Marco Hutter"
                    }
                ],
                "author_detail": {
                    "name": "Marco Hutter"
                },
                "author": "Marco Hutter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14985v1",
                "updated": "2025-09-18T14:15:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    15,
                    37,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T14:15:37Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    15,
                    37,
                    3,
                    261,
                    0
                ],
                "title": "PRISM: Product Retrieval In Shopping Carts using Hybrid Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: Product Retrieval In Shopping Carts using Hybrid Matching"
                },
                "summary": "Compared to traditional image retrieval tasks, product retrieval in retail\nsettings is even more challenging. Products of the same type from different\nbrands may have highly similar visual appearances, and the query image may be\ntaken from an angle that differs significantly from view angles of the stored\ncatalog images. Foundational models, such as CLIP and SigLIP, often struggle to\ndistinguish these subtle but important local differences. Pixel-wise matching\nmethods, on the other hand, are computationally expensive and incur\nprohibitively high matching times. In this paper, we propose a new, hybrid\nmethod, called PRISM, for product retrieval in retail settings by leveraging\nthe advantages of both vision-language model-based and pixel-wise matching\napproaches. To provide both efficiency/speed and finegrained retrieval\naccuracy, PRISM consists of three stages: 1) A vision-language model (SigLIP)\nis employed first to retrieve the top 35 most semantically similar products\nfrom a fixed gallery, thereby narrowing the search space significantly; 2) a\nsegmentation model (YOLO-E) is applied to eliminate background clutter; 3)\nfine-grained pixel-level matching is performed using LightGlue across the\nfiltered candidates. This framework enables more accurate discrimination\nbetween products with high inter-class similarity by focusing on subtle visual\ncues often missed by global models. Experiments performed on the ABV dataset\nshow that our proposed PRISM outperforms the state-of-the-art image retrieval\nmethods by 4.21% in top-1 accuracy while still remaining within the bounds of\nreal-time processing for practical retail deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compared to traditional image retrieval tasks, product retrieval in retail\nsettings is even more challenging. Products of the same type from different\nbrands may have highly similar visual appearances, and the query image may be\ntaken from an angle that differs significantly from view angles of the stored\ncatalog images. Foundational models, such as CLIP and SigLIP, often struggle to\ndistinguish these subtle but important local differences. Pixel-wise matching\nmethods, on the other hand, are computationally expensive and incur\nprohibitively high matching times. In this paper, we propose a new, hybrid\nmethod, called PRISM, for product retrieval in retail settings by leveraging\nthe advantages of both vision-language model-based and pixel-wise matching\napproaches. To provide both efficiency/speed and finegrained retrieval\naccuracy, PRISM consists of three stages: 1) A vision-language model (SigLIP)\nis employed first to retrieve the top 35 most semantically similar products\nfrom a fixed gallery, thereby narrowing the search space significantly; 2) a\nsegmentation model (YOLO-E) is applied to eliminate background clutter; 3)\nfine-grained pixel-level matching is performed using LightGlue across the\nfiltered candidates. This framework enables more accurate discrimination\nbetween products with high inter-class similarity by focusing on subtle visual\ncues often missed by global models. Experiments performed on the ABV dataset\nshow that our proposed PRISM outperforms the state-of-the-art image retrieval\nmethods by 4.21% in top-1 accuracy while still remaining within the bounds of\nreal-time processing for practical retail deployments."
                },
                "authors": [
                    {
                        "name": "Arda Kabadayi"
                    },
                    {
                        "name": "Senem Velipasalar"
                    },
                    {
                        "name": "Jiajing Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiajing Chen"
                },
                "author": "Jiajing Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20869v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20869v2",
                "updated": "2025-09-18T14:12:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    12,
                    45,
                    3,
                    261,
                    0
                ],
                "published": "2025-06-25T22:40:00Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    22,
                    40,
                    0,
                    2,
                    176,
                    0
                ],
                "title": "Engineering RAG Systems for Real-World Applications: Design,\n  Development, and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engineering RAG Systems for Real-World Applications: Design,\n  Development, and Evaluation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems are emerging as a key approach\nfor grounding Large Language Models (LLMs) in external knowledge, addressing\nlimitations in factual accuracy and contextual relevance. However, there is a\nlack of empirical studies that report on the development of RAG-based\nimplementations grounded in real-world use cases, evaluated through general\nuser involvement, and accompanied by systematic documentation of lessons\nlearned. This paper presents five domain-specific RAG applications developed\nfor real-world scenarios across governance, cybersecurity, agriculture,\nindustrial research, and medical diagnostics. Each system incorporates\nmultilingual OCR, semantic retrieval via vector embeddings, and domain-adapted\nLLMs, deployed through local servers or cloud APIs to meet distinct user needs.\nA web-based evaluation involving a total of 100 participants assessed the\nsystems across six dimensions: (i) Ease of Use, (ii) Relevance, (iii)\nTransparency, (iv) Responsiveness, (v) Accuracy, and (vi) Likelihood of\nRecommendation. Based on user feedback and our development experience, we\ndocumented twelve key lessons learned, highlighting technical, operational, and\nethical challenges affecting the reliability and usability of RAG systems in\npractice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems are emerging as a key approach\nfor grounding Large Language Models (LLMs) in external knowledge, addressing\nlimitations in factual accuracy and contextual relevance. However, there is a\nlack of empirical studies that report on the development of RAG-based\nimplementations grounded in real-world use cases, evaluated through general\nuser involvement, and accompanied by systematic documentation of lessons\nlearned. This paper presents five domain-specific RAG applications developed\nfor real-world scenarios across governance, cybersecurity, agriculture,\nindustrial research, and medical diagnostics. Each system incorporates\nmultilingual OCR, semantic retrieval via vector embeddings, and domain-adapted\nLLMs, deployed through local servers or cloud APIs to meet distinct user needs.\nA web-based evaluation involving a total of 100 participants assessed the\nsystems across six dimensions: (i) Ease of Use, (ii) Relevance, (iii)\nTransparency, (iv) Responsiveness, (v) Accuracy, and (vi) Likelihood of\nRecommendation. Based on user feedback and our development experience, we\ndocumented twelve key lessons learned, highlighting technical, operational, and\nethical challenges affecting the reliability and usability of RAG systems in\npractice."
                },
                "authors": [
                    {
                        "name": "Md Toufique Hasan"
                    },
                    {
                        "name": "Muhammad Waseem"
                    },
                    {
                        "name": "Kai-Kristian Kemell"
                    },
                    {
                        "name": "Ayman Asad Khan"
                    },
                    {
                        "name": "Mika Saari"
                    },
                    {
                        "name": "Pekka Abrahamsson"
                    }
                ],
                "author_detail": {
                    "name": "Pekka Abrahamsson"
                },
                "author": "Pekka Abrahamsson",
                "arxiv_doi": "10.1007/978-3-032-04200-2_10",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-032-04200-2_10",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.20869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20869v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the Proceedings of the 51st Euromicro Conference on\n  Software Engineering and Advanced Applications, SEAA 2025. Lecture Notes in\n  Computer Science, volume 16082, pages 143-158. Springer, 2026",
                "arxiv_journal_ref": "LNCS 16082, 143-158, 2026",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.11; I.2.6; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14979v1",
                "updated": "2025-09-18T14:08:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    8,
                    45,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T14:08:45Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    8,
                    45,
                    3,
                    261,
                    0
                ],
                "title": "What Matters in LLM-Based Feature Extractor for Recommender? A\n  Systematic Analysis of Prompts, Models, and Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Matters in LLM-Based Feature Extractor for Recommender? A\n  Systematic Analysis of Prompts, Models, and Adaptation"
                },
                "summary": "Using Large Language Models (LLMs) to generate semantic features has been\ndemonstrated as a powerful paradigm for enhancing Sequential Recommender\nSystems (SRS). This typically involves three stages: processing item text,\nextracting features with LLMs, and adapting them for downstream models.\nHowever, existing methods vary widely in prompting, architecture, and\nadaptation strategies, making it difficult to fairly compare design choices and\nidentify what truly drives performance. In this work, we propose RecXplore, a\nmodular analytical framework that decomposes the LLM-as-feature-extractor\npipeline into four modules: data processing, semantic feature extraction,\nfeature adaptation, and sequential modeling. Instead of proposing new\ntechniques, RecXplore revisits and organizes established methods, enabling\nsystematic exploration of each module in isolation. Experiments on four public\ndatasets show that simply combining the best designs from existing techniques\nwithout exhaustive search yields up to 18.7% relative improvement in NDCG@5 and\n12.7% in HR@5 over strong baselines. These results underscore the utility of\nmodular benchmarking for identifying effective design patterns and promoting\nstandardized research in LLM-enhanced recommendation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models (LLMs) to generate semantic features has been\ndemonstrated as a powerful paradigm for enhancing Sequential Recommender\nSystems (SRS). This typically involves three stages: processing item text,\nextracting features with LLMs, and adapting them for downstream models.\nHowever, existing methods vary widely in prompting, architecture, and\nadaptation strategies, making it difficult to fairly compare design choices and\nidentify what truly drives performance. In this work, we propose RecXplore, a\nmodular analytical framework that decomposes the LLM-as-feature-extractor\npipeline into four modules: data processing, semantic feature extraction,\nfeature adaptation, and sequential modeling. Instead of proposing new\ntechniques, RecXplore revisits and organizes established methods, enabling\nsystematic exploration of each module in isolation. Experiments on four public\ndatasets show that simply combining the best designs from existing techniques\nwithout exhaustive search yields up to 18.7% relative improvement in NDCG@5 and\n12.7% in HR@5 over strong baselines. These results underscore the utility of\nmodular benchmarking for identifying effective design patterns and promoting\nstandardized research in LLM-enhanced recommendation."
                },
                "authors": [
                    {
                        "name": "Kainan Shi"
                    },
                    {
                        "name": "Peilin Zhou"
                    },
                    {
                        "name": "Ge Wang"
                    },
                    {
                        "name": "Han Ding"
                    },
                    {
                        "name": "Fei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wang"
                },
                "arxiv_affiliation": "Xi'an Jiaotong University",
                "author": "Fei Wang",
                "arxiv_comment": "9 pages. Keywords: Recommender Systems, Large Language Models,\n  Sequential Recommendation, Feature Extraction",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3; I.2.6; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14966v1",
                "updated": "2025-09-18T13:59:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    13,
                    59,
                    24,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T13:59:24Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    13,
                    59,
                    24,
                    3,
                    261,
                    0
                ],
                "title": "RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D\n  Geometric Keypoint Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D\n  Geometric Keypoint Matching"
                },
                "summary": "The rapidly growing number of product categories in large-scale e-commerce\nmakes accurate object identification for automated packing in warehouses\nsubstantially more difficult. As the catalog grows, intra-class variability and\na long tail of rare or visually similar items increase, and when combined with\ndiverse packaging, cluttered containers, frequent occlusion, and large\nviewpoint changes-these factors amplify discrepancies between query and\nreference images, causing sharp performance drops for methods that rely solely\non 2D appearance features. Thus, we propose RoboEye, a two-stage identification\nframework that dynamically augments 2D semantic features with domain-adapted 3D\nreasoning and lightweight adapters to bridge training deployment gaps. In the\nfirst stage, we train a large vision model to extract 2D features for\ngenerating candidate rankings. A lightweight 3D-feature-awareness module then\nestimates 3D feature quality and predicts whether 3D re-ranking is necessary,\npreventing performance degradation and avoiding unnecessary computation. When\ninvoked, the second stage uses our robot 3D retrieval transformer, comprising a\n3D feature extractor that produces geometry-aware dense features and a\nkeypoint-based matcher that computes keypoint-correspondence confidences\nbetween query and reference images instead of conventional cosine-similarity\nscoring. Experiments show that RoboEye improves Recall@1 by 7.1% over the prior\nstate of the art (RoboLLM). Moreover, RoboEye operates using only RGB images,\navoiding reliance on explicit 3D inputs and reducing deployment costs. The code\nused in this paper is publicly available at:\nhttps://github.com/longkukuhi/RoboEye.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapidly growing number of product categories in large-scale e-commerce\nmakes accurate object identification for automated packing in warehouses\nsubstantially more difficult. As the catalog grows, intra-class variability and\na long tail of rare or visually similar items increase, and when combined with\ndiverse packaging, cluttered containers, frequent occlusion, and large\nviewpoint changes-these factors amplify discrepancies between query and\nreference images, causing sharp performance drops for methods that rely solely\non 2D appearance features. Thus, we propose RoboEye, a two-stage identification\nframework that dynamically augments 2D semantic features with domain-adapted 3D\nreasoning and lightweight adapters to bridge training deployment gaps. In the\nfirst stage, we train a large vision model to extract 2D features for\ngenerating candidate rankings. A lightweight 3D-feature-awareness module then\nestimates 3D feature quality and predicts whether 3D re-ranking is necessary,\npreventing performance degradation and avoiding unnecessary computation. When\ninvoked, the second stage uses our robot 3D retrieval transformer, comprising a\n3D feature extractor that produces geometry-aware dense features and a\nkeypoint-based matcher that computes keypoint-correspondence confidences\nbetween query and reference images instead of conventional cosine-similarity\nscoring. Experiments show that RoboEye improves Recall@1 by 7.1% over the prior\nstate of the art (RoboLLM). Moreover, RoboEye operates using only RGB images,\navoiding reliance on explicit 3D inputs and reducing deployment costs. The code\nused in this paper is publicly available at:\nhttps://github.com/longkukuhi/RoboEye."
                },
                "authors": [
                    {
                        "name": "Xingwu Zhang"
                    },
                    {
                        "name": "Guanxuan Li"
                    },
                    {
                        "name": "Zhuocheng Zhang"
                    },
                    {
                        "name": "Zijun Long"
                    }
                ],
                "author_detail": {
                    "name": "Zijun Long"
                },
                "author": "Zijun Long",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01566v2",
                "updated": "2025-09-18T13:42:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    13,
                    42,
                    32,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-01T15:51:30Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    15,
                    51,
                    30,
                    0,
                    244,
                    0
                ],
                "title": "CSRM-LLM: Embracing Multilingual LLMs for Cold-Start Relevance Matching\n  in Emerging E-commerce Markets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSRM-LLM: Embracing Multilingual LLMs for Cold-Start Relevance Matching\n  in Emerging E-commerce Markets"
                },
                "summary": "As global e-commerce platforms continue to expand, companies are entering new\nmarkets where they encounter cold-start challenges due to limited human labels\nand user behaviors. In this paper, we share our experiences in Coupang to\nprovide a competitive cold-start performance of relevance matching for emerging\ne-commerce markets. Specifically, we present a Cold-Start Relevance Matching\n(CSRM) framework, utilizing a multilingual Large Language Model (LLM) to\naddress three challenges: (1) activating cross-lingual transfer learning\nabilities of LLMs through machine translation tasks; (2) enhancing query\nunderstanding and incorporating e-commerce knowledge by retrieval-based query\naugmentation; (3) mitigating the impact of training label errors through a\nmulti-round self-distillation training strategy. Our experiments demonstrate\nthe effectiveness of CSRM-LLM and the proposed techniques, resulting in\nsuccessful real-world deployment and significant online gains, with a 45.8%\nreduction in defect ratio and a 0.866% uplift in session purchase rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As global e-commerce platforms continue to expand, companies are entering new\nmarkets where they encounter cold-start challenges due to limited human labels\nand user behaviors. In this paper, we share our experiences in Coupang to\nprovide a competitive cold-start performance of relevance matching for emerging\ne-commerce markets. Specifically, we present a Cold-Start Relevance Matching\n(CSRM) framework, utilizing a multilingual Large Language Model (LLM) to\naddress three challenges: (1) activating cross-lingual transfer learning\nabilities of LLMs through machine translation tasks; (2) enhancing query\nunderstanding and incorporating e-commerce knowledge by retrieval-based query\naugmentation; (3) mitigating the impact of training label errors through a\nmulti-round self-distillation training strategy. Our experiments demonstrate\nthe effectiveness of CSRM-LLM and the proposed techniques, resulting in\nsuccessful real-world deployment and significant online gains, with a 45.8%\nreduction in defect ratio and a 0.866% uplift in session purchase rate."
                },
                "authors": [
                    {
                        "name": "Yujing Wang"
                    },
                    {
                        "name": "Yiren Chen"
                    },
                    {
                        "name": "Huoran Li"
                    },
                    {
                        "name": "Chunxu Xu"
                    },
                    {
                        "name": "Yuchong Luo"
                    },
                    {
                        "name": "Xianghui Mao"
                    },
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Chunyang Ma"
                    },
                    {
                        "name": "Qiqi Jiang"
                    },
                    {
                        "name": "Yin Wang"
                    },
                    {
                        "name": "Fan Gao"
                    },
                    {
                        "name": "Wenting Mo"
                    },
                    {
                        "name": "Pei Wen"
                    },
                    {
                        "name": "Shantanu Kumar"
                    },
                    {
                        "name": "Taejin Park"
                    },
                    {
                        "name": "Yiwei Song"
                    },
                    {
                        "name": "Vijay Rajaram"
                    },
                    {
                        "name": "Tao Cheng"
                    },
                    {
                        "name": "Sonu Durgia"
                    },
                    {
                        "name": "Pranam Kolari"
                    }
                ],
                "author_detail": {
                    "name": "Pranam Kolari"
                },
                "author": "Pranam Kolari",
                "arxiv_comment": "7 pages, 3 figures, accepted by CIKM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14956v1",
                "updated": "2025-09-18T13:39:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    13,
                    39,
                    59,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T13:39:59Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    13,
                    39,
                    59,
                    3,
                    261,
                    0
                ],
                "title": "Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent\n  Systems"
                },
                "summary": "This paper proposes a novel architectural framework aimed at enhancing\nsecurity and reliability in multi-agent systems (MAS). A central component of\nthis framework is a network of Sentinel Agents, functioning as a distributed\nsecurity layer that integrates techniques such as semantic analysis via large\nlanguage models (LLMs), behavioral analytics, retrieval-augmented verification,\nand cross-agent anomaly detection. Such agents can potentially oversee\ninter-agent communications, identify potential threats, enforce privacy and\naccess controls, and maintain comprehensive audit records. Complementary to the\nidea of Sentinel Agents is the use of a Coordinator Agent. The Coordinator\nAgent supervises policy implementation, and manages agent participation. In\naddition, the Coordinator also ingests alerts from Sentinel Agents. Based on\nthese alerts, it can adapt policies, isolate or quarantine misbehaving agents,\nand contain threats to maintain the integrity of the MAS ecosystem. This\ndual-layered security approach, combining the continuous monitoring of Sentinel\nAgents with the governance functions of Coordinator Agents, supports dynamic\nand adaptive defense mechanisms against a range of threats, including prompt\ninjection, collusive agent behavior, hallucinations generated by LLMs, privacy\nbreaches, and coordinated multi-agent attacks. In addition to the architectural\ndesign, we present a simulation study where 162 synthetic attacks of different\nfamilies (prompt injection, hallucination, and data exfiltration) were injected\ninto a multi-agent conversational environment. The Sentinel Agents successfully\ndetected the attack attempts, confirming the practical feasibility of the\nproposed monitoring approach. The framework also offers enhanced system\nobservability, supports regulatory compliance, and enables policy evolution\nover time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a novel architectural framework aimed at enhancing\nsecurity and reliability in multi-agent systems (MAS). A central component of\nthis framework is a network of Sentinel Agents, functioning as a distributed\nsecurity layer that integrates techniques such as semantic analysis via large\nlanguage models (LLMs), behavioral analytics, retrieval-augmented verification,\nand cross-agent anomaly detection. Such agents can potentially oversee\ninter-agent communications, identify potential threats, enforce privacy and\naccess controls, and maintain comprehensive audit records. Complementary to the\nidea of Sentinel Agents is the use of a Coordinator Agent. The Coordinator\nAgent supervises policy implementation, and manages agent participation. In\naddition, the Coordinator also ingests alerts from Sentinel Agents. Based on\nthese alerts, it can adapt policies, isolate or quarantine misbehaving agents,\nand contain threats to maintain the integrity of the MAS ecosystem. This\ndual-layered security approach, combining the continuous monitoring of Sentinel\nAgents with the governance functions of Coordinator Agents, supports dynamic\nand adaptive defense mechanisms against a range of threats, including prompt\ninjection, collusive agent behavior, hallucinations generated by LLMs, privacy\nbreaches, and coordinated multi-agent attacks. In addition to the architectural\ndesign, we present a simulation study where 162 synthetic attacks of different\nfamilies (prompt injection, hallucination, and data exfiltration) were injected\ninto a multi-agent conversational environment. The Sentinel Agents successfully\ndetected the attack attempts, confirming the practical feasibility of the\nproposed monitoring approach. The framework also offers enhanced system\nobservability, supports regulatory compliance, and enables policy evolution\nover time."
                },
                "authors": [
                    {
                        "name": "Diego Gosmar"
                    },
                    {
                        "name": "Deborah A. Dahl"
                    }
                ],
                "author_detail": {
                    "name": "Deborah A. Dahl"
                },
                "author": "Deborah A. Dahl",
                "arxiv_comment": "25 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14954v1",
                "updated": "2025-09-18T13:38:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    13,
                    38,
                    47,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T13:38:47Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    13,
                    38,
                    47,
                    3,
                    261,
                    0
                ],
                "title": "Exploratory Movement Strategies for Texture Discrimination with a\n  Neuromorphic Tactile Sensor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploratory Movement Strategies for Texture Discrimination with a\n  Neuromorphic Tactile Sensor"
                },
                "summary": "We propose a neuromorphic tactile sensing framework for robotic texture\nclassification that is inspired by human exploratory strategies. Our system\nutilizes the NeuroTac sensor to capture neuromorphic tactile data during a\nseries of exploratory motions. We first tested six distinct motions for texture\nclassification under fixed environment: sliding, rotating, tapping, as well as\nthe combined motions: sliding+rotating, tapping+rotating, and tapping+sliding.\nWe chose sliding and sliding+rotating as the best motions based on final\naccuracy and the sample timing length needed to reach converged accuracy. In\nthe second experiment designed to simulate complex real-world conditions, these\ntwo motions were further evaluated under varying contact depth and speeds.\nUnder these conditions, our framework attained the highest accuracy of 87.33\\%\nwith sliding+rotating while maintaining an extremely low power consumption of\nonly 8.04 mW. These results suggest that the sliding+rotating motion is the\noptimal exploratory strategy for neuromorphic tactile sensing deployment in\ntexture classification tasks and holds significant promise for enhancing\nrobotic environmental interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a neuromorphic tactile sensing framework for robotic texture\nclassification that is inspired by human exploratory strategies. Our system\nutilizes the NeuroTac sensor to capture neuromorphic tactile data during a\nseries of exploratory motions. We first tested six distinct motions for texture\nclassification under fixed environment: sliding, rotating, tapping, as well as\nthe combined motions: sliding+rotating, tapping+rotating, and tapping+sliding.\nWe chose sliding and sliding+rotating as the best motions based on final\naccuracy and the sample timing length needed to reach converged accuracy. In\nthe second experiment designed to simulate complex real-world conditions, these\ntwo motions were further evaluated under varying contact depth and speeds.\nUnder these conditions, our framework attained the highest accuracy of 87.33\\%\nwith sliding+rotating while maintaining an extremely low power consumption of\nonly 8.04 mW. These results suggest that the sliding+rotating motion is the\noptimal exploratory strategy for neuromorphic tactile sensing deployment in\ntexture classification tasks and holds significant promise for enhancing\nrobotic environmental interaction."
                },
                "authors": [
                    {
                        "name": "Xingchen Xu"
                    },
                    {
                        "name": "Ao Li"
                    },
                    {
                        "name": "Benjamin Ward-Cherrier"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Ward-Cherrier"
                },
                "author": "Benjamin Ward-Cherrier",
                "arxiv_comment": "Accepted at IEEE/RSJ International Conference on Intelligent Robots\n  and Systems 2025. Please cite the proceedings version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22656v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22656v2",
                "updated": "2025-09-18T13:33:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    13,
                    33,
                    59,
                    3,
                    261,
                    0
                ],
                "published": "2025-07-30T13:15:37Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    13,
                    15,
                    37,
                    2,
                    211,
                    0
                ],
                "title": "A Multi-Scale Spatial Attention Network for Near-field MIMO Channel\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Scale Spatial Attention Network for Near-field MIMO Channel\n  Estimation"
                },
                "summary": "The deployment of extremely large-scale array (ELAA) brings higher spectral\nefficiency and spatial degree of freedom, but triggers issues on near-field\nchannel estimation.\n  Existing near-field channel estimation schemes primarily exploit sparsity in\nthe transform domain.\n  However, these schemes are sensitive to the transform matrix selection and\nthe stopping criteria.\n  Inspired by the success of deep learning (DL) in far-field channel\nestimation, this paper proposes a novel spatial-attention-based method for\nreconstructing extremely large-scale MIMO (XL-MIMO) channel.\n  Initially, the spatial antenna correlations of near-field channels are\nanalyzed as an expectation over the angle-distance space, which demonstrate\ncorrelation range of an antenna element varies with its position.\n  Due to the strong correlation between adjacent antenna elements, interactions\nof inter-subchannel are applied to describe inherent correlation of near-field\nchannels instead of inter-element.\n  Subsequently, a multi-scale spatial attention network (MsSAN) with the\ninter-subchannel correlation learning capabilities is proposed tailed to\nnear-field MIMO channel estimation.\n  We employ the multi-scale architecture to refine the subchannel size in\nMsSAN.\n  Specially, we inventively introduce the sum of dot products as spatial\nattention (SA) instead of cross-covariance to weight subchannel features at\ndifferent scales in the SA module.\n  Simulation results are presented to validate the proposed MsSAN achieves\nremarkable the inter-subchannel correlation learning capabilities and\noutperforms others in terms of near-field channel reconstruction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of extremely large-scale array (ELAA) brings higher spectral\nefficiency and spatial degree of freedom, but triggers issues on near-field\nchannel estimation.\n  Existing near-field channel estimation schemes primarily exploit sparsity in\nthe transform domain.\n  However, these schemes are sensitive to the transform matrix selection and\nthe stopping criteria.\n  Inspired by the success of deep learning (DL) in far-field channel\nestimation, this paper proposes a novel spatial-attention-based method for\nreconstructing extremely large-scale MIMO (XL-MIMO) channel.\n  Initially, the spatial antenna correlations of near-field channels are\nanalyzed as an expectation over the angle-distance space, which demonstrate\ncorrelation range of an antenna element varies with its position.\n  Due to the strong correlation between adjacent antenna elements, interactions\nof inter-subchannel are applied to describe inherent correlation of near-field\nchannels instead of inter-element.\n  Subsequently, a multi-scale spatial attention network (MsSAN) with the\ninter-subchannel correlation learning capabilities is proposed tailed to\nnear-field MIMO channel estimation.\n  We employ the multi-scale architecture to refine the subchannel size in\nMsSAN.\n  Specially, we inventively introduce the sum of dot products as spatial\nattention (SA) instead of cross-covariance to weight subchannel features at\ndifferent scales in the SA module.\n  Simulation results are presented to validate the proposed MsSAN achieves\nremarkable the inter-subchannel correlation learning capabilities and\noutperforms others in terms of near-field channel reconstruction."
                },
                "authors": [
                    {
                        "name": "Zhiming Zhu"
                    },
                    {
                        "name": "Shu Xu"
                    },
                    {
                        "name": "Jiexin Zhang"
                    },
                    {
                        "name": "Chunguo Li"
                    },
                    {
                        "name": "Yongming Huang"
                    },
                    {
                        "name": "Luxi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Luxi Yang"
                },
                "author": "Luxi Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22656v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22656v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14943v1",
                "updated": "2025-09-18T13:30:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    13,
                    30,
                    31,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T13:30:31Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    13,
                    30,
                    31,
                    3,
                    261,
                    0
                ],
                "title": "Explicit vs. Implicit Biographies: Evaluating and Adapting LLM\n  Information Extraction on Wikidata-Derived Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explicit vs. Implicit Biographies: Evaluating and Adapting LLM\n  Information Extraction on Wikidata-Derived Texts"
                },
                "summary": "Text Implicitness has always been challenging in Natural Language Processing\n(NLP), with traditional methods relying on explicit statements to identify\nentities and their relationships. From the sentence \"Zuhdi attends church every\nSunday\", the relationship between Zuhdi and Christianity is evident for a human\nreader, but it presents a challenge when it must be inferred automatically.\nLarge language models (LLMs) have proven effective in NLP downstream tasks such\nas text comprehension and information extraction (IE).\n  This study examines how textual implicitness affects IE tasks in pre-trained\nLLMs: LLaMA 2.3, DeepSeekV1, and Phi1.5. We generate two synthetic datasets of\n10k implicit and explicit verbalization of biographic information to measure\nthe impact on LLM performance and analyze whether fine-tuning implicit data\nimproves their ability to generalize in implicit reasoning tasks.\n  This research presents an experiment on the internal reasoning processes of\nLLMs in IE, particularly in dealing with implicit and explicit contexts. The\nresults demonstrate that fine-tuning LLM models with LoRA (low-rank adaptation)\nimproves their performance in extracting information from implicit texts,\ncontributing to better model interpretability and reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Implicitness has always been challenging in Natural Language Processing\n(NLP), with traditional methods relying on explicit statements to identify\nentities and their relationships. From the sentence \"Zuhdi attends church every\nSunday\", the relationship between Zuhdi and Christianity is evident for a human\nreader, but it presents a challenge when it must be inferred automatically.\nLarge language models (LLMs) have proven effective in NLP downstream tasks such\nas text comprehension and information extraction (IE).\n  This study examines how textual implicitness affects IE tasks in pre-trained\nLLMs: LLaMA 2.3, DeepSeekV1, and Phi1.5. We generate two synthetic datasets of\n10k implicit and explicit verbalization of biographic information to measure\nthe impact on LLM performance and analyze whether fine-tuning implicit data\nimproves their ability to generalize in implicit reasoning tasks.\n  This research presents an experiment on the internal reasoning processes of\nLLMs in IE, particularly in dealing with implicit and explicit contexts. The\nresults demonstrate that fine-tuning LLM models with LoRA (low-rank adaptation)\nimproves their performance in extracting information from implicit texts,\ncontributing to better model interpretability and reliability."
                },
                "authors": [
                    {
                        "name": "Alessandra Stramiglio"
                    },
                    {
                        "name": "Andrea Schimmenti"
                    },
                    {
                        "name": "Valentina Pasqual"
                    },
                    {
                        "name": "Marieke van Erp"
                    },
                    {
                        "name": "Francesco Sovrano"
                    },
                    {
                        "name": "Fabio Vitali"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Vitali"
                },
                "author": "Fabio Vitali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14930v1",
                "updated": "2025-09-18T13:07:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    13,
                    7,
                    53,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T13:07:53Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    13,
                    7,
                    53,
                    3,
                    261,
                    0
                ],
                "title": "Cross-Modal Knowledge Distillation for Speech Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Modal Knowledge Distillation for Speech Large Language Models"
                },
                "summary": "In this work, we present the first systematic evaluation of catastrophic\nforgetting and modality inequivalence in speech large language models, showing\nthat introducing speech capabilities can degrade knowledge and reasoning even\nwhen inputs remain textual, and performance further decreases with spoken\nqueries. To address these challenges, we propose a cross-modal knowledge\ndistillation framework that leverages both text-to-text and speech-to-text\nchannels to transfer knowledge from a text-based teacher model to a speech LLM.\nExtensive experiments on dialogue and audio understanding tasks validate the\neffectiveness of our approach in preserving textual knowledge, improving\ncross-modal alignment, and enhancing reasoning in speech-based interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present the first systematic evaluation of catastrophic\nforgetting and modality inequivalence in speech large language models, showing\nthat introducing speech capabilities can degrade knowledge and reasoning even\nwhen inputs remain textual, and performance further decreases with spoken\nqueries. To address these challenges, we propose a cross-modal knowledge\ndistillation framework that leverages both text-to-text and speech-to-text\nchannels to transfer knowledge from a text-based teacher model to a speech LLM.\nExtensive experiments on dialogue and audio understanding tasks validate the\neffectiveness of our approach in preserving textual knowledge, improving\ncross-modal alignment, and enhancing reasoning in speech-based interactions."
                },
                "authors": [
                    {
                        "name": "Enzhi Wang"
                    },
                    {
                        "name": "Qicheng Li"
                    },
                    {
                        "name": "Zhiyuan Tang"
                    },
                    {
                        "name": "Yuhang Jia"
                    }
                ],
                "author_detail": {
                    "name": "Yuhang Jia"
                },
                "author": "Yuhang Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14922v1",
                "updated": "2025-09-18T12:59:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    12,
                    59,
                    7,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T12:59:07Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    12,
                    59,
                    7,
                    3,
                    261,
                    0
                ],
                "title": "A Comparative Evaluation of Large Language Models for Persian Sentiment\n  Analysis and Emotion Detection in Social Media Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparative Evaluation of Large Language Models for Persian Sentiment\n  Analysis and Emotion Detection in Social Media Texts"
                },
                "summary": "This study presents a comprehensive comparative evaluation of four\nstate-of-the-art Large Language Models (LLMs)--Claude 3.7 Sonnet, DeepSeek-V3,\nGemini 2.0 Flash, and GPT-4o--for sentiment analysis and emotion detection in\nPersian social media texts. Comparative analysis among LLMs has witnessed a\nsignificant rise in recent years, however, most of these analyses have been\nconducted on English language tasks, creating gaps in understanding\ncross-linguistic performance patterns. This research addresses these gaps\nthrough rigorous experimental design using balanced Persian datasets containing\n900 texts for sentiment analysis (positive, negative, neutral) and 1,800 texts\nfor emotion detection (anger, fear, happiness, hate, sadness, surprise). The\nmain focus was to allow for a direct and fair comparison among different\nmodels, by using consistent prompts, uniform processing parameters, and by\nanalyzing the performance metrics such as precision, recall, F1-scores, along\nwith misclassification patterns. The results show that all models reach an\nacceptable level of performance, and a statistical comparison of the best three\nmodels indicates no significant differences among them. However, GPT-4o\ndemonstrated a marginally higher raw accuracy value for both tasks, while\nGemini 2.0 Flash proved to be the most cost-efficient. The findings indicate\nthat the emotion detection task is more challenging for all models compared to\nthe sentiment analysis task, and the misclassification patterns can represent\nsome challenges in Persian language texts. These findings establish performance\nbenchmarks for Persian NLP applications and offer practical guidance for model\nselection based on accuracy, efficiency, and cost considerations, while\nrevealing cultural and linguistic challenges that require consideration in\nmultilingual AI system deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a comprehensive comparative evaluation of four\nstate-of-the-art Large Language Models (LLMs)--Claude 3.7 Sonnet, DeepSeek-V3,\nGemini 2.0 Flash, and GPT-4o--for sentiment analysis and emotion detection in\nPersian social media texts. Comparative analysis among LLMs has witnessed a\nsignificant rise in recent years, however, most of these analyses have been\nconducted on English language tasks, creating gaps in understanding\ncross-linguistic performance patterns. This research addresses these gaps\nthrough rigorous experimental design using balanced Persian datasets containing\n900 texts for sentiment analysis (positive, negative, neutral) and 1,800 texts\nfor emotion detection (anger, fear, happiness, hate, sadness, surprise). The\nmain focus was to allow for a direct and fair comparison among different\nmodels, by using consistent prompts, uniform processing parameters, and by\nanalyzing the performance metrics such as precision, recall, F1-scores, along\nwith misclassification patterns. The results show that all models reach an\nacceptable level of performance, and a statistical comparison of the best three\nmodels indicates no significant differences among them. However, GPT-4o\ndemonstrated a marginally higher raw accuracy value for both tasks, while\nGemini 2.0 Flash proved to be the most cost-efficient. The findings indicate\nthat the emotion detection task is more challenging for all models compared to\nthe sentiment analysis task, and the misclassification patterns can represent\nsome challenges in Persian language texts. These findings establish performance\nbenchmarks for Persian NLP applications and offer practical guidance for model\nselection based on accuracy, efficiency, and cost considerations, while\nrevealing cultural and linguistic challenges that require consideration in\nmultilingual AI system deployment."
                },
                "authors": [
                    {
                        "name": "Kian Tohidi"
                    },
                    {
                        "name": "Kia Dashtipour"
                    },
                    {
                        "name": "Simone Rebora"
                    },
                    {
                        "name": "Sevda Pourfaramarz"
                    }
                ],
                "author_detail": {
                    "name": "Sevda Pourfaramarz"
                },
                "author": "Sevda Pourfaramarz",
                "arxiv_comment": "19 pages, 8 Figures, 9 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19026v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19026v3",
                "updated": "2025-09-18T12:56:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    12,
                    56,
                    38,
                    3,
                    261,
                    0
                ],
                "published": "2025-08-26T13:43:45Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    43,
                    45,
                    1,
                    238,
                    0
                ],
                "title": "MovieCORE: COgnitive REasoning in Movies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MovieCORE: COgnitive REasoning in Movies"
                },
                "summary": "This paper introduces MovieCORE, a novel video question answering (VQA)\ndataset designed to probe deeper cognitive understanding of movie content.\nUnlike existing datasets that focus on surface-level comprehension, MovieCORE\nemphasizes questions that engage System-2 thinking while remaining specific to\nthe video material. We present an innovative agentic brainstorming approach,\nutilizing multiple large language models (LLMs) as thought agents to generate\nand refine high-quality question-answer pairs. To evaluate dataset quality, we\ndevelop a set of cognitive tests assessing depth, thought-provocation\npotential, and syntactic complexity. We also propose a comprehensive evaluation\nscheme for assessing VQA model performance on deeper cognitive tasks. To\naddress the limitations of existing video-language models (VLMs), we introduce\nan agentic enhancement module, Agentic Choice Enhancement (ACE), which improves\nmodel reasoning capabilities post-training by up to 25%. Our work contributes\nto advancing movie understanding in AI systems and provides valuable insights\ninto the capabilities and limitations of current VQA models when faced with\nmore challenging, nuanced questions about cinematic content. Our project page,\ndataset and code can be found at\nhttps://joslefaure.github.io/assets/html/moviecore.html.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces MovieCORE, a novel video question answering (VQA)\ndataset designed to probe deeper cognitive understanding of movie content.\nUnlike existing datasets that focus on surface-level comprehension, MovieCORE\nemphasizes questions that engage System-2 thinking while remaining specific to\nthe video material. We present an innovative agentic brainstorming approach,\nutilizing multiple large language models (LLMs) as thought agents to generate\nand refine high-quality question-answer pairs. To evaluate dataset quality, we\ndevelop a set of cognitive tests assessing depth, thought-provocation\npotential, and syntactic complexity. We also propose a comprehensive evaluation\nscheme for assessing VQA model performance on deeper cognitive tasks. To\naddress the limitations of existing video-language models (VLMs), we introduce\nan agentic enhancement module, Agentic Choice Enhancement (ACE), which improves\nmodel reasoning capabilities post-training by up to 25%. Our work contributes\nto advancing movie understanding in AI systems and provides valuable insights\ninto the capabilities and limitations of current VQA models when faced with\nmore challenging, nuanced questions about cinematic content. Our project page,\ndataset and code can be found at\nhttps://joslefaure.github.io/assets/html/moviecore.html."
                },
                "authors": [
                    {
                        "name": "Gueter Josmy Faure"
                    },
                    {
                        "name": "Min-Hung Chen"
                    },
                    {
                        "name": "Jia-Fong Yeh"
                    },
                    {
                        "name": "Ying Cheng"
                    },
                    {
                        "name": "Hung-Ting Su"
                    },
                    {
                        "name": "Yung-Hao Tang"
                    },
                    {
                        "name": "Shang-Hong Lai"
                    },
                    {
                        "name": "Winston H. Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Winston H. Hsu"
                },
                "author": "Winston H. Hsu",
                "arxiv_comment": "Accepted for EMNLP'2025 Main Conference (Oral Presentation). Project\n  Page: https://joslefaure.github.io/assets/html/moviecore.html",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19026v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19026v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13456v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13456v3",
                "updated": "2025-09-18T12:48:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    12,
                    48,
                    34,
                    3,
                    261,
                    0
                ],
                "published": "2024-10-17T11:34:07Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    11,
                    34,
                    7,
                    3,
                    291,
                    0
                ],
                "title": "Unlocking Legal Knowledge: A Multilingual Dataset for Judicial\n  Summarization in Switzerland",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Legal Knowledge: A Multilingual Dataset for Judicial\n  Summarization in Switzerland"
                },
                "summary": "Legal research depends on headnotes: concise summaries that help lawyers\nquickly identify relevant cases. Yet, many court decisions lack them due to the\nhigh cost of manual annotation. To address this gap, we introduce the Swiss\nLandmark Decisions Summarization (SLDS) dataset containing 20K rulings from the\nSwiss Federal Supreme Court, each with headnotes in German, French, and\nItalian. SLDS has the potential to significantly improve access to legal\ninformation and transform legal research in Switzerland. We fine-tune open\nmodels (Qwen2.5, Llama 3.2, Phi-3.5) and compare them to larger general-purpose\nand reasoning-tuned LLMs, including GPT-4o, Claude 3.5 Sonnet, and the\nopen-source DeepSeek R1. Using an LLM-as-a-Judge framework, we find that\nfine-tuned models perform well in terms of lexical similarity, while larger\nmodels generate more legally accurate and coherent summaries. Interestingly,\nreasoning-focused models show no consistent benefit, suggesting that factual\nprecision is more important than deep reasoning in this task. We release SLDS\nunder a CC BY 4.0 license to support future research in cross-lingual legal\nsummarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal research depends on headnotes: concise summaries that help lawyers\nquickly identify relevant cases. Yet, many court decisions lack them due to the\nhigh cost of manual annotation. To address this gap, we introduce the Swiss\nLandmark Decisions Summarization (SLDS) dataset containing 20K rulings from the\nSwiss Federal Supreme Court, each with headnotes in German, French, and\nItalian. SLDS has the potential to significantly improve access to legal\ninformation and transform legal research in Switzerland. We fine-tune open\nmodels (Qwen2.5, Llama 3.2, Phi-3.5) and compare them to larger general-purpose\nand reasoning-tuned LLMs, including GPT-4o, Claude 3.5 Sonnet, and the\nopen-source DeepSeek R1. Using an LLM-as-a-Judge framework, we find that\nfine-tuned models perform well in terms of lexical similarity, while larger\nmodels generate more legally accurate and coherent summaries. Interestingly,\nreasoning-focused models show no consistent benefit, suggesting that factual\nprecision is more important than deep reasoning in this task. We release SLDS\nunder a CC BY 4.0 license to support future research in cross-lingual legal\nsummarization."
                },
                "authors": [
                    {
                        "name": "Luca Rolshoven"
                    },
                    {
                        "name": "Vishvaksenan Rasiah"
                    },
                    {
                        "name": "Srinanda Brügger Bose"
                    },
                    {
                        "name": "Sarah Hostettler"
                    },
                    {
                        "name": "Lara Burkhalter"
                    },
                    {
                        "name": "Matthias Stürmer"
                    },
                    {
                        "name": "Joel Niklaus"
                    }
                ],
                "author_detail": {
                    "name": "Joel Niklaus"
                },
                "author": "Joel Niklaus",
                "arxiv_comment": "Accepted to EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13456v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13456v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19176v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19176v3",
                "updated": "2025-09-18T12:24:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    12,
                    24,
                    53,
                    3,
                    261,
                    0
                ],
                "published": "2025-05-25T14:48:49Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    14,
                    48,
                    49,
                    6,
                    145,
                    0
                ],
                "title": "Assistant-Guided Mitigation of Teacher Preference Bias in LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assistant-Guided Mitigation of Teacher Preference Bias in LLM-as-a-Judge"
                },
                "summary": "LLM-as-a-Judge employs large language models (LLMs), such as GPT-4, to\nevaluate the quality of LLM-generated responses, gaining popularity for its\ncost-effectiveness and strong alignment with human evaluations. However,\ntraining proxy judge models using evaluation data generated by powerful teacher\nmodels introduces a critical yet previously overlooked issue: teacher\npreference bias, where the proxy judge model learns a biased preference for\nresponses from the teacher model. To tackle this problem, we propose a novel\nsetting that incorporates an additional assistant model, which is not biased\ntoward the teacher model's responses, to complement the training data. Building\non this setup, we introduce AGDe-Judge, a three-stage framework designed to\ndebias from both the labels and feedbacks in the training data. Extensive\nexperiments demonstrate that AGDe-Judge effectively reduces teacher preference\nbias while maintaining strong performance across six evaluation benchmarks.\nCode is available at https://github.com/Liuz233/AGDe-Judge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge employs large language models (LLMs), such as GPT-4, to\nevaluate the quality of LLM-generated responses, gaining popularity for its\ncost-effectiveness and strong alignment with human evaluations. However,\ntraining proxy judge models using evaluation data generated by powerful teacher\nmodels introduces a critical yet previously overlooked issue: teacher\npreference bias, where the proxy judge model learns a biased preference for\nresponses from the teacher model. To tackle this problem, we propose a novel\nsetting that incorporates an additional assistant model, which is not biased\ntoward the teacher model's responses, to complement the training data. Building\non this setup, we introduce AGDe-Judge, a three-stage framework designed to\ndebias from both the labels and feedbacks in the training data. Extensive\nexperiments demonstrate that AGDe-Judge effectively reduces teacher preference\nbias while maintaining strong performance across six evaluation benchmarks.\nCode is available at https://github.com/Liuz233/AGDe-Judge."
                },
                "authors": [
                    {
                        "name": "Zhuo Liu"
                    },
                    {
                        "name": "Moxin Li"
                    },
                    {
                        "name": "Xun Deng"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Fuli Feng"
                    }
                ],
                "author_detail": {
                    "name": "Fuli Feng"
                },
                "author": "Fuli Feng",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19176v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19176v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14899v1",
                "updated": "2025-09-18T12:21:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    12,
                    21,
                    30,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T12:21:30Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    12,
                    21,
                    30,
                    3,
                    261,
                    0
                ],
                "title": "CARGO: A Framework for Confidence-Aware Routing of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARGO: A Framework for Confidence-Aware Routing of Large Language Models"
                },
                "summary": "As large language models (LLMs) proliferate in scale, specialization, and\nlatency profiles, the challenge of routing user prompts to the most appropriate\nmodel has become increasingly critical for balancing performance and cost. We\nintroduce CARGO (Category-Aware Routing with Gap-based Optimization), a\nlightweight, confidence-aware framework for dynamic LLM selection. CARGO\nemploys a single embedding-based regressor trained on LLM-judged pairwise\ncomparisons to predict model performance, with an optional binary classifier\ninvoked when predictions are uncertain. This two-stage design enables precise,\ncost-aware routing without the need for human-annotated supervision. To capture\ndomain-specific behavior, CARGO also supports category-specific regressors\ntrained across five task groups: mathematics, coding, reasoning, summarization,\nand creative writing. Evaluated on four competitive LLMs (GPT-4o, Claude 3.5\nSonnet, DeepSeek V3, and Perplexity Sonar), CARGO achieves a top-1 routing\naccuracy of 76.4% and win rates ranging from 72% to 89% against individual\nexperts. These results demonstrate that confidence-guided, lightweight routing\ncan achieve expert-level performance with minimal overhead, offering a\npractical solution for real-world, multi-model LLM deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) proliferate in scale, specialization, and\nlatency profiles, the challenge of routing user prompts to the most appropriate\nmodel has become increasingly critical for balancing performance and cost. We\nintroduce CARGO (Category-Aware Routing with Gap-based Optimization), a\nlightweight, confidence-aware framework for dynamic LLM selection. CARGO\nemploys a single embedding-based regressor trained on LLM-judged pairwise\ncomparisons to predict model performance, with an optional binary classifier\ninvoked when predictions are uncertain. This two-stage design enables precise,\ncost-aware routing without the need for human-annotated supervision. To capture\ndomain-specific behavior, CARGO also supports category-specific regressors\ntrained across five task groups: mathematics, coding, reasoning, summarization,\nand creative writing. Evaluated on four competitive LLMs (GPT-4o, Claude 3.5\nSonnet, DeepSeek V3, and Perplexity Sonar), CARGO achieves a top-1 routing\naccuracy of 76.4% and win rates ranging from 72% to 89% against individual\nexperts. These results demonstrate that confidence-guided, lightweight routing\ncan achieve expert-level performance with minimal overhead, offering a\npractical solution for real-world, multi-model LLM deployments."
                },
                "authors": [
                    {
                        "name": "Amine Barrak"
                    },
                    {
                        "name": "Yosr Fourati"
                    },
                    {
                        "name": "Michael Olchawa"
                    },
                    {
                        "name": "Emna Ksontini"
                    },
                    {
                        "name": "Khalil Zoghlami"
                    }
                ],
                "author_detail": {
                    "name": "Khalil Zoghlami"
                },
                "author": "Khalil Zoghlami",
                "arxiv_journal_ref": "35th IEEE International Conference on Collaborative Advances in\n  Software and Computing 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07452v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07452v2",
                "updated": "2025-09-18T12:10:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    12,
                    10,
                    39,
                    3,
                    261,
                    0
                ],
                "published": "2025-05-12T11:33:55Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    11,
                    33,
                    55,
                    0,
                    132,
                    0
                ],
                "title": "SwarmSearch: Decentralized Search Engine with Self-Funding Economy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwarmSearch: Decentralized Search Engine with Self-Funding Economy"
                },
                "summary": "Centralized search engines control what we see, read, believe, and vote.\nConsequently, they raise concerns over information control, censorship, and\nbias. Decentralized search engines offer a remedy to this problem, but their\nadoption has been hindered by their inferior quality and lack of a\nself-sustaining economic framework. We present SwarmSearch, a fully\ndecentralized, AI-powered search engine with a self-funding architecture. Our\nsystem is designed for deployment within the decentralized file-sharing\nsoftware Tribler. SwarmSearch integrates volunteer-based with profit-driven\nmechanisms to foster an implicit marketplace for resources. Employing the\nstate-of-the-art of AI-based retrieval and relevance ranking, we also aim to\nclose the quality gap between decentralized search and centralized\nalternatives. Our system demonstrates high retrieval accuracy while showing\nrobustness in the presence of 50% adversarial nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Centralized search engines control what we see, read, believe, and vote.\nConsequently, they raise concerns over information control, censorship, and\nbias. Decentralized search engines offer a remedy to this problem, but their\nadoption has been hindered by their inferior quality and lack of a\nself-sustaining economic framework. We present SwarmSearch, a fully\ndecentralized, AI-powered search engine with a self-funding architecture. Our\nsystem is designed for deployment within the decentralized file-sharing\nsoftware Tribler. SwarmSearch integrates volunteer-based with profit-driven\nmechanisms to foster an implicit marketplace for resources. Employing the\nstate-of-the-art of AI-based retrieval and relevance ranking, we also aim to\nclose the quality gap between decentralized search and centralized\nalternatives. Our system demonstrates high retrieval accuracy while showing\nrobustness in the presence of 50% adversarial nodes."
                },
                "authors": [
                    {
                        "name": "Marcel Gregoriadis"
                    },
                    {
                        "name": "Rowdy Chotkan"
                    },
                    {
                        "name": "Petru Neague"
                    },
                    {
                        "name": "Johan Pouwelse"
                    }
                ],
                "author_detail": {
                    "name": "Johan Pouwelse"
                },
                "author": "Johan Pouwelse",
                "arxiv_doi": "10.1109/LCN65610.2025.11146295",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LCN65610.2025.11146295",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.07452v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07452v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Submitted for possible publication",
                "arxiv_journal_ref": "2025 IEEE 50th Conference on Local Computer Networks (LCN),\n  Sydney, Australia, 2025, pp. 1-10",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14884v1",
                "updated": "2025-09-18T12:06:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    12,
                    6,
                    40,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T12:06:40Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    12,
                    6,
                    40,
                    3,
                    261,
                    0
                ],
                "title": "Applying reinforcement learning to optical cavity locking tasks:\n  considerations on actor-critic architectures and real-time hardware\n  implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applying reinforcement learning to optical cavity locking tasks:\n  considerations on actor-critic architectures and real-time hardware\n  implementation"
                },
                "summary": "This proceedings contains our considerations made during and after fruitful\ndiscussions held at EuCAIFCon 2025. We explore the use of deep reinforcement\nlearning for autonomous locking of Fabry-Perot optical cavities in non-linear\nregimes, with relevance to gravitational-wave detectors. A custom Gymnasium\nenvironment with a time-domain simulator enabled training of agents such as\ndeep deterministic policy gradient, achieving reliable lock acquisition for\nboth low- and high-finesse cavities, including Virgo-like parameters. We also\ndiscuss possible improvements with Twin Delayed DDPG, Soft Actor Critic and\nmeta-reinforcement learning, as well as strategies for low-latency execution\nand off-line policy updates to address hardware limitations. These studies lay\nthe groundwork for future deployment of reinforcement learning-based control in\nreal optical setups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This proceedings contains our considerations made during and after fruitful\ndiscussions held at EuCAIFCon 2025. We explore the use of deep reinforcement\nlearning for autonomous locking of Fabry-Perot optical cavities in non-linear\nregimes, with relevance to gravitational-wave detectors. A custom Gymnasium\nenvironment with a time-domain simulator enabled training of agents such as\ndeep deterministic policy gradient, achieving reliable lock acquisition for\nboth low- and high-finesse cavities, including Virgo-like parameters. We also\ndiscuss possible improvements with Twin Delayed DDPG, Soft Actor Critic and\nmeta-reinforcement learning, as well as strategies for low-latency execution\nand off-line policy updates to address hardware limitations. These studies lay\nthe groundwork for future deployment of reinforcement learning-based control in\nreal optical setups."
                },
                "authors": [
                    {
                        "name": "Mateusz Bawaj"
                    },
                    {
                        "name": "Andrea Svizzeretto"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Svizzeretto"
                },
                "author": "Andrea Svizzeretto",
                "arxiv_comment": "EuCAIFCon2025 proceedings contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14882v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14882v1",
                "updated": "2025-09-18T12:00:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    12,
                    0,
                    7,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T12:00:07Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    12,
                    0,
                    7,
                    3,
                    261,
                    0
                ],
                "title": "Llama-Mimi: Speech Language Models with Interleaved Semantic and\n  Acoustic Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Llama-Mimi: Speech Language Models with Interleaved Semantic and\n  Acoustic Tokens"
                },
                "summary": "We propose Llama-Mimi, a speech language model that uses a unified tokenizer\nand a single Transformer decoder to jointly model sequences of interleaved\nsemantic and acoustic tokens. Comprehensive evaluation shows that Llama-Mimi\nachieves state-of-the-art performance in acoustic consistency and possesses the\nability to preserve speaker identity. Our analysis further demonstrates that\nincreasing the number of quantizers improves acoustic fidelity but degrades\nlinguistic performance, highlighting the inherent challenge of maintaining\nlong-term coherence. We additionally introduce an LLM-as-a-Judge-based\nevaluation to assess the spoken content quality of generated outputs. Our\nmodels, code, and speech samples are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Llama-Mimi, a speech language model that uses a unified tokenizer\nand a single Transformer decoder to jointly model sequences of interleaved\nsemantic and acoustic tokens. Comprehensive evaluation shows that Llama-Mimi\nachieves state-of-the-art performance in acoustic consistency and possesses the\nability to preserve speaker identity. Our analysis further demonstrates that\nincreasing the number of quantizers improves acoustic fidelity but degrades\nlinguistic performance, highlighting the inherent challenge of maintaining\nlong-term coherence. We additionally introduce an LLM-as-a-Judge-based\nevaluation to assess the spoken content quality of generated outputs. Our\nmodels, code, and speech samples are publicly available."
                },
                "authors": [
                    {
                        "name": "Issa Sugiura"
                    },
                    {
                        "name": "Shuhei Kurita"
                    },
                    {
                        "name": "Yusuke Oda"
                    },
                    {
                        "name": "Ryuichiro Higashinaka"
                    }
                ],
                "author_detail": {
                    "name": "Ryuichiro Higashinaka"
                },
                "author": "Ryuichiro Higashinaka",
                "arxiv_comment": "5 pages, 1 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14882v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14882v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18650v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18650v2",
                "updated": "2025-09-18T11:58:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    58,
                    13,
                    3,
                    261,
                    0
                ],
                "published": "2025-02-25T21:19:27Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    21,
                    19,
                    27,
                    1,
                    56,
                    0
                ],
                "title": "Single- vs. Dual-Prompt Dialogue Generation with LLMs for Job Interviews\n  in Human Resources",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single- vs. Dual-Prompt Dialogue Generation with LLMs for Job Interviews\n  in Human Resources"
                },
                "summary": "Optimizing language models for use in conversational agents requires large\nquantities of example dialogues. Increasingly, these dialogues are\nsynthetically generated by using powerful large language models (LLMs),\nespecially in domains where obtaining authentic human data is challenging. One\nsuch domain is human resources (HR). In this context, we compare two LLM-based\ndialogue generation methods for producing HR job interviews, and assess which\nmethod generates higher-quality dialogues, i.e., those more difficult to\ndistinguish from genuine human discourse. The first method uses a single prompt\nto generate the complete interview dialogue. The second method uses two agents\nthat converse with each other. To evaluate dialogue quality under each method,\nwe ask a judge LLM to determine whether AI was used for interview generation,\nusing pairwise interview comparisons. We empirically find that, at the expense\nof a sixfold increase in token count, interviews generated with the dual-prompt\nmethod achieve a win rate 2 to 10 times higher than those generated with the\nsingle-prompt method. This difference remains consistent regardless of whether\nGPT-4o or Llama 3.3 70B is used for either interview generation or quality\njudging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing language models for use in conversational agents requires large\nquantities of example dialogues. Increasingly, these dialogues are\nsynthetically generated by using powerful large language models (LLMs),\nespecially in domains where obtaining authentic human data is challenging. One\nsuch domain is human resources (HR). In this context, we compare two LLM-based\ndialogue generation methods for producing HR job interviews, and assess which\nmethod generates higher-quality dialogues, i.e., those more difficult to\ndistinguish from genuine human discourse. The first method uses a single prompt\nto generate the complete interview dialogue. The second method uses two agents\nthat converse with each other. To evaluate dialogue quality under each method,\nwe ask a judge LLM to determine whether AI was used for interview generation,\nusing pairwise interview comparisons. We empirically find that, at the expense\nof a sixfold increase in token count, interviews generated with the dual-prompt\nmethod achieve a win rate 2 to 10 times higher than those generated with the\nsingle-prompt method. This difference remains consistent regardless of whether\nGPT-4o or Llama 3.3 70B is used for either interview generation or quality\njudging."
                },
                "authors": [
                    {
                        "name": "Joachim De Baer"
                    },
                    {
                        "name": "A. Seza Doğruöz"
                    },
                    {
                        "name": "Thomas Demeester"
                    },
                    {
                        "name": "Chris Develder"
                    }
                ],
                "author_detail": {
                    "name": "Chris Develder"
                },
                "author": "Chris Develder",
                "arxiv_comment": "12 pages. Accepted to the Fourth Workshop on Generation, Evaluation\n  and Metrics (GEM^2) at ACL 2025. ACL Anthology version available at\n  https://aclanthology.org/2025.gem-1.74",
                "arxiv_journal_ref": "Proceedings of the Fourth Workshop on Generation, Evaluation and\n  Metrics (GEM^2), pages 947-957, Vienna, Austria and virtual meeting, August\n  2025. Association for Computational Linguistics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18650v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18650v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18042v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18042v2",
                "updated": "2025-09-18T11:55:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    55,
                    2,
                    3,
                    261,
                    0
                ],
                "published": "2025-02-25T10:02:12Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    10,
                    2,
                    12,
                    1,
                    56,
                    0
                ],
                "title": "VLM-E2E: Enhancing End-to-End Autonomous Driving with Multimodal Driver\n  Attention Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLM-E2E: Enhancing End-to-End Autonomous Driving with Multimodal Driver\n  Attention Fusion"
                },
                "summary": "Human drivers adeptly navigate complex scenarios by utilizing rich\nattentional semantics, but the current autonomous systems struggle to replicate\nthis ability, as they often lose critical semantic information when converting\n2D observations into 3D space. In this sense, it hinders their effective\ndeployment in dynamic and complex environments. Leveraging the superior scene\nunderstanding and reasoning abilities of Vision-Language Models (VLMs), we\npropose VLM-E2E, a novel framework that uses the VLMs to enhance training by\nproviding attentional cues. Our method integrates textual representations into\nBird's-Eye-View (BEV) features for semantic supervision, which enables the\nmodel to learn richer feature representations that explicitly capture the\ndriver's attentional semantics. By focusing on attentional semantics, VLM-E2E\nbetter aligns with human-like driving behavior, which is critical for\nnavigating dynamic and complex environments. Furthermore, we introduce a\nBEV-Text learnable weighted fusion strategy to address the issue of modality\nimportance imbalance in fusing multimodal information. This approach\ndynamically balances the contributions of BEV and text features, ensuring that\nthe complementary information from visual and textual modalities is effectively\nutilized. By explicitly addressing the imbalance in multimodal fusion, our\nmethod facilitates a more holistic and robust representation of driving\nenvironments. We evaluate VLM-E2E on the nuScenes dataset and achieve\nsignificant improvements in perception, prediction, and planning over the\nbaseline end-to-end model, showcasing the effectiveness of our\nattention-enhanced BEV representation in enabling more accurate and reliable\nautonomous driving tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human drivers adeptly navigate complex scenarios by utilizing rich\nattentional semantics, but the current autonomous systems struggle to replicate\nthis ability, as they often lose critical semantic information when converting\n2D observations into 3D space. In this sense, it hinders their effective\ndeployment in dynamic and complex environments. Leveraging the superior scene\nunderstanding and reasoning abilities of Vision-Language Models (VLMs), we\npropose VLM-E2E, a novel framework that uses the VLMs to enhance training by\nproviding attentional cues. Our method integrates textual representations into\nBird's-Eye-View (BEV) features for semantic supervision, which enables the\nmodel to learn richer feature representations that explicitly capture the\ndriver's attentional semantics. By focusing on attentional semantics, VLM-E2E\nbetter aligns with human-like driving behavior, which is critical for\nnavigating dynamic and complex environments. Furthermore, we introduce a\nBEV-Text learnable weighted fusion strategy to address the issue of modality\nimportance imbalance in fusing multimodal information. This approach\ndynamically balances the contributions of BEV and text features, ensuring that\nthe complementary information from visual and textual modalities is effectively\nutilized. By explicitly addressing the imbalance in multimodal fusion, our\nmethod facilitates a more holistic and robust representation of driving\nenvironments. We evaluate VLM-E2E on the nuScenes dataset and achieve\nsignificant improvements in perception, prediction, and planning over the\nbaseline end-to-end model, showcasing the effectiveness of our\nattention-enhanced BEV representation in enabling more accurate and reliable\nautonomous driving tasks."
                },
                "authors": [
                    {
                        "name": "Pei Liu"
                    },
                    {
                        "name": "Haipeng Liu"
                    },
                    {
                        "name": "Haichao Liu"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Jinxin Ni"
                    },
                    {
                        "name": "Jun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jun Ma"
                },
                "author": "Jun Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18042v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18042v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14880v1",
                "updated": "2025-09-18T11:51:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    51,
                    20,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T11:51:20Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    51,
                    20,
                    3,
                    261,
                    0
                ],
                "title": "From Hype to Insight: Rethinking Large Language Model Integration in\n  Visual Speech Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Hype to Insight: Rethinking Large Language Model Integration in\n  Visual Speech Recognition"
                },
                "summary": "Advances in self-supervised encoders have improved Visual Speech Recognition\n(VSR). Recent approaches integrating these encoders with LLM decoders improves\ntranscription accuracy; however, it remains unclear whether these gains stem\nfrom visual understanding or stronger language modeling. In this work, we\nsystematically evaluate LLM decoders by freezing or selectively updating the\nvisual encoder, scaling decoder size, comparing adaptation strategies and\narchitectures, and varying training data across LRS2, LRS3, and their\ncombination. Evaluation on LRS2, LRS3, and WildVSR shows that scaling and\nadaptation yield limited improvements, while combining datasets enhances\ngeneralization. Semantic analysis reveals that gains arise primarily from\nlexical rather than semantic processing. Our Llama-2-13B model trained on the\ncombined set achieves 24.7\\% WER on LRS3 and 47.0\\% on WildVSR, establishing\nSOTA among models trained without additional supervision. Our findings indicate\nLLM decoders refine contextual reasoning rather than visual features,\nemphasizing the need for stronger visual encoders to drive meaningful progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in self-supervised encoders have improved Visual Speech Recognition\n(VSR). Recent approaches integrating these encoders with LLM decoders improves\ntranscription accuracy; however, it remains unclear whether these gains stem\nfrom visual understanding or stronger language modeling. In this work, we\nsystematically evaluate LLM decoders by freezing or selectively updating the\nvisual encoder, scaling decoder size, comparing adaptation strategies and\narchitectures, and varying training data across LRS2, LRS3, and their\ncombination. Evaluation on LRS2, LRS3, and WildVSR shows that scaling and\nadaptation yield limited improvements, while combining datasets enhances\ngeneralization. Semantic analysis reveals that gains arise primarily from\nlexical rather than semantic processing. Our Llama-2-13B model trained on the\ncombined set achieves 24.7\\% WER on LRS3 and 47.0\\% on WildVSR, establishing\nSOTA among models trained without additional supervision. Our findings indicate\nLLM decoders refine contextual reasoning rather than visual features,\nemphasizing the need for stronger visual encoders to drive meaningful progress."
                },
                "authors": [
                    {
                        "name": "Rishabh Jain"
                    },
                    {
                        "name": "Naomi Harte"
                    }
                ],
                "author_detail": {
                    "name": "Naomi Harte"
                },
                "author": "Naomi Harte",
                "arxiv_comment": "submitted to ICASSP 2026. This work has been submitted to the IEEE\n  for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11686v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11686v2",
                "updated": "2025-09-18T11:44:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    44,
                    9,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-15T08:38:01Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    8,
                    38,
                    1,
                    0,
                    258,
                    0
                ],
                "title": "Do Code Semantics Help? A Comprehensive Study on Execution Trace-Based\n  Information for Code Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Code Semantics Help? A Comprehensive Study on Execution Trace-Based\n  Information for Code Large Language Models"
                },
                "summary": "Code Large Language Models (Code LLMs) have opened a new era in programming\nwith their impressive capabilities. However, recent research has revealed\ncritical limitations in their ability to reason about runtime behavior and\nunderstand the actual functionality of programs, which poses significant\nchallenges for their post-training and practical deployment. Specifically, Code\nLLMs encounter two principal issues: (1) a lack of proficiency in reasoning\nabout program execution behavior, as they struggle to interpret what programs\nactually do during runtime, and (2) the inconsistent and fragmented\nrepresentation of semantic information, such as execution traces, across\nexisting methods, which hinders their ability to generalize and reason\neffectively. These challenges underscore the necessity for more systematic\napproaches to enhance the reasoning capabilities of Code LLMs. To address these\nissues, we introduce a generic framework to support integrating semantic\ninformation~(e.g., execution trace) to code task-relevant prompts, and conduct\na comprehensive study to explore the role of semantic information in enhancing\nthe reasoning ability of Code LLMs accordingly. Specifically, we focus on\ninvestigating the usefulness of trace-based semantic information in boosting\nsupervised fine-tuning~(SFT) and post-phase inference of Code LLMs. The\nexperimental results surprisingly disagree with previous works and demonstrate\nthat semantic information has limited usefulness for SFT and test time scaling\nof Code LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Large Language Models (Code LLMs) have opened a new era in programming\nwith their impressive capabilities. However, recent research has revealed\ncritical limitations in their ability to reason about runtime behavior and\nunderstand the actual functionality of programs, which poses significant\nchallenges for their post-training and practical deployment. Specifically, Code\nLLMs encounter two principal issues: (1) a lack of proficiency in reasoning\nabout program execution behavior, as they struggle to interpret what programs\nactually do during runtime, and (2) the inconsistent and fragmented\nrepresentation of semantic information, such as execution traces, across\nexisting methods, which hinders their ability to generalize and reason\neffectively. These challenges underscore the necessity for more systematic\napproaches to enhance the reasoning capabilities of Code LLMs. To address these\nissues, we introduce a generic framework to support integrating semantic\ninformation~(e.g., execution trace) to code task-relevant prompts, and conduct\na comprehensive study to explore the role of semantic information in enhancing\nthe reasoning ability of Code LLMs accordingly. Specifically, we focus on\ninvestigating the usefulness of trace-based semantic information in boosting\nsupervised fine-tuning~(SFT) and post-phase inference of Code LLMs. The\nexperimental results surprisingly disagree with previous works and demonstrate\nthat semantic information has limited usefulness for SFT and test time scaling\nof Code LLM."
                },
                "authors": [
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Xiaofei Xie"
                    },
                    {
                        "name": "Qiang Hu"
                    },
                    {
                        "name": "Shangqing Liu"
                    },
                    {
                        "name": "Yi Li"
                    }
                ],
                "author_detail": {
                    "name": "Yi Li"
                },
                "author": "Yi Li",
                "arxiv_comment": "EMNLP2025-findings https://openreview.net/forum?id=d4ICISW2T4",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11686v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11686v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15213v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15213v2",
                "updated": "2025-09-18T11:35:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    35,
                    1,
                    3,
                    261,
                    0
                ],
                "published": "2025-08-21T03:53:35Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    53,
                    35,
                    3,
                    233,
                    0
                ],
                "title": "Select to Know: An Internal-External Knowledge Self-Selection Framework\n  for Domain-Specific Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Select to Know: An Internal-External Knowledge Self-Selection Framework\n  for Domain-Specific Question Answering"
                },
                "summary": "Large Language Models (LLMs) perform well in general QA but often struggle in\ndomain-specific scenarios. Retrieval-Augmented Generation (RAG) introduces\nexternal knowledge but suffers from hallucinations and latency due to noisy\nretrievals. Continued pretraining internalizes domain knowledge but is costly\nand lacks cross-domain flexibility. We attribute this challenge to the\nlong-tail distribution of domain knowledge, which leaves partial yet useful\ninternal knowledge underutilized. We further argue that knowledge acquisition\nshould be progressive, mirroring human learning: first understanding concepts,\nthen applying them to complex reasoning. To address this, we propose Selct2Know\n(S2K), a cost-effective framework that internalizes domain knowledge through an\ninternal-external knowledge self-selection strategy and selective supervised\nfine-tuning. We also introduce a structured reasoning data generation pipeline\nand integrate GRPO to enhance reasoning ability. Experiments on medical, legal,\nand financial QA benchmarks show that S2K consistently outperforms existing\nmethods and matches domain-pretrained LLMs with significantly lower cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) perform well in general QA but often struggle in\ndomain-specific scenarios. Retrieval-Augmented Generation (RAG) introduces\nexternal knowledge but suffers from hallucinations and latency due to noisy\nretrievals. Continued pretraining internalizes domain knowledge but is costly\nand lacks cross-domain flexibility. We attribute this challenge to the\nlong-tail distribution of domain knowledge, which leaves partial yet useful\ninternal knowledge underutilized. We further argue that knowledge acquisition\nshould be progressive, mirroring human learning: first understanding concepts,\nthen applying them to complex reasoning. To address this, we propose Selct2Know\n(S2K), a cost-effective framework that internalizes domain knowledge through an\ninternal-external knowledge self-selection strategy and selective supervised\nfine-tuning. We also introduce a structured reasoning data generation pipeline\nand integrate GRPO to enhance reasoning ability. Experiments on medical, legal,\nand financial QA benchmarks show that S2K consistently outperforms existing\nmethods and matches domain-pretrained LLMs with significantly lower cost."
                },
                "authors": [
                    {
                        "name": "Bolei He"
                    },
                    {
                        "name": "Xinran He"
                    },
                    {
                        "name": "Run Shao"
                    },
                    {
                        "name": "Shanfu Shu"
                    },
                    {
                        "name": "Xianwei Xue"
                    },
                    {
                        "name": "Mingquan Cheng"
                    },
                    {
                        "name": "Haifeng Li"
                    },
                    {
                        "name": "Zhenhua Ling"
                    }
                ],
                "author_detail": {
                    "name": "Zhenhua Ling"
                },
                "author": "Zhenhua Ling",
                "arxiv_comment": "EMNLP2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15213v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15213v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20013v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20013v2",
                "updated": "2025-09-18T11:32:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    32,
                    15,
                    3,
                    261,
                    0
                ],
                "published": "2025-05-26T14:03:37Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    14,
                    3,
                    37,
                    0,
                    146,
                    0
                ],
                "title": "WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought\n  in Reflection, Branching, and Rollback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought\n  in Reflection, Branching, and Rollback"
                },
                "summary": "Web agents powered by Large Language Models (LLMs) show promise for\nnext-generation AI, but their limited reasoning in uncertain, dynamic web\nenvironments hinders robust deployment. In this paper, we identify key\nreasoning skills essential for effective web agents, i.e., reflection &\nlookahead, branching, and rollback, and curate trajectory data that exemplifies\nthese abilities by reconstructing the agent's (inference-time) reasoning\nalgorithms into chain-of-thought rationales. We conduct experiments in the\nagent self-improving benchmark, OpenWebVoyager, and demonstrate that distilling\nsalient reasoning patterns into the backbone LLM via simple fine-tuning can\nsubstantially enhance its performance. Our approach yields significant\nimprovements across multiple benchmarks, including WebVoyager, Mind2web-live,\nand SimpleQA (web search), highlighting the potential of targeted reasoning\nskill enhancement for web agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web agents powered by Large Language Models (LLMs) show promise for\nnext-generation AI, but their limited reasoning in uncertain, dynamic web\nenvironments hinders robust deployment. In this paper, we identify key\nreasoning skills essential for effective web agents, i.e., reflection &\nlookahead, branching, and rollback, and curate trajectory data that exemplifies\nthese abilities by reconstructing the agent's (inference-time) reasoning\nalgorithms into chain-of-thought rationales. We conduct experiments in the\nagent self-improving benchmark, OpenWebVoyager, and demonstrate that distilling\nsalient reasoning patterns into the backbone LLM via simple fine-tuning can\nsubstantially enhance its performance. Our approach yields significant\nimprovements across multiple benchmarks, including WebVoyager, Mind2web-live,\nand SimpleQA (web search), highlighting the potential of targeted reasoning\nskill enhancement for web agents."
                },
                "authors": [
                    {
                        "name": "Minda Hu"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Jianshu Zhang"
                    },
                    {
                        "name": "Junyu Ma"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Jingyan Zhou"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Irwin King"
                    }
                ],
                "author_detail": {
                    "name": "Irwin King"
                },
                "author": "Irwin King",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20013v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20013v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14856v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14856v1",
                "updated": "2025-09-18T11:24:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    24,
                    9,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T11:24:09Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    24,
                    9,
                    3,
                    261,
                    0
                ],
                "title": "CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End\n  Code Review Evaluation in Python Projects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End\n  Code Review Evaluation in Python Projects"
                },
                "summary": "Automated code review (CR) is a key application for Large Language Models\n(LLMs), but progress is hampered by a \"reality gap\": existing benchmarks\nevaluate models on isolated sub-tasks using simplified, context-poor data. This\nfails to reflect the holistic context-rich nature of real-world CR. To bridge\nthis gap, we introduce CodeFuse-CR-Bench, the first comprehensiveness-aware\nbenchmark for repository-level CR evaluation. CodeFuse-CR-Bench comprises 601\nhigh-quality instances from 70 Python projects covering nine Pull-Request (PR)\nproblem domains, where each instance provides rich, multi-faceted context\nincluding the associated issue, PR details, and repository state, enabling\nend-to-end evaluation. Beyond superficial metrics, we also propose a novel\nevaluation framework that combines rule-based checks for location and syntax\nwith model-based judgments of review quality. We present the first large-scale\nassessment of state-of-the-art LLMs on this comprehensive CR task. Our results\nestablish crucial baselines and reveal that (1) no single LLM dominates all\naspects of CR; (2) Gemini 2.5 Pro achieves the highest comprehensive\nperformance; and (3) different LLMs exhibit varying robustness to redundant\ncontext. These findings highlight the necessity of holistic, multi-dimensional\nevaluation and provide actionable insights for advancing truly intelligent yet\npractical CR assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated code review (CR) is a key application for Large Language Models\n(LLMs), but progress is hampered by a \"reality gap\": existing benchmarks\nevaluate models on isolated sub-tasks using simplified, context-poor data. This\nfails to reflect the holistic context-rich nature of real-world CR. To bridge\nthis gap, we introduce CodeFuse-CR-Bench, the first comprehensiveness-aware\nbenchmark for repository-level CR evaluation. CodeFuse-CR-Bench comprises 601\nhigh-quality instances from 70 Python projects covering nine Pull-Request (PR)\nproblem domains, where each instance provides rich, multi-faceted context\nincluding the associated issue, PR details, and repository state, enabling\nend-to-end evaluation. Beyond superficial metrics, we also propose a novel\nevaluation framework that combines rule-based checks for location and syntax\nwith model-based judgments of review quality. We present the first large-scale\nassessment of state-of-the-art LLMs on this comprehensive CR task. Our results\nestablish crucial baselines and reveal that (1) no single LLM dominates all\naspects of CR; (2) Gemini 2.5 Pro achieves the highest comprehensive\nperformance; and (3) different LLMs exhibit varying robustness to redundant\ncontext. These findings highlight the necessity of holistic, multi-dimensional\nevaluation and provide actionable insights for advancing truly intelligent yet\npractical CR assistants."
                },
                "authors": [
                    {
                        "name": "Hanyang Guo"
                    },
                    {
                        "name": "Xunjin Zheng"
                    },
                    {
                        "name": "Zihan Liao"
                    },
                    {
                        "name": "Hang Yu"
                    },
                    {
                        "name": "Peng DI"
                    },
                    {
                        "name": "Ziyin Zhang"
                    },
                    {
                        "name": "Hong-Ning Dai"
                    }
                ],
                "author_detail": {
                    "name": "Hong-Ning Dai"
                },
                "author": "Hong-Ning Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14856v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14856v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14851v1",
                "updated": "2025-09-18T11:16:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    16,
                    9,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T11:16:09Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    16,
                    9,
                    3,
                    261,
                    0
                ],
                "title": "Empathy-R1: A Chain-of-Empathy and Reinforcement Learning Framework for\n  Long-Form Mental Health Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empathy-R1: A Chain-of-Empathy and Reinforcement Learning Framework for\n  Long-Form Mental Health Support"
                },
                "summary": "Empathy is critical for effective mental health support, especially when\naddressing Long Counseling Texts (LCTs). However, existing Large Language\nModels (LLMs) often generate replies that are semantically fluent but lack the\nstructured reasoning necessary for genuine psychological support, particularly\nin a Chinese context. To bridge this gap, we introduce Empathy-R1, a novel\nframework that integrates a Chain-of-Empathy (CoE) reasoning process with\nReinforcement Learning (RL) to enhance response quality for LCTs. Inspired by\ncognitive-behavioral therapy, our CoE paradigm guides the model to sequentially\nreason about a help-seeker's emotions, causes, and intentions, making its\nthinking process both transparent and interpretable. Our framework is empowered\nby a new large-scale Chinese dataset, Empathy-QA, and a two-stage training\nprocess. First, Supervised Fine-Tuning instills the CoE's reasoning structure.\nSubsequently, RL, guided by a dedicated reward model, refines the therapeutic\nrelevance and contextual appropriateness of the final responses. Experiments\nshow that Empathy-R1 achieves strong performance on key automatic metrics. More\nimportantly, human evaluations confirm its superiority, showing a clear\npreference over strong baselines and achieving a Win@1 rate of 44.30% on our\nnew benchmark. By enabling interpretable and contextually nuanced responses,\nEmpathy-R1 represents a significant advancement in developing responsible and\ngenuinely beneficial AI for mental health support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empathy is critical for effective mental health support, especially when\naddressing Long Counseling Texts (LCTs). However, existing Large Language\nModels (LLMs) often generate replies that are semantically fluent but lack the\nstructured reasoning necessary for genuine psychological support, particularly\nin a Chinese context. To bridge this gap, we introduce Empathy-R1, a novel\nframework that integrates a Chain-of-Empathy (CoE) reasoning process with\nReinforcement Learning (RL) to enhance response quality for LCTs. Inspired by\ncognitive-behavioral therapy, our CoE paradigm guides the model to sequentially\nreason about a help-seeker's emotions, causes, and intentions, making its\nthinking process both transparent and interpretable. Our framework is empowered\nby a new large-scale Chinese dataset, Empathy-QA, and a two-stage training\nprocess. First, Supervised Fine-Tuning instills the CoE's reasoning structure.\nSubsequently, RL, guided by a dedicated reward model, refines the therapeutic\nrelevance and contextual appropriateness of the final responses. Experiments\nshow that Empathy-R1 achieves strong performance on key automatic metrics. More\nimportantly, human evaluations confirm its superiority, showing a clear\npreference over strong baselines and achieving a Win@1 rate of 44.30% on our\nnew benchmark. By enabling interpretable and contextually nuanced responses,\nEmpathy-R1 represents a significant advancement in developing responsible and\ngenuinely beneficial AI for mental health support."
                },
                "authors": [
                    {
                        "name": "Xianrong Yao"
                    },
                    {
                        "name": "Dong She"
                    },
                    {
                        "name": "Chenxu Zhang"
                    },
                    {
                        "name": "Yimeng Zhang"
                    },
                    {
                        "name": "Yueru Sun"
                    },
                    {
                        "name": "Noman Ahmed"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Zhanpeng Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhanpeng Jin"
                },
                "author": "Zhanpeng Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14850v1",
                "updated": "2025-09-18T11:15:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    15,
                    27,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T11:15:27Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    15,
                    27,
                    3,
                    261,
                    0
                ],
                "title": "Beam Squint Assisted Joint Angle-Distance Localization for Near-Field\n  Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beam Squint Assisted Joint Angle-Distance Localization for Near-Field\n  Communications"
                },
                "summary": "With the advent of extremely large-scale MIMO (XL-MIMO), mmWave/THz bands and\nultra-wideband transmission, future 6G systems demand real-time positioning\nwith centimeter or even millimeter level accuracy. This paper addresses the\npronounced near-field beam squint problem caused by phase shifter based\nbeamforming in wideband near-field scenarios and proposes a beam squint\nassisted joint angle-distance localization scheme. The key idea is to employ\ntrue-time-delay (TTD) units together with phase shifters (PS) to synthesize a\ncontrollable joint angle-distance (JAD) trajectory that establishes a unique\nmapping between subcarriers and spatial locations, enabling single scan\nacquisition of target angle and range. To implement this paradigm efficiently,\nwe design a coarse to fine two stage estimator: a low complexity coarse stage\nbased on subcarrier power peaks for user separation and candidate region\nselection, followed by a local high resolution refinement stage that applies\nspatial smoothing and near-field multiple signal classification (MUSIC) over\nmultiple subcarriers and fuses the resulting spectra by geometric averaging to\nsuppress spurious peaks. We theoretically prove the correctness and uniqueness\nof the MUSIC spatial spectrum peak under the proposed near-field steering\nmodel, and derive the Cram\\'er-Rao lower bound (CRLB) for joint angle-distance\nestimation. Simulation results in single and multi-user scenarios validate that\nthe proposed method achieves very high accuracy and robustness, significantly\noutperforming conventional two-step approaches, and is promising for practical\n6G sensing and localization deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of extremely large-scale MIMO (XL-MIMO), mmWave/THz bands and\nultra-wideband transmission, future 6G systems demand real-time positioning\nwith centimeter or even millimeter level accuracy. This paper addresses the\npronounced near-field beam squint problem caused by phase shifter based\nbeamforming in wideband near-field scenarios and proposes a beam squint\nassisted joint angle-distance localization scheme. The key idea is to employ\ntrue-time-delay (TTD) units together with phase shifters (PS) to synthesize a\ncontrollable joint angle-distance (JAD) trajectory that establishes a unique\nmapping between subcarriers and spatial locations, enabling single scan\nacquisition of target angle and range. To implement this paradigm efficiently,\nwe design a coarse to fine two stage estimator: a low complexity coarse stage\nbased on subcarrier power peaks for user separation and candidate region\nselection, followed by a local high resolution refinement stage that applies\nspatial smoothing and near-field multiple signal classification (MUSIC) over\nmultiple subcarriers and fuses the resulting spectra by geometric averaging to\nsuppress spurious peaks. We theoretically prove the correctness and uniqueness\nof the MUSIC spatial spectrum peak under the proposed near-field steering\nmodel, and derive the Cram\\'er-Rao lower bound (CRLB) for joint angle-distance\nestimation. Simulation results in single and multi-user scenarios validate that\nthe proposed method achieves very high accuracy and robustness, significantly\noutperforming conventional two-step approaches, and is promising for practical\n6G sensing and localization deployments."
                },
                "authors": [
                    {
                        "name": "Aibiao Zhang"
                    },
                    {
                        "name": "Weizheng Zhang"
                    },
                    {
                        "name": "Chiya Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chiya Zhang"
                },
                "author": "Chiya Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14844v1",
                "updated": "2025-09-18T11:10:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    10,
                    24,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T11:10:24Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    10,
                    24,
                    3,
                    261,
                    0
                ],
                "title": "Non-Intrusive Parametrized-Background Data-Weak Reconstruction of\n  Cardiac Displacement Fields from Sparse MRI-like Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-Intrusive Parametrized-Background Data-Weak Reconstruction of\n  Cardiac Displacement Fields from Sparse MRI-like Observations"
                },
                "summary": "Personalized cardiac diagnostics require accurate reconstruction of\nmyocardial displacement fields from sparse clinical imaging data, yet current\nmethods often demand intrusive access to computational models. In this work, we\napply the non-intrusive Parametrized-Background Data-Weak (PBDW) approach to\nthree-dimensional (3D) cardiac displacement field reconstruction from limited\nMagnetic Resonance Image (MRI)-like observations. Our implementation requires\nonly solution snapshots -- no governing equations, assembly routines, or solver\naccess -- enabling immediate deployment across commercial and research codes\nusing different constitutive models. Additionally, we introduce two\nenhancements: an H-size minibatch worst-case Orthogonal Matching Pursuit (wOMP)\nalgorithm that improves Sensor Selection (SS) computational efficiency while\nmaintaining reconstruction accuracy, and memory optimization techniques\nexploiting block matrix structures in vectorial problems. We demonstrate the\neffectiveness of the method through validation on a 3D left ventricular model\nwith simulated scar tissue. Starting with noise-free reconstruction, we\nsystematically incorporate Gaussian noise and spatial sparsity mimicking\nrealistic MRI acquisition protocols. Results show exceptional accuracy in\nnoise-free conditions (relative L2 error of order O(1e-5)), robust performance\nwith 10% noise (relative L2 error of order O(1e-2)), and effective\nreconstruction from sparse measurements (relative L2 error of order O(1e-2)).\nThe online reconstruction achieves four-order-of-magnitude computational\nspeed-up compared to full Finite Element (FE) simulations, with reconstruction\ntimes under one tenth of second for sparse scenarios, demonstrating significant\npotential for integration into clinical cardiac modeling workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized cardiac diagnostics require accurate reconstruction of\nmyocardial displacement fields from sparse clinical imaging data, yet current\nmethods often demand intrusive access to computational models. In this work, we\napply the non-intrusive Parametrized-Background Data-Weak (PBDW) approach to\nthree-dimensional (3D) cardiac displacement field reconstruction from limited\nMagnetic Resonance Image (MRI)-like observations. Our implementation requires\nonly solution snapshots -- no governing equations, assembly routines, or solver\naccess -- enabling immediate deployment across commercial and research codes\nusing different constitutive models. Additionally, we introduce two\nenhancements: an H-size minibatch worst-case Orthogonal Matching Pursuit (wOMP)\nalgorithm that improves Sensor Selection (SS) computational efficiency while\nmaintaining reconstruction accuracy, and memory optimization techniques\nexploiting block matrix structures in vectorial problems. We demonstrate the\neffectiveness of the method through validation on a 3D left ventricular model\nwith simulated scar tissue. Starting with noise-free reconstruction, we\nsystematically incorporate Gaussian noise and spatial sparsity mimicking\nrealistic MRI acquisition protocols. Results show exceptional accuracy in\nnoise-free conditions (relative L2 error of order O(1e-5)), robust performance\nwith 10% noise (relative L2 error of order O(1e-2)), and effective\nreconstruction from sparse measurements (relative L2 error of order O(1e-2)).\nThe online reconstruction achieves four-order-of-magnitude computational\nspeed-up compared to full Finite Element (FE) simulations, with reconstruction\ntimes under one tenth of second for sparse scenarios, demonstrating significant\npotential for integration into clinical cardiac modeling workflows."
                },
                "authors": [
                    {
                        "name": "Francesco C. Mantegazza"
                    },
                    {
                        "name": "Federica Caforio"
                    },
                    {
                        "name": "Christoph Augustin"
                    },
                    {
                        "name": "Matthias A. F. Gsell"
                    },
                    {
                        "name": "Gundolf Haase"
                    },
                    {
                        "name": "Elias Karabelas"
                    }
                ],
                "author_detail": {
                    "name": "Elias Karabelas"
                },
                "author": "Elias Karabelas",
                "arxiv_comment": "42 pages, 12 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65M60, 74L15, 92C10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.1.8; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14834v1",
                "updated": "2025-09-18T10:55:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    10,
                    55,
                    33,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T10:55:33Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    10,
                    55,
                    33,
                    3,
                    261,
                    0
                ],
                "title": "LLM Agents at the Roundtable: A Multi-Perspective and Dialectical\n  Reasoning Framework for Essay Scoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Agents at the Roundtable: A Multi-Perspective and Dialectical\n  Reasoning Framework for Essay Scoring"
                },
                "summary": "The emergence of large language models (LLMs) has brought a new paradigm to\nautomated essay scoring (AES), a long-standing and practical application of\nnatural language processing in education. However, achieving human-level\nmulti-perspective understanding and judgment remains a challenge. In this work,\nwe propose Roundtable Essay Scoring (RES), a multi-agent evaluation framework\ndesigned to perform precise and human-aligned scoring under a zero-shot\nsetting. RES constructs evaluator agents based on LLMs, each tailored to a\nspecific prompt and topic context. Each agent independently generates a\ntrait-based rubric and conducts a multi-perspective evaluation. Then, by\nsimulating a roundtable-style discussion, RES consolidates individual\nevaluations through a dialectical reasoning process to produce a final holistic\nscore that more closely aligns with human evaluation. By enabling collaboration\nand consensus among agents with diverse evaluation perspectives, RES\noutperforms prior zero-shot AES approaches. Experiments on the ASAP dataset\nusing ChatGPT and Claude show that RES achieves up to a 34.86% improvement in\naverage QWK over straightforward prompting (Vanilla) methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) has brought a new paradigm to\nautomated essay scoring (AES), a long-standing and practical application of\nnatural language processing in education. However, achieving human-level\nmulti-perspective understanding and judgment remains a challenge. In this work,\nwe propose Roundtable Essay Scoring (RES), a multi-agent evaluation framework\ndesigned to perform precise and human-aligned scoring under a zero-shot\nsetting. RES constructs evaluator agents based on LLMs, each tailored to a\nspecific prompt and topic context. Each agent independently generates a\ntrait-based rubric and conducts a multi-perspective evaluation. Then, by\nsimulating a roundtable-style discussion, RES consolidates individual\nevaluations through a dialectical reasoning process to produce a final holistic\nscore that more closely aligns with human evaluation. By enabling collaboration\nand consensus among agents with diverse evaluation perspectives, RES\noutperforms prior zero-shot AES approaches. Experiments on the ASAP dataset\nusing ChatGPT and Claude show that RES achieves up to a 34.86% improvement in\naverage QWK over straightforward prompting (Vanilla) methods."
                },
                "authors": [
                    {
                        "name": "Jinhee Jang"
                    },
                    {
                        "name": "Ayoung Moon"
                    },
                    {
                        "name": "Minkyoung Jung"
                    },
                    {
                        "name": "YoungBin Kim. Seung Jin Lee"
                    }
                ],
                "author_detail": {
                    "name": "YoungBin Kim. Seung Jin Lee"
                },
                "author": "YoungBin Kim. Seung Jin Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14829v1",
                "updated": "2025-09-18T10:45:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    10,
                    45,
                    26,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T10:45:26Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    10,
                    45,
                    26,
                    3,
                    261,
                    0
                ],
                "title": "RulER: Automated Rule-Based Semantic Error Localization and Repair for\n  Code Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RulER: Automated Rule-Based Semantic Error Localization and Repair for\n  Code Translation"
                },
                "summary": "Automated code translation aims to convert programs between different\nprogramming languages while maintaining their functionality. Due to the\nimperfections of code translation models, the generated translations may\ncontain errors that compromise their reliability. Existing automated debugging\nmethods for code translation rely on code alignments and repair patch templates\nto locate and fix erroneous translations. However, existing methods lack\nreliable references to construct code alignments and design repair patch\ntemplates, which significantly impacts their localization accuracy and repair\neffectiveness. To address these limitations, we reintroduce code translation\nrules and propose a rule-based debugging method for code translation, called\nRulER. RulER automatically derives code translation rules from correct\ntranslations generated by LLMs, enabling the efficient collection of diverse\ntranslation rules. In addition, RulER dynamically combines the existing rules\non expandable nodes like expressions and tokens to further adaptively align\nmore statements. These rules capture clear and detailed structural\ncorrespondences between source and target programming languages. Therefore,\nthey can serve as reliable and reusable references for code alignment and\nrepair template design, enabling RulER to locate and fix translation errors\neffectively. Our evaluation of RulER on Java-to-C++ and Python-to-C++\ntranslations produced by four code translation models demonstrates that RulER\noutperforms state-of-the-art methods, BatFix and TransMap. Our experimental\nresults show that RulER outperformed the best baseline by 20% and 272% in terms\nof error localization rates and repair success rates, respectively. RulER\nexhibits superior repair performance compared to directly prompting LLMs for\npatch generation, demonstrating a promising methodology for extracting and\nleveraging coding knowledge from LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated code translation aims to convert programs between different\nprogramming languages while maintaining their functionality. Due to the\nimperfections of code translation models, the generated translations may\ncontain errors that compromise their reliability. Existing automated debugging\nmethods for code translation rely on code alignments and repair patch templates\nto locate and fix erroneous translations. However, existing methods lack\nreliable references to construct code alignments and design repair patch\ntemplates, which significantly impacts their localization accuracy and repair\neffectiveness. To address these limitations, we reintroduce code translation\nrules and propose a rule-based debugging method for code translation, called\nRulER. RulER automatically derives code translation rules from correct\ntranslations generated by LLMs, enabling the efficient collection of diverse\ntranslation rules. In addition, RulER dynamically combines the existing rules\non expandable nodes like expressions and tokens to further adaptively align\nmore statements. These rules capture clear and detailed structural\ncorrespondences between source and target programming languages. Therefore,\nthey can serve as reliable and reusable references for code alignment and\nrepair template design, enabling RulER to locate and fix translation errors\neffectively. Our evaluation of RulER on Java-to-C++ and Python-to-C++\ntranslations produced by four code translation models demonstrates that RulER\noutperforms state-of-the-art methods, BatFix and TransMap. Our experimental\nresults show that RulER outperformed the best baseline by 20% and 272% in terms\nof error localization rates and repair success rates, respectively. RulER\nexhibits superior repair performance compared to directly prompting LLMs for\npatch generation, demonstrating a promising methodology for extracting and\nleveraging coding knowledge from LLMs."
                },
                "authors": [
                    {
                        "name": "Shuo Jin"
                    },
                    {
                        "name": "Songqiang Chen"
                    },
                    {
                        "name": "Xiaoyuan Xie"
                    },
                    {
                        "name": "Shing-Chi Cheung"
                    }
                ],
                "author_detail": {
                    "name": "Shing-Chi Cheung"
                },
                "author": "Shing-Chi Cheung",
                "arxiv_comment": "Due to the limitation \"The abstract field cannot be longer than 1,920\n  characters\", the abstract here is shorter than that in the PDF file",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14824v1",
                "updated": "2025-09-18T10:32:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    10,
                    32,
                    52,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T10:32:52Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    10,
                    32,
                    52,
                    3,
                    261,
                    0
                ],
                "title": "Confirmation Bias as a Cognitive Resource in LLM-Supported Deliberation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confirmation Bias as a Cognitive Resource in LLM-Supported Deliberation"
                },
                "summary": "Large language models (LLMs) are increasingly used in group decision-making,\nbut their influence risks fostering conformity and reducing epistemic\nvigilance. Drawing on the Argumentative Theory of Reasoning, we argue that\nconfirmation bias, often seen as detrimental, can be harnessed as a resource\nwhen paired with critical evaluation. We propose a three-step process in which\nindividuals first generate ideas independently, then use LLMs to refine and\narticulate them, and finally engage with LLMs as epistemic provocateurs to\nanticipate group critique. This framing positions LLMs as tools for scaffolding\ndisagreement, helping individuals prepare for more productive group\ndiscussions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used in group decision-making,\nbut their influence risks fostering conformity and reducing epistemic\nvigilance. Drawing on the Argumentative Theory of Reasoning, we argue that\nconfirmation bias, often seen as detrimental, can be harnessed as a resource\nwhen paired with critical evaluation. We propose a three-step process in which\nindividuals first generate ideas independently, then use LLMs to refine and\narticulate them, and finally engage with LLMs as epistemic provocateurs to\nanticipate group critique. This framing positions LLMs as tools for scaffolding\ndisagreement, helping individuals prepare for more productive group\ndiscussions."
                },
                "authors": [
                    {
                        "name": "Sander de Jong"
                    },
                    {
                        "name": "Rune Møberg Jacobsen"
                    },
                    {
                        "name": "Niels van Berkel"
                    }
                ],
                "author_detail": {
                    "name": "Niels van Berkel"
                },
                "author": "Niels van Berkel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14814v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14814v1",
                "updated": "2025-09-18T10:15:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    10,
                    15,
                    52,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T10:15:52Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    10,
                    15,
                    52,
                    3,
                    261,
                    0
                ],
                "title": "ReCoVeR the Target Language: Language Steering without Sacrificing Task\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReCoVeR the Target Language: Language Steering without Sacrificing Task\n  Performance"
                },
                "summary": "As they become increasingly multilingual, Large Language Models (LLMs)\nexhibit more language confusion, i.e., they tend to generate answers in a\nlanguage different from the language of the prompt or the answer language\nexplicitly requested by the user. In this work, we propose ReCoVeR (REducing\nlanguage COnfusion in VEctor Representations), a novel lightweight approach for\nreducing language confusion based on language-specific steering vectors. We\nfirst isolate language vectors with the help of multi-parallel corpus and then\neffectively leverage those vectors for effective LLM steering via fixed (i.e.,\nunsupervised) as well as trainable steering functions. Our extensive\nevaluation, encompassing three benchmarks and 18 languages, shows that ReCoVeR\neffectively mitigates language confusion in both monolingual and cross-lingual\nsetups while at the same time -- and in contrast to prior language steering\nmethods -- retaining task performance. Our data code is available at\nhttps://github.com/hSterz/recover.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As they become increasingly multilingual, Large Language Models (LLMs)\nexhibit more language confusion, i.e., they tend to generate answers in a\nlanguage different from the language of the prompt or the answer language\nexplicitly requested by the user. In this work, we propose ReCoVeR (REducing\nlanguage COnfusion in VEctor Representations), a novel lightweight approach for\nreducing language confusion based on language-specific steering vectors. We\nfirst isolate language vectors with the help of multi-parallel corpus and then\neffectively leverage those vectors for effective LLM steering via fixed (i.e.,\nunsupervised) as well as trainable steering functions. Our extensive\nevaluation, encompassing three benchmarks and 18 languages, shows that ReCoVeR\neffectively mitigates language confusion in both monolingual and cross-lingual\nsetups while at the same time -- and in contrast to prior language steering\nmethods -- retaining task performance. Our data code is available at\nhttps://github.com/hSterz/recover."
                },
                "authors": [
                    {
                        "name": "Hannah Sterz"
                    },
                    {
                        "name": "Fabian David Schmidt"
                    },
                    {
                        "name": "Goran Glavaš"
                    },
                    {
                        "name": "Ivan Vulić"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Vulić"
                },
                "author": "Ivan Vulić",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14814v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14809v1",
                "updated": "2025-09-18T10:09:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    10,
                    9,
                    2,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T10:09:02Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    10,
                    9,
                    2,
                    3,
                    261,
                    0
                ],
                "title": "Comparative Performance Analysis of Different Hybrid NOMA Schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Performance Analysis of Different Hybrid NOMA Schemes"
                },
                "summary": "Hybrid non-orthogonal multiple access (H-NOMA), which combines the advantages\nof pure NOMA and conventional OMA organically, has emerged as a highly\npromising multiple access technology for future wireless networks. Recent\nstudies have proposed various H-NOMA systems by employing different successive\ninterference cancellation (SIC) methods for the NOMA transmission phase.\nHowever, existing analyses typically assume a fixed channel gain order between\npaired users, despite the fact that channel coefficients follow random\ndistribution, leading to their magnitude relationships inherently stochastic\nand time varying. This paper analyzes the performance of three H-NOMA schemes\nunder stochastic channel gain ordering: a) fixed order SIC (FSIC) aided H-NOMA\nscheme; b) hybrid SIC with non-power adaptation (HSIC-NPA) aided H-NOMA scheme;\nc) hybrid SIC with power adaptation (HSIC-PA) aided H-NOMA scheme. Theoretical\nanalysis derives closed-form expressions for the probability that H-NOMA\nschemes underperform conventional OMA. Asymptotic results in the high\nsignal-to-noise ratio (SNR) regime are also developed. Simulation results\nvalidate our analysis and demonstrate the performance of H-NOMA schemes across\ndifferent SNR scenarios, providing a theoretical foundation for the deployment\nof H-NOMA in next-generation wireless systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid non-orthogonal multiple access (H-NOMA), which combines the advantages\nof pure NOMA and conventional OMA organically, has emerged as a highly\npromising multiple access technology for future wireless networks. Recent\nstudies have proposed various H-NOMA systems by employing different successive\ninterference cancellation (SIC) methods for the NOMA transmission phase.\nHowever, existing analyses typically assume a fixed channel gain order between\npaired users, despite the fact that channel coefficients follow random\ndistribution, leading to their magnitude relationships inherently stochastic\nand time varying. This paper analyzes the performance of three H-NOMA schemes\nunder stochastic channel gain ordering: a) fixed order SIC (FSIC) aided H-NOMA\nscheme; b) hybrid SIC with non-power adaptation (HSIC-NPA) aided H-NOMA scheme;\nc) hybrid SIC with power adaptation (HSIC-PA) aided H-NOMA scheme. Theoretical\nanalysis derives closed-form expressions for the probability that H-NOMA\nschemes underperform conventional OMA. Asymptotic results in the high\nsignal-to-noise ratio (SNR) regime are also developed. Simulation results\nvalidate our analysis and demonstrate the performance of H-NOMA schemes across\ndifferent SNR scenarios, providing a theoretical foundation for the deployment\nof H-NOMA in next-generation wireless systems."
                },
                "authors": [
                    {
                        "name": "Ning Wang"
                    },
                    {
                        "name": "Chenyu Zhang"
                    },
                    {
                        "name": "Yanshi Sun"
                    },
                    {
                        "name": "Minghui Min"
                    },
                    {
                        "name": "Shiyin Li"
                    }
                ],
                "author_detail": {
                    "name": "Shiyin Li"
                },
                "author": "Shiyin Li",
                "arxiv_comment": "9 pages, 6 figures. Paper submitted to IEEE Internet of Things\n  Journal, paper ID IoT-55019-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14804v1",
                "updated": "2025-09-18T09:59:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    9,
                    59,
                    55,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T09:59:55Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    9,
                    59,
                    55,
                    3,
                    261,
                    0
                ],
                "title": "Towards Building Speech Large Language Models for Multitask\n  Understanding in Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Building Speech Large Language Models for Multitask\n  Understanding in Low-Resource Languages"
                },
                "summary": "Speech large language models (SLLMs) built on speech encoders, adapters, and\nLLMs demonstrate remarkable multitask understanding performance in\nhigh-resource languages such as English and Chinese. However, their\neffectiveness substantially degrades in low-resource languages such as Thai.\nThis limitation arises from three factors: (1) existing commonly used speech\nencoders, like the Whisper family, underperform in low-resource languages and\nlack support for broader spoken language understanding tasks; (2) the ASR-based\nalignment paradigm requires training the entire SLLM, leading to high\ncomputational cost; (3) paired speech-text data in low-resource languages is\nscarce. To overcome these challenges in the low-resource language Thai, we\nintroduce XLSR-Thai, the first self-supervised learning (SSL) speech encoder\nfor Thai. It is obtained by continuously training the standard SSL XLSR model\non 36,000 hours of Thai speech data. Furthermore, we propose U-Align, a\nspeech-text alignment method that is more resource-efficient and\nmultitask-effective than typical ASR-based alignment. Finally, we present\nThai-SUP, a pipeline for generating Thai spoken language understanding data\nfrom high-resource languages, yielding the first Thai spoken language\nunderstanding dataset of over 1,000 hours. Multiple experiments demonstrate the\neffectiveness of our methods in building a Thai multitask-understanding SLLM.\nWe open-source XLSR-Thai and Thai-SUP to facilitate future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech large language models (SLLMs) built on speech encoders, adapters, and\nLLMs demonstrate remarkable multitask understanding performance in\nhigh-resource languages such as English and Chinese. However, their\neffectiveness substantially degrades in low-resource languages such as Thai.\nThis limitation arises from three factors: (1) existing commonly used speech\nencoders, like the Whisper family, underperform in low-resource languages and\nlack support for broader spoken language understanding tasks; (2) the ASR-based\nalignment paradigm requires training the entire SLLM, leading to high\ncomputational cost; (3) paired speech-text data in low-resource languages is\nscarce. To overcome these challenges in the low-resource language Thai, we\nintroduce XLSR-Thai, the first self-supervised learning (SSL) speech encoder\nfor Thai. It is obtained by continuously training the standard SSL XLSR model\non 36,000 hours of Thai speech data. Furthermore, we propose U-Align, a\nspeech-text alignment method that is more resource-efficient and\nmultitask-effective than typical ASR-based alignment. Finally, we present\nThai-SUP, a pipeline for generating Thai spoken language understanding data\nfrom high-resource languages, yielding the first Thai spoken language\nunderstanding dataset of over 1,000 hours. Multiple experiments demonstrate the\neffectiveness of our methods in building a Thai multitask-understanding SLLM.\nWe open-source XLSR-Thai and Thai-SUP to facilitate future research."
                },
                "authors": [
                    {
                        "name": "Mingchen Shao"
                    },
                    {
                        "name": "Bingshen Mu"
                    },
                    {
                        "name": "Chengyou Wang"
                    },
                    {
                        "name": "Hai Li"
                    },
                    {
                        "name": "Ying Yan"
                    },
                    {
                        "name": "Zhonghua Fu"
                    },
                    {
                        "name": "Lei Xie"
                    }
                ],
                "author_detail": {
                    "name": "Lei Xie"
                },
                "author": "Lei Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14803v1",
                "updated": "2025-09-18T09:56:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    9,
                    56,
                    45,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T09:56:45Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    9,
                    56,
                    45,
                    3,
                    261,
                    0
                ],
                "title": "OnlineMate: An LLM-Based Multi-Agent Companion System for Cognitive\n  Support in Online Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OnlineMate: An LLM-Based Multi-Agent Companion System for Cognitive\n  Support in Online Learning"
                },
                "summary": "In online learning environments, students often lack personalized peer\ninteractions, which play a crucial role in supporting cognitive development and\nlearning engagement. Although previous studies have utilized large language\nmodels (LLMs) to simulate interactive dynamic learning environments for\nstudents, these interactions remain limited to conversational exchanges,\nlacking insights and adaptations to the learners' individualized learning and\ncognitive states. As a result, students' interest in discussions with AI\nlearning companions is low, and they struggle to gain inspiration from such\ninteractions. To address this challenge, we propose OnlineMate, a multi-agent\nlearning companion system driven by LLMs that integrates the Theory of Mind\n(ToM). OnlineMate is capable of simulating peer-like agent roles, adapting to\nlearners' cognitive states during collaborative discussions, and inferring\ntheir psychological states, such as misunderstandings, confusion, or\nmotivation. By incorporating Theory of Mind capabilities, the system can\ndynamically adjust its interaction strategies to support the development of\nhigher-order thinking and cognition. Experimental results in simulated learning\nscenarios demonstrate that OnlineMate effectively fosters deep learning and\ndiscussions while enhancing cognitive engagement in online educational\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In online learning environments, students often lack personalized peer\ninteractions, which play a crucial role in supporting cognitive development and\nlearning engagement. Although previous studies have utilized large language\nmodels (LLMs) to simulate interactive dynamic learning environments for\nstudents, these interactions remain limited to conversational exchanges,\nlacking insights and adaptations to the learners' individualized learning and\ncognitive states. As a result, students' interest in discussions with AI\nlearning companions is low, and they struggle to gain inspiration from such\ninteractions. To address this challenge, we propose OnlineMate, a multi-agent\nlearning companion system driven by LLMs that integrates the Theory of Mind\n(ToM). OnlineMate is capable of simulating peer-like agent roles, adapting to\nlearners' cognitive states during collaborative discussions, and inferring\ntheir psychological states, such as misunderstandings, confusion, or\nmotivation. By incorporating Theory of Mind capabilities, the system can\ndynamically adjust its interaction strategies to support the development of\nhigher-order thinking and cognition. Experimental results in simulated learning\nscenarios demonstrate that OnlineMate effectively fosters deep learning and\ndiscussions while enhancing cognitive engagement in online educational\nsettings."
                },
                "authors": [
                    {
                        "name": "Xian Gao"
                    },
                    {
                        "name": "Zongyun Zhang"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Yuzhuo Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhuo Fu"
                },
                "author": "Yuzhuo Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12536v2",
                "updated": "2025-09-18T09:46:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    9,
                    46,
                    4,
                    3,
                    261,
                    0
                ],
                "published": "2025-08-18T00:14:24Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    0,
                    14,
                    24,
                    0,
                    230,
                    0
                ],
                "title": "jXBW: Fast Substructure Search for Large-Scale JSONL Datasets with LLM\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "jXBW: Fast Substructure Search for Large-Scale JSONL Datasets with LLM\n  Applications"
                },
                "summary": "JSON Lines (JSONL) is widely used for managing large collections of\nsemi-structured data, ranging from large language model (LLM) prompts to\nchemical compound records and geospatial datasets. A key operation is\nsubstructure search, which identifies all JSON objects containing a query\npattern. This task underpins applications such as drug discovery (querying\ncompounds for functional groups), prompt engineering (extracting prompts with\nschema fragments), and geospatial analytics (finding entities with nested\nattributes). However, existing methods are inefficient: traversal requires\nexhaustive tree matching, succinct JSON representations save space but do not\naccelerate search, and XML-based approaches incur conversion overhead and\nsemantic mismatches. We present jXBW, a compressed index for efficient\nsubstructure search over JSONL. jXBW introduces three innovations: (i) a merged\ntree representation that consolidates repeated structures, (ii) a succinct tree\nindex based on the eXtended Burrows--Wheeler Transform (XBW), and (iii) a\nthree-phase algorithm for substructure search. These enable query-dependent\ncomplexity, where cost depends on query characteristics rather than dataset\nsize, while retaining succinct space. This resolves a key bottleneck in\nretrieval-augmented generation (RAG) systems requiring structure-aware\nretrieval. Experiments on seven real datasets, including PubChem (1M compounds)\nand OSM geospatial data (6.6M objects), achieve up to 4,700$\\times$ speedup\nover tree-based methods and over $6\\times 10^6$ speedup relative to XML-based\napproaches. jXBW makes JSONL substructure search practical for the first time,\nopening opportunities for large-scale LLM-based analytics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JSON Lines (JSONL) is widely used for managing large collections of\nsemi-structured data, ranging from large language model (LLM) prompts to\nchemical compound records and geospatial datasets. A key operation is\nsubstructure search, which identifies all JSON objects containing a query\npattern. This task underpins applications such as drug discovery (querying\ncompounds for functional groups), prompt engineering (extracting prompts with\nschema fragments), and geospatial analytics (finding entities with nested\nattributes). However, existing methods are inefficient: traversal requires\nexhaustive tree matching, succinct JSON representations save space but do not\naccelerate search, and XML-based approaches incur conversion overhead and\nsemantic mismatches. We present jXBW, a compressed index for efficient\nsubstructure search over JSONL. jXBW introduces three innovations: (i) a merged\ntree representation that consolidates repeated structures, (ii) a succinct tree\nindex based on the eXtended Burrows--Wheeler Transform (XBW), and (iii) a\nthree-phase algorithm for substructure search. These enable query-dependent\ncomplexity, where cost depends on query characteristics rather than dataset\nsize, while retaining succinct space. This resolves a key bottleneck in\nretrieval-augmented generation (RAG) systems requiring structure-aware\nretrieval. Experiments on seven real datasets, including PubChem (1M compounds)\nand OSM geospatial data (6.6M objects), achieve up to 4,700$\\times$ speedup\nover tree-based methods and over $6\\times 10^6$ speedup relative to XML-based\napproaches. jXBW makes JSONL substructure search practical for the first time,\nopening opportunities for large-scale LLM-based analytics."
                },
                "authors": [
                    {
                        "name": "Yasuo Tabei"
                    }
                ],
                "author_detail": {
                    "name": "Yasuo Tabei"
                },
                "author": "Yasuo Tabei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19800v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19800v2",
                "updated": "2025-09-18T09:38:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    9,
                    38,
                    31,
                    3,
                    261,
                    0
                ],
                "published": "2025-05-26T10:31:26Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    10,
                    31,
                    26,
                    0,
                    146,
                    0
                ],
                "title": "MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs"
                },
                "summary": "Metadata extraction is essential for cataloging and preserving datasets,\nenabling effective research discovery and reproducibility, especially given the\ncurrent exponential growth in scientific research. While Masader (Alyafeai et\nal.,2021) laid the groundwork for extracting a wide range of metadata\nattributes from Arabic NLP datasets' scholarly articles, it relies heavily on\nmanual annotation. In this paper, we present MOLE, a framework that leverages\nLarge Language Models (LLMs) to automatically extract metadata attributes from\nscientific papers covering datasets of languages other than Arabic. Our\nschema-driven methodology processes entire documents across multiple input\nformats and incorporates robust validation mechanisms for consistent output.\nAdditionally, we introduce a new benchmark to evaluate the research progress on\nthis task. Through systematic analysis of context length, few-shot learning,\nand web browsing integration, we demonstrate that modern LLMs show promising\nresults in automating this task, highlighting the need for further future work\nimprovements to ensure consistent and reliable performance. We release the\ncode: https://github.com/IVUL-KAUST/MOLE and dataset:\nhttps://huggingface.co/datasets/IVUL-KAUST/MOLE for the research community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metadata extraction is essential for cataloging and preserving datasets,\nenabling effective research discovery and reproducibility, especially given the\ncurrent exponential growth in scientific research. While Masader (Alyafeai et\nal.,2021) laid the groundwork for extracting a wide range of metadata\nattributes from Arabic NLP datasets' scholarly articles, it relies heavily on\nmanual annotation. In this paper, we present MOLE, a framework that leverages\nLarge Language Models (LLMs) to automatically extract metadata attributes from\nscientific papers covering datasets of languages other than Arabic. Our\nschema-driven methodology processes entire documents across multiple input\nformats and incorporates robust validation mechanisms for consistent output.\nAdditionally, we introduce a new benchmark to evaluate the research progress on\nthis task. Through systematic analysis of context length, few-shot learning,\nand web browsing integration, we demonstrate that modern LLMs show promising\nresults in automating this task, highlighting the need for further future work\nimprovements to ensure consistent and reliable performance. We release the\ncode: https://github.com/IVUL-KAUST/MOLE and dataset:\nhttps://huggingface.co/datasets/IVUL-KAUST/MOLE for the research community."
                },
                "authors": [
                    {
                        "name": "Zaid Alyafeai"
                    },
                    {
                        "name": "Maged S. Al-Shaibani"
                    },
                    {
                        "name": "Bernard Ghanem"
                    }
                ],
                "author_detail": {
                    "name": "Bernard Ghanem"
                },
                "author": "Bernard Ghanem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19800v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14781v1",
                "updated": "2025-09-18T09:34:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    9,
                    34,
                    5,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T09:34:05Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    9,
                    34,
                    5,
                    3,
                    261,
                    0
                ],
                "title": "LEAP: LLM Inference on Scalable PIM-NoC Architecture with Balanced\n  Dataflow and Fine-Grained Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEAP: LLM Inference on Scalable PIM-NoC Architecture with Balanced\n  Dataflow and Fine-Grained Parallelism"
                },
                "summary": "Large language model (LLM) inference has been a prevalent demand in daily\nlife and industries. The large tensor sizes and computing complexities in LLMs\nhave brought challenges to memory, computing, and databus. This paper proposes\na computation/memory/communication co-designed non-von Neumann accelerator by\naggregating processing-in-memory (PIM) and computational network-on-chip (NoC),\ntermed LEAP. The matrix multiplications in LLMs are assigned to PIM or NoC\nbased on the data dynamicity to maximize data locality. Model partition and\nmapping are optimized by heuristic design space exploration. Dedicated\nfine-grained parallelism and tiling techniques enable high-throughput dataflow\nacross the distributed resources in PIM and NoC. The architecture is evaluated\non Llama 1B/8B/13B models and shows $\\sim$2.55$\\times$ throughput (tokens/sec)\nimprovement and $\\sim$71.94$\\times$ energy efficiency (tokens/Joule) boost\ncompared to the A100 GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference has been a prevalent demand in daily\nlife and industries. The large tensor sizes and computing complexities in LLMs\nhave brought challenges to memory, computing, and databus. This paper proposes\na computation/memory/communication co-designed non-von Neumann accelerator by\naggregating processing-in-memory (PIM) and computational network-on-chip (NoC),\ntermed LEAP. The matrix multiplications in LLMs are assigned to PIM or NoC\nbased on the data dynamicity to maximize data locality. Model partition and\nmapping are optimized by heuristic design space exploration. Dedicated\nfine-grained parallelism and tiling techniques enable high-throughput dataflow\nacross the distributed resources in PIM and NoC. The architecture is evaluated\non Llama 1B/8B/13B models and shows $\\sim$2.55$\\times$ throughput (tokens/sec)\nimprovement and $\\sim$71.94$\\times$ energy efficiency (tokens/Joule) boost\ncompared to the A100 GPU."
                },
                "authors": [
                    {
                        "name": "Yimin Wang"
                    },
                    {
                        "name": "Yue Jiet Chong"
                    },
                    {
                        "name": "Xuanyao Fong"
                    }
                ],
                "author_detail": {
                    "name": "Xuanyao Fong"
                },
                "author": "Xuanyao Fong",
                "arxiv_comment": "Accepted to the 2025 International Conference on Computer-Aided\n  Design (ICCAD'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01513v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01513v3",
                "updated": "2025-09-18T09:26:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    9,
                    26,
                    24,
                    3,
                    261,
                    0
                ],
                "published": "2025-03-03T13:26:01Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    13,
                    26,
                    1,
                    0,
                    62,
                    0
                ],
                "title": "Evaluation and Facilitation of Online Discussions in the LLM Era: A\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation and Facilitation of Online Discussions in the LLM Era: A\n  Survey"
                },
                "summary": "We present a survey of methods for assessing and enhancing the quality of\nonline discussions, focusing on the potential of LLMs. While online discourses\naim, at least in theory, to foster mutual understanding, they often devolve\ninto harmful exchanges, such as hate speech, threatening social cohesion and\ndemocratic values. Recent advancements in LLMs enable artificial facilitation\nagents to not only moderate content, but also actively improve the quality of\ninteractions. Our survey synthesizes ideas from NLP and Social Sciences to\nprovide (a) a new taxonomy on discussion quality evaluation, (b) an overview of\nintervention and facilitation strategies, (c) along with a new taxonomy of\nconversation facilitation datasets, (d) an LLM-oriented roadmap of good\npractices and future research directions, from technological and societal\nperspectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a survey of methods for assessing and enhancing the quality of\nonline discussions, focusing on the potential of LLMs. While online discourses\naim, at least in theory, to foster mutual understanding, they often devolve\ninto harmful exchanges, such as hate speech, threatening social cohesion and\ndemocratic values. Recent advancements in LLMs enable artificial facilitation\nagents to not only moderate content, but also actively improve the quality of\ninteractions. Our survey synthesizes ideas from NLP and Social Sciences to\nprovide (a) a new taxonomy on discussion quality evaluation, (b) an overview of\nintervention and facilitation strategies, (c) along with a new taxonomy of\nconversation facilitation datasets, (d) an LLM-oriented roadmap of good\npractices and future research directions, from technological and societal\nperspectives."
                },
                "authors": [
                    {
                        "name": "Katerina Korre"
                    },
                    {
                        "name": "Dimitris Tsirmpas"
                    },
                    {
                        "name": "Nikos Gkoumas"
                    },
                    {
                        "name": "Emma Cabalé"
                    },
                    {
                        "name": "Danai Myrtzani"
                    },
                    {
                        "name": "Theodoros Evgeniou"
                    },
                    {
                        "name": "Ion Androutsopoulos"
                    },
                    {
                        "name": "John Pavlopoulos"
                    }
                ],
                "author_detail": {
                    "name": "John Pavlopoulos"
                },
                "author": "John Pavlopoulos",
                "arxiv_comment": "To appear in EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01513v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01513v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14778v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14778v1",
                "updated": "2025-09-18T09:25:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    9,
                    25,
                    57,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T09:25:57Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    9,
                    25,
                    57,
                    3,
                    261,
                    0
                ],
                "title": "OpenLens AI: Fully Autonomous Research Agent for Health Infomatics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenLens AI: Fully Autonomous Research Agent for Health Infomatics"
                },
                "summary": "Health informatics research is characterized by diverse data modalities,\nrapid knowledge expansion, and the need to integrate insights across biomedical\nscience, data analytics, and clinical practice. These characteristics make it\nparticularly well-suited for agent-based approaches that can automate knowledge\nexploration, manage complex workflows, and generate clinically meaningful\noutputs. Recent progress in large language model (LLM)-based agents has\ndemonstrated promising capabilities in literature synthesis, data analysis, and\neven end-to-end research execution. However, existing systems remain limited\nfor health informatics because they lack mechanisms to interpret medical\nvisualizations and often overlook domain-specific quality requirements. To\naddress these gaps, we introduce OpenLens AI, a fully automated framework\ntailored to health informatics. OpenLens AI integrates specialized agents for\nliterature review, data analysis, code generation, and manuscript preparation,\nenhanced by vision-language feedback for medical visualization and quality\ncontrol for reproducibility. The framework automates the entire research\npipeline, producing publication-ready LaTeX manuscripts with transparent and\ntraceable workflows, thereby offering a domain-adapted solution for advancing\nhealth informatics research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Health informatics research is characterized by diverse data modalities,\nrapid knowledge expansion, and the need to integrate insights across biomedical\nscience, data analytics, and clinical practice. These characteristics make it\nparticularly well-suited for agent-based approaches that can automate knowledge\nexploration, manage complex workflows, and generate clinically meaningful\noutputs. Recent progress in large language model (LLM)-based agents has\ndemonstrated promising capabilities in literature synthesis, data analysis, and\neven end-to-end research execution. However, existing systems remain limited\nfor health informatics because they lack mechanisms to interpret medical\nvisualizations and often overlook domain-specific quality requirements. To\naddress these gaps, we introduce OpenLens AI, a fully automated framework\ntailored to health informatics. OpenLens AI integrates specialized agents for\nliterature review, data analysis, code generation, and manuscript preparation,\nenhanced by vision-language feedback for medical visualization and quality\ncontrol for reproducibility. The framework automates the entire research\npipeline, producing publication-ready LaTeX manuscripts with transparent and\ntraceable workflows, thereby offering a domain-adapted solution for advancing\nhealth informatics research."
                },
                "authors": [
                    {
                        "name": "Yuxiao Cheng"
                    },
                    {
                        "name": "Jinli Suo"
                    }
                ],
                "author_detail": {
                    "name": "Jinli Suo"
                },
                "author": "Jinli Suo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14778v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14778v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21830v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21830v4",
                "updated": "2025-09-18T09:23:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    9,
                    23,
                    30,
                    3,
                    261,
                    0
                ],
                "published": "2025-07-29T14:08:09Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    14,
                    8,
                    9,
                    1,
                    210,
                    0
                ],
                "title": "DualSG: A Dual-Stream Explicit Semantic-Guided Multivariate Time Series\n  Forecasting Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DualSG: A Dual-Stream Explicit Semantic-Guided Multivariate Time Series\n  Forecasting Framework"
                },
                "summary": "Multivariate Time Series Forecasting plays a key role in many applications.\nRecent works have explored using Large Language Models for MTSF to take\nadvantage of their reasoning abilities. However, many methods treat LLMs as\nend-to-end forecasters, which often leads to a loss of numerical precision and\nforces LLMs to handle patterns beyond their intended design. Alternatively,\nmethods that attempt to align textual and time series modalities within latent\nspace frequently encounter alignment difficulty. In this paper, we propose to\ntreat LLMs not as standalone forecasters, but as semantic guidance modules\nwithin a dual-stream framework. We propose DualSG, a dual-stream framework that\nprovides explicit semantic guidance, where LLMs act as Semantic Guides to\nrefine rather than replace traditional predictions. As part of DualSG, we\nintroduce Time Series Caption, an explicit prompt format that summarizes trend\npatterns in natural language and provides interpretable context for LLMs,\nrather than relying on implicit alignment between text and time series in the\nlatent space. We also design a caption-guided fusion module that explicitly\nmodels inter-variable relationships while reducing noise and computation.\nExperiments on real-world datasets from diverse domains show that DualSG\nconsistently outperforms 15 state-of-the-art baselines, demonstrating the value\nof explicitly combining numerical forecasting with semantic guidance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate Time Series Forecasting plays a key role in many applications.\nRecent works have explored using Large Language Models for MTSF to take\nadvantage of their reasoning abilities. However, many methods treat LLMs as\nend-to-end forecasters, which often leads to a loss of numerical precision and\nforces LLMs to handle patterns beyond their intended design. Alternatively,\nmethods that attempt to align textual and time series modalities within latent\nspace frequently encounter alignment difficulty. In this paper, we propose to\ntreat LLMs not as standalone forecasters, but as semantic guidance modules\nwithin a dual-stream framework. We propose DualSG, a dual-stream framework that\nprovides explicit semantic guidance, where LLMs act as Semantic Guides to\nrefine rather than replace traditional predictions. As part of DualSG, we\nintroduce Time Series Caption, an explicit prompt format that summarizes trend\npatterns in natural language and provides interpretable context for LLMs,\nrather than relying on implicit alignment between text and time series in the\nlatent space. We also design a caption-guided fusion module that explicitly\nmodels inter-variable relationships while reducing noise and computation.\nExperiments on real-world datasets from diverse domains show that DualSG\nconsistently outperforms 15 state-of-the-art baselines, demonstrating the value\nof explicitly combining numerical forecasting with semantic guidance."
                },
                "authors": [
                    {
                        "name": "Kuiye Ding"
                    },
                    {
                        "name": "Fanda Fan"
                    },
                    {
                        "name": "Yao Wang"
                    },
                    {
                        "name": "Ruijie jian"
                    },
                    {
                        "name": "Xiaorui Wang"
                    },
                    {
                        "name": "Luqi Gong"
                    },
                    {
                        "name": "Yishan Jiang"
                    },
                    {
                        "name": "Chunjie Luo"
                    },
                    {
                        "name": "Jianfeng Zhan"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Zhan"
                },
                "author": "Jianfeng Zhan",
                "arxiv_doi": "10.1145/3746027.3755458",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746027.3755458",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.21830v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21830v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper has been accepted by ACM Multimedia 2025 (ACM MM 2025)",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14760v1",
                "updated": "2025-09-18T09:08:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    9,
                    8,
                    53,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T09:08:53Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    9,
                    8,
                    53,
                    3,
                    261,
                    0
                ],
                "title": "Reasoning over Boundaries: Enhancing Specification Alignment via\n  Test-time Delibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning over Boundaries: Enhancing Specification Alignment via\n  Test-time Delibration"
                },
                "summary": "Large language models (LLMs) are increasingly applied in diverse real-world\nscenarios, each governed by bespoke behavioral and safety specifications (spec)\ncustom-tailored by users or organizations. These spec, categorized into\nsafety-spec and behavioral-spec, vary across scenarios and evolve with changing\npreferences and requirements. We formalize this challenge as specification\nalignment, focusing on LLMs' ability to follow dynamic, scenario-specific spec\nfrom both behavioral and safety perspectives. To address this challenge, we\npropose Align3, a lightweight method that employs Test-Time Deliberation (TTD)\nwith hierarchical reflection and revision to reason over the specification\nboundaries. We further present SpecBench, a unified benchmark for measuring\nspecification alignment, covering 5 scenarios, 103 spec, and 1,500 prompts.\nExperiments on 15 reasoning and 18 instruct models with several TTD methods,\nincluding Self-Refine, TPO, and MoreThink, yield three key findings: (i)\ntest-time deliberation enhances specification alignment; (ii) Align3 advances\nthe safety-helpfulness trade-off frontier with minimal overhead; (iii)\nSpecBench effectively reveals alignment gaps. These results highlight the\npotential of test-time deliberation as an effective strategy for reasoning over\nthe real-world specification boundaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly applied in diverse real-world\nscenarios, each governed by bespoke behavioral and safety specifications (spec)\ncustom-tailored by users or organizations. These spec, categorized into\nsafety-spec and behavioral-spec, vary across scenarios and evolve with changing\npreferences and requirements. We formalize this challenge as specification\nalignment, focusing on LLMs' ability to follow dynamic, scenario-specific spec\nfrom both behavioral and safety perspectives. To address this challenge, we\npropose Align3, a lightweight method that employs Test-Time Deliberation (TTD)\nwith hierarchical reflection and revision to reason over the specification\nboundaries. We further present SpecBench, a unified benchmark for measuring\nspecification alignment, covering 5 scenarios, 103 spec, and 1,500 prompts.\nExperiments on 15 reasoning and 18 instruct models with several TTD methods,\nincluding Self-Refine, TPO, and MoreThink, yield three key findings: (i)\ntest-time deliberation enhances specification alignment; (ii) Align3 advances\nthe safety-helpfulness trade-off frontier with minimal overhead; (iii)\nSpecBench effectively reveals alignment gaps. These results highlight the\npotential of test-time deliberation as an effective strategy for reasoning over\nthe real-world specification boundaries."
                },
                "authors": [
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Yafu Li"
                    },
                    {
                        "name": "Xuyang Hu"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Zhilin Wang"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "10 pages main text, 52 pages total (including appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14758v1",
                "updated": "2025-09-18T09:06:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    9,
                    6,
                    37,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T09:06:37Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    9,
                    6,
                    37,
                    3,
                    261,
                    0
                ],
                "title": "Designing Latent Safety Filters using Pre-Trained Vision Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing Latent Safety Filters using Pre-Trained Vision Models"
                },
                "summary": "Ensuring safety of vision-based control systems remains a major challenge\nhindering their deployment in critical settings. Safety filters have gained\nincreased interest as effective tools for ensuring the safety of classical\ncontrol systems, but their applications in vision-based control settings have\nso far been limited. Pre-trained vision models (PVRs) have been shown to be\neffective perception backbones for control in various robotics domains. In this\npaper, we are interested in examining their effectiveness when used for\ndesigning vision-based safety filters. We use them as backbones for classifiers\ndefining failure sets, for Hamilton-Jacobi (HJ) reachability-based safety\nfilters, and for latent world models. We discuss the trade-offs between\ntraining from scratch, fine-tuning, and freezing the PVRs when training the\nmodels they are backbones for. We also evaluate whether one of the PVRs is\nsuperior across all tasks, evaluate whether learned world models or Q-functions\nare better for switching decisions to safe policies, and discuss practical\nconsiderations for deploying these PVRs on resource-constrained devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring safety of vision-based control systems remains a major challenge\nhindering their deployment in critical settings. Safety filters have gained\nincreased interest as effective tools for ensuring the safety of classical\ncontrol systems, but their applications in vision-based control settings have\nso far been limited. Pre-trained vision models (PVRs) have been shown to be\neffective perception backbones for control in various robotics domains. In this\npaper, we are interested in examining their effectiveness when used for\ndesigning vision-based safety filters. We use them as backbones for classifiers\ndefining failure sets, for Hamilton-Jacobi (HJ) reachability-based safety\nfilters, and for latent world models. We discuss the trade-offs between\ntraining from scratch, fine-tuning, and freezing the PVRs when training the\nmodels they are backbones for. We also evaluate whether one of the PVRs is\nsuperior across all tasks, evaluate whether learned world models or Q-functions\nare better for switching decisions to safe policies, and discuss practical\nconsiderations for deploying these PVRs on resource-constrained devices."
                },
                "authors": [
                    {
                        "name": "Ihab Tabbara"
                    },
                    {
                        "name": "Yuxuan Yang"
                    },
                    {
                        "name": "Ahmad Hamzeh"
                    },
                    {
                        "name": "Maxwell Astafyev"
                    },
                    {
                        "name": "Hussein Sibai"
                    }
                ],
                "author_detail": {
                    "name": "Hussein Sibai"
                },
                "author": "Hussein Sibai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07356v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07356v3",
                "updated": "2025-09-18T09:03:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    9,
                    3,
                    43,
                    3,
                    261,
                    0
                ],
                "published": "2025-07-10T00:42:59Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    0,
                    42,
                    59,
                    3,
                    191,
                    0
                ],
                "title": "UniTracker: Learning Universal Whole-Body Motion Tracker for Humanoid\n  Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniTracker: Learning Universal Whole-Body Motion Tracker for Humanoid\n  Robots"
                },
                "summary": "Achieving expressive and generalizable whole-body motion control is essential\nfor deploying humanoid robots in real-world environments. In this work, we\npropose UniTracker, a three-stage training framework that enables robust and\nscalable motion tracking across a wide range of human behaviors. In the first\nstage, we train a teacher policy with privileged observations to generate\nhigh-quality actions. In the second stage, we introduce a Conditional\nVariational Autoencoder (CVAE) to model a universal student policy that can be\ndeployed directly on real hardware. The CVAE structure allows the policy to\nlearn a global latent representation of motion, enhancing generalization to\nunseen behaviors and addressing the limitations of standard MLP-based policies\nunder partial observations. Unlike pure MLPs that suffer from drift in global\nattributes like orientation, our CVAE-student policy incorporates global intent\nduring training by aligning a partial-observation prior to the full-observation\nencoder. In the third stage, we introduce a fast adaptation module that\nfine-tunes the universal policy on harder motion sequences that are difficult\nto track directly. This adaptation can be performed both for single sequences\nand in batch mode, further showcasing the flexibility and scalability of our\napproach. We evaluate UniTracker in both simulation and real-world settings\nusing a Unitree G1 humanoid, demonstrating strong performance in motion\ndiversity, tracking accuracy, and deployment robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving expressive and generalizable whole-body motion control is essential\nfor deploying humanoid robots in real-world environments. In this work, we\npropose UniTracker, a three-stage training framework that enables robust and\nscalable motion tracking across a wide range of human behaviors. In the first\nstage, we train a teacher policy with privileged observations to generate\nhigh-quality actions. In the second stage, we introduce a Conditional\nVariational Autoencoder (CVAE) to model a universal student policy that can be\ndeployed directly on real hardware. The CVAE structure allows the policy to\nlearn a global latent representation of motion, enhancing generalization to\nunseen behaviors and addressing the limitations of standard MLP-based policies\nunder partial observations. Unlike pure MLPs that suffer from drift in global\nattributes like orientation, our CVAE-student policy incorporates global intent\nduring training by aligning a partial-observation prior to the full-observation\nencoder. In the third stage, we introduce a fast adaptation module that\nfine-tunes the universal policy on harder motion sequences that are difficult\nto track directly. This adaptation can be performed both for single sequences\nand in batch mode, further showcasing the flexibility and scalability of our\napproach. We evaluate UniTracker in both simulation and real-world settings\nusing a Unitree G1 humanoid, demonstrating strong performance in motion\ndiversity, tracking accuracy, and deployment robustness."
                },
                "authors": [
                    {
                        "name": "Kangning Yin"
                    },
                    {
                        "name": "Weishuai Zeng"
                    },
                    {
                        "name": "Ke Fan"
                    },
                    {
                        "name": "Minyue Dai"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Zheng Tian"
                    },
                    {
                        "name": "Jingbo Wang"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "arxiv_comment": "three-stage universal motion tracker for humanoid robots",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07356v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07356v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14752v1",
                "updated": "2025-09-18T08:56:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    8,
                    56,
                    31,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T08:56:31Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    8,
                    56,
                    31,
                    3,
                    261,
                    0
                ],
                "title": "KAIO: A Collection of More Challenging Korean Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KAIO: A Collection of More Challenging Korean Questions"
                },
                "summary": "With the advancement of mid/post-training techniques, LLMs are pushing their\nboundaries at an accelerated pace. Legacy benchmarks saturate quickly (e.g.,\nbroad suites like MMLU over the years, newer ones like GPQA-D even faster),\nwhich makes frontier progress hard to track. The problem is especially acute in\nKorean: widely used benchmarks are fewer, often translated or narrow in scope,\nand updated more slowly, so saturation and contamination arrive sooner.\nAccordingly, at this moment, there is no Korean benchmark capable of evaluating\nand ranking frontier models. To bridge this gap, we introduce KAIO, a Korean,\nmath-centric benchmark that stresses long-chain reasoning. Unlike recent Korean\nsuites that are at or near saturation, KAIO remains far from saturated: the\nbest-performing model, GPT-5, attains 62.8, followed by Gemini-2.5-Pro (52.3).\nOpen models such as Qwen3-235B and DeepSeek-R1 cluster falls below 30,\ndemonstrating substantial headroom, enabling robust tracking of frontier\nprogress in Korean. To reduce contamination, KAIO will remain private and be\nserved via a held-out evaluator until the best publicly known model reaches at\nleast 80% accuracy, after which we will release the set and iterate to a harder\nversion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of mid/post-training techniques, LLMs are pushing their\nboundaries at an accelerated pace. Legacy benchmarks saturate quickly (e.g.,\nbroad suites like MMLU over the years, newer ones like GPQA-D even faster),\nwhich makes frontier progress hard to track. The problem is especially acute in\nKorean: widely used benchmarks are fewer, often translated or narrow in scope,\nand updated more slowly, so saturation and contamination arrive sooner.\nAccordingly, at this moment, there is no Korean benchmark capable of evaluating\nand ranking frontier models. To bridge this gap, we introduce KAIO, a Korean,\nmath-centric benchmark that stresses long-chain reasoning. Unlike recent Korean\nsuites that are at or near saturation, KAIO remains far from saturated: the\nbest-performing model, GPT-5, attains 62.8, followed by Gemini-2.5-Pro (52.3).\nOpen models such as Qwen3-235B and DeepSeek-R1 cluster falls below 30,\ndemonstrating substantial headroom, enabling robust tracking of frontier\nprogress in Korean. To reduce contamination, KAIO will remain private and be\nserved via a held-out evaluator until the best publicly known model reaches at\nleast 80% accuracy, after which we will release the set and iterate to a harder\nversion."
                },
                "authors": [
                    {
                        "name": "Nahyun Lee"
                    },
                    {
                        "name": "Guijin Son"
                    },
                    {
                        "name": "Hyunwoo Ko"
                    },
                    {
                        "name": "Kyubeen Han"
                    }
                ],
                "author_detail": {
                    "name": "Kyubeen Han"
                },
                "author": "Kyubeen Han",
                "arxiv_comment": "4 pages paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14750v1",
                "updated": "2025-09-18T08:54:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    8,
                    54,
                    20,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T08:54:20Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    8,
                    54,
                    20,
                    3,
                    261,
                    0
                ],
                "title": "Enhancing Retrieval Augmentation via Adversarial Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Retrieval Augmentation via Adversarial Collaboration"
                },
                "summary": "Retrieval-augmented Generation (RAG) is a prevalent approach for\ndomain-specific LLMs, yet it is often plagued by \"Retrieval Hallucinations\"--a\nphenomenon where fine-tuned models fail to recognize and act upon poor-quality\nretrieved documents, thus undermining performance. To address this, we propose\nthe Adversarial Collaboration RAG (AC-RAG) framework. AC-RAG employs two\nheterogeneous agents: a generalist Detector that identifies knowledge gaps, and\na domain-specialized Resolver that provides precise solutions. Guided by a\nmoderator, these agents engage in an adversarial collaboration, where the\nDetector's persistent questioning challenges the Resolver's expertise. This\ndynamic process allows for iterative problem dissection and refined knowledge\nretrieval. Extensive experiments show that AC-RAG significantly improves\nretrieval accuracy and outperforms state-of-the-art RAG methods across various\nvertical domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented Generation (RAG) is a prevalent approach for\ndomain-specific LLMs, yet it is often plagued by \"Retrieval Hallucinations\"--a\nphenomenon where fine-tuned models fail to recognize and act upon poor-quality\nretrieved documents, thus undermining performance. To address this, we propose\nthe Adversarial Collaboration RAG (AC-RAG) framework. AC-RAG employs two\nheterogeneous agents: a generalist Detector that identifies knowledge gaps, and\na domain-specialized Resolver that provides precise solutions. Guided by a\nmoderator, these agents engage in an adversarial collaboration, where the\nDetector's persistent questioning challenges the Resolver's expertise. This\ndynamic process allows for iterative problem dissection and refined knowledge\nretrieval. Extensive experiments show that AC-RAG significantly improves\nretrieval accuracy and outperforms state-of-the-art RAG methods across various\nvertical domains."
                },
                "authors": [
                    {
                        "name": "Letian Zhang"
                    },
                    {
                        "name": "Guanghao Meng"
                    },
                    {
                        "name": "Xudong Ren"
                    },
                    {
                        "name": "Yiming Wang"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    }
                ],
                "author_detail": {
                    "name": "Shu-Tao Xia"
                },
                "author": "Shu-Tao Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14749v1",
                "updated": "2025-09-18T08:54:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    8,
                    54,
                    17,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T08:54:17Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    8,
                    54,
                    17,
                    3,
                    261,
                    0
                ],
                "title": "Evaluating Large Language Models for Cross-Lingual Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models for Cross-Lingual Retrieval"
                },
                "summary": "Multi-stage information retrieval (IR) has become a widely-adopted paradigm\nin search. While Large Language Models (LLMs) have been extensively evaluated\nas second-stage reranking models for monolingual IR, a systematic large-scale\ncomparison is still lacking for cross-lingual IR (CLIR). Moreover, while prior\nwork shows that LLM-based rerankers improve CLIR performance, their evaluation\nsetup relies on lexical retrieval with machine translation (MT) for the first\nstage. This is not only prohibitively expensive but also prone to error\npropagation across stages. Our evaluation on passage-level and document-level\nCLIR reveals that further gains can be achieved with multilingual bi-encoders\nas first-stage retrievers and that the benefits of translation diminishes with\nstronger reranking models. We further show that pairwise rerankers based on\ninstruction-tuned LLMs perform competitively with listwise rerankers. To the\nbest of our knowledge, we are the first to study the interaction between\nretrievers and rerankers in two-stage CLIR with LLMs. Our findings reveal that,\nwithout MT, current state-of-the-art rerankers fall severely short when\ndirectly applied in CLIR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-stage information retrieval (IR) has become a widely-adopted paradigm\nin search. While Large Language Models (LLMs) have been extensively evaluated\nas second-stage reranking models for monolingual IR, a systematic large-scale\ncomparison is still lacking for cross-lingual IR (CLIR). Moreover, while prior\nwork shows that LLM-based rerankers improve CLIR performance, their evaluation\nsetup relies on lexical retrieval with machine translation (MT) for the first\nstage. This is not only prohibitively expensive but also prone to error\npropagation across stages. Our evaluation on passage-level and document-level\nCLIR reveals that further gains can be achieved with multilingual bi-encoders\nas first-stage retrievers and that the benefits of translation diminishes with\nstronger reranking models. We further show that pairwise rerankers based on\ninstruction-tuned LLMs perform competitively with listwise rerankers. To the\nbest of our knowledge, we are the first to study the interaction between\nretrievers and rerankers in two-stage CLIR with LLMs. Our findings reveal that,\nwithout MT, current state-of-the-art rerankers fall severely short when\ndirectly applied in CLIR."
                },
                "authors": [
                    {
                        "name": "Longfei Zuo"
                    },
                    {
                        "name": "Pingjun Hong"
                    },
                    {
                        "name": "Oliver Kraus"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "Robert Litschko"
                    }
                ],
                "author_detail": {
                    "name": "Robert Litschko"
                },
                "author": "Robert Litschko",
                "arxiv_comment": "Accepted at EMNLP 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14745v1",
                "updated": "2025-09-18T08:48:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    8,
                    48,
                    32,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T08:48:32Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    8,
                    48,
                    32,
                    3,
                    261,
                    0
                ],
                "title": "On the Use of Agentic Coding: An Empirical Study of Pull Requests on\n  GitHub",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Use of Agentic Coding: An Empirical Study of Pull Requests on\n  GitHub"
                },
                "summary": "Large language models (LLMs) are increasingly being integrated into software\ndevelopment processes. The ability to generate code and submit pull requests\nwith minimal human intervention, through the use of autonomous AI agents, is\npoised to become a standard practice. However, little is known about the\npractical usefulness of these pull requests and the extent to which their\ncontributions are accepted in real-world projects. In this paper, we\nempirically study 567 GitHub pull requests (PRs) generated using Claude Code,\nan agentic coding tool, across 157 diverse open-source projects. Our analysis\nreveals that developers tend to rely on agents for tasks such as refactoring,\ndocumentation, and testing. The results indicate that 83.8% of these\nagent-assisted PRs are eventually accepted and merged by project maintainers,\nwith 54.9% of the merged PRs are integrated without further modification. The\nremaining 45.1% require additional changes benefit from human revisions,\nespecially for bug fixes, documentation, and adherence to project-specific\nstandards. These findings suggest that while agent-assisted PRs are largely\nacceptable, they still benefit from human oversight and refinement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being integrated into software\ndevelopment processes. The ability to generate code and submit pull requests\nwith minimal human intervention, through the use of autonomous AI agents, is\npoised to become a standard practice. However, little is known about the\npractical usefulness of these pull requests and the extent to which their\ncontributions are accepted in real-world projects. In this paper, we\nempirically study 567 GitHub pull requests (PRs) generated using Claude Code,\nan agentic coding tool, across 157 diverse open-source projects. Our analysis\nreveals that developers tend to rely on agents for tasks such as refactoring,\ndocumentation, and testing. The results indicate that 83.8% of these\nagent-assisted PRs are eventually accepted and merged by project maintainers,\nwith 54.9% of the merged PRs are integrated without further modification. The\nremaining 45.1% require additional changes benefit from human revisions,\nespecially for bug fixes, documentation, and adherence to project-specific\nstandards. These findings suggest that while agent-assisted PRs are largely\nacceptable, they still benefit from human oversight and refinement."
                },
                "authors": [
                    {
                        "name": "Miku Watanabe"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Yutaro Kashiwa"
                    },
                    {
                        "name": "Brittany Reid"
                    },
                    {
                        "name": "Hajimu Iida"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14735v1",
                "updated": "2025-09-18T08:37:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    8,
                    37,
                    11,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T08:37:11Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    8,
                    37,
                    11,
                    3,
                    261,
                    0
                ],
                "title": "Decoupled Proxy Alignment: Mitigating Language Prior Conflict for\n  Multimodal Alignment in MLLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoupled Proxy Alignment: Mitigating Language Prior Conflict for\n  Multimodal Alignment in MLLM"
                },
                "summary": "Multimodal large language models (MLLMs) have gained significant attention\ndue to their impressive ability to integrate vision and language modalities.\nRecent advancements in MLLMs have primarily focused on improving performance\nthrough high-quality datasets, novel architectures, and optimized training\nstrategies. However, in this paper, we identify a previously overlooked issue,\nlanguage prior conflict, a mismatch between the inherent language priors of\nlarge language models (LLMs) and the language priors in training datasets. This\nconflict leads to suboptimal vision-language alignment, as MLLMs are prone to\nadapting to the language style of training samples. To address this issue, we\npropose a novel training method called Decoupled Proxy Alignment (DPA). DPA\nintroduces two key innovations: (1) the use of a proxy LLM during pretraining\nto decouple the vision-language alignment process from language prior\ninterference, and (2) dynamic loss adjustment based on visual relevance to\nstrengthen optimization signals for visually relevant tokens. Extensive\nexperiments demonstrate that DPA significantly mitigates the language prior\nconflict, achieving superior alignment performance across diverse datasets,\nmodel families, and scales. Our method not only improves the effectiveness of\nMLLM training but also shows exceptional generalization capabilities, making it\na robust approach for vision-language alignment. Our code is available at\nhttps://github.com/fnlp-vision/DPA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have gained significant attention\ndue to their impressive ability to integrate vision and language modalities.\nRecent advancements in MLLMs have primarily focused on improving performance\nthrough high-quality datasets, novel architectures, and optimized training\nstrategies. However, in this paper, we identify a previously overlooked issue,\nlanguage prior conflict, a mismatch between the inherent language priors of\nlarge language models (LLMs) and the language priors in training datasets. This\nconflict leads to suboptimal vision-language alignment, as MLLMs are prone to\nadapting to the language style of training samples. To address this issue, we\npropose a novel training method called Decoupled Proxy Alignment (DPA). DPA\nintroduces two key innovations: (1) the use of a proxy LLM during pretraining\nto decouple the vision-language alignment process from language prior\ninterference, and (2) dynamic loss adjustment based on visual relevance to\nstrengthen optimization signals for visually relevant tokens. Extensive\nexperiments demonstrate that DPA significantly mitigates the language prior\nconflict, achieving superior alignment performance across diverse datasets,\nmodel families, and scales. Our method not only improves the effectiveness of\nMLLM training but also shows exceptional generalization capabilities, making it\na robust approach for vision-language alignment. Our code is available at\nhttps://github.com/fnlp-vision/DPA."
                },
                "authors": [
                    {
                        "name": "Chenkun Tan"
                    },
                    {
                        "name": "Pengyu Wang"
                    },
                    {
                        "name": "Shaojun Zhou"
                    },
                    {
                        "name": "Botian Jiang"
                    },
                    {
                        "name": "Zhaowei Li"
                    },
                    {
                        "name": "Dong Zhang"
                    },
                    {
                        "name": "Xinghao Wang"
                    },
                    {
                        "name": "Yaqian Zhou"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "Accepted by Findings of EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18614v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18614v4",
                "updated": "2025-09-18T08:19:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    8,
                    19,
                    20,
                    3,
                    261,
                    0
                ],
                "published": "2025-05-24T09:28:09Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    9,
                    28,
                    9,
                    5,
                    144,
                    0
                ],
                "title": "MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song\n  Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song\n  Translation"
                },
                "summary": "Lyrics translation requires both accurate semantic transfer and preservation\nof musical rhythm, syllabic structure, and poetic style. In animated musicals,\nthe challenge intensifies due to alignment with visual and auditory cues. We\nintroduce Multilingual Audio-Video Lyrics Benchmark for Animated Song\nTranslation (MAVL), the first multilingual, multimodal benchmark for singable\nlyrics translation. By integrating text, audio, and video, MAVL enables richer\nand more expressive translations than text-only approaches. Building on this,\nwe propose Syllable-Constrained Audio-Video LLM with Chain-of-Thought\nSylAVL-CoT, which leverages audio-video cues and enforces syllabic constraints\nto produce natural-sounding lyrics. Experimental results demonstrate that\nSylAVL-CoT significantly outperforms text-based models in singability and\ncontextual accuracy, emphasizing the value of multimodal, multilingual\napproaches for lyrics translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lyrics translation requires both accurate semantic transfer and preservation\nof musical rhythm, syllabic structure, and poetic style. In animated musicals,\nthe challenge intensifies due to alignment with visual and auditory cues. We\nintroduce Multilingual Audio-Video Lyrics Benchmark for Animated Song\nTranslation (MAVL), the first multilingual, multimodal benchmark for singable\nlyrics translation. By integrating text, audio, and video, MAVL enables richer\nand more expressive translations than text-only approaches. Building on this,\nwe propose Syllable-Constrained Audio-Video LLM with Chain-of-Thought\nSylAVL-CoT, which leverages audio-video cues and enforces syllabic constraints\nto produce natural-sounding lyrics. Experimental results demonstrate that\nSylAVL-CoT significantly outperforms text-based models in singability and\ncontextual accuracy, emphasizing the value of multimodal, multilingual\napproaches for lyrics translation."
                },
                "authors": [
                    {
                        "name": "Woohyun Cho"
                    },
                    {
                        "name": "Youngmin Kim"
                    },
                    {
                        "name": "Sunghyun Lee"
                    },
                    {
                        "name": "Youngjae Yu"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Yu"
                },
                "author": "Youngjae Yu",
                "arxiv_comment": "Accepted to EMNLP 2025, Project Page:\n  https://k1064190.github.io/papers/paper1.html, our codes and datasets are\n  available at https://github.com/k1064190/MAVL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18614v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18614v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14723v1",
                "updated": "2025-09-18T08:16:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    8,
                    16,
                    21,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T08:16:21Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    8,
                    16,
                    21,
                    3,
                    261,
                    0
                ],
                "title": "Transcoder-based Circuit Analysis for Interpretable Single-Cell\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transcoder-based Circuit Analysis for Interpretable Single-Cell\n  Foundation Models"
                },
                "summary": "Single-cell foundation models (scFMs) have demonstrated state-of-the-art\nperformance on various tasks, such as cell-type annotation and perturbation\nresponse prediction, by learning gene regulatory networks from large-scale\ntranscriptome data. However, a significant challenge remains: the\ndecision-making processes of these models are less interpretable compared to\ntraditional methods like differential gene expression analysis. Recently,\ntranscoders have emerged as a promising approach for extracting interpretable\ndecision circuits from large language models (LLMs). In this work, we train a\ntranscoder on the cell2sentence (C2S) model, a state-of-the-art scFM. By\nleveraging the trained transcoder, we extract internal decision-making circuits\nfrom the C2S model. We demonstrate that the discovered circuits correspond to\nreal-world biological mechanisms, confirming the potential of transcoders to\nuncover biologically plausible pathways within complex single-cell models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single-cell foundation models (scFMs) have demonstrated state-of-the-art\nperformance on various tasks, such as cell-type annotation and perturbation\nresponse prediction, by learning gene regulatory networks from large-scale\ntranscriptome data. However, a significant challenge remains: the\ndecision-making processes of these models are less interpretable compared to\ntraditional methods like differential gene expression analysis. Recently,\ntranscoders have emerged as a promising approach for extracting interpretable\ndecision circuits from large language models (LLMs). In this work, we train a\ntranscoder on the cell2sentence (C2S) model, a state-of-the-art scFM. By\nleveraging the trained transcoder, we extract internal decision-making circuits\nfrom the C2S model. We demonstrate that the discovered circuits correspond to\nreal-world biological mechanisms, confirming the potential of transcoders to\nuncover biologically plausible pathways within complex single-cell models."
                },
                "authors": [
                    {
                        "name": "Sosuke Hosokawa"
                    },
                    {
                        "name": "Toshiharu Kawakami"
                    },
                    {
                        "name": "Satoshi Kodera"
                    },
                    {
                        "name": "Masamichi Ito"
                    },
                    {
                        "name": "Norihiko Takeda"
                    }
                ],
                "author_detail": {
                    "name": "Norihiko Takeda"
                },
                "author": "Norihiko Takeda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13775v2",
                "updated": "2025-09-18T08:09:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    8,
                    9,
                    19,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-17T07:45:09Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    45,
                    9,
                    2,
                    260,
                    0
                ],
                "title": "Exploring Data and Parameter Efficient Strategies for Arabic Dialect\n  Identifications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Data and Parameter Efficient Strategies for Arabic Dialect\n  Identifications"
                },
                "summary": "This paper discusses our exploration of different data-efficient and\nparameter-efficient approaches to Arabic Dialect Identification (ADI). In\nparticular, we investigate various soft-prompting strategies, including\nprefix-tuning, prompt-tuning, P-tuning, and P-tuning V2, as well as LoRA\nreparameterizations. For the data-efficient strategy, we analyze hard prompting\nwith zero-shot and few-shot inferences to analyze the dialect identification\ncapabilities of Large Language Models (LLMs). For the parameter-efficient PEFT\napproaches, we conducted our experiments using Arabic-specific encoder models\non several major datasets. We also analyzed the n-shot inferences on\nopen-source decoder-only models, a general multilingual model (Phi-3.5), and an\nArabic-specific one(SILMA). We observed that the LLMs generally struggle to\ndifferentiate the dialectal nuances in the few-shot or zero-shot setups. The\nsoft-prompted encoder variants perform better, while the LoRA-based fine-tuned\nmodels perform best, even surpassing full fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper discusses our exploration of different data-efficient and\nparameter-efficient approaches to Arabic Dialect Identification (ADI). In\nparticular, we investigate various soft-prompting strategies, including\nprefix-tuning, prompt-tuning, P-tuning, and P-tuning V2, as well as LoRA\nreparameterizations. For the data-efficient strategy, we analyze hard prompting\nwith zero-shot and few-shot inferences to analyze the dialect identification\ncapabilities of Large Language Models (LLMs). For the parameter-efficient PEFT\napproaches, we conducted our experiments using Arabic-specific encoder models\non several major datasets. We also analyzed the n-shot inferences on\nopen-source decoder-only models, a general multilingual model (Phi-3.5), and an\nArabic-specific one(SILMA). We observed that the LLMs generally struggle to\ndifferentiate the dialectal nuances in the few-shot or zero-shot setups. The\nsoft-prompted encoder variants perform better, while the LoRA-based fine-tuned\nmodels perform best, even surpassing full fine-tuning."
                },
                "authors": [
                    {
                        "name": "Vani Kanjirangat"
                    },
                    {
                        "name": "Ljiljana Dolamic"
                    },
                    {
                        "name": "Fabio Rinaldi"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Rinaldi"
                },
                "author": "Fabio Rinaldi",
                "arxiv_comment": "4 main pages, 4 additional, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14718v1",
                "updated": "2025-09-18T08:04:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    8,
                    4,
                    49,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T08:04:49Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    8,
                    4,
                    49,
                    3,
                    261,
                    0
                ],
                "title": "ToolSample: Dual Dynamic Sampling Methods with Curriculum Learning for\n  RL-based Tool Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolSample: Dual Dynamic Sampling Methods with Curriculum Learning for\n  RL-based Tool Learning"
                },
                "summary": "While reinforcement learning (RL) is increasingly used for LLM-based tool\nlearning, its efficiency is often hampered by an overabundance of simple\nsamples that provide diminishing learning value as training progresses.\nExisting dynamic sampling techniques are ill-suited for the multi-task\nstructure and fine-grained reward mechanisms inherent to tool learning. This\npaper introduces Dynamic Sampling with Curriculum Learning (DSCL), a framework\nspecifically designed to address this challenge by targeting the unique\ncharacteristics of tool learning: its multiple interdependent sub-tasks and\nmulti-valued reward functions. DSCL features two core components: Reward-Based\nDynamic Sampling, which uses multi-dimensional reward statistics (mean and\nvariance) to prioritize valuable data, and Task-Based Dynamic Curriculum\nLearning, which adaptively focuses training on less-mastered sub-tasks. Through\nextensive experiments, we demonstrate that DSCL significantly improves training\nefficiency and model performance over strong baselines, achieving a 3.29\\%\nimprovement on the BFCLv3 benchmark. Our method provides a tailored solution\nthat effectively leverages the complex reward signals and sub-task dynamics\nwithin tool learning to achieve superior results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While reinforcement learning (RL) is increasingly used for LLM-based tool\nlearning, its efficiency is often hampered by an overabundance of simple\nsamples that provide diminishing learning value as training progresses.\nExisting dynamic sampling techniques are ill-suited for the multi-task\nstructure and fine-grained reward mechanisms inherent to tool learning. This\npaper introduces Dynamic Sampling with Curriculum Learning (DSCL), a framework\nspecifically designed to address this challenge by targeting the unique\ncharacteristics of tool learning: its multiple interdependent sub-tasks and\nmulti-valued reward functions. DSCL features two core components: Reward-Based\nDynamic Sampling, which uses multi-dimensional reward statistics (mean and\nvariance) to prioritize valuable data, and Task-Based Dynamic Curriculum\nLearning, which adaptively focuses training on less-mastered sub-tasks. Through\nextensive experiments, we demonstrate that DSCL significantly improves training\nefficiency and model performance over strong baselines, achieving a 3.29\\%\nimprovement on the BFCLv3 benchmark. Our method provides a tailored solution\nthat effectively leverages the complex reward signals and sub-task dynamics\nwithin tool learning to achieve superior results."
                },
                "authors": [
                    {
                        "name": "Zihao Feng"
                    },
                    {
                        "name": "Xiaoxue Wang"
                    },
                    {
                        "name": "Bowen Wu"
                    },
                    {
                        "name": "Hailong Cao"
                    },
                    {
                        "name": "Tiejun Zhao"
                    },
                    {
                        "name": "Qun Yu"
                    },
                    {
                        "name": "Baoxun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Baoxun Wang"
                },
                "author": "Baoxun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14712v1",
                "updated": "2025-09-18T07:57:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    7,
                    57,
                    18,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T07:57:18Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    7,
                    57,
                    18,
                    3,
                    261,
                    0
                ],
                "title": "From Ground Trust to Truth: Disparities in Offensive Language Judgments\n  on Contemporary Korean Political Discourse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Ground Trust to Truth: Disparities in Offensive Language Judgments\n  on Contemporary Korean Political Discourse"
                },
                "summary": "Although offensive language continually evolves over time, even recent\nstudies using LLMs have predominantly relied on outdated datasets and rarely\nevaluated the generalization ability on unseen texts. In this study, we\nconstructed a large-scale dataset of contemporary political discourse and\nemployed three refined judgments in the absence of ground truth. Each judgment\nreflects a representative offensive language detection method and is carefully\ndesigned for optimal conditions. We identified distinct patterns for each\njudgment and demonstrated tendencies of label agreement using a leave-one-out\nstrategy. By establishing pseudo-labels as ground trust for quantitative\nperformance assessment, we observed that a strategically designed single\nprompting achieves comparable performance to more resource-intensive methods.\nThis suggests a feasible approach applicable in real-world settings with\ninherent constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although offensive language continually evolves over time, even recent\nstudies using LLMs have predominantly relied on outdated datasets and rarely\nevaluated the generalization ability on unseen texts. In this study, we\nconstructed a large-scale dataset of contemporary political discourse and\nemployed three refined judgments in the absence of ground truth. Each judgment\nreflects a representative offensive language detection method and is carefully\ndesigned for optimal conditions. We identified distinct patterns for each\njudgment and demonstrated tendencies of label agreement using a leave-one-out\nstrategy. By establishing pseudo-labels as ground trust for quantitative\nperformance assessment, we observed that a strategically designed single\nprompting achieves comparable performance to more resource-intensive methods.\nThis suggests a feasible approach applicable in real-world settings with\ninherent constraints."
                },
                "authors": [
                    {
                        "name": "Seunguk Yu"
                    },
                    {
                        "name": "Jungmin Yun"
                    },
                    {
                        "name": "Jinhee Jang"
                    },
                    {
                        "name": "Youngbin Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngbin Kim"
                },
                "author": "Youngbin Kim",
                "arxiv_comment": "EMNLP 2025 findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14711v1",
                "updated": "2025-09-18T07:57:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    7,
                    57,
                    6,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T07:57:06Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    7,
                    57,
                    6,
                    3,
                    261,
                    0
                ],
                "title": "LLM4MG: Adapting Large Language Model for Multipath Generation via\n  Synesthesia of Machines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4MG: Adapting Large Language Model for Multipath Generation via\n  Synesthesia of Machines"
                },
                "summary": "Based on Synesthesia of Machines (SoM), a large language model (LLM) is\nadapted for multipath generation (LLM4MG) for the first time. Considering a\ntypical sixth-generation (6G) vehicle-to-infrastructure (V2I) scenario, a new\nmulti-modal sensing-communication dataset is constructed, named SynthSoM-V2I,\nincluding channel multipath information, millimeter wave (mmWave) radar sensory\ndata, RGB-D images, and light detection and ranging (LiDAR) point clouds. Based\non the SynthSoM-V2I dataset, the proposed LLM4MG leverages Large Language Model\nMeta AI (LLaMA) 3.2 for multipath generation via multi-modal sensory data. The\nproposed LLM4MG aligns the multi-modal feature space with the LLaMA semantic\nspace through feature extraction and fusion networks. To further achieve\ngeneral knowledge transfer from the pre-trained LLaMA for multipath generation\nvia multi-modal sensory data, the low-rank adaptation (LoRA)\nparameter-efficient fine-tuning and propagation-aware prompt engineering are\nexploited. Simulation results demonstrate that the proposed LLM4MG outperforms\nconventional deep learning-based methods in terms of line-of-sight\n(LoS)/non-LoS (NLoS) classification with accuracy of 92.76%, multipath\npower/delay generation precision with normalized mean square error (NMSE) of\n0.099/0.032, and cross-vehicular traffic density (VTD), cross-band, and\ncross-scenario generalization. The utility of the proposed LLM4MG is validated\nby real-world generalization. The necessity of high-precision multipath\ngeneration for system design is also demonstrated by channel capacity\ncomparison.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Based on Synesthesia of Machines (SoM), a large language model (LLM) is\nadapted for multipath generation (LLM4MG) for the first time. Considering a\ntypical sixth-generation (6G) vehicle-to-infrastructure (V2I) scenario, a new\nmulti-modal sensing-communication dataset is constructed, named SynthSoM-V2I,\nincluding channel multipath information, millimeter wave (mmWave) radar sensory\ndata, RGB-D images, and light detection and ranging (LiDAR) point clouds. Based\non the SynthSoM-V2I dataset, the proposed LLM4MG leverages Large Language Model\nMeta AI (LLaMA) 3.2 for multipath generation via multi-modal sensory data. The\nproposed LLM4MG aligns the multi-modal feature space with the LLaMA semantic\nspace through feature extraction and fusion networks. To further achieve\ngeneral knowledge transfer from the pre-trained LLaMA for multipath generation\nvia multi-modal sensory data, the low-rank adaptation (LoRA)\nparameter-efficient fine-tuning and propagation-aware prompt engineering are\nexploited. Simulation results demonstrate that the proposed LLM4MG outperforms\nconventional deep learning-based methods in terms of line-of-sight\n(LoS)/non-LoS (NLoS) classification with accuracy of 92.76%, multipath\npower/delay generation precision with normalized mean square error (NMSE) of\n0.099/0.032, and cross-vehicular traffic density (VTD), cross-band, and\ncross-scenario generalization. The utility of the proposed LLM4MG is validated\nby real-world generalization. The necessity of high-precision multipath\ngeneration for system design is also demonstrated by channel capacity\ncomparison."
                },
                "authors": [
                    {
                        "name": "Ziwei Huang"
                    },
                    {
                        "name": "Shiliang Lu"
                    },
                    {
                        "name": "Lu Bai"
                    },
                    {
                        "name": "Xuesong Cai"
                    },
                    {
                        "name": "Xiang Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Cheng"
                },
                "author": "Xiang Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14705v1",
                "updated": "2025-09-18T07:51:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    7,
                    51,
                    4,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T07:51:04Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    7,
                    51,
                    4,
                    3,
                    261,
                    0
                ],
                "title": "Secure Short-Packet Communications for RIS-Assisted AAV Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure Short-Packet Communications for RIS-Assisted AAV Networks"
                },
                "summary": "Advancements toward 6G have intensified demands for ultra-reliable\nlow-latency communication, positioning shortpacket communications as a critical\ntechnology for autonomous aerial vehicle (AAV) networks. However, the open\nbroadcast nature introduces significant security vulnerabilities. Although\nphysical-layer security offers a low-complexity solution by exploiting wireless\nchannel randomness, the AAV communication performance severely degrades in\nweak-coverage or non-line-of sight scenarios. To overcome these limitations,\nthis paper proposes a short-packet communications framework for AAV networks\nthat leverages reconfigurable intelligent surfaces (RIS) with the aim of\nextending coverage and enhancing secrecy capabilities. Analytical frameworks\nare developed to evaluate the average secrecy throughput (AST) in finite\nblocklength constraints for both external and internal avesdropping scenarios,\nwhich incorporates non-orthogonal multiple access with imperfect successive\ninterference cancellation. Asymptotic approximations of AST are derived as\ntransmit power approaches infinity. Furthermore, we formulate a blocklength\noptimization problem to maximize the AST, effectively resolving the trade-offs\namong delay, reliability, and secrecy. Extensive simulations validate the\nanalytical frameworks, which reveal that large-scale RIS deployment\nsignificantly boosts AST, and the power allocation coefficient exhibits dual\neffects in the internal eavesdropping scenario. These observations provide\nuseful insights for designing reliable and secure lowlatency AAV communications\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements toward 6G have intensified demands for ultra-reliable\nlow-latency communication, positioning shortpacket communications as a critical\ntechnology for autonomous aerial vehicle (AAV) networks. However, the open\nbroadcast nature introduces significant security vulnerabilities. Although\nphysical-layer security offers a low-complexity solution by exploiting wireless\nchannel randomness, the AAV communication performance severely degrades in\nweak-coverage or non-line-of sight scenarios. To overcome these limitations,\nthis paper proposes a short-packet communications framework for AAV networks\nthat leverages reconfigurable intelligent surfaces (RIS) with the aim of\nextending coverage and enhancing secrecy capabilities. Analytical frameworks\nare developed to evaluate the average secrecy throughput (AST) in finite\nblocklength constraints for both external and internal avesdropping scenarios,\nwhich incorporates non-orthogonal multiple access with imperfect successive\ninterference cancellation. Asymptotic approximations of AST are derived as\ntransmit power approaches infinity. Furthermore, we formulate a blocklength\noptimization problem to maximize the AST, effectively resolving the trade-offs\namong delay, reliability, and secrecy. Extensive simulations validate the\nanalytical frameworks, which reveal that large-scale RIS deployment\nsignificantly boosts AST, and the power allocation coefficient exhibits dual\neffects in the internal eavesdropping scenario. These observations provide\nuseful insights for designing reliable and secure lowlatency AAV communications\nsystems."
                },
                "authors": [
                    {
                        "name": "Huiling Liu"
                    },
                    {
                        "name": "Junshan Luo"
                    },
                    {
                        "name": "Shilian Wang"
                    },
                    {
                        "name": "Fanggang Wang"
                    },
                    {
                        "name": "Theodoros A. Tsiftsis"
                    },
                    {
                        "name": "Symeon Chatzinotas"
                    }
                ],
                "author_detail": {
                    "name": "Symeon Chatzinotas"
                },
                "author": "Symeon Chatzinotas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14704v1",
                "updated": "2025-09-18T07:50:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    7,
                    50,
                    4,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T07:50:04Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    7,
                    50,
                    4,
                    3,
                    261,
                    0
                ],
                "title": "The NazoNazo Benchmark: A Cost-Effective and Extensible Test of\n  Insight-Based Reasoning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NazoNazo Benchmark: A Cost-Effective and Extensible Test of\n  Insight-Based Reasoning in LLMs"
                },
                "summary": "Benchmark saturation and contamination undermine confidence in LLM\nevaluation. We present Nazonazo, a cost-effective and extensible benchmark\nbuilt from Japanese children's riddles to test insight-based reasoning. Items\nare short (mostly one sentence), require no specialized domain knowledge, and\ncan be generated at scale, enabling rapid refresh of blind sets when leakage is\nsuspected. We evaluate 38 frontier models and 126 adults on 120 riddles. No\nmodel except for GPT-5 is comparable to human performance, which achieves a\n52.9% mean accuracy. Model comparison on extended 201 items shows that\nreasoning models significantly outperform non-reasoning peers, while model size\nshows no reliable association with accuracy. Beyond aggregate accuracy, an\ninformal candidate-tracking analysis of thought logs reveals many cases of\nverification failure: models often produce the correct solution among\nintermediate candidates yet fail to select it as the final answer, which we\nillustrate with representative examples observed in multiple models. Nazonazo\nthus offers a cost-effective, scalable, and easily renewable benchmark format\nthat addresses the current evaluation crisis while also suggesting a recurrent\nmeta-cognitive weakness, providing clear targets for future control and\ncalibration methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmark saturation and contamination undermine confidence in LLM\nevaluation. We present Nazonazo, a cost-effective and extensible benchmark\nbuilt from Japanese children's riddles to test insight-based reasoning. Items\nare short (mostly one sentence), require no specialized domain knowledge, and\ncan be generated at scale, enabling rapid refresh of blind sets when leakage is\nsuspected. We evaluate 38 frontier models and 126 adults on 120 riddles. No\nmodel except for GPT-5 is comparable to human performance, which achieves a\n52.9% mean accuracy. Model comparison on extended 201 items shows that\nreasoning models significantly outperform non-reasoning peers, while model size\nshows no reliable association with accuracy. Beyond aggregate accuracy, an\ninformal candidate-tracking analysis of thought logs reveals many cases of\nverification failure: models often produce the correct solution among\nintermediate candidates yet fail to select it as the final answer, which we\nillustrate with representative examples observed in multiple models. Nazonazo\nthus offers a cost-effective, scalable, and easily renewable benchmark format\nthat addresses the current evaluation crisis while also suggesting a recurrent\nmeta-cognitive weakness, providing clear targets for future control and\ncalibration methods."
                },
                "authors": [
                    {
                        "name": "Masaharu Mizumoto"
                    },
                    {
                        "name": "Dat Nguyen"
                    },
                    {
                        "name": "Zhiheng Han"
                    },
                    {
                        "name": "Jiyuan Fang"
                    },
                    {
                        "name": "Heyuan Guan"
                    },
                    {
                        "name": "Xingfu Li"
                    },
                    {
                        "name": "Naoya Shiraishi"
                    },
                    {
                        "name": "Xuyang Tian"
                    },
                    {
                        "name": "Yo Nakawake"
                    },
                    {
                        "name": "Le Minh Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Le Minh Nguyen"
                },
                "author": "Le Minh Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]